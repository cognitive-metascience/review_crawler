<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">38105</article-id><article-id pub-id-type="doi">10.7554/eLife.38105</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>How biological attention mechanisms improve task performance in a large-scale visual system model</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-112230"><name><surname>Lindsay</surname><given-names>Grace W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9904-7471</contrib-id><email>gracewlindsay@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-113602"><name><surname>Miller</surname><given-names>Kenneth D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1433-0647</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Center for Theoretical Neuroscience, College of Physicians and Surgeons</institution><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Mortimer B. Zuckerman Mind Brain Behaviour Institute</institution><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Swartz Program in Theoretical Neuroscience</institution><institution>Kavli Institute for Brain Science</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Neuroscience</institution><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>van Gerven</surname><given-names>Marcel</given-names></name><role>Reviewing Editor</role><aff><institution>Radboud Universiteit</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>01</day><month>10</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e38105</elocation-id><history><date date-type="received" iso-8601-date="2018-05-05"><day>05</day><month>05</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-09-28"><day>28</day><month>09</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Lindsay et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Lindsay et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-38105-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.38105.001</object-id><p>How does attentional modulation of neural activity enhance performance? Here we use a deep convolutional neural network as a large-scale model of the visual system to address this question. We model the feature similarity gain model of attention, in which attentional modulation is applied according to neural stimulus tuning. Using a variety of visual tasks, we show that neural modulations of the kind and magnitude observed experimentally lead to performance changes of the kind and magnitude observed experimentally. We find that, at earlier layers, attention applied according to tuning does not successfully propagate through the network, and has a weaker impact on performance than attention applied according to values computed for optimally modulating higher areas. This raises the question of whether biological attention might be applied at least in part to optimize function rather than strictly according to tuning. We suggest a simple experiment to distinguish these alternatives.</p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.38105.002</object-id><title>eLife digest</title><p>Imagine you have lost your cell phone. Your eyes scan the cluttered table in front of you, searching for its familiar blue case. But what is happening within the visual areas of your brain while you search? One possibility is that neurons that represent relevant features such as 'blue' and 'rectangular' increase their activity. This might help you spot your phone among all the other objects on the table.</p><p>Paying attention to specific features improves our performance on visual tasks that require detecting those features. The 'feature similarity gain model' proposes that this is because attention increases the activity of neurons sensitive to specific target features, such as ‘blue’ in the example above. But is this how the brain solves such challenges in practice? Previous studies examining this issue have relied on correlations. They have shown that increases in neural activity correlate with improved performance on visual tasks. But correlation does not imply causation.</p><p>Lindsay and Miller have now used a computer model of the brain’s visual pathway to examine whether changes in neural activity cause improved performance. The model was trained to use feature similarity gain to detect an object within a set of photographs. As predicted, changes in activity like those that occur in the brain did indeed improve the model’s performance. Moreover, activity changes at later stages of the model's processing pathway produced bigger improvements than activity changes earlier in the pathway. This may explain why attention affects neural activity more at later stages in the visual pathway.</p><p>But feature similarity gain is not the only possible explanation for the results. Lindsay and Miller show that another pattern of activity change also enhanced the model’s performance, and propose an experiment to distinguish between the two possibilities. Overall, these findings increase our understanding of how the brain processes sensory information. Work is ongoing to teach computers to process images as efficiently as the human visual system. The computer model used in this study is similar to those used in state-of-the-art computer vision. These findings could thus help advance artificial sensory processing too.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>convolutional neural networks</kwd><kwd>visual attention</kwd><kwd>gain modulation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DBI-1707398</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T32 NS064929</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000324</institution-id><institution>Gatsby Charitable Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006785</institution-id><institution>Google</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Lindsay</surname><given-names>Grace W</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IIS-1704938</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Implementing neural changes associated with attention in a deep neural network causes performance changes that mimic those observed in humans and macaques.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Covert visual attention—applied according to spatial location or visual features—has been shown repeatedly to enhance performance on challenging visual tasks (<xref ref-type="bibr" rid="bib13">Carrasco, 2011</xref>). To explore the neural mechanisms behind this enhancement, neural responses to the same visual input are compared under different task conditions. Such experiments have identified numerous neural modulations associated with attention, including changes in firing rates, noise levels, and correlated activity (<xref ref-type="bibr" rid="bib92">Treue, 2001</xref>; <xref ref-type="bibr" rid="bib17">Cohen and Maunsell, 2009</xref>; <xref ref-type="bibr" rid="bib25">Fries et al., 2001</xref>; <xref ref-type="bibr" rid="bib56">Maunsell and Cook, 2002</xref>). But how do these neural activity changes impact performance? Previous theoretical studies have offered helpful insights on how attention may work to enhance performance (<xref ref-type="bibr" rid="bib67">Navalpakkam and Itti, 2007</xref>; <xref ref-type="bibr" rid="bib77">Rolls and Deco, 2006</xref>; <xref ref-type="bibr" rid="bib94">Tsotsos et al., 1995</xref>; <xref ref-type="bibr" rid="bib14">Cave, 1999</xref>; <xref ref-type="bibr" rid="bib30">Hamker and Worcester, 2002</xref>; <xref ref-type="bibr" rid="bib99">Wolfe, 1994</xref>; <xref ref-type="bibr" rid="bib29">Hamker, 1999</xref>; <xref ref-type="bibr" rid="bib23">Eckstein et al., 2009</xref>; <xref ref-type="bibr" rid="bib7">Borji and Itti, 2014</xref>; <xref ref-type="bibr" rid="bib98">Whiteley and Sahani, 2012</xref>; <xref ref-type="bibr" rid="bib11">Bundesen, 1990</xref>; <xref ref-type="bibr" rid="bib90">Treisman and Gelade, 1980</xref>; <xref ref-type="bibr" rid="bib97">Verghese, 2001</xref>; <xref ref-type="bibr" rid="bib16">Chikkerur et al., 2010</xref>). However, much of this work is either based on small, hand-designed models or lacks direct mechanistic interpretability. Here, we utilize a large-scale model of the ventral visual stream to explore the extent to which neural changes like those observed experimentally can lead to performance enhancements on realistic visual tasks. Specifically, we use a deep convolutional neural network trained to perform object classification to test effects of the feature similarity gain model of attention (<xref ref-type="bibr" rid="bib91">Treue and Martínez Trujillo, 1999</xref>).</p><p>Deep convolutional neural networks (CNNs) are popular tools in the machine learning and computer vision communities for performing challenging visual tasks (<xref ref-type="bibr" rid="bib75">Rawat and Wang, 2017</xref>). Their architecture—comprised of layers of convolutions, nonlinearities, and response pooling—was designed to mimic the retinotopic and hierarchical nature of the mammalian visual system (<xref ref-type="bibr" rid="bib75">Rawat and Wang, 2017</xref>). Models of a similar form have been used to study the biological underpinnings of object recognition for decades (<xref ref-type="bibr" rid="bib27">Fukushima, 1988</xref>; <xref ref-type="bibr" rid="bib76">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib84">Serre et al., 2007</xref>). Recently it has been shown that when these networks are trained to successfully perform object classification on real-world images, the intermediate representations learned are remarkably similar to those of the primate visual system, making CNNs state-of-the-art models of the ventral stream (<xref ref-type="bibr" rid="bib101">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Khaligh-Razavi et al., 2017</xref>; <xref ref-type="bibr" rid="bib41">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib42">Kheradpisheh et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Kar et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Cadena et al., 2017</xref>; <xref ref-type="bibr" rid="bib93">Tripp, 2017</xref>; <xref ref-type="bibr" rid="bib50">Love et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Kubilius et al., 2016</xref>). A key finding has been the correspondence between different areas in the ventral stream and layers in the deep CNNs, with early convolutional layers best able to capture the representation of V1 and middle and higher layers best able to capture V4 and IT, respectively (<xref ref-type="bibr" rid="bib28">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib24">Eickenberg et al., 2017</xref>; <xref ref-type="bibr" rid="bib101">Yamins et al., 2014</xref>). The generalizability of these networks is limited, however, and the models are not able to match all elements of visual behaviour (<xref ref-type="bibr" rid="bib95">Ullman et al., 2016</xref>; <xref ref-type="bibr" rid="bib2">Azulay and Weiss, 2018</xref>; <xref ref-type="bibr" rid="bib3">Baker et al., 2018</xref>). But given that CNNs can reach near-human performance on some visual tasks and have architectural and representational similarities to the visual system, they are well-positioned for exploring how neural correlates of attention can impact behaviour.</p><p>One popular framework to describe attention’s effects on firing rates is the feature similarity gain model (FSGM). This model, introduced by Treue and Martinez-Trujillo, claims that a neuron’s activity is multiplicatively scaled up (or down) according to how much it prefers (or doesn’t prefer) the properties of the attended stimulus (<xref ref-type="bibr" rid="bib91">Treue and Martínez Trujillo, 1999</xref>; <xref ref-type="bibr" rid="bib55">Martinez-Trujillo and Treue, 2004</xref>). Attention to a certain visual attribute, such as a specific orientation or color, is generally referred to as feature-based attention (FBA). FBA effects are spatially global: if a task performed at one location in the visual field activates attention to a particular feature, neurons that represent that feature across the visual field will be affected (<xref ref-type="bibr" rid="bib103">Zhang and Luck, 2009</xref>; <xref ref-type="bibr" rid="bib79">Saenz et al., 2002</xref>). Overall, this leads to a general shift in the representation of the neural population towards that of the attended stimulus (<xref ref-type="bibr" rid="bib20">Çukur et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Kaiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib72">Peelen and Kastner, 2011</xref>). Spatial attention implies that a particular portion of the visual field is being attended. According to the FSGM, spatial location is treated as an attribute like any other. Therefore, a neuron’s modulation due to attention can be predicted by how well it’s preferred features and spatial receptive field align with the features or location of the attended stimulus. The effects of combined feature and spatial attention have been found to be additive (<xref ref-type="bibr" rid="bib32">Hayden and Gallant, 2009</xref>).</p><p>A debated issue in the attention literature is where in the visual stream attention effects can be seen. Many studies of attention focus on V4 and MT/MST (<xref ref-type="bibr" rid="bib92">Treue, 2001</xref>), as these areas have reliable attentional effects. Some studies do find effects at earlier areas (<xref ref-type="bibr" rid="bib65">Moro et al., 2010</xref>), though they tend to be weaker and occur later in the visual response (<xref ref-type="bibr" rid="bib38">Kastner and Pinsk, 2004</xref>). Therefore, a leading hypothesis is that attention signals, coming from prefrontal areas (<xref ref-type="bibr" rid="bib63">Moore and Armstrong, 2003</xref>; <xref ref-type="bibr" rid="bib62">Monosov et al., 2011</xref>; <xref ref-type="bibr" rid="bib6">Bichot et al., 2015</xref>; <xref ref-type="bibr" rid="bib44">Kornblith and Tsao, 2017</xref>), target later visual areas, and the feedback connections that those areas send to earlier ones cause the weaker effects seen there later (<xref ref-type="bibr" rid="bib10">Buffalo et al., 2010</xref>; <xref ref-type="bibr" rid="bib51">Luck et al., 1997</xref>).</p><p>In this study, we define the FSGM of attention mathematically and implement it in a deep CNN. By applying attention at different layers in the network and for different tasks, we see how neural changes at one area propagate through the network and change performance.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The network used in this study—VGG-16, (<xref ref-type="bibr" rid="bib85">Simonyan and Zisserman, 2014</xref>)—is shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref> and explained in Materials and methods, 'Network Model'. Briefly, at each convolutional layer, the application of a given convolutional filter results in a feature map, which is a 2-D grid of artificial neurons that represent how well the bottom-up input at each location aligns with the filter. Each layer has multiple feature maps. Therefore a 'retinotopic’ layout is built into the structure of the network, and the same visual features are represented across that retinotopy (akin to how cells that prefer a given orientation exist at all locations across the V1 retinotopy). This network was explored in (<xref ref-type="bibr" rid="bib28">Güçlü and van Gerven, 2015</xref>), where it was shown that early convolutional layers of this CNN are best at predicting activity of voxels in V1, while late convolutional layers are best at predicting activity of voxels in the object-selective lateral occipital area (LO).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.38105.003</object-id><label>Figure 1.</label><caption><title>Network architecture and feature-based attention task setup.</title><p>(<bold>A</bold>) The model used is a pre-trained deep neural network (VGG-16) that contains 13 convolutional layers (labelled in gray, number of feature maps given in parenthesis) and is trained on the ImageNet dataset to do 1000-way object classification. All convolutional filters are 3 × 3. (<bold>B</bold>) Modified architecture for feature-based attention tasks. To perform our feature-based attention tasks, the final layer that was implementing 1000-way softmax classification is replaced by binary classifiers (logistic regression), one for each category tested (two shown here, 20 total). These binary classifiers are trained on standard ImageNet images. (<bold>C</bold>) Test images for feature-based attention tasks. Merged images (left) contain two transparently overlaid ImageNet images of different categories. Array images (right) contain four ImageNet images on a 2 × 2 grid. Both are 224 × 224 pixels. These images are fed into the network and the binary classifiers are used to label the presence or absence of the given category. (<bold>D</bold>) Performance of binary classifiers. Box plots describe values over 20 different object categories (median marked in red, box indicates lower to upper quartile values and whiskers extend to full range, with the exception of outliers marked as dots). ‘Standard’ images are regular ImageNet images not used in the binary classifier training set.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig1-v2"/></fig><sec id="s2-1"><title>The relationship between tuning and classification</title><p>The feature similarity gain model of attention posits that neural activity is modulated by attention in proportion to how strongly a neuron prefers the attended features, as assessed by its tuning. However, the relationship between a neuron’s tuning and its ability to influence downstream readouts remains a difficult one to investigate biologically. We use our hierarchical model to explore this question. We do so by using back propagation to calculate 'gradient values', which we compare to tuning curves (see Materials and methods, 'Object category gradient calculations' and 'Tuning values' for details). Gradient values indicate the ways in which feature map activities should change in order to make the network more likely to classify an image as being of a certain object category. Tuning values represent the degree to which the feature map responds preferentially to images of a given category. If there is a correspondence between tuning and classification, a feature map that prefers a given object category (that is, responds strongly to it) should also have a high positive gradient value for that category. In <xref ref-type="fig" rid="fig2">Figure 2A</xref> we show gradient values and tuning curves for three example feature maps. In <xref ref-type="fig" rid="fig2">Figure 2C</xref>, we show the average correlation coefficients between tuning values and gradient values for all feature maps at each of the 13 convolutional layers. As can be seen, tuning curves in all layers show higher correlation with gradient values than expected by chance (as assayed by shuffled controls), but this correlation is relatively low, increasing across layers from about .2 to .5. Overall tuning quality also increases with layer depth (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), but less strongly.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.38105.004</object-id><label>Figure 2.</label><caption><title>Relationship between feature map tuning and gradient values.</title><p>(<bold>A</bold>) Example tuning values (green, left axis) and gradient values (purple, right axis) of three different feature maps from three different layers (identified in titles, layers as labelled in <xref ref-type="fig" rid="fig1">Figure 1A</xref>) over the 20 tested object categories. Tuning values indicate how the response to a category differs from the mean response; gradient values indicate how activity should change in order to classify input as from the category. Correlation coefficients between tuning curves and gradient values given in titles. All gradient and tuning values available in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref> (<bold>B</bold>) Tuning quality across layers. Tuning quality is defined per feature map as the maximum absolute tuning value of that feature map. Box plots show distribution across feature maps for each layer. Average tuning quality for shuffled data: .372 ± .097 (this value does not vary significantly across layers) (<bold>C</bold>) Correlation coefficients between tuning curves and gradient value curves averaged over feature maps and plotted across layers (errorbars ± S.E.M., data values in blue and shuffled controls in orange). (<bold>D</bold>) Distributions of gradient values when tuning is strong. In red, histogram of gradient values associated with tuning values larger than one (i.e. for feature maps that strongly prefer the category), across all feature maps in layers 10, 11, 12, and 13. For comparison, histograms of gradient values associated with tuning values less than one are shown in black (counts are separately normalized for visibility, as the population in black is much larger than that in red).</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.38105.005</object-id><label>Figure 2—source data 1.</label><caption><title>Object tuning curves and gradients.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-38105-fig2-data1-v2.txt"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig2-v2"/></fig><p>Even at the highest layers, there can be serious discrepancies between tuning and gradient values. In <xref ref-type="fig" rid="fig2">Figure 2D</xref>, we show the gradient values of feature maps at the final four convolutional layers, segregated according to tuning value. In red are gradient values that correspond to tuning values greater than one (for example, category 12 for the feature map in the middle pane of <xref ref-type="fig" rid="fig2">Figure 2A</xref>). As these distributions show, strong tuning values can be associated with weak or even negative gradient values. Negative gradient values indicate that increasing the activity of that feature map makes the network less likely to categorize the image as the given category. Therefore, even feature maps that strongly prefer a category (and are only a few layers from the classifier) still may not be involved in its classification, or even be inversely related to it. This is aligned with a recent neural network ablation study that shows category selectivity does not predict impact on classification (<xref ref-type="bibr" rid="bib64">Morcos et al., 2018</xref>).</p></sec><sec id="s2-2"><title>Feature-based attention improves performance on challenging object classification tasks</title><p>To determine if manipulation according to tuning values can enhance performance, we created challenging visual images composed of multiple objects for the network to classify. These test images are of two types: merged (two object images transparently overlaid, such as in <xref ref-type="bibr" rid="bib83">Serences et al., 2004</xref>) or array (four object images arranged on a grid) (see <xref ref-type="fig" rid="fig1">Figure 1C</xref> examples). The task for the network is to detect the presence of a given object category in these images. It does so using a series of binary classifiers trained on standard images of these objects, which replace the last layer of the network (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The performance of these classifiers on the test images indicates that this is a challenging task for the network (64.4% on merged images and 55.6% on array, <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Chance is 50%), and thus a good opportunity to see the effects of attention.</p><p>We implement feature-based attention in this network by modulating the activity of units in each feature map according to how strongly the feature map prefers the attended object category (see Materials and methods, 'Tuning values' and 'How attention is applied'). A schematic of this is shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. The slope of the activation function of units in a given feature map is scaled according to the tuning value of that feature map for the attended category (positive tuning values increase the slope while negative tuning values decrease it). Thus the impact of attention on activity is multiplicative and bi-directional.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.38105.006</object-id><label>Figure 3.</label><caption><title>Effects of applying feature-based attention on object category tasks.</title><p>(<bold>A</bold>) Schematic of how attention modulates the activity function. All units in a feature map are modulated the same way. The slope of the activation function is altered based on the tuning (or gradient) value, <inline-formula><mml:math id="inf1"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, of a given feature map (here, the <inline-formula><mml:math id="inf2"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> feature map in the <inline-formula><mml:math id="inf3"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer) for the attended category, <inline-formula><mml:math id="inf4"><mml:mi>c</mml:mi></mml:math></inline-formula>, along with an overall strength parameter <inline-formula><mml:math id="inf5"><mml:mi>β</mml:mi></mml:math></inline-formula>. <inline-formula><mml:math id="inf6"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> Is the input to this unit from the previous layer. For more information, see Materials and methods, 'How attention is applied'. (<bold>B</bold>) Average increase in binary classification performance as a function of layer at which attention is applied (solid line represents using tuning values, dashed line using gradient values, errorbars ± S.E.M.). In all cases, best performing strength from the range tested is used for each instance. Performance shown separately for merged (left) and array (right) images. Gradients perform significantly (<inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>.05</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>) better than tuning at layers 5 – 8 (p=4.6e<sup>-3</sup>, 2.6e<sup>-5</sup>, 6.5e<sup>-3</sup>, 4.4e<sup>-3</sup>) for merged images and 5 – 9 (p=3.1e<sup>-2</sup>, 2.3e<sup>-4</sup>, 4.2e<sup>-2</sup>, 6.1e<sup>-3</sup>, 3.1e<sup>-2</sup>) for array images. Raw performance values in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p><p><supplementary-material id="fig3sdata1"><object-id pub-id-type="doi">10.7554/eLife.38105.009</object-id><label>Figure 3—source data 1.</label><caption><title>Performance changes with attention. </title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-38105-fig3-data1-v2.txt"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38105.007</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Effect of applying attention to all layers or all feature maps uniformly.</title><p>(<bold>A</bold>) Effect of applying attention at all layers simultaneously for the category detection task. Performance increase in merged (left) and array (right) image tasks when attention is applied with tuning curves (red) or gradients (blue). Range of strengths tested is one-tenth that of the range tested when applying attention at only one layer and best-performing strength for each category is used. Errorbars are ± S.E.M. (<bold>B</bold>) Same as (<bold>A</bold>) but for orientation detection task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) (<bold>C</bold>) Control experiment. Instead of using tuning values or gradient values to determine how activity modulates feature maps, all feature maps are scaled by the same amount. Best-performing strengths are used for each category. These results show that merely scaling activity is insufficient to create the performance gains seen when attention is applied in a specific manner. Note: these results are independent of the layer at which the modulation takes place because <inline-formula><mml:math id="inf9"><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>∗</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf10"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig3-figsupp1-v2"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38105.008</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Alternative forms of attention.</title><p>(<bold>A</bold>) Schematics of how attention can modulate the activity function. Feature-based attention modulates feature maps according to their tuning values but this modulation can scale the activity multiplicatively or additively, and can either only enhance feature maps that prefer the attended category (positive-only) or also decrease the activity of feature maps that do not prefer it (bidirectional). See Materials and methods, 'Implementation options' for details of these implementations. The main body of this paper only uses multiplicative bi-directional attention. (<bold>B</bold>) Comparison of binary classification performance when attention is applied in each of the four ways described in (<bold>A</bold>). Considering the combination of attention applied to a given category at a given layer/layers as an instance (20 categories * 14 layer options = 280 instances), histograms (left axis) show how often the given option is the best performing, for merged (top) and array (bottom) images. Average increase in binary classification performance for each option also shown (right axis, averaged across all instances, errorbars ± S.E.M.).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig3-figsupp2-v2"/></fig></fig-group><p>The effects of attention are measured when attention is applied in this way at each layer individually (<xref ref-type="fig" rid="fig3">Figure 3B</xref>; solid lines) or all layers simultaneously (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>, red). For both image types (merged and array), attention enhances performance and there is a clear increase in performance enhancement as attention is applied at later layers in the network (numbering is as in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). In particular, attention applied at the final convolutional layer performs best, leading to an 18.8% percentage point increase in binary classification on the merged images task and 22.8% increase on the array images task. Thus, FSGM-like effects can have large beneficial impacts on performance.</p><p>Attention applied at all layers simultaneously does not lead to better performance than attention applied at any individual layer (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). We also performed a control experiment to ensure that nonspecific scaling of activity does not alone enhance performance (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>).</p><p>Some components of the FSGM are debated, for example whether attention impacts responses multiplicatively or additively (<xref ref-type="bibr" rid="bib8">Boynton, 2009</xref>; <xref ref-type="bibr" rid="bib5">Baruni et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="bib59">McAdams and Maunsell, 1999</xref> ), and whether the activity of cells that do not prefer the attended stimulus is actually suppressed (<xref ref-type="bibr" rid="bib9">Bridwell and Srinivasan, 2012</xref>; <xref ref-type="bibr" rid="bib67">Navalpakkam and Itti, 2007</xref>). Comparisons of different variants of the FSGM can be seen in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>. In general, multiplicative and bidirectional effects work best.</p><p>We also measure performance when attention is applied using gradient values rather than tuning values (these gradient values are calculated to maximize performance on the binary classification task, rather than classify the image as a given category; therefore technically they differ from those shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, however in practice they are strongly correlated. See Materials and methods, 'Object category gradient calculations' and 'Gradient values' for details). Attention applied using gradient values shows the same layer-wise trend as when using tuning values. It also reaches the same performance enhancement peak when attention is applied at the final layers. The major difference, however, comes when attention is applied at middle layers of the network. Here, attention applied according to gradient values outperforms that of tuning values.</p></sec><sec id="s2-3"><title>Attention strength and the trade-off between increasing true and false positives</title><p>In the previous section, we examined the best possible effects of attention by choosing the strength for each layer and category that optimized performance. Here, we look at how performance changes as we vary the overall strength (<inline-formula><mml:math id="inf11"><mml:mi>β</mml:mi></mml:math></inline-formula>) of attention.</p><p>In <xref ref-type="fig" rid="fig4">Figure 4A</xref> we break the binary classification performance into true and false positive rates. Here, each colored line indicates a different category and increasing dot size represents increasing strength of attention. Ideally, true positives would increase without an equivalent increase (and possibly with a decrease) in false positive rates. If they increase in tandem, attention does not have a net beneficial effect. Looking at the effects of applying attention at different layers, we can see that attention at lower layers is less effective at moving the performance in this space and that movement is in somewhat random directions, although there is an average increase in performance with moderate attentional strength. With attention applied at later layers, true positive rates are more likely to increase for moderate attentional strengths, while substantial false positive rate increases occur only with higher strengths. Thus, when attention is applied with modest strength at layer 13, most categories see a substantial increase in true positives with only modest increases in false positives. As strength continues to increase however, false positives increase substantially and eventually lead to a net decrease in overall classifier performance (representing as crossing the dotted line in <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.38105.010</object-id><label>Figure 4.</label><caption><title>Effects of varying attention strength</title><p>(<bold>A</bold>) Effect of increasing attention strength (<inline-formula><mml:math id="inf12"><mml:mi>β</mml:mi></mml:math></inline-formula>) in true and false positive rate space for attention applied at each of four layers (layer indicated in bottom right of each panel, attention applied using tuning values). Each line represents performance for an individual category (only 10 categories shown for visibility), with each increase in dot size representing a .15 increase in <inline-formula><mml:math id="inf13"><mml:mi>β</mml:mi></mml:math></inline-formula>. Baseline (no attention) values are subtracted for each category such that all start at (0,0). The black dotted line represents equal changes in true and false positive rates. (<bold>B</bold>) Comparisons from experimental data. The true and false positive rates from six experiments in four previously published studies are shown for conditions of increasing attentional strength (solid lines). Cat-Drawings = (<xref ref-type="bibr" rid="bib54">Lupyan and Ward, 2013</xref>), Exp. 1; Cat-Images=(<xref ref-type="bibr" rid="bib54">Lupyan and Ward, 2013</xref>),Exp. 2; Objects=(<xref ref-type="bibr" rid="bib43">Koivisto and Kahila, 2017</xref>), Letter-Aud.=(<xref ref-type="bibr" rid="bib53">Lupyan and Spivey, 2010</xref>), Exp. 1; Letter-Vis.=(<xref ref-type="bibr" rid="bib53">Lupyan and Spivey, 2010</xref>), Exp. 2. Ori-Change=(<xref ref-type="bibr" rid="bib58">Mayo and Maunsell, 2016</xref>). See Materials and methods, 'Experimental data' for details of experiments. Dotted lines show model results for merged images, averaged over all 20 categories, when attention is applied using either tuning (TC) or gradient (Grad) values at layer 13. Model results are shown for attention applied with increasing strengths (starting at 0, with each increasing dot size representing a .15 increase in <inline-formula><mml:math id="inf14"><mml:mi>β</mml:mi></mml:math></inline-formula>). Receiver operating curve (ROC) for the model using merged images, which corresponds to the effect of changing the threshold in the final, readout layer, is shown in gray. Raw performance values in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38105.011</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Negatively applying attention and best-performing strengths.</title><p>(<bold>A</bold>) Effect of strength increase in true and false positive rate space when tuning values are negated. Negated tuning values have the same overall level of positive and negative modulation but in the opposite direction of tuning for a given category. Plot same as in <xref ref-type="fig" rid="fig4">Figure 4A</xref>. Layer attention applied at indicated in gray. Attention applied in this way decreases true positives, and to a lesser extent false positives (the initial false positive rate when no attention is applied is very low). (<bold>B</bold>) Mean best performing strength (<inline-formula><mml:math id="inf15"><mml:mi>β</mml:mi></mml:math></inline-formula> value; using regular non-negated attention) across categories as a function of the layer attention is applied at, according to merged images task. Errorbars ± S.E.M.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig4-figsupp1-v2"/></fig></fig-group><p>Applying attention according to negated tuning values leads to a decrease in true and false positive values with increasing attention strength, which decreases overall performance (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). This verifies that the effects of attention are not from non-specific changes in activity.</p><p>Experimentally, when switching from no or neutral attention, neurons in MT showed an average increase in activity of 7% when attending their preferred motion direction (and similar decrease when attending the non-preferred) (<xref ref-type="bibr" rid="bib55">Martinez-Trujillo and Treue, 2004</xref>). In our model, when <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>.75</mml:mn></mml:mrow></mml:math></inline-formula> (roughly the value at which performance peaks at later layers; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>), given the magnitude of the tuning values (average magnitude: .38), attention scales activity by an average of 28.5%. This value refers to how much activity is modulated in comparison to the <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> condition, which is probably more comparable to passive or anesthetized viewing, as task engagement has been shown to scale neural responses generally (<xref ref-type="bibr" rid="bib70">Page and Duffy, 2008</xref>). This complicates the relationship between modulation strength in our model and the values reported in the data.</p><p>To allow for a more direct comparison, in <xref ref-type="fig" rid="fig4">Figure 4B</xref>, we collected the true and false positive rates obtained experimentally during different object detection tasks (explained in Materials and methods, 'Experimental data'), and plotted them in comparison to the model results when attention is applied at layer 13 using tuning values (pink line) or gradient value (brown line). Five experiments (second through sixth studies) are human studies. In all of these, uncued trials are those in which no information about the upcoming visual stimulus is given, and therefore attention strength is assumed to be low. In cued trials, the to-be-detected category is cued before the presentation of a challenging visual stimulus, allowing attention to be applied to that object or category.</p><p>The majority of these experiments show a concurrent increase in both true and false positive rates as attention strength is increased. The rates in the uncued conditions (smaller dots) are generally higher than the rates produced by the <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> condition in our model, consistent with neutrally cued conditions corresponding to <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. We find (see Materials and methods, 'Experimental data'), that the average corresponding <inline-formula><mml:math id="inf20"><mml:mi>β</mml:mi></mml:math></inline-formula> value for the neutral conditions is .37 and for the attended conditions .51. Because attention scales activity by <inline-formula><mml:math id="inf21"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf22"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the tuning value), these changes correspond to a <inline-formula><mml:math id="inf23"><mml:mo>≈</mml:mo></mml:math></inline-formula>5% change in activity.</p><p>The first dataset included in the plot (Ori-Change; yellow line in <xref ref-type="fig" rid="fig4">Figure 4B</xref>) comes from a macaque change detection study (see Materials and methods, 'Experimental data' for details). Because the attention cue was only 80% valid, attention strength could be of three levels: low (for the uncued stimuli on cued trials), medium (for both stimuli on neutrally-cued trials), or high (for the cued stimuli on cued trials). Like the other studies, this study shows a concurrent increase in both true positive (correct change detection) and false positive (premature response) rates with increasing attention strength. For the model to achieve the performance changes observed between low and medium attention a roughly 12% activity change is needed, but average V4 firing rates recorded during this task show an increase of only 3.6%. This discrepancy may suggest that changes in correlations (<xref ref-type="bibr" rid="bib17">Cohen and Maunsell, 2009</xref>) or firing rate changes in areas aside from V4 also make important contributions to observed performance changes.</p><p>Thus, according to our model, the size of experimentally observed performance changes is broadly consistent with the size of experimentally observed neural changes. While other factors are likely also relevant for performance changes, this rough alignment between the magnitude of firing rate changes and magnitude of performance changes supports the idea that the former could be a major causal factor for the latter. In addition, the fact that the model can capture this relationship provides further support for its usefulness as a model of the biology.</p><p>Finally, we show the change in true and false positive rates when the threshold of the final layer binary classifier is varied (a ‘receiver operating characteristic’ analysis, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, gray line; no attention was applied during this analysis). Comparing this to the pink line, it is clear that varying the strength of attention applied at the final convolutional layer has more favorable performance effects than altering the classifier threshold (which corresponds to an additive effect of attention at the classifier layer). This points to the limitations that could come from attention targeting only downstream readout areas.</p><p>Overall, the model roughly matches experiments in the amount of neural modulation needed to create the observed changes in true and false positive rates. However, it is clear that the details of the experimental setup are relevant, and changes aside from firing rate and/or outside the ventral stream also likely play a role (<xref ref-type="bibr" rid="bib67">Navalpakkam and Itti, 2007</xref>).</p></sec><sec id="s2-4"><title>Feature-based attention enhances performance on orientation detection task</title><p>Some of the results presented above, particularly those related to the layer at which attention is applied, may be influenced by the fact that we are using an object categorization task. To see if results are comparable using the simpler stimuli frequently used in macaque studies, we created an orientation detection task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Here, binary classifiers trained on full-field oriented gratings are tested using images that contain two gratings of different orientation and color. The performance of these binary classifiers without attention is above chance (distribution across orientations shown in inset of <xref ref-type="fig" rid="fig5">Figure 5A</xref>). The performance of the binary classifier associated with vertical orientation (0 degrees) was abnormally high (92% correct without attention, other orientations average 60.25%. This likely reflects the over-representation of vertical lines in the training images) and this orientation was excluded from further performance analysis.</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.38105.012</object-id><label>Figure 5.</label><caption><title>Attention task and results using oriented gratings.</title><p>(<bold>A</bold>) Orientation detection task. Like with the object category detection tasks, separate binary classifiers trained to detect each of 9 different orientations replaced the final layer of the network. Test images included two oriented gratings of different color and orientation located at 2 of 4 quadrants. Inset shows performance over nine orientations without attention (<bold>B</bold>) Orientation tuning quality as a function of layer. (<bold>C</bold>) Average correlation coefficient between orientation tuning curves and gradient curves across layers (blue). Shuffled correlation values in orange. Errorbars are ± S.E.M. (<bold>D</bold>) Comparison of performance on orientation detection task when attention is determined by tuning values (solid line) or gradient values (dashed line) and applied at different layers. As in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, best performing strength is used in all cases. Errorbars are ±S.E.M. Gradients perform significantly (p=1.9e -2) better than tuning at layer 7. Raw performance values available in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>. (<bold>E</bold>) Change in signal detection values and performance (perent correct) when attention is applied in different ways—spatial (red), feature according to tuning (solid blue), feature according to gradients (dashed blue), and both spatial and feature (according to tuning, black)—for the task of detecting a given orientation in a given quadrant. Top row is when attention is applied at layer 13 and bottom when applied at layer 4. Raw performance values available in <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>.</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.38105.014</object-id><label>Figure 5—source data 1.</label><caption><title>Performance on orientation detection task.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-38105-fig5-data1-v2.txt"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><object-id pub-id-type="doi">10.7554/eLife.38105.015</object-id><label>Figure 5—source data 2.</label><caption><title>Performance on spatial and feature-based attention task.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-38105-fig5-data2-v2.txt"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig5-v2"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38105.013</object-id><label>Figure 5—figure supplement 1.</label><caption><title>True and false positive changes with spatial and feature-based attention.</title><p>(<bold>A</bold>) Effect of strength increase in true and false positive rate space when attention is applied according to quadrant, orientation, or both in the orientation detection task. Rates averaged over orientations/locations. Increasing dot size corresponds to .2 increase in <inline-formula><mml:math id="inf24"><mml:mi>β</mml:mi></mml:math></inline-formula> each. No-attention rates are subtracted and the black dotted line indicates equal increase in true and false positives. Layer attention applied at indicated in gray.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig5-figsupp1-v2"/></fig></fig-group><p>Attention is applied according to orientation tuning values of the feature maps (tuning quality by layer is shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>) and tested across layers. We find (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, solid line and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>, red) that the trend in this task is similar to that of the object task: applying attention at later layers leads to larger performance increases (14.4% percentage point increase at layer 10). This is despite the fact that orientation tuning quality peaks in the middle layers.</p><p>We also calculate the gradient values for this orientation detection task. While overall the correlations between gradient values and tuning values are lower (and even negative for early layers), the average correlation still increases with layer (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), as with the category detection task. Importantly, while this trend in correlation exists in both detection tasks tested here, it is not a universal feature of the network or an artifact of how these values are calculated. Indeed, an opposite pattern in the correlation between orientation tuning and gradient values is shown when using attention to orientation to classify the color of a stimulus with the attended orientation (see 'Recordings show how feature similarity gain effects propagate', and Materials and methods, 'Oriented grating attention tasks' and 'Gradient values').</p><p>The results of applying attention according to gradient values is shown in <xref ref-type="fig" rid="fig5">Figure 5D</xref> (dashed line). Here again, using gradient value creates similar trends as using tuning values, with gradient values performing better in the middle layers.</p></sec><sec id="s2-5"><title>Feature-based attention primarily influences criteria and spatial attention primarily influences sensitivity</title><p>Signal detection theory is frequently used to characterize the effects of attention on performance (<xref ref-type="bibr" rid="bib97">Verghese, 2001</xref>). Here, we use a joint feature-spatial attention task to explore effects of attention in the model. The task uses the same two-grating stimuli described above. The same binary orientation classifiers are used and the task of the model is to determine if a given orientation is present in a given quadrant of the image. Performance is then measured when attention is applied to an orientation, a quadrant, or both an orientation and a quadrant (effects are combined additively, for more, see Materials and methods, 'How attention is applied'). Two key signal detection measurements are computed: criteria and sensitivity. Criteria is a measure of the threshold that’s used to mark an input as positive, with a higher criteria leading to fewer positives; sensitivity is a measure of the separation between the two populations (positives and negatives), with higher sensitivity indicating a greater separation.</p><p><xref ref-type="fig" rid="fig5">Figure 5E</xref> shows that both spatial and feature-based attention influence sensitivity and criteria. However, feature-based attention decreases criteria more than spatial attention does. Intuitively, feature-based attention shifts the representations of all stimuli in the direction of the attended category, implicitly lowering the detection threshold. Starting from a high threshold, this can lead to the observed behavioural pattern wherein true positives increase before false positives do. Sensitivity increases more for spatial attention alone than for feature-based attention alone, indicating that spatial attention amplifies differences in the representation of whichever features are present. These general trends hold regardless of the layer at which attention is applied and whether feature-based attention is applied using tuning curves or gradients. Changes in true and false positive rates for this task can be seen explicitly in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p><p>In line with our results, spatial attention was found experimentally to increase sensitivity and (less reliably) decrease criteria (<xref ref-type="bibr" rid="bib31">Hawkins et al., 1990</xref>; <xref ref-type="bibr" rid="bib22">Downing, 1988</xref>). Furthermore, feature-based attention is known to decrease criteria, with lesser effects on sensitivity (<xref ref-type="bibr" rid="bib74">Rahnev et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Bang and Rahnev, 2017</xref>; though see <xref ref-type="bibr" rid="bib87">Stein and Peelen, 2015</xref>). A study that looked explicitly at the different effects of spatial and category-based attention (<xref ref-type="bibr" rid="bib88">Stein and Peelen, 2017</xref>) found that spatial attention increases sensitivity more than category-based attention (most visible in their Experiment 3c, which uses natural images), and the effects of the two are additive.</p><p>Attention and priming are known to impact neural activity beyond pure sensory areas (<xref ref-type="bibr" rid="bib45">Krauzlis et al., 2013</xref>; <xref ref-type="bibr" rid="bib19">Crapse et al., 2018</xref>). This idea is borne out by a study that aimed to isolate the neural changes associated with sensitivity and criteria changes (<xref ref-type="bibr" rid="bib52">Luo and Maunsell, 2015</xref>) In this study, the authors designed behavioural tasks that encouraged changes in behavioural sensitivity or criteria exclusively: high sensitivity was encouraged by associating a given stimulus location with higher overall reward, while high criteria was encouraged by rewarding correct rejects more than hits (and vice versa for low sensitivity/criteria). Differences in V4 neural activity were observed between trials using high versus low sensitivity stimuli. No differences were observed between trials using high versus low criteria stimuli. This indicates that areas outside of the ventral stream (or at least outside V4) are capable of impacting criteria (<xref ref-type="bibr" rid="bib86">Sridharan et al., 2017</xref>). Importantly, it does not mean that changes in V4 don’t impact criteria, but merely that those changes can be countered by the impact of changes in other areas. Indeed, to create sessions wherein sensitivity was varied without any change in criteria, the authors had to increase the relative correct reject reward (i.e., increase the criteria) at locations of high absolute reward, which may have been needed to counter a decrease in criteria induced by attention-related changes in V4 (similarly, they had to decrease the correct reject reward at low reward locations). Our model demonstrates clearly how such effects from sensory areas alone can impact detection performance, which, in turn highlights the role downstream areas may play in determining the final behavioural outcome.</p></sec><sec id="s2-6"><title>Recordings show how feature similarity gain effects propagate</title><p>To explore how attention applied at one location in the network impacts activity later on, we apply attention at various layers and 'record' activity at others (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, in response to full field oriented gratings). In particular, we record activity of feature maps at all layers while applying attention at layers 2, 6, 8, 10, or 12 individually.</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.38105.016</object-id><label>Figure 6.</label><caption><title>How attention-induced activity changes propagate through the network.</title><p>(<bold>A</bold>) Recording setup. The spatially averaged activity of feature maps at each layer was recorded (left) while attention was applied at layers 2, 6, 8, 10, or 12 individually. Activity was in response to a full field oriented grating. (<bold>B</bold>) Schematic of metric used to test for the feature similarity gain model. Activity when a given orientation is present and attended is divided by the activity when no attention is applied, giving a set of activity ratios. Ordering these ratios from most to least preferred orientation and fitting a line to them gives the slope and intercept values plotted in (<bold>C</bold>). Intercept values are plotted in terms of how they differ from 1, so positive values are an intercept greater than 1. (FSGM predicts negative slope and positive intercept). (<bold>C</bold>) The median slope (solid line) and intercept (dashed line) values as described in (<bold>B</bold>) plotted for each layer when attention is applied to the layer indicated by the line color as labelled in (<bold>A</bold>). On the left, attention applied according to tuning values and on the right, attention applied according to gradient values. Raw slope and intercept values when using tuning curves available in <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref> and for gradients in <xref ref-type="supplementary-material" rid="fig6sdata2">Figure 6—source data 2</xref>. (<bold>D</bold>) Fraction of feature maps displaying feature matching behaviour at each layer when attention is applied at the layer indicated by line color. Shown for attention applied according to tuning (solid lines) and gradient values (dashed line).</p><p><supplementary-material id="fig6sdata1"><object-id pub-id-type="doi">10.7554/eLife.38105.019</object-id><label>Figure 6—source data 1.</label><caption><title>Intercepts and slopes from gradient-applied attention.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-38105-fig6-data1-v2.txt"/></supplementary-material></p><p><supplementary-material id="fig6sdata2"><object-id pub-id-type="doi">10.7554/eLife.38105.020</object-id><label>Figure 6—source data 2.</label><caption><title>Intercepts and slopes from tuning curve-applied attention.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-38105-fig6-data2-v2.txt"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig6-v2"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38105.017</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Feature-based attention at one layer often suppresses activity of the attended features at later layers.</title><p>Activity ratios are shown for when attention is applied (according to tuning) at various layers individually and activity is recorded from that layer and later layers. In all cases, the category attended was the same as the one present in the input image (standard ImageNet images used to ensure that these results are not influenced by the presence of other category features in the input). Histograms are of ratios of feature map activity when attention is applied to the category divided by activity when no attention is applied, split according to whether the feature map prefers (red) or does not prefer (black) the attended category. In many cases, feature maps that prefer the attended category have activity ratios less than one, indicating that attention at a lower layer decreases the activity of feature maps that prefer the attended category. The misalignment between lower and later layers is starker the larger the distance between the attended and recorded layers. For example, when looking at layer 12, attention applied at layer two appears to increase and decrease feature map activity equally, without respect to category preference. This demonstrates the ability of attention at a lower layer to change activity in ways opposite to the effects of attention at the recorded layer.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig6-figsupp1-v2"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.38105.018</object-id><label>Figure 6—figure supplement 2.</label><caption><title>Correlating activity changes with performance changes.</title><p>(<bold>A</bold>) A new measure of activity changes inspired by gradient values. The gray vector represents the average pattern of neural activity in response to images the classifier indicates as containing the given orientation (i.e., positively-classified in the absence of attention, whether or not the orientation was present in the image). The blue vector (activity without attention) and red vector (activity when attention is applied) are then made using images that do contain the given orientation. Assuming that attention makes activity look more like activity during positive classification, this measure compares the cosine of the angle between the positively-classified and with-attention vectors to the cosine of the angle between the positively-classified and without-attention vectors. We use <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as the measure, but results are similar using <inline-formula><mml:math id="inf26"><mml:mi>θ</mml:mi></mml:math></inline-formula>. See Materials and methods, 'Correlating activity changes with performance' for how this is calculated (<bold>B</bold>) Using the same color scheme as <xref ref-type="fig" rid="fig6">Figure 6</xref>, this plot shows how attention applied at different layers causes activity changes throughout the network, as measured by the vector method introduced in (<bold>A</bold>). Specifically, the cosine of the angle between the positively-classified and without-attention vectors is subtracted from the cosine of the angle between the positively-classified and with-attention vectors. Solid lines indicate median value of this difference (across images) when attention is applied with tuning curves and dashed line when applied with gradients. (<bold>C</bold>) How activity changes correlate with performance changes. The correlation coefficient between the change in true positive rate with attention and activity changes as measured by: difference in cosines of angles (blue line) or feature similarity gain model-like behaviour (orange line). Activity and performance changes are collected when attention is applied at different layers individually (using a range of strengths) according to tuning curves (left) or gradient values (right). Activity is recorded at and after the layer at which attention is applied. For a given layer L, the correlation coefficient is thus computed across data points, where there is one data point for each combination of orientation, strength of attention applied, and layer (l <inline-formula><mml:math id="inf27"><mml:mo>≤</mml:mo></mml:math></inline-formula> L) at which attention is applied. A bootstrap analysis determined that at layers 1, 2, 3, 4, and 5 the vector angle method had significantly (<inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>.05</mml:mn></mml:mrow></mml:math></inline-formula>) higher correlation with performance for both application options than the FSGM measure.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig6-figsupp2-v2"/></fig></fig-group><p>To understand the activity changes occurring at each layer, we use an analysis from (<xref ref-type="bibr" rid="bib55">Martinez-Trujillo and Treue, 2004</xref>) that was designed to test for FSGM-like effects and is explained in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. Here, the activity of a feature map in response to a given orientation when attention is applied is divided by the activity in response to the same orientation without attention. These ratios are organized according to the feature map’s orientation preference (most to least) and a line is fit to them. According to the FSGM of attention, this ratio should be greater than one for more preferred orientations and less than one for less preferred, creating a line with an intercept greater than one and negative slope.</p><p>In <xref ref-type="fig" rid="fig6">Figure 6C</xref>, we plot the median value of the slopes and intercepts across all feature maps at a layer, when attention is applied at different layers (indicated by color). When attention is applied directly at a layer according to its tuning values (left), FSGM effects are seen by default (intercept values are plotted in terms of how they differ from one; comparable average values from (<xref ref-type="bibr" rid="bib55">Martinez-Trujillo and Treue, 2004</xref>) are intercept: .06 and slope: 0.0166, but note we are using <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for the no-attention condition in the model which, as mentioned earlier, is not necessarily the best analogue for no-attention conditions experimentally. Therefore we use these measures to show qualitative effects). As these activity changes propagate through the network, however, the FSGM effects wear off, suggesting that activating units tuned for a stimulus at one layer does not necessarily activate cells tuned for that stimulus at the next. This misalignment between tuning at one layer and the next explains why attention applied at all layers simultaneously isn’t more effective (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). In fact, applying attention to a category at one layer can actually have effects that counteract attention at a later layer (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><p>In <xref ref-type="fig" rid="fig6">Figure 6C</xref> (right), we show the same analysis, but while applying attention according to gradient values. The effects at the layer at which attention is applied do not look strongly like FSGM, however FSGM properties evolve as the activity changes propagate through the network, leading to clear FSGM-like effects at the final layer. Finding FSGM-like behaviour in neural data could thus be a result of FSGM effects at that area or non-FSGM effects at an earlier area (here, attention applied according to gradients which, especially at earlier layers, are not aligned with tuning).</p><p>An alternative model of the neural effects of attention—the feature matching (FM) model—suggests that the effect of attention is to amplify the activity of a neuron whenever the stimulus in its receptive field matches the attended stimulus. In <xref ref-type="fig" rid="fig6">Figure 6D</xref>, we calculate the fraction of feature maps at a given layer that show feature matching behaviour (defined as having activity ratios greater than one when the stimulus orientation matches the attended orientation for both preferred and anti-preferred orientations). As early as one layer post-attention, some feature maps start showing feature matching behaviour. The fact that the attention literature contains conflicting findings regarding the feature similarity gain model versus the feature matching model (<xref ref-type="bibr" rid="bib66">Motter, 1994</xref>; <xref ref-type="bibr" rid="bib78">Ruff and Born, 2015</xref>) may result from this finding that FSGM effects can turn into FM effects as they propagate through the network. In particular, this mechanism can explain the observations that feature matching behaviour is observed more in FEF than V4 (<xref ref-type="bibr" rid="bib105">Zhou and Desimone, 2011</xref>) and that match information is more easily read out from perirhinal cortex than IT (<xref ref-type="bibr" rid="bib69">Pagan et al., 2013</xref>).</p><p>We also investigated the extent to which measures of attention’s neural effects correlate with changes in performance (see Materials and methods, 'Correlating activity changes with performance'). For this we developed a new, experimentally-feasible way of calculating attention’s effects on neural activity that is inspired by the gradient-based approach to attention (that is, it focuses on classification rather than tuning). We show (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>) that this new measure better correlates with performance changes than the FSGM measure of activity changes, particularly at earlier layers.</p><p>There is a simple experiment that would distinguish whether factors beyond tuning, such as gradients, play a role in guiding attention. It requires using two tasks with very different objectives (which should produce different gradients) but with the same attentional cue. An example is described in <xref ref-type="fig" rid="fig7">Figure 7</xref>. Here, the two tasks used would be an orientation-based color classification task (two gratings each with their own color and orientation are simultaneously shown, and the task is to report the color of the grating with the attended orientation) and an orientation detection task (report if the attended orientation is present or absent in the image). In both cases, attention is cued according to orientation. But gradient-based attention will produce different neural modulations for the two tasks, while the FSGM predicts identical modulations (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Thus, an experiment that recorded from the same neurons during both tasks could distinguish between tuning-based and gradient-based attention.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.38105.021</object-id><label>Figure 7.</label><caption><title>A proposed experiment to distinguish between tuning-based and gradient-based attention</title><p>(<bold>A</bold>) ‘Cross-featural’ attention task. Here, the final layer of the network is replaced with a color classifier and the task is to classify the color of the attended orientation in a two-orientation stimulus. Importantly, in both this and the orientation detection task (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), a subject performing the task would be cued to attend to an orientation. (<bold>B</bold>) The correlation coefficient between the gradient values calculated for this task and orientation tuning values (as in <xref ref-type="fig" rid="fig5">Figure 5C</xref>). Correlation peaks at lower layers for this task. (<bold>C</bold>) Correlation between tuning values for the two tasks (blue) and between gradient values for the two tasks (orange). If attention does target cells based on tuning, the modulation would be the same in both the color classification task and the orientation detection task. If a gradient-based targeting is used, no (or even a slight anti-) correlation is expected. Tuning and gradient values available in <xref ref-type="supplementary-material" rid="fig7sdata1">Figure 7—source data 1</xref>.</p><p><supplementary-material id="fig7sdata1"><object-id pub-id-type="doi">10.7554/eLife.38105.022</object-id><label>Figure 7—source data 1.</label><caption><title>Orientation tuning curves and gradients.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-38105-fig7-data1-v2.txt"/></supplementary-material></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-38105-fig7-v2"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we utilized a deep convolutional neural network (CNN) as a model of the visual system to probe the relationship between modulation of neural activity, as in attention, and performance. Specifically, we formally define the feature similarity gain model (FSGM) of attention (the basic tenets of which have been described in several experimental studies) as a multiplicative modulation of neuronal activity proportional to the neuron’s mean-subtracted feature tuning. This formalization allows us to investigate the FSGM’s ability to enhance a CNN’s performance on challenging visual tasks. We found that, across a variety of tasks, neural activity changes matching the type and magnitude of those observed experimentally can indeed lead to performance changes of the kind and magnitude observed experimentally.</p><p>We used the full observability of the model to investigate the relationship between tuning and function. We compared attention applied according to feature tuning (the FSGM) with attention designed to optimally modulate activity to improve performance (as determined by the gradient of performance with respect to the neural activity). Attention applied according to tuning does not successfully propagate from lower or middle to higher layers; that is, enhancing the activity of neurons that most prefer a given category at lower layers need not selectively enhance the activity of neurons preferring that category at higher layers. As a result, attention applied according to the FSGM performs poorly when applied at early to middle layers, while attention applied according to gradients at these layers performs better.</p><p>Attention is most effective applied at later layers (e.g., layers 9–13), where tuning and gradient values are better correlated. According to (<xref ref-type="bibr" rid="bib28">Güçlü and van Gerven, 2015</xref>), these layers correspond most to areas V4 and LO. Such areas are known and studied for reliably showing attentional effects, whereas earlier areas such as V1 are generally not (<xref ref-type="bibr" rid="bib51">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="bib1">Abdelhack and Kamitani, 2018</xref>). In a study involving detection of objects in natural scenes, the strength of category-specific preparatory activity in object selective cortex was correlated with performance, whereas such preparatory activity in V1 was anti-correlated with performance (<xref ref-type="bibr" rid="bib72">Peelen and Kastner, 2011</xref>). This is in line with our finding that feature-based attention effects at earlier areas can counter the beneficial effects of that attention at later areas (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><p>Our work raises the question: is attention applied simply according to tuning or is it targeted to best optimize function on a given task? We suggested a simple experiment (<xref ref-type="fig" rid="fig7">Figure 7</xref>) that would reveal whether non-tuning factors, such as gradients, guide attentional modulation. In (<xref ref-type="bibr" rid="bib15">Chelazzi et al., 1998</xref>) the correlation coefficient between an index of tuning and an index of attentional modulation was .52 for a population of V4 neurons, suggesting factors other than selectivity influence attention. Furthermore, many attention studies, including that one, use only preferred and anti-preferred stimuli and therefore don’t include a thorough investigation of the relationship between tuning and attentional modulation. (<xref ref-type="bibr" rid="bib55">Martinez-Trujillo and Treue, 2004</xref>) uses multiple stimuli to provide support for the FSGM, however the interpretation is limited by the fact that they only report population averages. (<xref ref-type="bibr" rid="bib78">Ruff and Born, 2015</xref>) investigated the relationship between tuning strength and the strength of attentional modulation on a cell-by-cell basis. While they did find a correlation (particularly for binocular disparity tuning), it was relatively weak, which leaves room for the possibility that tuning is not the primary factor that determines attentional modulation. Local connectivity is also likely to play a role, as a correlation between normalization and attentional modulation has been shown (<xref ref-type="bibr" rid="bib68">Ni et al., 2012</xref>).</p><p>A major challenge for understanding the biological implementation of selective attention is determining how such a precise attentional signal is carried by feedback connections. We believe that it is plausible that the visual system can learn the connections needed to carry out gradient-based attention. For example, if a high-level neuron related to the classification of an image sends a feedback connection to lower areas, an anti-Hebbian post-pre spike timing-dependent learning rule would strengthen the connection from the high level neuron to the low level one, if the lower level one causes the firing of the higher. In this way, neurons in later areas can learn to target the cells in earlier areas that caused them to fire. In contrast, it is actually more difficult to imagine how higher areas could learn the connections needed to target neurons according to their tuning, as in the FSGM. The machine learning literature on attention and learning may inspire other useful hypotheses on underlying brain mechanisms (<xref ref-type="bibr" rid="bib100">Xu et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Lillicrap et al., 2016</xref>).</p><p>The concept of attention has been introduced in these models previously in the machine learning literature (<xref ref-type="bibr" rid="bib60">Mnih et al., 2014</xref>). Generally, this kind of attention relates to what would be called overt spatial attention in the neuroscience literature. That is, the attention mechanism serially selects areas of the input image for further processing, rather than modulating the activity of neurons representing those areas (as in our model of spatial attention). Other work has been done using attention to selectively process image features (<xref ref-type="bibr" rid="bib89">Stollenga et al., 2014</xref>) and it would be interesting to compare the workings of that model to the feature-based attention used in our study.</p><p>While CNNs have representations that are similar to the ventral stream, they lack many biological details including recurrent connections, dynamics, cell types, and noisy responses. Preliminary work has shown that these elements can be incorporated into a CNN structure, and attention can enhance performance in this more biologically-realistic architecture (<xref ref-type="bibr" rid="bib49">Lindsay et al., 2017</xref>). Furthermore, while the current work does not include neural noise independent of the stimulus, the fact that a given image is presented in many contexts (different merged images or different array images) can be thought of as a form of highly structured noise that does produce variable responses to the same image.</p><p>Another biological detail that this model lacks is 'skip connections,’ where one layer feeds into both the layer directly after it and deeper layers after that (<xref ref-type="bibr" rid="bib33">He et al., 2016</xref>; <xref ref-type="bibr" rid="bib35">Huang et al., 2017</xref>) as in connections from V2 to V4 or V4 to parietal areas (<xref ref-type="bibr" rid="bib96">Ungerleider et al., 2008</xref>). Our results regarding propagation of changes through the network suggest that synaptic distance from the classifier is a relevant feature—one that is less straight forward to determine in a network with skip connections.</p><p>Because experimenters can easily control the image, defining a cell’s function in terms of how it responds to stimuli makes practical sense. However, it may be that thinking about visual areas in terms of their synaptic distance from decision-making areas such as prefrontal cortex (<xref ref-type="bibr" rid="bib34">Heekeren et al., 2004</xref>) can be more useful for the study of attention than thinking in terms of their distance from the retina. Thus far, coarse stimulation protocols have found a relationship between the tuning of neural populations and their impact on perception (<xref ref-type="bibr" rid="bib61">Moeller et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">DeAngelis et al., 1998</xref>; <xref ref-type="bibr" rid="bib81">Salzman et al., 1990</xref>). However, studies of the relationship between tuning and choice probabilities suggest that a neuron’s preferred stimulus is not always an indication of its causal role in classification (<xref ref-type="bibr" rid="bib102">Zaidel et al., 2017</xref>; <xref ref-type="bibr" rid="bib73">Purushothaman and Bradley, 2005</xref>), though see (<xref ref-type="bibr" rid="bib39">Katz et al., 2016</xref>). Targeted stimulation protocols and a more fine-grained ability to determine both upstream drivers of, and downstream responses driven by, stimulated neurons will be needed to better address these issues.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Key resources</title><p>The weights for the model ('VGG-16') came from <xref ref-type="bibr" rid="bib26">Frossard (2017)</xref> (RRID <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_016494">SCR_016494</ext-link>).</p></sec><sec id="s4-2"><title>Network model</title><p>This work uses a deep convolutional neural network (CNN) as a model of the ventral visual stream. Convolutional neural networks are feed forward artificial neural networks that consist of a few basic operations repeated in sequence, key among them being the convolution. The specific CNN architecture used in the study comes from <xref ref-type="bibr" rid="bib85">Simonyan and Zisserman, 2014</xref> (VGG-16D) and is shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref> (a previous variant of this work used a smaller network (<xref ref-type="bibr" rid="bib48">Lindsay, 2015</xref>). For this study, all the layers of the CNN except the final classifier layer were pre-trained using back propagation on the ImageNet classification task, which involves doing 1000-way object categorization (weights provided by <xref ref-type="bibr" rid="bib26">Frossard, 2017</xref>). The training of the top layer is described in subsequent sections. Here we describe the basic workings of the CNN model we use, with details available in <xref ref-type="bibr" rid="bib85">Simonyan and Zisserman, 2014</xref>.</p><p>The activity values of the units in each convolutional layer are the result of applying a 2-D spatial convolution to the layer below, followed by positive rectification (rectified linear ’ReLu’ nonlinearity):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>⋆</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf30"><mml:mo>⋆</mml:mo></mml:math></inline-formula> indicates convolution, and <inline-formula><mml:math id="inf31"><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, 0 otherwise. <inline-formula><mml:math id="inf33"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="inf34"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> convolutional filter at the <inline-formula><mml:math id="inf35"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer. The application of each filter results in a 2-D feature map (the number of filters used varies across layers and is given in parenthesis in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). <inline-formula><mml:math id="inf36"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the activity of the unit at the <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> spatial location in the <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> feature map at the <inline-formula><mml:math id="inf39"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer. <inline-formula><mml:math id="inf40"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is thus the activity of all units at the layer below the <inline-formula><mml:math id="inf41"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer. The input to the network is a 224 by 224 pixel RGB image, and thus the first convolution is applied to these pixel values. Convolutional filters are 3 × 3. For the purposes of this study the convolutional layers are most relevant, and will be referred to according to their numbering in <xref ref-type="fig" rid="fig1">Figure 1A</xref> (numbers in parentheses indicate number of feature maps per layer).</p><p>Max pooling layers reduce the size of the feature maps by taking the maximum activity value of units in a given feature map in non-overlapping 2 × 2 windows. Through this, the size of the feature maps decreases after each max pooling (layers 1 and 2: 224 × 224; 3 and 4: 112 × 112; 5, 6, and 7: 56 × 56. 8, 9, and 10: 28 × 28; 11, 12, and 13: 14 × 14).</p><p>The final two layers before the classifier are each fully-connected to the layer below them, with the number of units per layer given in parenthesis in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Therefore, connections exist from all units from all feature maps in the last convolutional layer (layer 13) to all 4096 units of the next layer, and so on. The top readout layer of the network in (<xref ref-type="bibr" rid="bib85">Simonyan and Zisserman, 2014</xref>) contained 1000 units upon which a softmax classifier was used to output a ranked list of category labels for a given image. Looking at the top-5 error rate (wherein an image is correctly labelled if the true category appears in the top five categories given by the network), this network achieved 92.7% accuracy. With the exception of the gradient calculations described below, we did not use this 1000-way classifier, but rather replaced it with a series of binary classifiers.</p></sec><sec id="s4-3"><title>Object category attention tasks</title><p>The tasks we use to probe the effects of feature-based attention in this network involve determining if a given object category is present in an image or not, similar to tasks used in (<xref ref-type="bibr" rid="bib88">Stein and Peelen, 2017</xref>; <xref ref-type="bibr" rid="bib71">Peelen et al., 2009</xref>; <xref ref-type="bibr" rid="bib43">Koivisto and Kahila, 2017</xref>). To have the network perform this specific task, we replaced the final layer in the network with a series of binary classifiers, one for each category tested (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We tested a total of 20 categories: paintbrush, wall clock, seashore, paddlewheel, padlock, garden spider, long-horned beetle, cabbage butterfly, toaster, greenhouse, bakery, stone wall, artichoke, modem, football helmet, stage, mortar, consomme, dough, bathtub. Binary classifiers were trained using ImageNet images taken from the 2014 validation set (and were therefore not used in the training of the original model). A total of 35 unique true positive images were used for training for each category, and each training batch was balanced with 35 true negative images taken from the remaining 19 categories. The results shown here come from using logistic regression as the binary classifier, though trends in performance are similar if support vector machines are used.</p><p>Once these binary classifiers are trained, they are then used to classify more challenging test images. Experimental results suggest that classifiers trained on unattended and isolated object images are appropriate for reading out attended objects in cluttered images (<xref ref-type="bibr" rid="bib104">Zhang et al., 2011</xref>). These test images are composed of multiple individual images (drawn from the 20 categories) and are of two types: 'merged’ and 'array’. Merged images are generated by transparently overlaying two images, each from a different category (specifically, pixel values from each are divided by two and then summed). Array images are composed of four separate images (all from different categories) that are scaled down to 112 by 112 pixels and placed on a two by two grid. The images that comprise these test images also come from the 2014 validation set, but are separate from those used to train the binary classifiers. See examples of each in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. Test image sets are balanced (50% do contain the given category and 50% do not, 150 total test images per category). Both true positive and true negative rates are recorded and overall performance is the average of these rates.</p></sec><sec id="s4-4"><title>Object category gradient calculations</title><p>When neural networks are trained via back propagation, gradients are calculated that indicate how a given weight in the network impacts the final classification. We use this same method to determine how a given unit’s activity impacts the final classification. Specifically, we input a 'merged’ image (wherein one of the images belongs to the category of interest) to the network. We then use gradient calculations to determine the changes in activity that would move the 1000-way classifier toward classifying that image as belonging to the category of interest (i.e. rank that category highest). We average these activity changes over images and over all units in a feature map. This gives a single value per feature map:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where H and W are the spatial dimensions of layer <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the total number of images from the category (here <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>35</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the merged images used were generated from the same images used to generate tuning curves, described below). <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is the error of the 1000-way classifier in response to image <inline-formula><mml:math id="inf46"><mml:mi>n</mml:mi></mml:math></inline-formula>, which is defined as the cross entropy between the activity vector of the final layer (after the soft-max operation) and a one-hot vector, wherein the correct label is the only non-zero entry. Because we are interested in activity changes that would decrease the error value, we negate this term. The gradient value we end up with thus indicates how the feature map’s activity would need to change to make the network more likely to classify an image as the desired category. Repeating this procedure for each category, we obtain a set of gradient values (one for each category, akin to a tuning curve), for each feature map: <inline-formula><mml:math id="inf47"><mml:mrow><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>g</mml:mi></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Note that, as these values result from applying the chain rule through layers of the network, they can be very small, especially for the earliest layers. For this study, the sign and relative magnitudes are of more interest than the absolute values.</p></sec><sec id="s4-5"><title>Oriented grating attention tasks</title><p>In addition to attending to object categories, we also test attention on simpler stimuli. In the orientation detection task, the network detects the presence of a given orientation in an image. Again, the final layer of the network is replaced by a series of binary classifiers, one for each of 9 orientations (0, 20, 40, 60, 80, 100, 120, 140, and 160 degrees. Gratings had a frequency of. 025 cycles/pixel). The training sets for each were balanced (50% had only the given orientation and 50% had one of 8 other orientations) and composed of full field (224 by 224 pixel) oriented gratings in red, blue, green, orange, or purple (to increase the diversity of the training images, they were randomly degraded by setting blocks of pixels ranging uniformly from 0% to 70% of the image to 0 at random). Test images were each composed of two oriented gratings of different orientation and color (same options as training images). Each of these gratings were of size 112 by 112 pixels and placed randomly in a quadrant while the remaining two quadrants were black (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Again, the test sets were balanced and performance was measured as the average of the true positive and true negative rates (100 test images per orientation).</p><p>These same test images were used for a task wherein the network had to classify the color of the grating that had the attended orientation (cross-featural task paradigms like this are commonly used in attention studies, such as <xref ref-type="bibr" rid="bib80">Sàenz et al., 2003</xref>). For this, the final layer of the network was replaced with a 5-way softmax color classifier. This color classifier was trained using the same full field oriented gratings used to train the binary classifiers (therefore, the network saw each color at all orientation values).</p><p>For another analysis, a joint feature and spatial attention task was used. This task is almost identical to the setup of the orientation detection task, except that the searched-for orientation would only appear in one of the four quadrants. Therefore, performance could be measured when applying feature-based attention to the searched-for orientation, spatial attention to the quadrant in which it could appear, or both.</p></sec><sec id="s4-6"><title>How attention is applied</title><p>This study aims to test variations of the feature similarity gain model of attention, wherein neural activity is modulated by attention according to how much the neuron prefers the attended stimulus. To replicate this in our model, we therefore must first determine the extent to which units in the network prefer different stimuli ('tuning values'). When attention is applied to a given category, for example, units’ activities are modulated according to these values.</p><sec id="s4-6-1"><title>Tuning values</title><p>To determine tuning to the 20 object categories used, we presented the network with images of each object category (the same images on which the binary classifiers were trained) and measured the relative activity levels. Because feature-based attention is a spatially global phenomenon (<xref ref-type="bibr" rid="bib103">Zhang and Luck, 2009</xref>; <xref ref-type="bibr" rid="bib79">Saenz et al., 2002</xref>), we treat all units in a feature map identically, and calculate tuning by averaging over them.</p><p>Specifically, for the <inline-formula><mml:math id="inf48"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> feature map in the <inline-formula><mml:math id="inf49"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer, we define <inline-formula><mml:math id="inf50"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> as the activity in response to image <inline-formula><mml:math id="inf51"><mml:mi>n</mml:mi></mml:math></inline-formula>, averaged over all units in the feature map (i.e., over the spatial dimensions). Averaging these values over all images in the training sets (<inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>35</mml:mn></mml:mrow></mml:math></inline-formula> images per category, 20 categories. <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>700</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) gives the mean activity of the feature map <inline-formula><mml:math id="inf54"><mml:mrow><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msup><mml:mstyle displaystyle="true" mathsize="140%"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Tuning values are defined for each object category, <inline-formula><mml:math id="inf55"><mml:mi>c</mml:mi></mml:math></inline-formula> as:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>That is, a feature map’s tuning value for a given category is merely the average activity of that feature map in response to images of that category, with the mean activity under all image categories subtracted, divided by the standard deviation of the activity across all images. These tuning values determine how the feature map is modulated when attention is applied to the category. Taking these values as a vector over all categories, <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, gives a tuning curve for the feature map. We define the overall tuning quality of a feature map as its maximum absolute tuning value: <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. To determine expected tuning quality by chance, we shuffled the responses to individual images across category and feature map at a given layer and calculated tuning quality for this shuffled data.</p><p>We also define the category with the highest tuning value as that feature map’s most preferred, and the category with the lowest (most negative) value as the least or anti-preferred.</p><p>We apply the same procedure to generate tuning curves for orientation by using the full field gratings used to train the orientation detection classifiers. The orientation tuning values were used when applying attention in these tasks.</p><p>When measuring how correlated tuning values are with gradient values, shuffled comparisons are used. To do this shuffling, correlation coefficients are calculated from pairing each feature map’s tuning values with a random other feature map’s gradient values.</p></sec><sec id="s4-6-2"><title>Gradient values</title><p>In addition to applying attention according to tuning, we also attempt to generate the 'best possible’ attentional modulation by utilizing gradient values. These gradient values are calculated slightly differently from those described above ('Object category gradient calculations'), because they are meant to represent how feature map activity should change in order to increase binary classification performance, rather than just increase the chance of classifying an image as a certain object.</p><p>The error functions used to calculate gradient values for the category and orientation detection tasks were for the binary classifiers associated with each object/orientation. A balanced set of test images was used. Therefore a feature map’s gradient value for a given object/orientation is the averaged activity change that would increase binary classification performance for that object/orientation. Note that on images that the network already classifies correctly, gradients are zero. Therefore, the gradient values are driven by the errors: false negatives (classifying an image as not containing the category when it does) and false positives (classifying an image as containing the category when it does not). In our detection tasks, the former error is more prevalent than the latter, and thus is the dominant impact on the gradient values. Because of this, gradient values calculated this way end up very similar to those described in Materials and methods, 'Object category gradient calculations', as they are driven by a push to positively classify the input as the given category.</p><p>The same procedure was used to generate gradient values for the color classification task. Here, gradients were calculated using the 5-way color classifier: for a given orientation, the color of that orientation in the test image was used as the correct label, and gradients were calculated that would lead to the network correctly classifying the color. Averaging over many images of different colors gives one value per orientation that represents how a feature map’s activity should change in order to make the network better at classifying the color of that orientation.</p><p>In the orientation detection task, the test images used for gradient calculations (50 images per orientation) differed from those used to assess performance. For the object detection task, images used for gradient calculations (45 per category; preliminary tests for some categories using 90 images gave similar results) were drawn from the same pool as, but different from, those used to test detection performance. Gradient values were calculated separately for merged and array images.</p></sec><sec id="s4-6-3"><title>Spatial attention</title><p>In the feature similarity gain model of attention, attention is applied according to how much a cell prefers the attended feature, and location is considered a feature like any other. In CNNs, each feature map results from applying the same filter at different spatial locations. Therefore, the 2-D position of a unit in a feature map represents more or less the spatial location to which that unit responds. Via the max-pooling layers, the size of each feature map shrinks deeper in the network, and each unit responds to a larger area of image space, but the 'retinotopy' is still preserved. Thus, when we apply spatial attention to a given area of the image, we enhance the activity of units in that area of the feature maps and decrease the activity of units in other areas. In this study, spatial attention is applied to a given quadrant of the image.</p></sec><sec id="s4-6-4"><title>Implementation options</title><p>The values discussed above determine how strongly different feature maps or units should be modulated under different attentional conditions. We will now lay out the different implementation options for that modulation. The multiplicative bidirectional form of attention is used throughout this paper (with the exception of <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> where it is compared to the others). Other implementations are only used for the Supplementary Results.</p><p>First, the modulation can be multiplicative or additive. That is, when attending to category <inline-formula><mml:math id="inf58"><mml:mi>c</mml:mi></mml:math></inline-formula>, the slope of the rectified linear units can be multiplied by the tuning value for category <inline-formula><mml:math id="inf59"><mml:mi>c</mml:mi></mml:math></inline-formula> weighted by the strength parameter, <inline-formula><mml:math id="inf60"><mml:mi>β</mml:mi></mml:math></inline-formula>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>with <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> representing input to the unit coming from layer <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Alternatively, a weighted version of the tuning value can be added before the rectified linear unit:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>β</mml:mi><mml:msubsup><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Strength of attention is varied via the strength parameter, <inline-formula><mml:math id="inf63"><mml:mi>β</mml:mi></mml:math></inline-formula>. For the additive effect, manipulations are multiplied by <inline-formula><mml:math id="inf64"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the average activity level across all units of layer <inline-formula><mml:math id="inf65"><mml:mi>l</mml:mi></mml:math></inline-formula> in response to all images (for each of the 13 layers respectively: 20, 100, 150, 150, 240, 240, 150, 150, 80, 20, 20, 10, 1). When gradient values are used in place of tuning values, we normalize them by the maximum value at a layer, to be the same order of magnitude as the tuning values: <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Recall that for feature-based attention all units in a feature map are modulated the same way, as feature-based attention has been found to be spatially global. In the case of spatial attention, however, tuning values are not used and a unit’s modulation is dependent on its location in the feature map. Specifically, the tuning value term is set to +1 if the <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> position of the unit is in the attended quadrant and to −1 otherwise. For feature-based attention tasks, <inline-formula><mml:math id="inf68"><mml:mi>β</mml:mi></mml:math></inline-formula> ranged from 0 to a maximum of 11.85 (object attention) and 0 to 4.8 (orientation attention). For spatial attention tasks, it ranged from 0 to 1.</p><p>Next, we chose whether attention only enhances units that prefer the attended feature, or also decreases activity of those that don’t prefer it. For the latter, the tuning values are used as-is. For the former, the tuning values are positively-rectified: <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>Combining these two factors, there are four implementation options: additive positive-only, multiplicative positive-only, additive bidirectional, and multiplicative bidirectional.</p><p>The final option is the layer in the network at which attention is applied. We try attention at all convolutional layers individually and, in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, simultaneously (when applying simultaneously the strength range tested is a tenth of that when applying to a single layer).</p></sec></sec><sec id="s4-7"><title>Signal detection calculations</title><p>For the joint spatial-feature attention task (<xref ref-type="fig" rid="fig5">Figure 5</xref>), we calculated criteria (<inline-formula><mml:math id="inf70"><mml:mi>c</mml:mi></mml:math></inline-formula>, 'threshold') and sensitivity (<inline-formula><mml:math id="inf71"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>) using true (TP) and false (FP) positive rates as follows (<xref ref-type="bibr" rid="bib52">Luo and Maunsell, 2015</xref>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf72"><mml:mrow><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the inverse cumulative normal distribution function. <inline-formula><mml:math id="inf73"><mml:mi>c</mml:mi></mml:math></inline-formula> is a measure of the distance from a neutral threshold situated between the mean of the true negative and true positive distributions. Thus, a positive <inline-formula><mml:math id="inf74"><mml:mi>c</mml:mi></mml:math></inline-formula> indicates a stricter threshold (fewer inputs classified as positive) and a negative <inline-formula><mml:math id="inf75"><mml:mi>c</mml:mi></mml:math></inline-formula> indicates a more lenient threshold (more inputs classified as positive). The sensitivity was calculated as:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This measures the distance between the means of the distributions for true negative and two positives. Thus, a larger <inline-formula><mml:math id="inf76"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> indicates better sensitivity.</p><p>To prevent the individual terms in these expressions from going to <inline-formula><mml:math id="inf77"><mml:mrow><mml:mo>±</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula>, false positive rates of <inline-formula><mml:math id="inf78"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>.01</mml:mn></mml:mrow></mml:math></inline-formula> were set to <inline-formula><mml:math id="inf79"><mml:mrow><mml:mn>.01</mml:mn></mml:mrow></mml:math></inline-formula> and true positive rates of <inline-formula><mml:math id="inf80"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>.99</mml:mn></mml:mrow></mml:math></inline-formula> were set to <inline-formula><mml:math id="inf81"><mml:mrow><mml:mn>.99</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-8"><title>Assessment of feature similarity gain model and feature matching behaviour</title><p>In <xref ref-type="fig" rid="fig6">Figure 6</xref>, we examined the effects that applying attention at certain layers in the network (specifically 2, 6, 8, 10, and 12) has on activity of units at other layers. Attention was applied with <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The recording setup is designed to mimic the analysis of (<xref ref-type="bibr" rid="bib55">Martinez-Trujillo and Treue, 2004</xref>). Here, the images presented to the network are full-field oriented gratings of all orientation-color combinations. Feature map activity is measured as the spatially averaged activity of all units in a feature map in response to an image. Activity in response to a given orientation is further averaged over all colors. We calculate the ratio of activity when attention is applied to a given orientation (and the orientation is present in the image) over activity in response to the same image when no attention is applied. These ratios are then organized according to orientation preference: the most preferred is at location 0, then the average of next two most preferred at location 1, and so on with the average of the two least preferred orientations at location 4 (the reason for averaging of pairs is to match <xref ref-type="bibr" rid="bib55">Martinez-Trujillo and Treue, 2004</xref> as closely as possible). Fitting a line to these points gives a slope and intercept for each feature map (lines are fit using the least squares method). FSGM predicts a negative slope and an intercept greater than one.</p><p>To test for signs of feature matching behaviour, each feature map’s preferred (most positive tuning value) and anti-preferred (most negative tuning value) orientations are determined. Activity is recorded when attention is applied to the preferred or anti-preferred orientation and activity ratios are calculated. According to the FSGM, activity when the preferred orientation is attended should be greater than when the anti-preferred is attended, regardless of whether the image is of the preferred or anti-preferred orientation. According to the feature matching (FM) model, however, activity when attending the presented orientation should be greater than activity when attending an absent orientation, regardless of whether the orientation is preferred or not. Therefore, we say that a feature map is displaying feature matching behaviour if (1) activity is greater when attending the preferred orientation when the preferred is present versus when the anti-preferred is present, and (2) activity is greater when attending the anti-preferred orientation when the anti-preferred is present versus when the preferred is present. The second criteria distinguishes feature matching behaviour from FSGM.</p></sec><sec id="s4-9"><title>Correlating activity changes with performance</title><p>In <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>, we use two different measures of attention-induced activity changes in order to probe the relationship between activity and classification performance. In both cases, the network is performing the orientation detection task described in <xref ref-type="fig" rid="fig5">Figure 5A</xref> and performance is measured only in terms of true positive rates. Because we know attention to increase both true and false positive rates, we would expect a positive correlation between activity changes and true positive performance, but a negative correlation between activity changes and true negative rates. This predicts that activity changes will have a monotonic relationship with true positive performance, but an inverted U-shaped relationship with total performance. Since we are calculating correlation coefficients of activity with performance, which measure a linear relationship, we use the rate of true positives as our measure of performance.</p><p>The first measure is meant to capture feature similarity gain model-like behaviour in a way similar to the metric described in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. The main difference is that that measure is calculated over a population of images of different stimuli, whereas the variant introduced here can be calculated on an image-by-image basis. Images that contain a given orientation are shown to the network and the spatially-averaged activity of feature maps is recorded when attention is applied to that orientation and when it is not. The ratio of these activities is then plotted against each feature map’s tuning value for the orientation. According to the FSGM, this ratio should be above one for feature maps with positive tuning values and less than one for those with negative tuning values. Therefore, we use the slope of the line fitted to these ratios plotted as a function of tuning values as an indication of the extent to which activity is FSGM-like (with positive slopes more FSGM-like). The median slope over a set of images of a given orientation is paired with the change in performance on those images with attention. This gives one pair for each combination of orientation, strength (<inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>.15</mml:mn><mml:mo>,</mml:mo><mml:mn>.30</mml:mn><mml:mo>,</mml:mo><mml:mn>.45</mml:mn><mml:mo>,</mml:mo><mml:mn>.60</mml:mn><mml:mo>,</mml:mo><mml:mn>.75</mml:mn><mml:mo>,</mml:mo><mml:mn>.90</mml:mn></mml:mrow></mml:math></inline-formula>), and layer at which attention was applied (activity changes are only recorded if attention was applied at or before the recorded layer). The correlation coefficient between these value pairs is plotted as the orange line in <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2C</xref>.</p><p>The second measure aims to characterize activity in terms of its downstream effects, rather than the contents of the input ('Vector Angle' measure, see <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2A</xref> for a visualization). It is therefore more aligned with the gradient-based approach to attention rather than tuning, and is thus related to 'choice probability' measures (<xref ref-type="bibr" rid="bib102">Zaidel et al., 2017</xref>; <xref ref-type="bibr" rid="bib73">Purushothaman and Bradley, 2005</xref>). First, for a particular orientation, images that both do and do not contain that orientation are shown to the network. Activity (spatially-averaged over each feature map) in response to images classified as containing the orientation (i.e., both true and false positives) is averaged in order to construct a vector in activity space that represents positive classification for a given layer. To reduce complications of working with vectors in high dimensions, principal components are found that capture at least 90% of the variance of the activity in response to all images, and all computations are done in this lower dimensional space. The next step is to determine if attention moves activity in a given layer closer to this direction of positive classification. For this, only images that contain the given orientation are used. For each image, the cosine of the angle between the positive-classification vector and the activity in response to the image is calculated. The median of these angles over a set of images is calculated separately for when attention is applied and when it is not. The difference between these medians (with-attention minus without-attention) is paired with the change in performance that comes with attention on those images. Then the same correlation calculation is done with these pairs as described above.</p><p>The outcome of these analyses is a correlation coefficient between the measure of activity changes and performance changes. This gives two values per layer: one for the FSGM-like measure and one for the vector angle measure. To determine if these two values are significantly different, we performed a bootstrap analysis. For this, correlation coefficients were recalculated using simulated data made by sampling with replacement from the true data. We do this 100 times and perform a two-sided t-test to test for differences between the two measures.</p></sec><sec id="s4-10"><title>Experimental data</title><p>Model results were compared to previously published data coming from several studies. In <xref ref-type="bibr" rid="bib54">Lupyan and Ward, 2013</xref>, a category detection task was performed using stereogram stimuli (on object present trials, the object image was presented to one eye and a noise mask to another). The presentation of the visual stimuli was preceded by a verbal cue that indicated the object category that would later be queried (cued trials) or by meaningless noise (uncued trials). After visual stimulus presentation, subjects were asked if an object was present and, if so, if the object was from the cued category (categories were randomized for uncued trials). In Experiment 1 ('Cat-Drawings' in <xref ref-type="fig" rid="fig4">Figure 4B</xref>), the object images were line drawings (one per category) and the stimuli were presented for 1.5 s. In Experiment 2 ('Cat-Images'), the object images were grayscale photographs (multiple per category) and presented for 6 s (of note: this presumably allows for several rounds of feedback processing, in contrast to our purelfeed forwardrd model). True positives were counted as trials wherein a given object category was present and the subject correctly indicated its presence when queried. False positives were trials wherein no category was present and subjects indicated that the queried category was present.</p><p>In <xref ref-type="bibr" rid="bib53">Lupyan and Spivey (2010)</xref>, a similar detection task was used. Here, subjects detected the presence of an uppercase letter that (on target present trials) was presented rapidly and followed by a mask. Prior to the visual stimulus, a visual ('Letter-Vis') or audio ('Letter-Aud') cue indicated a target letter. After the visual stimulus, the subjects were required to indicate whether any letter was present. True positives were trials in which a letter was present and the subject indicated it (only uncued trials or validly cued trials—where the cued letter was the letter shown—were considered here). False positives were trials where no letter was present and the subject indicated that one was.</p><p>The task in <xref ref-type="bibr" rid="bib43">Koivisto and Kahila (2017)</xref> was also an object category detection task ('Objects'). Here, an array of several images was flashed on the screen with one image marked as the target. All images were color photographs of objects in natural scenes. In certain blocks, the subjects knew in advance which category they would later be queried about (cued trials). On other trials, the queried category was only revealed after the visual stimulus (uncued). True positives were trials in which the subject indicated the presence of the queried category when it did exist in the target image. False positives were trials in which the subject indicated the presence of the cued category when it was not in the target image. Data from trials using basic category levels with masks were used for this study.</p><p>Finally, we include one study using macaques ('Ori-Change') wherein both neural and performance changes were measured (<xref ref-type="bibr" rid="bib58">Mayo and Maunsell, 2016</xref>). In this task, subjects had to report a change in orientation that could occur in one of two stimuli. On cued trials, the change occurred in the cued stimulus in 80% of trials and the uncued stimulus in 20% of trials. On neutrally-cued trials, subjects were not given prior information about where the change was likely to occur (50% at each stimulus). Therefore performance could be compared under conditions of low (uncued stimuli), medium (neutrally cued stimuli), and high (cued stimuli) attention strength. Correct detection of an orientation change in a given stimulus (indicated by a saccade) is considered a true positive and a saccade to the stimulus prior to any orientation change is considered a false positive. True negatives are defined as correct detection of a change in the uncued stimulus (as this means the subject correctly did not perceive a change in the stimulus under consideration) and false negatives correspond to a lack of response to an orientation change. While this task includes a spatial attention component, it is still useful as a test of feature-based attention effects. Previous work has demonstrated that, during a change detection task, feature-based attention is deployed to the pre-change features of a stimulus (<xref ref-type="bibr" rid="bib18">Cohen and Maunsell, 2011</xref>; <xref ref-type="bibr" rid="bib57">Mayo et al., 2015</xref>). Therefore, because the pre-change stimuli are of differing orientations, the cueing paradigm used here controls the strength of attention to orientation as well.</p><p>In cases where the true and false positive rates were not published, they were obtained via personal communications with the authors. Not all changes in performance were statistically significant, but we plot them to show general trends.</p><p>We calculate the activity changes required in the model to achieve the behavioural changes observed experimentally by using the data plotted in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. We determine the average <inline-formula><mml:math id="inf84"><mml:mi>β</mml:mi></mml:math></inline-formula> value for the neutral and cued conditions by finding the <inline-formula><mml:math id="inf85"><mml:mi>β</mml:mi></mml:math></inline-formula> value of the point on the model line nearest to the given data point. Specifically, we average the <inline-formula><mml:math id="inf86"><mml:mi>β</mml:mi></mml:math></inline-formula> values found for the four datasets whose experiments are most similar to our merged image task (Cat-Drawings, Cat-Images, Letter-Aud, and Letter-Vis).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We are very grateful to the authors who so readily shared details of their behavioural data upon request: J Patrick Mayo, Gary Lupyan, and Mika Koivisto. We further thank J Patrick Mayo for helpful comments on the manuscript.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Funding acquisition, Investigation, Visualization, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>Supervision, Funding acquisition, Methodology, Writing—review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.38105.023</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-38105-transrepform-v2.pdf"/></supplementary-material><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The weights for the model used are linked to in the study. The data resulting from simulations have been packaged and are available on Dryad (doi:10.5061/dryad.jc14081). The analysis code are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/gwl2108/CNN_attention">https://github.com/gwl2108/CNN_attention</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/CNN_attention">https://github.com/elifesciences-publications/CNN_attention</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Data from: How biological attention mechanisms improve task performance in a large-scale visual system model</data-title><source>Available at Dryad Digital Repository under a CC0 Public Domain Dedication</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.jc14081</pub-id></element-citation></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdelhack</surname> <given-names>M</given-names></name><name><surname>Kamitani</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sharpening of hierarchical visual feature representations of blurred images</article-title><source>Eneuro</source><volume>5</volume><elocation-id>ENEURO.0443-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0443-17.2018</pub-id><pub-id pub-id-type="pmid">29756028</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Azulay</surname> <given-names>A</given-names></name><name><surname>Weiss</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title> Why do deep convolutional networks generalize so poorly to small image transformations? </article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1805.12177">https://arxiv.org/abs/1805.12177</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname> <given-names>N</given-names></name><name><surname>Lu</surname> <given-names>H</given-names></name><name><surname>Erlikhman</surname> <given-names>G</given-names></name><name><surname>Kellman</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep convolutional networks do not make classifications based on global object shape</article-title><source>Journal of Vision</source><volume>18</volume><fpage>904</fpage><pub-id pub-id-type="doi">10.1167/18.10.904</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname> <given-names>JW</given-names></name><name><surname>Rahnev</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stimulus expectation alters decision criterion but not sensory signal in perceptual decision making</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>17072</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-16885-2</pub-id><pub-id pub-id-type="pmid">29213117</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baruni</surname> <given-names>JK</given-names></name><name><surname>Lau</surname> <given-names>B</given-names></name><name><surname>Salzman</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reward expectation differentially modulates attentional behavior and activity in visual area V4</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1656</fpage><lpage>1663</lpage><pub-id pub-id-type="doi">10.1038/nn.4141</pub-id><pub-id pub-id-type="pmid">26479590</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bichot</surname> <given-names>NP</given-names></name><name><surname>Heard</surname> <given-names>MT</given-names></name><name><surname>DeGennaro</surname> <given-names>EM</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A source for Feature-Based attention in the prefrontal cortex</article-title><source>Neuron</source><volume>88</volume><fpage>832</fpage><lpage>844</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.10.001</pub-id><pub-id pub-id-type="pmid">26526392</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borji</surname> <given-names>A</given-names></name><name><surname>Itti</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal attentional modulation of a neural population</article-title><source>Frontiers in Computational Neuroscience</source><volume>8</volume><elocation-id>34</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2014.00034</pub-id><pub-id pub-id-type="pmid">24723881</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A framework for describing the effects of attention on visual responses</article-title><source>Vision Research</source><volume>49</volume><fpage>1129</fpage><lpage>1143</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.11.001</pub-id><pub-id pub-id-type="pmid">19038281</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bridwell</surname> <given-names>DA</given-names></name><name><surname>Srinivasan</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Distinct attention networks for feature enhancement and suppression in vision</article-title><source>Psychological Science</source><volume>23</volume><fpage>1151</fpage><lpage>1158</lpage><pub-id pub-id-type="doi">10.1177/0956797612440099</pub-id><pub-id pub-id-type="pmid">22923337</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buffalo</surname> <given-names>EA</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Landman</surname> <given-names>R</given-names></name><name><surname>Liang</surname> <given-names>H</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A backward progression of attentional effects in the ventral stream</article-title><source>PNAS</source><volume>107</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1073/pnas.0907658106</pub-id><pub-id pub-id-type="pmid">20007766</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bundesen</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>A theory of visual attention</article-title><source>Psychological Review</source><volume>97</volume><fpage>523</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.97.4.523</pub-id><pub-id pub-id-type="pmid">2247540</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cadena</surname> <given-names>SA</given-names></name><name><surname>Denfield</surname> <given-names>GH</given-names></name><name><surname>Walker</surname> <given-names>EY</given-names></name><name><surname>Gatys</surname> <given-names>LA</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Ecker</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep convolutional models improve predictions of macaque v1 responses to natural images</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/201764</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual attention: the past 25 years</article-title><source>Vision Research</source><volume>51</volume><fpage>1484</fpage><lpage>1525</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.04.012</pub-id><pub-id pub-id-type="pmid">21549742</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cave</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The FeatureGate model of visual selection</article-title><source>Psychological Research</source><volume>62</volume><fpage>182</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1007/s004260050050</pub-id><pub-id pub-id-type="pmid">10490397</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chelazzi</surname> <given-names>L</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Responses of neurons in inferior temporal cortex during memory-guided visual search</article-title><source>Journal of Neurophysiology</source><volume>80</volume><fpage>2918</fpage><lpage>2940</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.80.6.2918</pub-id><pub-id pub-id-type="pmid">9862896</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chikkerur</surname> <given-names>S</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Tan</surname> <given-names>C</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>What and where: a bayesian inference theory of attention</article-title><source>Vision Research</source><volume>50</volume><fpage>2233</fpage><lpage>2247</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.05.013</pub-id><pub-id pub-id-type="pmid">20493206</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention improves performance primarily by reducing interneuronal correlations</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1594</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1038/nn.2439</pub-id><pub-id pub-id-type="pmid">19915566</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Using neuronal populations to study the mechanisms underlying spatial and feature attention</article-title><source>Neuron</source><volume>70</volume><fpage>1192</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.029</pub-id><pub-id pub-id-type="pmid">21689604</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crapse</surname> <given-names>TB</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name><name><surname>Basso</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A role for the superior colliculus in decision criteria</article-title><source>Neuron</source><volume>97</volume><fpage>181</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.006</pub-id><pub-id pub-id-type="pmid">29301100</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Çukur</surname> <given-names>T</given-names></name><name><surname>Nishimoto</surname> <given-names>S</given-names></name><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention during natural vision warps semantic representation across the human brain</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>763</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1038/nn.3381</pub-id><pub-id pub-id-type="pmid">23603707</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Cumming</surname> <given-names>BG</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical area MT and the perception of stereoscopic depth</article-title><source>Nature</source><volume>394</volume><fpage>677</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1038/29299</pub-id><pub-id pub-id-type="pmid">9716130</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Expectancy and visual-spatial attention: effects on perceptual quality</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>14</volume><fpage>188</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.14.2.188</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckstein</surname> <given-names>MP</given-names></name><name><surname>Peterson</surname> <given-names>MF</given-names></name><name><surname>Pham</surname> <given-names>BT</given-names></name><name><surname>Droll</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Statistical decision theory to relate neurons to behavior in the study of covert visual attention</article-title><source>Vision Research</source><volume>49</volume><fpage>1097</fpage><lpage>1128</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.12.008</pub-id><pub-id pub-id-type="pmid">19138699</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickenberg</surname> <given-names>M</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Seeing it all: convolutional network layers map the function of the human visual system</article-title><source>NeuroImage</source><volume>152</volume><fpage>184</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.001</pub-id><pub-id pub-id-type="pmid">27777172</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Reynolds</surname> <given-names>JH</given-names></name><name><surname>Rorie</surname> <given-names>AE</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modulation of oscillatory neuronal synchronization by selective visual attention</article-title><source>Science</source><volume>291</volume><fpage>1560</fpage><lpage>1563</lpage><pub-id pub-id-type="doi">10.1126/science.1055465</pub-id><pub-id pub-id-type="pmid">11222864</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Frossard</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>VGG in TensorFlow</article-title><ext-link ext-link-type="uri" xlink:href="https://www.cs.toronto.edu/ frossard/post/vgg16">https://www.cs.toronto.edu/ frossard/post/vgg16</ext-link><date-in-citation iso-8601-date="2017-03-01">March 1, 2017</date-in-citation></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukushima</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Neocognitron: a hierarchical neural network capable of visual pattern recognition</article-title><source>Neural Networks</source><volume>1</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/0893-6080(88)90014-7</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname> <given-names>U</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hamker</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="1999">1999</year><chapter-title><italic>The Role of Feedback Connections in Task-Driven Visual Search</italic></chapter-title><source>Connectionist Models in Cognitive Neuroscience</source><publisher-name>Springer</publisher-name><fpage>252</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1007/978-1-4471-0813-9_22</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hamker</surname> <given-names>FH</given-names></name><name><surname>Worcester</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title><italic>Object Detection in Natural Scenes by Feedback</italic></chapter-title><source>International Workshop on Biologically Motivated Computer Vision</source><volume>407</volume><publisher-name>Springer</publisher-name><fpage>398</fpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>HL</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Mouloua</surname> <given-names>M</given-names></name><name><surname>Downing</surname> <given-names>CJ</given-names></name><name><surname>Woodward</surname> <given-names>DP</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Visual attention modulates signal detectability</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>16</volume><fpage>802</fpage><lpage>811</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.16.4.802</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname> <given-names>BY</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Combined effects of spatial and feature-based attention on responses of V4 neurons</article-title><source>Vision Research</source><volume>49</volume><fpage>1182</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.06.011</pub-id><pub-id pub-id-type="pmid">18619996</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heekeren</surname> <given-names>HR</given-names></name><name><surname>Marrett</surname> <given-names>S</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A general mechanism for perceptual decision-making in the human brain</article-title><source>Nature</source><volume>431</volume><fpage>859</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1038/nature02966</pub-id><pub-id pub-id-type="pmid">15483614</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>G</given-names></name><name><surname>Liu</surname> <given-names>Z</given-names></name><name><surname>van der Maaten</surname> <given-names>L</given-names></name><name><surname>Weinberger</surname> <given-names>KQ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Densely connected convolutional networks</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2017.243</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neural dynamics of attentional selection in natural scenes</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>10522</fpage><lpage>10528</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1385-16.2016</pub-id><pub-id pub-id-type="pmid">27733605</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kar</surname> <given-names>K</given-names></name><name><surname>Kubilius</surname> <given-names>J</given-names></name><name><surname>Issa</surname> <given-names>E</given-names></name><name><surname>Schmidt</surname> <given-names>K</given-names></name><name><surname>DiCarlo</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title><italic>Evidence that feedback is required for object identity inferences computed by the ventral stream</italic></article-title><conf-name>Computational and Systems Neuroscience (Cosyne)</conf-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname> <given-names>S</given-names></name><name><surname>Pinsk</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual attention as a multilevel selection process</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>4</volume><fpage>483</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.3758/CABN.4.4.483</pub-id><pub-id pub-id-type="pmid">15849892</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname> <given-names>LN</given-names></name><name><surname>Yates</surname> <given-names>JL</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Huk</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociated functional significance of decision-related activity in the primate dorsal stream</article-title><source>Nature</source><volume>535</volume><fpage>285</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1038/nature18617</pub-id><pub-id pub-id-type="pmid">27376476</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Henriksson</surname> <given-names>L</given-names></name><name><surname>Kay</surname> <given-names>K</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fixed versus mixed RSA: Explaining visual representations by fixed and mixed feature sets from shallow and deep computational models</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>184</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.10.007</pub-id><pub-id pub-id-type="pmid">28298702</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kheradpisheh</surname> <given-names>SR</given-names></name><name><surname>Ghodrati</surname> <given-names>M</given-names></name><name><surname>Ganjtabesh</surname> <given-names>M</given-names></name><name><surname>Masquelier</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep networks can resemble human Feed-forward vision in invariant object recognition</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>32672</elocation-id><pub-id pub-id-type="doi">10.1038/srep32672</pub-id><pub-id pub-id-type="pmid">27601096</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koivisto</surname> <given-names>M</given-names></name><name><surname>Kahila</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Top-down preparation modulates visual categorization but not subjective awareness of objects presented in natural backgrounds</article-title><source>Vision Research</source><volume>133</volume><fpage>73</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2017.01.005</pub-id><pub-id pub-id-type="pmid">28202397</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kornblith</surname> <given-names>S</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How thoughts arise from sights: inferotemporal and prefrontal contributions to vision</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>208</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.08.016</pub-id><pub-id pub-id-type="pmid">28942219</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauzlis</surname> <given-names>RJ</given-names></name><name><surname>Lovejoy</surname> <given-names>LP</given-names></name><name><surname>Zénon</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Superior colliculus and visual spatial attention</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>165</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170249</pub-id><pub-id pub-id-type="pmid">23682659</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname> <given-names>J</given-names></name><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>HP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep neural networks as a computational model for human shape sensitivity</article-title><source>PLoS Computational Biology</source><volume>12</volume><elocation-id>e1004896</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004896</pub-id><pub-id pub-id-type="pmid">27124699</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Cownden</surname> <given-names>D</given-names></name><name><surname>Tweed</surname> <given-names>DB</given-names></name><name><surname>Akerman</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id><pub-id pub-id-type="pmid">27824044</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lindsay</surname> <given-names>GW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Feature-based attention in convolutional neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.06408">https://arxiv.org/abs/1511.06408</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lindsay</surname> <given-names>GW</given-names></name><name><surname>Rubin</surname> <given-names>DB</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The stabilized supralinear network replicates neural and performance correlates of attention</article-title><conf-name>Computational and Systems Neuroscience (Cosyne)</conf-name></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Love</surname> <given-names>BC</given-names></name><name><surname>Guest</surname> <given-names>O</given-names></name><name><surname>Slomka</surname> <given-names>P</given-names></name><name><surname>Navarro</surname> <given-names>VM</given-names></name><name><surname>Wasserman</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title><italic>Deep networks as models of human and animal categorization</italic></article-title><conf-name>CogSci 2018</conf-name></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Chelazzi</surname> <given-names>L</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Neural mechanisms of spatial selective attention in Areas V1, V2, and V4 of macaque visual cortex</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>24</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.1.24</pub-id><pub-id pub-id-type="pmid">9120566</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>TZ</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal modulations in visual cortex are associated with only one of multiple components of attention</article-title><source>Neuron</source><volume>86</volume><fpage>1182</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.007</pub-id><pub-id pub-id-type="pmid">26050038</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lupyan</surname> <given-names>G</given-names></name><name><surname>Spivey</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Making the invisible visible: verbal but not visual cues enhance visual detection</article-title><source>PLoS ONE</source><volume>5</volume><elocation-id>e11452</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0011452</pub-id><pub-id pub-id-type="pmid">20628646</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lupyan</surname> <given-names>G</given-names></name><name><surname>Ward</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Language can boost otherwise unseen objects into visual awareness</article-title><source>PNAS</source><volume>110</volume><fpage>14196</fpage><lpage>14201</lpage><pub-id pub-id-type="doi">10.1073/pnas.1303312110</pub-id><pub-id pub-id-type="pmid">23940323</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Feature-based attention increases the selectivity of population responses in primate visual cortex</article-title><source>Current Biology</source><volume>14</volume><fpage>744</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.04.028</pub-id><pub-id pub-id-type="pmid">15120065</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname> <given-names>JHR</given-names></name><name><surname>Cook</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The role of attention in visual processing</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>357</volume><fpage>1063</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1107</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mayo</surname> <given-names>JP</given-names></name><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A refined neuronal population measure of visual attention</article-title><source>PLoS One</source><volume>10</volume><elocation-id>e0136570</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0136570</pub-id><pub-id pub-id-type="pmid">26296083</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mayo</surname> <given-names>JP</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Graded neuronal modulations related to visual spatial attention</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>5353</fpage><lpage>5361</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0192-16.2016</pub-id><pub-id pub-id-type="pmid">27170131</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname> <given-names>CJ</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>431</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-01-00431.1999</pub-id><pub-id pub-id-type="pmid">9870971</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mnih</surname> <given-names>V</given-names></name><name><surname>Heess</surname> <given-names>N</given-names></name><name><surname>Graves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Recurrent Models of Visual Attention</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><fpage>2204</fpage><lpage>2212</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Crapse</surname> <given-names>T</given-names></name><name><surname>Chang</surname> <given-names>L</given-names></name><name><surname>Tsao</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The effect of face patch microstimulation on perception of faces and objects</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>743</fpage><lpage>752</lpage><pub-id pub-id-type="doi">10.1038/nn.4527</pub-id><pub-id pub-id-type="pmid">28288127</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monosov</surname> <given-names>IE</given-names></name><name><surname>Sheinberg</surname> <given-names>DL</given-names></name><name><surname>Thompson</surname> <given-names>KG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The effects of prefrontal cortex inactivation on object responses of single neurons in the inferotemporal cortex during visual search</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>15956</fpage><lpage>15961</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2995-11.2011</pub-id><pub-id pub-id-type="pmid">22049438</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname> <given-names>T</given-names></name><name><surname>Armstrong</surname> <given-names>KM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Selective gating of visual signals by microstimulation of frontal cortex</article-title><source>Nature</source><volume>421</volume><fpage>370</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1038/nature01341</pub-id><pub-id pub-id-type="pmid">12540901</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Morcos</surname> <given-names>AS</given-names></name><name><surname>Barrett</surname> <given-names>DGT</given-names></name><name><surname>Rabinowitz</surname> <given-names>NC</given-names></name><name><surname>Botvinick</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>On the importance of single directions for generalization</article-title><source>arXiv </source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.06959">https://arxiv.org/abs/1803.06959</ext-link></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moro</surname> <given-names>SI</given-names></name><name><surname>Tolboom</surname> <given-names>M</given-names></name><name><surname>Khayat</surname> <given-names>PS</given-names></name><name><surname>Roelfsema</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neuronal activity in the visual cortex reveals the temporal order of cognitive operations</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>16293</fpage><lpage>16303</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1256-10.2010</pub-id><pub-id pub-id-type="pmid">21123575</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motter</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Neural correlates of feature selective memory and pop-out in extrastriate area V4</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>2190</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-04-02190.1994</pub-id><pub-id pub-id-type="pmid">8158265</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navalpakkam</surname> <given-names>V</given-names></name><name><surname>Itti</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Search goal tunes visual features optimally</article-title><source>Neuron</source><volume>53</volume><fpage>605</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.01.018</pub-id><pub-id pub-id-type="pmid">17296560</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ni</surname> <given-names>AM</given-names></name><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Tuned normalization explains the size of attention modulations</article-title><source>Neuron</source><volume>73</volume><fpage>803</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.006</pub-id><pub-id pub-id-type="pmid">22365552</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagan</surname> <given-names>M</given-names></name><name><surname>Urban</surname> <given-names>LS</given-names></name><name><surname>Wohl</surname> <given-names>MP</given-names></name><name><surname>Rust</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Signals in inferotemporal and perirhinal cortex suggest an untangling of visual target information</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1132</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1038/nn.3433</pub-id><pub-id pub-id-type="pmid">23792943</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname> <given-names>WK</given-names></name><name><surname>Duffy</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical neuronal responses to optic flow are shaped by visual strategies for steering</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>727</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm109</pub-id><pub-id pub-id-type="pmid">17621608</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural mechanisms of rapid natural scene categorization in human visual cortex</article-title><source>Nature</source><volume>460</volume><fpage>94</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1038/nature08103</pub-id><pub-id pub-id-type="pmid">19506558</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A neural basis for real-world visual search in human occipitotemporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>12125</fpage><lpage>12130</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101042108</pub-id><pub-id pub-id-type="pmid">21730192</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purushothaman</surname> <given-names>G</given-names></name><name><surname>Bradley</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural population code for fine perceptual decisions in area MT</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1038/nn1373</pub-id><pub-id pub-id-type="pmid">15608633</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahnev</surname> <given-names>D</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prior expectation modulates the interaction between sensory and prefrontal regions in the human brain</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>10741</fpage><lpage>10748</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1478-11.2011</pub-id><pub-id pub-id-type="pmid">21775617</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rawat</surname> <given-names>W</given-names></name><name><surname>Wang</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep convolutional neural networks for image classification: a comprehensive review</article-title><source>Neural Computation</source><volume>29</volume><fpage>2352</fpage><lpage>2449</lpage><pub-id pub-id-type="doi">10.1162/neco_a_00990</pub-id><pub-id pub-id-type="pmid">28599112</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname> <given-names>ET</given-names></name><name><surname>Deco</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Attention in natural scenes: neurophysiological and computational bases</article-title><source>Neural Networks</source><volume>19</volume><fpage>1383</fpage><lpage>1394</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2006.08.007</pub-id><pub-id pub-id-type="pmid">17011749</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Born</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Feature attention for binocular disparity in primate area MT depends on tuning strength</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>1545</fpage><lpage>1555</lpage><pub-id pub-id-type="doi">10.1152/jn.00772.2014</pub-id><pub-id pub-id-type="pmid">25505115</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saenz</surname> <given-names>M</given-names></name><name><surname>Buracas</surname> <given-names>GT</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Global effects of feature-based attention in human visual cortex</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>631</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1038/nn876</pub-id><pub-id pub-id-type="pmid">12068304</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sàenz</surname> <given-names>M</given-names></name><name><surname>Buraĉas</surname> <given-names>GT</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Global feature-based attention for motion and color</article-title><source>Vision Research</source><volume>43</volume><fpage>629</fpage><lpage>637</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(02)00595-3</pub-id><pub-id pub-id-type="pmid">12604099</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salzman</surname> <given-names>CD</given-names></name><name><surname>Britten</surname> <given-names>KH</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Cortical microstimulation influences perceptual judgements of motion direction</article-title><source>Nature</source><volume>346</volume><fpage>174</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1038/346174a0</pub-id><pub-id pub-id-type="pmid">2366872</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeliger</surname> <given-names>K</given-names></name><name><surname>Fritsche</surname> <given-names>M</given-names></name><name><surname>Güçlü</surname> <given-names>U</given-names></name><name><surname>Schoenmakers</surname> <given-names>S</given-names></name><name><surname>Schoffelen</surname> <given-names>J-M</given-names></name><name><surname>Bosch</surname> <given-names>SE</given-names></name><name><surname>van Gerven</surname> <given-names>MAJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cnn-based encoding and decoding of visual object recognition in space and time</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/118091</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname> <given-names>JT</given-names></name><name><surname>Schwarzbach</surname> <given-names>J</given-names></name><name><surname>Courtney</surname> <given-names>SM</given-names></name><name><surname>Golay</surname> <given-names>X</given-names></name><name><surname>Yantis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Control of object-based attention in human cortex</article-title><source>Cerebral Cortex</source><volume>14</volume><fpage>1346</fpage><lpage>1357</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh095</pub-id><pub-id pub-id-type="pmid">15166105</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Wolf</surname> <given-names>L</given-names></name><name><surname>Bileschi</surname> <given-names>S</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Robust object recognition with cortex-like mechanisms</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>29</volume><fpage>411</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.56</pub-id><pub-id pub-id-type="pmid">17224612</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Zisserman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Very deep convolutional networks for large-scale image recognition.</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sridharan</surname> <given-names>D</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Moore</surname> <given-names>T</given-names></name><name><surname>Knudsen</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Does the superior colliculus control perceptual sensitivity or choice Bias during attention? evidence from a multialternative decision framework</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>480</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4505-14.2017</pub-id><pub-id pub-id-type="pmid">28100734</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname> <given-names>T</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Content-specific expectations enhance stimulus detectability by increasing perceptual sensitivity</article-title><source>Journal of Experimental Psychology: General</source><volume>144</volume><fpage>1089</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1037/xge0000109</pub-id><pub-id pub-id-type="pmid">26460783</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname> <given-names>T</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Object detection in natural scenes: independent effects of spatial and category-based attention</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>79</volume><fpage>738</fpage><lpage>752</lpage><pub-id pub-id-type="doi">10.3758/s13414-017-1279-8</pub-id><pub-id pub-id-type="pmid">28138945</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stollenga</surname> <given-names>MF</given-names></name><name><surname>Masci</surname> <given-names>J</given-names></name><name><surname>Gomez</surname> <given-names>F</given-names></name><name><surname>Schmidhuber</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Deep networks with internal selective attention through feedback connections</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><fpage>3545</fpage><lpage>3553</lpage></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treisman</surname> <given-names>AM</given-names></name><name><surname>Gelade</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>A feature-integration theory of attention</article-title><source>Cognitive Psychology</source><volume>12</volume><fpage>97</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(80)90005-5</pub-id><pub-id pub-id-type="pmid">7351125</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname> <given-names>S</given-names></name><name><surname>Martínez Trujillo</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title><source>Nature</source><volume>399</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/21176</pub-id><pub-id pub-id-type="pmid">10376597</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural correlates of attention in primate visual cortex</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01814-2</pub-id><pub-id pub-id-type="pmid">11311383</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tripp</surname> <given-names>BP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Similarities and differences between stimulus tuning in theinferotemporal visual cortex and convolutional networks</article-title><conf-name>Neural Networks (IJCNN), 2017 International Joint Conference</conf-name><fpage>3551</fpage><lpage>3560</lpage></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsotsos</surname> <given-names>JK</given-names></name><name><surname>Culhane</surname> <given-names>SM</given-names></name><name><surname>Kei Wai</surname> <given-names>WY</given-names></name><name><surname>Lai</surname> <given-names>Y</given-names></name><name><surname>Davis</surname> <given-names>N</given-names></name><name><surname>Nuflo</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Modeling visual attention via selective tuning</article-title><source>Artificial Intelligence</source><volume>78</volume><fpage>507</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(95)00025-9</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullman</surname> <given-names>S</given-names></name><name><surname>Assif</surname> <given-names>L</given-names></name><name><surname>Fetaya</surname> <given-names>E</given-names></name><name><surname>Harari</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Atoms of recognition in human and computer vision</article-title><source>PNAS</source><volume>113</volume><fpage>2744</fpage><lpage>2749</lpage><pub-id pub-id-type="doi">10.1073/pnas.1513198113</pub-id><pub-id pub-id-type="pmid">26884200</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Galkin</surname> <given-names>TW</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name><name><surname>Gattass</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical connections of area V4 in the macaque</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>477</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm061</pub-id><pub-id pub-id-type="pmid">17548798</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verghese</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Visual search and attention: a signal detection theory approach</article-title><source>Neuron</source><volume>31</volume><fpage>523</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00392-0</pub-id><pub-id pub-id-type="pmid">11545712</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whiteley</surname> <given-names>L</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Attention in a bayesian framework</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>100</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00100</pub-id><pub-id pub-id-type="pmid">22712010</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Guided search 2.0 A revised model of visual search</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>1</volume><fpage>202</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.3758/BF03200774</pub-id><pub-id pub-id-type="pmid">24203471</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>K</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name><name><surname>Kiros</surname> <given-names>R</given-names></name><name><surname>Cho</surname> <given-names>K</given-names></name><name><surname>Courville</surname> <given-names>A</given-names></name><name><surname>Salakhudinov</surname> <given-names>R</given-names></name><name><surname>Zemel</surname> <given-names>R</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Show, attend and tell: neural image caption generation with visual attention</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>2048</fpage><lpage>2057</lpage></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Seibert</surname> <given-names>D</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaidel</surname> <given-names>A</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoupled choice-driven and stimulus-related activity in parietal neurons may be misrepresented by choice probabilities</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00766-3</pub-id><pub-id pub-id-type="pmid">28959018</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Feature-based attention modulates feedforward visual processing</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>24</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1038/nn.2223</pub-id><pub-id pub-id-type="pmid">19029890</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Bichot</surname> <given-names>NP</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Poggio</surname> <given-names>TA</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Object decoding with attention in inferior temporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>8850</fpage><lpage>8855</lpage><pub-id pub-id-type="doi">10.1073/pnas.1100999108</pub-id><pub-id pub-id-type="pmid">21555594</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>H</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Feature-based attention in the frontal eye field and area V4 during visual search</article-title><source>Neuron</source><volume>70</volume><fpage>1205</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.04.032</pub-id><pub-id pub-id-type="pmid">21689605</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.38105.029</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>van Gerven</surname><given-names>Marcel</given-names></name><role>Reviewing Editor</role><aff><institution>Radboud Universiteit</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewer</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;How biological attention mechanisms improve task performance in a large-scale visual system model&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Marcel van Gerven as the Reviewing Editor, and the evaluation has been overseen by Sabine Kastner as the Senior Editor. The following individual involved in review of your submission has agreed to reveal his identity: Marius Peelen (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors present an approach to examining the feature similarity model of attention by incorporating attentional modulation in a convolutional neural network for object categorization. They find that task performance enhancements with attention can roughly approximate those found experimentally, but most interestingly, only when attention is applied to the later layers of the network. The authors demonstrate a dissociation between layers in the strength of tuning and the performance enhancement achieved by applying attention. This represents an interesting contribution to theories of attention to the extent that one considers CNNs a useful model for biological vision. Another key contribution is the distinction between gradient-based and tuning-based feedback. The findings obtained here in neural networks make several new predictions for biological experiments and raise fundamental questions about how tuning affects behavior.</p><p>Essential revisions:</p><p>Introduction, second paragraph: CNNs are introduced as great models of the ventral stream, but an increasing number of studies shows that the features used in these CNNs to classify objects are very different from those used by humans (e.g., Azulay and Weiss, 2018; Baker, Lu, Erlikhman, and Kelleman VSS 2018; Ullman et al., 2016). Some caution may be warranted.</p><p>In Figure 3 performance increase across layers is shown. This plot is created by using the best performing weighting parameter <italic>β</italic>. To dissociate the effect of <italic>β</italic> and of the attentional modulation f, please add a control condition in which f is set to one. The reasoning is that varying <italic>β</italic> alone and picking the best <italic>β</italic> may already induce performance changes.</p><p>In Figure 3 the results for modulating all layers are shown. It is felt that the conclusions drawn from these results are unsupported. Picking <italic>β</italic> at 1/10 of the optimal <italic>β</italic> for each layer does not constitute an optimal setting for modulating all layers. Also, modulations in early layers may negatively impact activity changes in later layers. Hence, the authors cannot exclude the possibility that modulation of all layers simultaneously could actually help. Results and interpretations of attention applied to all layers should therefore be removed from the paper.</p><p>In the subsection “Attention Strength and the Tradeoff between Increasing True and False Positives”, you compare the change in the magnitude of neural activation in the CNN to the changes in primate brains. It was not clear how to interpret these results. Can these magnitudes be meaningfully compared? What can we conclude from this?</p><p>In the subsection “Feature-based Attention Primarily Influences Criteria and Spatial Attention Primarily Influences Sensitivity”, it is argued that FBA works through a criterion shift rather than by increasing sensitivity, with FBA shifting the representation of all stimuli in the direction of the attended category. But earlier you show that FBA selectively increases TP (relative to FP), which suggests an increase in sensitivity. (Also, Figure 4E appears to show a positive effect of FBA (L13) on sensitivity). Please clarify.</p><p>For many of the analyses results of both types of feedback are shown. However, for some comparisons only tuning-based results are shown (e.g., see the aforementioned subsection). Why? Please ensure consistency throughout.</p><p>It is unclear why a new method for quantifying attention is introduced in Figure 7, or how the &quot;FSGM-like&quot; measure is related to feature matching and the activity ratios already discussed. Please motivate or restrict to feature matching and activity ratios. In general, the paper is a dense read due to the various analysis and metrics. Any steps towards simplification of the presentation will aid the reader.</p><p>The claim that the new measure of attention (Figure 7A), or the alternative measures of attention for that matter, is experimentally testable seems unsupported. In particular, getting with and without attention activity in response to images that are not classified as the target orientation is not possible to measure in experiments with humans or animals. Subjects are stochastic in their judgments. One could however, measure with and without attention responses to ambiguous stimuli that elicit near chance performance. This metric would then become very similar to a population version of the well-studied &quot;choice probability&quot; metric. This connection should at least be discussed.</p><p>It would be more useful to the experimental community to recast the orientation task and analysis more in terms of what would be measured empirically. For instance presenting the task in terms of correctly identified target orientation as a function of the presented orientations rotation from the target. This may be outside the scope of current manuscript though.</p><p>What would be the biological mechanism that can account for tuning-based and gradient-based feedback? Especially the gradient based approach seems to be hard to defend from a biological point of view. How would putative decision-related areas have access to this gradient information? Some words should be spent on this in the Discussion section.</p><p>Please mention relevant related work:</p><p>- Katz et al., 2016, related to the relationship between tuning and influence on decisions.</p><p>- Abdelhack and Kamitani, 2018, related to subsection “Recordings Show How Feature Similarity Gain Effects Propagate” (and Figure 7) showing that the activity in response to misclassified stimuli shifts towards the activity in response to correctly classified stimuli when attention is turned on.</p><p>- Stein and Peelen, 2015, related to the subsection “Feature-based Attention Primarily Influences Criteria and Spatial Attention Primarily Influences Sensitivity”, arguing that FBA in human experiments does not lead to an increase in sensitivity (see also work by Carrasco on effects of FBA on discrimination tasks).</p><p>- Discussion, fifth paragraph: Ni, Ray, and Maunsell, 2012, would appear to be very relevant to this Discussion section. Those authors found that strength of normalization was as strong a factor as tuning in the strength of attentional effects.</p><p>- The neural network community developed various models that implement some form of attention. See the Attention section in Hassabis et al., Neuron, 2017. The present work should be contrasted with the papers mentioned there.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.38105.030</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Introduction, second paragraph: CNNs are introduced as great models of the ventral stream, but an increasing number of studies shows that the features used in these CNNs to classify objects are very different from those used by humans (e.g., Azulay and Weiss, 2018; Baker, Lu, Erlikhman, and Kelleman VSS 2018; Ullman et al., 2016). Some caution may be warranted.</p></disp-quote><p>​The language and references have been updated to reflect this (Introduction, second paragraph).</p><disp-quote content-type="editor-comment"><p>In Figure 3 performance increase across layers is shown. This plot is created by using the best performing weighting parameter β. To dissociate the effect of β and of the attentional modulation f, please add a control condition in which f is set to one. The reasoning is that varying β alone and picking the best β may already induce performance changes.</p></disp-quote><p>This control has been added as Figure 3—figure supplement 1C, subsection “Feature-based Attention Improves Performance on Challenging Object Classification Tasks”, fourth paragraph.</p><disp-quote content-type="editor-comment"><p>In Figure 3 the results for modulating all layers are shown. It is felt that the conclusions drawn from these results are unsupported. Picking β at 1/10 of the optimal β for each layer does not constitute an optimal setting for modulating all layers. Also, modulations in early layers may negatively impact activity changes in later layers. Hence, the authors cannot exclude the possibility that modulation of all layers simultaneously could actually help. Results and interpretations of attention applied to all layers should therefore be removed from the paper.</p></disp-quote><p>We shifted the all-layer results to supplementary figures. Although it is true that we have not exhaustively explored the possibilities with modulation at all layers, we believe that the particular case we did study is of sufficient interest to readers to be included in the supplement. What we have shown is that modulating all layers, with 1/10 the strength of a single layer, improves performance about as much as modulation of the best single layer. Figure 3—figure supplements 1A and B, subsection “Feature-based Attention Improves Performance on Challenging Object Classification Tasks”, fourth paragraph.</p><disp-quote content-type="editor-comment"><p>In the subsection “Attention Strength and the Tradeoff between Increasing True and False Positives”, you compare the change in the magnitude of neural activation in the CNN to the changes in primate brains. It was not clear how to interpret these results. Can these magnitudes be meaningfully compared? What can we conclude from this?</p></disp-quote><p>We feel that the main take-away from that calculation is that the neural changes needed to cause observed performance chances are neither absurdly large nor absurdly small when compared to the what is observed experimentally in neurons. Therefore, we believe that as an order-of-magnitude comparison it provides some insight and reassurance that the model may be working similarly to the biology. We have shortened this section to make it clearer that this is the intended interpretation (subsection “Attention Strength and the Tradeoff between Increasing True and False Positives”, eighth paragraph).</p><disp-quote content-type="editor-comment"><p>In the subsection “Feature-based Attention Primarily Influences Criteria and Spatial Attention Primarily Influences Sensitivity”, it is argued that FBA works through a criterion shift rather than by increasing sensitivity, with FBA shifting the representation of all stimuli in the direction of the attended category. But earlier you show that FBA selectively increases TP (relative to FP), which suggests an increase in sensitivity. (Also, Figure 4E appears to show a positive effect of FBA (L13) on sensitivity). Please clarify.</p></disp-quote><p>We’d like to make two points of clarification to the reviewers on this regard:</p><p>First, the claim we make about feature-based attention (FBA) vs. spatial attention is that FBA has a larger impact on criteria than sensitivity and vice versa for spatial attention. Thus, we do not claim that there are no sensitivity changes coming from FBA, only that the criteria changes are more dominant.</p><p>Second, the fact that FBA first (with increasing modulation strength) increases TP (true positives) much more than FP (false positives), and only later (for much stronger modulation strength) increases FP more than TP, need not suggest an increase in sensitivity (where increasing sensitivity means increasing the separation between the distribution of TP and FP). This outcome could result solely from a criteria change (a decrease in threshold), as shown by the example in <xref ref-type="fig" rid="respfig1">Author response image 1</xref>.</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.38105.025</object-id><label>Author response image 1.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-38105-resp-fig1-v2"/></fig><p>Here, the shaded area representing true positives will increase as the threshold moves leftward, before the false positives increase.</p><p>The text reflects these points (subsection “Feature-based Attention Primarily Influences Criteria and Spatial Attention Primarily Influences Sensitivity”, second paragraph).</p><disp-quote content-type="editor-comment"><p>For many of the analyses results of both types of feedback are shown. However, for some comparisons only tuning-based results are shown (e.g., see the aforementioned subsection). Why? Please ensure consistency throughout.</p></disp-quote><p>The gradient-based results were omitted in Figure 5E only due to the already crowded nature of that graph. Gradient results have now been added to that figure.</p><disp-quote content-type="editor-comment"><p>It is unclear why a new method for quantifying attention is introduced in Figure 7, or how the &quot;FSGM-like&quot; measure is related to feature matching and the activity ratios already discussed. Please motivate or restrict to feature matching and activity ratios. In general, the paper is a dense read due to the various analysis and metrics. Any steps towards simplification of the presentation will aid the reader.</p></disp-quote><p>The feature-similarity gain model (FSGM) measure in Figure 6 was taken to match a measure used in previous experimental work. It requires the presentation of multiple orientations in order to calculate the impact of attention on a given cell. That is, it measures how the effects of attention change across stimuli for a single cell. In Figure 7, we wanted to test for FSGM-like activity changes for a single stimulus, because this allows us to best study the correlation of FSGM-like activity changes with performance changes (because performance varies from stimulus to stimulus). Therefore we had to measure how the effects of attention change across cells, i.e. across the population as a whole, for responses to a single orientation (or even a single image). That is why we introduce a new “FSGM-like” measure in Figure 7. Both measures test whether attention multiplicatively increases (decreases) the activity of cells that are tuned to prefer (anti-prefer) the attended stimulus. The difference is simply whether the attention vs. no-attention activity ratios used to fit the line are taken from the same cell (which for our model means the same feature map) in response to different orientations (Figure 6B), or from all the feature maps in response to the same orientation (Figure 7). Our new population measure, like the single-cell measure which was used previously in experiments, can be easily implemented by experimentalists. We agree with the reviewers that we did not make our reasons for using a new measure, and the precise difference between the two measures, clear, and we have now made these points clear in our revision.</p><p>Having a measure of FSGM-like activity that could be calculated per-orientation allowed us to make a more fine-grained analysis of the correlation between FSGM-like activity changes and performance changes (because we did not have to average performance over all orientations). We show these correlations in Figure 7C. In that figure we also introduced a second measure of attention-induced activity changes (vector angle). This measure was introduced to see if a in hopes of finding a different measure of activity changes that better correlates with performance changes than the FSGM measure. The vector angle measure was meant to be an experimentally tractable measure that better reflects gradients rather than FSGM-like tuning curves, following our findings that the gradients should be most potent in affecting performance. We show that, at early layers, the vector angle measure of attention-induced activity changes does indeed better correlate with performance changes than the FSGM-like measure.</p><p>However, we agree with the reviewers that Figure 7 and the associated new analyses are complex and could be confusing. Yet we believe the proposal of different measures of activity changes and the exploration of their correlation with performance changes are valuable contributions from this kind of modeling work. Furthermore the results of Figure 7 were modest rather than striking. We therefore referenced the findings of these analyses in the main text and moved Figure 7, and a detailed explanation of the different measures and their motivations, to the supplementary materials.</p><p>In its place, we have included, as Figure 7, a figure illustrating the point we think will be of most interest to experimentalists at this point in our paper, namely a figure illustrating an experiment to clearly distinguish whether attention is applied according to gradients rather than (as always assumed up to this point) according to tuning. We previously only addressed this experiment in the Discussion, but we believe it is best to make it a prominent and illustrated part of the Results.</p><p>This new figure is now Figure 7, and the old Figure 7 has been moved to Figure 6—figure supplement 2. The analysis techniques in Figure 6—figure supplement 2 are referenced in the sixth paragraph of the subsection “Recordings Show How Feature Similarity Gain Effects Propagate”, and further explanation has been added in the Materials and methods subsection “Assessment of Feature Similarity Gain Model and Feature Matching Behavior” as well as to the caption of Figure 6—figure supplement 2. The experiment (Figure 7) is described in the last paragraph of the subsection “Recordings Show How Feature Similarity Gain Effects Propagate”.</p><disp-quote content-type="editor-comment"><p>The claim that the new measure of attention (Figure 7A), or the alternative measures of attention for that matter, is experimentally testable seems unsupported. In particular, getting with and without attention activity in response to images that are not classified as the target orientation is not possible to measure in experiments with humans or animals. Subjects are stochastic in their judgments. One could however, measure with and without attention responses to ambiguous stimuli that elicit near chance performance. This metric would then become very similar to a population version of the well-studied &quot;choice probability&quot; metric. This connection should at least be discussed.</p></disp-quote><p>The alternative measures (we take this to mean the FSGM measures of Figure 7 vs. Figure 6) have already, in the Figure 6 version, been used experimentally, and we see no reason why the population-based version of Figure 7 should not also be usable. We understand the rest of this comment to be referring to the “new measure”, namely the vector-angle method of assaying attention-induced activity changes. We agree with the reviewers that behavioral responses will be stochastic, whereas those in our model are deterministic, but disagree that this renders the vector-angle method unusable. As the reviewers suggest in referring to a threshold task, so long as the method is applied to images in which, without attention, there is a significant percentage of mis-categorizations, and attention improves performance, the measure will be usable: the method only requires that population responses be measured (1) without attention for mis-categorization (2) without attention for correct categorization and (3) with attention for both cases. We believe introducing this measure to experimentalists, which should better test the ability of attention to modulate gradients rather than tuning, will be a valuable contribution, although again, we have largely put this in the supplement.</p><p>Furthermore, it is not necessary for the blue and red vectors in Figure 7A to be calculated only using images that were negatively-classified without attention. The same result would be expected if all images were used. To be specific, a collection of images, some of which contained the desired orientation and some that did not, could be shown to a subject (who is not deploying attention to the orientation). The positive-classification vector (gray vector in 7A) would be made from the responses when an image was positively-classified (regardless of whether the desired orientation was present). The blue vector would be made from the responses to all images that contained the desired orientation (regardless of how they were classified), and the red vector would be from the responses to images that contained the desired orientation (regardless of how they were classified) but when attention was applied to that orientation. In this setting we would still expect the red vector (attention present) to be closer to positive classification vector than the blue (attention absent).</p><p>The results of calculating these vectors this way are shown in the green lines in <xref ref-type="fig" rid="respfig2">Author response image 2</xref> (with original analyses for comparison), and give results virtually identical to those of our previously-used definition of vector angle (blue lines).</p><fig id="respfig2"><object-id pub-id-type="doi">10.7554/eLife.38105.026</object-id><label>Author response image 2.</label><caption/><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-38105-resp-fig2-v2"/></fig><p>Figure 6—figure supplement 2 now uses the new definition of the Vector Angle measure (that which uses all images, green line in <xref ref-type="fig" rid="respfig2">Author response image 2</xref>). The last paragraph of the subsection “Assessment of Feature Similarity Gain Model and Feature Matching Behavior” has been changed to reflect that.</p><disp-quote content-type="editor-comment"><p>It would be more useful to the experimental community to recast the orientation task and analysis more in terms of what would be measured empirically. For instance presenting the task in terms of correctly identified target orientation as a function of the presented orientations rotation from the target. This may be outside the scope of current manuscript though.</p></disp-quote><p>We agree that relating the performance trends in these models more to behavioral data from various experimentally-used tasks would be interesting, but indeed outside the scope of the current manuscript.</p><disp-quote content-type="editor-comment"><p>What would be the biological mechanism that can account for tuning-based and gradient-based feedback? Especially the gradient based approach seems to be hard to defend from a biological point of view. How would putative decision-related areas have access to this gradient information? Some words should be spent on this in the Discussion section.</p></disp-quote><p>We believe there are means by which the gradient-based feedback could be learned biologically, through correlation-based or spike-timing-based learning rules in which, if lower-level activity is followed by high-level activity, the feedback from those higher-level units to the corresponding lower-level units is strengthened. We have fleshed this out in the Discussion (fifth paragraph).</p><disp-quote content-type="editor-comment"><p>Please mention relevant related work:</p><p>- Katz et al., 2016, related to the relationship between tuning and influence on decisions.</p><p>- Abdelhack and Kamitani, 2018, related to subsection “Recordings Show How Feature Similarity Gain Effects Propagate” (and Figure 7) showing that the activity in response to misclassified stimuli shifts towards the activity in response to correctly classified stimuli when attention is turned on.</p><p>- Stein and Peelen, 2015, related to the subsection “Feature-based Attention Primarily Influences Criteria and Spatial Attention Primarily Influences Sensitivity”, arguing that FBA in human experiments does not lead to an increase in sensitivity (see also work by Carrasco on effects of FBA on discrimination tasks).</p><p>- Discussion, fifth paragraph: Ni, Ray, and Maunsell, 2012, would appear to be very relevant to this Discussion section. Those authors found that strength of normalization was as strong a factor as tuning in the strength of attentional effects.</p><p>- The neural network community developed various models that implement some form of attention. See the Attention section in Hassabis et al., Neuron, 2017. The present work should be contrasted with the papers mentioned there.</p></disp-quote><p>These citations have been included in the Discussion.</p></body></sub-article></article>