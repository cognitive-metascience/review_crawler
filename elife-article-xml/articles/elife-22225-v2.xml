<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">22225</article-id><article-id pub-id-type="doi">10.7554/eLife.22225</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Fundamental bound on the persistence and capacity of short-term memory stored as graded persistent activity</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-96249"><name><surname>Koyluoglu</surname><given-names>Onur Ozan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8512-4755</contrib-id><email>ozan.koyluoglu@berkeley.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-71804"><name><surname>Pertzov</surname><given-names>Yoni</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-71805"><name><surname>Manohar</surname><given-names>Sanjay</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0735-4349</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-22967"><name><surname>Husain</surname><given-names>Masud</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-4557"><name><surname>Fiete</surname><given-names>Ila R</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Electrical Engineering and Computer Science</institution><institution>University of California, Berkeley</institution><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology</institution><institution>Hebrew University</institution><addr-line><named-content content-type="city">Jerusalem</named-content></addr-line><country>Israel</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Experimental Psychology</institution><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Center for Learning and Memory</institution><institution>University of Texas at Austin</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-19176"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role>Reviewing Editor</role><aff><institution>New York University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="update" publication-format="electronic"><month>09</month><day>20</day><year>2017</year></pub-date><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>09</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e22225</elocation-id><history><date date-type="received" iso-8601-date="2016-10-08"><day>08</day><month>10</month><year>2016</year></date><date date-type="accepted" iso-8601-date="2017-08-25"><day>25</day><month>08</month><year>2017</year></date></history><permissions><copyright-statement>Â© 2017, Koyluoglu et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Koyluoglu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-22225-v2.pdf"/><related-object ext-link-type="url" xlink:href="https://elifesciences.org/articles/e22225v1"><date date-type="v1" iso-8601-date="2017-09-07"><day>07</day><month>09</month><year>2017</year></date></related-object><related-object ext-link-type="url" xlink:href="https://elifesciences.org/articles/e22225v2"><date date-type="v2" iso-8601-date="2017-09-20"><day>20</day><month>09</month><year>2017</year></date></related-object><abstract><object-id pub-id-type="doi">10.7554/eLife.22225.001</object-id><p>It is widely believed that persistent neural activity underlies short-term memory. Yet, as we show, the degradation of information stored directly in such networks behaves differently from human short-term memory performance. We build a more general framework where memory is viewed as a problem of passing information through noisy channels whose degradation characteristics resemble those of persistent activity networks. If the brain first encoded the information appropriately before passing the information into such networks, the information can be stored substantially more faithfully. Within this framework, we derive a fundamental lower-bound on recall precision, which declines with storage duration and number of stored items. We show that human performance, though inconsistent with models involving direct (uncoded) storage in persistent activity networks, can be well-fit by the theoretical bound. This finding is consistent with the view that if the brain stores information in patterns of persistent activity, it might use codes that minimize the effects of noise, motivating the search for such codes in the brain.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>short term memory</kwd><kwd>information theory</kwd><kwd>forgetting</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IIS-1464349</award-id><principal-award-recipient><name><surname>Koyluoglu</surname><given-names>Onur Ozan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003977</institution-id><institution>Israel Science Foundation</institution></institution-wrap></funding-source><award-id>1747/14</award-id><principal-award-recipient><name><surname>Pertzov</surname><given-names>Yoni</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>MRC Clinician Scientist Fellowship</institution></institution-wrap></funding-source><award-id>MR/P00878X</award-id><principal-award-recipient><name><surname>Manohar</surname><given-names>Sanjay</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000272</institution-id><institution>National Institute for Health Research</institution></institution-wrap></funding-source><award-id>Oxford Biomedical Centre</award-id><principal-award-recipient><name><surname>Husain</surname><given-names>Masud</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Husain</surname><given-names>Masud</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IIS-1148973</award-id><principal-award-recipient><name><surname>Fiete</surname><given-names>Ila R</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Fiete</surname><given-names>Ila R</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><award-id>Faculty Scholar Award</award-id><principal-award-recipient><name><surname>Fiete</surname><given-names>Ila R</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A fundamental lower-bound on memory recall precision, which declines with storage duration and number of stored items, is derived, and human performance is shown to be well-fit by this theoretical bound.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Short-term memory, which refers to the brainâs temporary buffer of readily usable information, is considered to be a critical component of general intelligence (<xref ref-type="bibr" rid="bib23">Conway et al., 2003</xref>). Despite considerable interest in understanding the neural mechanisms that limit short-term memory, the issue remains relatively unsettled. Human working memory is a complex phenomenon, involving not just short-term memory but executive selection and processing, operating on multiple timescales and across multiple brain areas (<xref ref-type="bibr" rid="bib38">Jonides et al., 2008</xref>). In this study, we restrict ourselves to obtaining limits on short-term memory performance purely due to noise in persistent activity networks, if analog information is stored directly into these networks, or if it is first well-encoded to make the stored states robust to ongoing noise.</p><p>Short-term memory experiments quantify the precision of memory recall. Typically in such experiments, subjects are briefly presented with sensory inputs, which are then removed. After a delay the subjects are asked to estimate from memory some feature of the input. Consistent with everyday experience, memory <italic>capacity</italic> is severely limited, restricted to just a handful of items (<xref ref-type="bibr" rid="bib52">Miller, 1956</xref>), and recall performance is worse when there are more items to be remembered. <italic>Persistence</italic> can also be limited, though forgetting over time is a less severe constraint than capacity: several experiments show that recall performance declines with delay (<xref ref-type="bibr" rid="bib42">Luck and Vogel, 1997</xref>; <xref ref-type="bibr" rid="bib38">Jonides et al., 2008</xref>; <xref ref-type="bibr" rid="bib8">Barrouillet et al., 2009</xref>; <xref ref-type="bibr" rid="bib9">Barrouillet et al., 2011</xref>; <xref ref-type="bibr" rid="bib7">Barrouillet et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Pertzov et al., 2013</xref>; <xref ref-type="bibr" rid="bib82">Wilken and Ma, 2004</xref>; <xref ref-type="bibr" rid="bib10">Bays et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Pertzov et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Anderson et al., 2011</xref>), at least when many items are stored in memory.</p><p>Efforts in experimental and theoretical psychology to understand the nature of these memory constraints (<xref ref-type="bibr" rid="bib4">Atkinson and Shiffrin, 1968</xref>) have led to quantification of human memory performance, and to phenomenological models that can fit limitations in capacity (<xref ref-type="bibr" rid="bib87">Zhang and Luck, 2008</xref>; <xref ref-type="bibr" rid="bib11">Bays and Husain, 2008</xref>; <xref ref-type="bibr" rid="bib77">van den Berg et al., 2012</xref>) or in persistence (<xref ref-type="bibr" rid="bib82">Wilken and Ma, 2004</xref>; <xref ref-type="bibr" rid="bib7">Barrouillet et al., 2012</xref>). They have also led to controversy: about whether memory consists of discrete âslotsâ for a limited maximum number of items (<xref ref-type="bibr" rid="bib52">Miller, 1956</xref>; <xref ref-type="bibr" rid="bib26">Cowan, 2001</xref>; <xref ref-type="bibr" rid="bib87">Zhang and Luck, 2008</xref>) or is more continuously allocable across a larger, variable number of items (<xref ref-type="bibr" rid="bib77">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib11">Bays and Husain, 2008</xref>); about whether forgetting in short-term memory can be attributed in part to some inherent temporal decay of an activity or memory variable over time (<xref ref-type="bibr" rid="bib7">Barrouillet et al., 2012</xref>; <xref ref-type="bibr" rid="bib20">Campoy, 2012</xref>; <xref ref-type="bibr" rid="bib60">Ricker and Cowan, 2014</xref>; <xref ref-type="bibr" rid="bib88">Zhang and Luck, 2009</xref>) or is, as more widely supported, primarily due to interference across stored items (<xref ref-type="bibr" rid="bib41">Lewandowsky et al., 2009</xref>).</p><p>These controversies have been difficult to resolve in part because different experimental paradigms lend support to different models, while in some cases the resolution of memory performance data is not high enough to adjuciate between models. In addition, psychological models of memory performance make little contact with its neural underpinnings; thus, it is difficult to mediate between them on the basis of mechanism or electrophysiological studies.</p><p>On the mechanistic side, persistent neural activity has been widely hypothesized to form the substrate for short-term memory. The hypothesis is based on a corpus of electrophysiological work establishing a link between short-term memory and persistent neural activity (<xref ref-type="bibr" rid="bib30">Funahashi, 2006</xref>; <xref ref-type="bibr" rid="bib67">Smith and Jonides, 1998</xref>; <xref ref-type="bibr" rid="bib83">Wimmer et al., 2014</xref>). Neural network models of analog persistent activity predict a degradation of information over time (<xref ref-type="bibr" rid="bib22">Compte et al., 2000</xref>; <xref ref-type="bibr" rid="bib17">Brody et al., 2003</xref>; <xref ref-type="bibr" rid="bib15">Boucheny et al., 2005</xref>; <xref ref-type="bibr" rid="bib18">Burak and Fiete, 2009</xref>; <xref ref-type="bibr" rid="bib31">Fung et al., 2010</xref>; <xref ref-type="bibr" rid="bib53">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib19">Burak and Fiete, 2012</xref>; <xref ref-type="bibr" rid="bib81">Wei et al., 2012</xref>), because of noise in synaptic and neural activation. If individual analog features are assumed to be directly stored as variables in such persistent activity networks, the time course of degradation of persistent activity should directly predict the time course of degradation in short-term memory performance. However, these models do not typically consider the direct storage of multiple variables (but see (<xref ref-type="bibr" rid="bib81">Wei et al., 2012</xref>) ), and in general their predictions have not been directly compared against human psychophysics experiments in which the memory load and delay period are varied.</p><p>In the present work, we make the following contributions: (1) Generate psychophysics predictions for information degradation as a function of delay period and number of stored items, if information is stored directly, without recoding, in persistent activity neural networks of a fixed total size; (2) Generate psychophysics predictions (though the use of joint source-channel coding theory) for a model that assumes information is restructured by encoding and decoding stages before and after storage in persistent activity neural networks; (3) Compare these models to new analog measurements (<xref ref-type="bibr" rid="bib56">Pertzov et al., 2017</xref>) of human memory performance on an analog task as the demands on both maintenance duration and capacity are varied.</p><p>We show that the direct storage predictions are at odds with human memory performance. We propose that noisy storage systems, such as persistent activity networks, may be viewed as noisy channels through which information is passed, to be accessed at another time. We use the theory of <italic>channel coding</italic> and <italic>joint source-channel coding</italic> to derive the information-theoretic upper-bound on the achievable accuracy of short-term memory as a function of time and number of items to be remembered, assuming a core of graded persistent activity networks. According to the channel coding view, the brain might strategically restructure information before storing it, to use the available neurons in a way that minimizes the impact of noise upon the ability to retrieve that information later. We apply our framework, which requires the assumption of additional encoding and decoding stages in the memory process, to psychophysical data obtained using the technique of delayed estimation (<xref ref-type="bibr" rid="bib44">Ma et al., 2014</xref>), which provides a sensitive measure of short-term memory recall using a continuous, analog response space, rather than discrete (Yes/No) binary recall responses.</p><p>We show that empirical results are in substantially better agreement with the functional form of the theoretical bound than with predictions from a model of direct storage of information in persistent activity networks.</p><p>Our treatment of the memory problem is distinct from other recent approaches rooted in information theory (<xref ref-type="bibr" rid="bib16">Brady et al., 2009</xref>; <xref ref-type="bibr" rid="bib66">Sims et al., 2012</xref>), which consider only <italic>source coding</italic> â they assume that internal representations have a limited number of states, then compute the minimal distortion achievable in representing an analog variable with these limited states, after redundancy reduction and other compression. All representations are noise-free. By contrast, our central focus is precisely on noise and its effects on memory degradation <italic>over time</italic>, because theÂ stored states areÂ assumed to diffuse or random-walk across the set of possible stored states. The emphasis on representation with noise involves <italic>channel coding</italic> as the central element of our analysis.</p><p>Our present work is also complementary to efforts to understand short-term memory as rooted in variables other than persistent activity, for instance the possibility that short-term synaptic plasticity, through facilitation (<xref ref-type="bibr" rid="bib53">Mongillo et al., 2008</xref>; <xref ref-type="bibr" rid="bib6">Barak and Tsodyks, 2014</xref>; <xref ref-type="bibr" rid="bib50">Mi et al., 2017</xref>), might âsilentlyâ (<xref ref-type="bibr" rid="bib73">Stokes, 2015</xref>) store short-term memory, which is reactivated and accessed through intermittent neural activity (<xref ref-type="bibr" rid="bib43">Lundqvist et al., 2016</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Analog measurement of human short-term memory</title><p>We consider data from subjects performing a delayed estimation task (<xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1âsource data 1</xref>). We briefly summarize the paradigm and the main findings; a more detailed description can be found in <xref ref-type="bibr" rid="bib56">Pertzov et al. (2017)</xref> Subjects view a display with several (<inline-formula><mml:math id="inf1"><mml:mi>K</mml:mi></mml:math></inline-formula>) differently colored and oriented bars that are subsequently removed for the storage (delay) period. Following the storage period, subjects were cued by one of the colored bars in the display, now randomly oriented, and asked to rotate it to its remembered orientation. Bar orientations in the display were drawn randomly from the uniform distribution over all angles (thus the range of orientations lies in the circular interval <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the report of the subject was recorded as an analog value, to allow for more detailed and quantitative comparisons with theory (<xref ref-type="bibr" rid="bib77">van den Berg et al., 2012</xref>). Importantly, both the number of items (<inline-formula><mml:math id="inf3"><mml:mi>K</mml:mi></mml:math></inline-formula>) and the storage duration (<inline-formula><mml:math id="inf4"><mml:mi>T</mml:mi></mml:math></inline-formula>) were varied.</p><p>When only a single item had to be remembered, the length of the storage interval had no statistically significant influence on the distribution of responses over the intervals considered (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, with different delays marked by different shades and line styles; errors <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi/><mml:mo>&lt;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> degrees, effect of delay: <inline-formula><mml:math id="inf6"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>36</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; errors between <inline-formula><mml:math id="inf7"><mml:mrow><mml:mn>30</mml:mn><mml:mo>-</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> degrees: <inline-formula><mml:math id="inf8"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>36</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). By contrast, response accuracy degraded significantly with delay duration when there were 6 items in the stimulus (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; true orientation subtracted from all responses to provide a common center at 0 degrees). The number of very precise responses decreased (errors <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi/><mml:mo>&lt;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> degrees, effect of delay: <inline-formula><mml:math id="inf10"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>36</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>6.15</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), with a corresponding increase in the number of trials with large errors (e.g. errors between <inline-formula><mml:math id="inf11"><mml:mrow><mml:mn>30</mml:mn><mml:mo>-</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> degrees, effect of delay: <inline-formula><mml:math id="inf12"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>36</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>5.4</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.004</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.22225.002</object-id><label>Figure 1.</label><caption><title>Human performance on an analog delayed orientation matching task with variable item number and storage duration.</title><p>(<bold>A</bold>) Setup of a delayed orientation estimation task to probe human short-term memory. A variable number of bars with different colors and uniformly randomly drawn orientations are presented for 500 msec. Following a variable delay, the subjects are asked to adjust the orientation of a cue bar, by using a dial, to match the remembered orientation of the bar of the same color from the presentation. (<bold>B</bold>) Distribution of responses for one item, plotted so the target orientation is centered at zero. Different shades and line styles represent different delays. Note that responses did not vary significantly with storage duration. (<bold>C</bold>) Distribution of responses for six items varies with storage duration. (<bold>D</bold>) Mean squared error of recall on the task of <xref ref-type="fig" rid="fig1">Figure 1A</xref> (averaged across subjects and trials, and normalized by <inline-formula><mml:math id="inf13"><mml:msup><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">(</mml:mo><mml:msup><mml:mn mathsize="125%" mathvariant="normal">180</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">â</mml:mo></mml:msup><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">)</mml:mo></mml:mrow><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msup></mml:math></inline-formula>, the square of the range of the stored variable), as item number and delay duration are systematically varied. Error bars denote SEM across participants.</p><p><supplementary-material id="fig1sdata1"><object-id pub-id-type="doi">10.7554/eLife.22225.004</object-id><label>Figure 1âsource data 1.</label><caption><title>Experiment data used in the manuscript.</title><p>Subjects view a display with several (<inline-formula><mml:math id="inf14"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula>) differently colored and oriented bars that are subsequently removed for the storage (delay) period. Following the storage period, subjects were cued by one of the colored bars in the display, now randomly oriented, and asked to rotate it to its remembered orientation. Bar orientations in the display were drawn randomly from the uniform distribution over all angles (thus the range of orientations lies in the circular interval <inline-formula><mml:math id="inf15"><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">[</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathsize="125%">Ï</mml:mi><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">]</mml:mo></mml:mrow></mml:math></inline-formula>) and the report of the subject was recorded as an analog value. (See also [<xref ref-type="bibr" rid="bib56">Pertzov et al., 2017</xref>]).</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-22225-fig1-data1-v2.zip"/></supplementary-material> </p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22225.003</object-id><label>Figure 1âfigure supplement 1.</label><caption><title>Similar variance statistics for bounded versus unbounded domains over range relevant for performance data.</title><p>Circular nature of memory variable is unimportant in computing response statistics. (<bold>A</bold>) A normal distribution with standard deviation <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi mathsize="125%">Ï</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.2</mml:mn></mml:mrow></mml:math></inline-formula> over an unbounded domain (solid black curve), together with the corresponding wrapped normal distribution (dashed blue), wrapped around the circular interval <inline-formula><mml:math id="inf17"><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">[</mml:mo><mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">-</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.5</mml:mn></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">,</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.5</mml:mn><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">)</mml:mo></mml:mrow></mml:math></inline-formula>. Note the strong similarity of these two distributions: the wrapped normal barely deviates from the corresponding normal distribution for this value of standard deviation (corresponding to a variance of <inline-formula><mml:math id="inf18"><mml:mrow><mml:msup><mml:mi mathsize="125%">Ï</mml:mi><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msup><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.04</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Left: the computed standard deviation of the distributions in (<bold>A</bold>), as a function of the standard deviation <inline-formula><mml:math id="inf19"><mml:mi mathsize="125%">Ï</mml:mi></mml:math></inline-formula> of the unbounded normal distribution used to generate both the distributions (solid black: normal; dashed blue: wrapped normal). Right: The difference of the two curves from plot at top. Note that the computed standard deviation in the wrapped normal distribution only departs substantially (by more than 5%) from that of the normal distribution around <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi mathsize="125%">Ï</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.3</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf21"><mml:mrow><mml:msup><mml:mi mathsize="125%">Ï</mml:mi><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:msup><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">â¼</mml:mo><mml:mn mathsize="125%" mathvariant="normal">0.09</mml:mn></mml:mrow></mml:math></inline-formula>. All the responses in the experiments, both the across-subject averages reported in the main paper as well as the individual performance averages reported in the Appendix, exhibit a MSE (normalized by squared range) smaller than <inline-formula><mml:math id="inf22"><mml:mn mathsize="125%" mathvariant="normal">0.09</mml:mn></mml:math></inline-formula>. Thus, there is little effect of the boundedness of the angular variable in the results, even though the range of the coded variable is only <inline-formula><mml:math id="inf23"><mml:mi mathsize="125%">Ï</mml:mi></mml:math></inline-formula> radians.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig1-figsupp1-v2"/></fig></fig-group><p>Overall, the squared error in recalling an itemâs orientation (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), averaged over subjects, increased with delay duration (<inline-formula><mml:math id="inf24"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>27</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>49</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) and also with item number (<inline-formula><mml:math id="inf25"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>27</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>48</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). The data show a clear interaction between storage interval duration and set size (<inline-formula><mml:math id="inf26"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mn>81</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>17</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), apparent as steeper degradation slopes for larger set-sizes. In summary, for a small number of items (e.g. <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), increasing the storage duration does not strongly affect performance, but for any fixed delay, increasing item number has a more profound effect.</p><p>Finally, at all tested delays and item numbers, the squared errors are much smaller than the squared range of the circular variable, and any sub-linearities in the curves cannot be attributed to the inevitable saturation of a growing variance on a circular domain (<xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>).</p></sec><sec id="s2-2"><title>Information degradation in persistent activity networks</title><p>In this and all following sections, we start from the hypothesis that persistent neural activity underlies short-term information storage in the brain. The hypothesis is founded on evidence of a relationship between the stored variable and specific patterns of elevated (or depressed) neural activity (<xref ref-type="bibr" rid="bib76">Taube, 1998</xref>; <xref ref-type="bibr" rid="bib1">Aksay et al., 2001</xref>) that persist into the memory storage period and terminate when the task concludes, and on findings that fluctuations in delay-period neural activity can be predictive of variations in memory performance (<xref ref-type="bibr" rid="bib30">Funahashi, 2006</xref>; <xref ref-type="bibr" rid="bib67">Smith and Jonides, 1998</xref>; <xref ref-type="bibr" rid="bib14">Blair and Sharp, 1995</xref>; <xref ref-type="bibr" rid="bib51">Miller et al., 1996</xref>; <xref ref-type="bibr" rid="bib62">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib74">SupÃ¨r et al., 2001</xref>; <xref ref-type="bibr" rid="bib36">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib83">Wimmer et al., 2014</xref>).</p><p>Neural network models like the ring attractor generate an activity bump that is a steady state of the network and thus persists when the input is removed, <xref ref-type="fig" rid="fig2">Figure 2A</xref>. All rotations of the canonical activity bump form a one-dimensional continuum of steady states, <xref ref-type="fig" rid="fig2">Figure 2B</xref>. Relatively straightforward extensions of the ring network can generate 2D or higher-dimensional manifoldsÂ of persistent states. However, any noise in network activity, for instance in form of stochastic spiking (<xref ref-type="bibr" rid="bib69">Softky and Koch, 1993</xref>; <xref ref-type="bibr" rid="bib63">Shadlen and Newsome, 1994</xref>), leads to lateral random drift along the manifold in the form of a diffusive (Ornstein-Uhlenbeck) random walk (<xref ref-type="bibr" rid="bib22">Compte et al., 2000</xref>; <xref ref-type="bibr" rid="bib17">Brody et al., 2003</xref>; <xref ref-type="bibr" rid="bib15">Boucheny et al., 2005</xref>; <xref ref-type="bibr" rid="bib84">Wu et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Burak and Fiete, 2009</xref>; <xref ref-type="bibr" rid="bib31">Fung et al., 2010</xref>; <xref ref-type="bibr" rid="bib19">Burak and Fiete, 2012</xref>), <xref ref-type="fig" rid="fig2">Figure 2CâD</xref>.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.22225.005</object-id><label>Figure 2.</label><caption><title>Analog persistent activity networks and information decay over time.</title><p>(<bold>A</bold>) In a ring network, each neuron excites its immediate neighbors and inhibits all the rest (weight profiles not shown). A single bump of activity (green) is a steady state of such a network of such a network, as are all its translations around the ring. (<bold>B</bold>) A âstate-spaceâ view of activity in the ring network: each axis represents the activity of one neuron in the network; if there are <inline-formula><mml:math id="inf28"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula> neurons in the network, this state-space plot is <inline-formula><mml:math id="inf29"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula>-dimensional. Any point inside the state space represents some possible instantaneous configuration of activity in the <inline-formula><mml:math id="inf30"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula> neurons. The grey curve represents the set of steady states, which traces a 1-dimensional manifold because the stable states are just translations of a canonical activity bump along a single dimension. (<bold>C</bold>) Top: Grey: a schematic non-noisy activity bump; black vertical lines: schematic spikes emitted by neurons after the state is initialized according to the grey curve. Black curve: A best-fit activity profile for the emitted spikes is shifted relative to the original grey bump simply because of the stochastic spikes. Bottom: the state space view of (<bold>B</bold>), with the addition of the state corresponding to the non-noisy initial activity bump (grey filled circle), the noisy spiking state (black cross), and the projection of the noisy spiking state to the best-fit or closest non-noisy activity profile (black filled circle). (<bold>D</bold>) Over longer periods of time, activity fluctuations seen in (<bold>C</bold>) drive a diffusive drift (random walk) along the manifold of stable states, with a squared error that grows linearly with time.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig2-v2"/></fig><p>A defining feature of such random walks is that the squared deviation of the stored state relative to its initial value will grow linearly with elapsed time over short times, <xref ref-type="fig" rid="fig2">Figure 2D</xref>, with a proportionality constant <inline-formula><mml:math id="inf31"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf32"><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:math></inline-formula> is the diffusivity) that depends on quantities like the size of the network and the peak firing rate of neurons (<xref ref-type="bibr" rid="bib19">Burak and Fiete, 2012</xref>).</p></sec><sec id="s2-3"><title>Memory modeled as direct storage in persistent activity networks</title><p>Suppose that the variables in a short-term memory task were <italic>directly</italic> transferred to persistent activity neural networks with a manifold of fixed points that matched the topology of the represented variable. Thus, <inline-formula><mml:math id="inf33"><mml:mi>K</mml:mi></mml:math></inline-formula> circular variables would be stored, entry-by-entry, in <inline-formula><mml:math id="inf34"><mml:mi>K</mml:mi></mml:math></inline-formula> 1-dimensional (1D) ring networks (<xref ref-type="bibr" rid="bib13">Ben-Yishai et al., 1995</xref>). (Alternatively, the <inline-formula><mml:math id="inf35"><mml:mi>K</mml:mi></mml:math></inline-formula> variables could be stored in a single network with a <inline-formula><mml:math id="inf36"><mml:mi>K</mml:mi></mml:math></inline-formula>-dimensional manifold of stable states, as described in the Appendix; the performance in neural costs and in fit to the data of this version of direct storage is worse than with storage in <inline-formula><mml:math id="inf37"><mml:mi>K</mml:mi></mml:math></inline-formula> 1D networks, thus we focus on banks of 1D networks.)</p><p>When <inline-formula><mml:math id="inf38"><mml:mi>N</mml:mi></mml:math></inline-formula> neural resources (e.g. composed of <inline-formula><mml:math id="inf39"><mml:mi>N</mml:mi></mml:math></inline-formula> sets of <inline-formula><mml:math id="inf40"><mml:mi>M</mml:mi></mml:math></inline-formula> neurons each, for a total of <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> neurons) are split into <inline-formula><mml:math id="inf42"><mml:mi>K</mml:mi></mml:math></inline-formula> networks, each network is left with <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> resources (<inline-formula><mml:math id="inf44"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> neurons in our example) for storage of a 1D variable. We know from (<xref ref-type="bibr" rid="bib19">Burak and Fiete, 2012</xref>) that the diffusivity of the state in each of these 1D persistent activity networks will scale as the inverse of the number of neurons and of the peak firing rate per neuron. In other words, the diffusion coefficient is given by <inline-formula><mml:math id="inf45"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf46"><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:math></inline-formula> is a diffusivity parameter independent of <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> (but <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). So long as the squared error remains small compared to the squared range of the variable, it will grow linearly in time at a rate given by <inline-formula><mml:math id="inf49"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mover accent="true"><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (indeed, in the psychophysical data, the squared error remains small compared to the squared range of the angular variable; see <xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>). Therefore the mean squared error (MSE) is given by:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="script">ð</mml:mi></mml:mrow></mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mi>T</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The only free parameter in the expression for MSE as a function of time and item number is the ratio <inline-formula><mml:math id="inf50"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula>. Because the inverse diffusivity parameter <inline-formula><mml:math id="inf51"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> scales with the number of neurons (<inline-formula><mml:math id="inf52"><mml:mi>M</mml:mi></mml:math></inline-formula> in our example) when <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> are held fixed, the product <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is proportional to the total number of neurons (<inline-formula><mml:math id="inf55"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). This ratio therefore functions as a combined neural resource parameter.</p></sec><sec id="s2-4"><title>Direct storage is a poor model of memory performance</title><p>To fit the theory of direct storage to psychophysics data, we find a single best-fit value (with weighted least-squares) of the free parameter <inline-formula><mml:math id="inf56"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> across all item numbers and storage durations. For each item number curve, the fits are additionally anchored to the shortest storage period point (<inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms), which serves as a proxy for <italic>baseline</italic> performance at zero delay. Such baseline errors close to zero delay â which may be due to limitations in sensory perception, attentional constraints, constraints on the rate of information encoding (loading) into memory, or other factors â are not the subject of the present study, which seeks to describe how performance will <italic>deteriorate over time</italic> relative to the zero-delay baseline, as a function of storage duration and item number.</p><p>As can be seen in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, the direct storage theory provides a poor match to human memory performance (<inline-formula><mml:math id="inf58"><mml:mi>p</mml:mi></mml:math></inline-formula> values that the data occur by sampling from the model, excluding the <inline-formula><mml:math id="inf59"><mml:mn>100</mml:mn></mml:math></inline-formula> ms time-point: <inline-formula><mml:math id="inf60"><mml:mrow><mml:mn>0.07</mml:mn><mml:mo>,</mml:mo><mml:mn>0.38</mml:mn><mml:mo>,</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for 1 item; <inline-formula><mml:math id="inf61"><mml:mrow><mml:mn>0.39</mml:mn><mml:mo>,</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> for 2 items; <inline-formula><mml:math id="inf62"><mml:mrow><mml:mn>0.09</mml:mn><mml:mo>,</mml:mo><mml:mn>0.29</mml:mn><mml:mo>,</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:math></inline-formula> for 4 items, and <inline-formula><mml:math id="inf63"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for 6). These <inline-formula><mml:math id="inf64"><mml:mi>p</mml:mi></mml:math></inline-formula>-values strongly suggest rejection of the model.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.22225.006</object-id><label>Figure 3.</label><caption><title>Comparison of direct and coded storage models using persistent activity networks with human memory performance.</title><p>(<bold>A</bold>) Lines: predictions from the direct storage model for human memory. The theory specifies all curves with a single free parameter, after shifting each curve to the measured value of performance at the shortest delay interval of 100 ms. Fits performed by weighted least squares (weights are inverse SEM). (<bold>B</bold>) Similar to (<bold>A</bold>), but parameters fit by ordinary least-squares to only the 6-item curve; note the discrepancy in the 1- and 2-item fits. (<bold>CâE</bold>) Information (<inline-formula><mml:math id="inf65"><mml:mi mathsize="125%">Ï</mml:mi></mml:math></inline-formula>) is directly transmitted (or stored) in a noisy channel, and at the end an estimate of <inline-formula><mml:math id="inf66"><mml:mover accent="true"><mml:mi mathsize="125%">Ï</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> of <inline-formula><mml:math id="inf67"><mml:mi mathsize="125%">Ï</mml:mi></mml:math></inline-formula> is recovered. (<bold>C</bold>) A scenario involving space-to-earth communication. (<bold>D</bold>) The scenario for direct storage in noisy memory banks (the nosy channels); the encoder and decoder are simply the identity transformation in the case of direct storage and hence do nothing. (<bold>E</bold>) The <inline-formula><mml:math id="inf68"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula> pieces of information in the <inline-formula><mml:math id="inf69"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula>-dimensional vector <inline-formula><mml:math id="inf70"><mml:mi mathsize="125%">Ï</mml:mi></mml:math></inline-formula> are each represented in one of <inline-formula><mml:math id="inf71"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula> continuous attractor neural networks of size <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">/</mml:mo><mml:mi mathsize="125%">K</mml:mi></mml:mrow></mml:math></inline-formula> neurons each. Each attractor representation accumulates squared error linearly over time and inversely with <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">/</mml:mo><mml:mi mathsize="125%">K</mml:mi></mml:mrow></mml:math></inline-formula>. (<bold>FâH</bold>) Same as (<bold>CâE</bold>), but here information is first encoded (<inline-formula><mml:math id="inf74"><mml:mrow><mml:mi mathsize="125%">Ï</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">â</mml:mo><mml:mrow><mml:mi mathsize="125%" mathvariant="bold">ð</mml:mi><mml:mo mathsize="88%" mathvariant="normal" stretchy="false">â¢</mml:mo><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">(</mml:mo><mml:mi mathsize="125%">Ï</mml:mi><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) with appropriate structure and redundancy to combat the channel noise. A good encoder-decoder pair can return an estimate <inline-formula><mml:math id="inf75"><mml:mover accent="true"><mml:mi mathsize="125%">Ï</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> that has lower error than the direct strategy, even with similar resource use, mitigating the effects of channel noise for high-fidelity information preservation. (<bold>H</bold>) The <inline-formula><mml:math id="inf76"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula>-dimensional <inline-formula><mml:math id="inf77"><mml:mi mathsize="125%" mathvariant="bold-italic">Ï</mml:mi></mml:math></inline-formula> is encoded as the (<inline-formula><mml:math id="inf78"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula>-dimensional) codeword <inline-formula><mml:math id="inf79"><mml:mi mathsize="125%" mathvariant="bold">ð±</mml:mi></mml:math></inline-formula>, each entry of which is stored in one of <inline-formula><mml:math id="inf80"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula> persistent activity networks. Squared error in the channel grows linearly with time as before; however, the resources used to build <inline-formula><mml:math id="inf81"><mml:mi mathsize="125%">K</mml:mi></mml:math></inline-formula> channels of quality <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">(</mml:mo><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">/</mml:mo><mml:mi mathsize="125%">K</mml:mi></mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">)</mml:mo></mml:mrow><mml:mo mathsize="88%" mathvariant="normal" stretchy="false">â¢</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn></mml:mrow><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">/</mml:mo><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:mrow><mml:mo mathsize="88%" mathvariant="normal" stretchy="false">â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic" mathsize="125%">ð</mml:mi></mml:mrow></mml:math></inline-formula> from before are redirected into building <inline-formula><mml:math id="inf83"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula> channels of poorer quality <inline-formula><mml:math id="inf84"><mml:mrow><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">/</mml:mo><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:mrow><mml:mo mathsize="88%" mathvariant="normal" stretchy="false">â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic" mathsize="125%">ð</mml:mi></mml:mrow></mml:math></inline-formula> (assuming <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">&gt;</mml:mo><mml:mi mathsize="125%">K</mml:mi></mml:mrow></mml:math></inline-formula>). The decoder estimates <inline-formula><mml:math id="inf86"><mml:mi mathsize="125%" mathvariant="bold-italic">Ï</mml:mi></mml:math></inline-formula> from <inline-formula><mml:math id="inf87"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula>-dimensional output <inline-formula><mml:math id="inf88"><mml:mi mathsize="125%" mathvariant="bold">ð²</mml:mi></mml:math></inline-formula>. (<bold>I</bold>) Same as (<bold>A</bold>), but the model lines are the lower-bound on mean-squared error obtained from an information-theoretic model of memory with good coding. (Model fit by weighted least-squares; the theory specifies all curves with two free parameters, after shifting each curve to the measured value of performance at the shortest delay interval of 100 ms).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22225.007</object-id><label>Figure 3âfigure supplement 1.</label><caption><title>Cross-validated comparison of the direct and well-coded storage models after leaving out <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn></mml:mrow></mml:math></inline-formula>s datapoints.</title><p>The A) direct and B) well-coded storage models are fit to the data, excluding the datapoints at time (<inline-formula><mml:math id="inf90"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1</mml:mn></mml:mrow></mml:math></inline-formula>s). This is a leave-one-out or jackknife cross-validation procedure. The well-coded model predicts the withheld datapoints with smaller error than the uncoded/direct coding model. Direct model: Sum of weighted least-squares error (WLS error): 103.3984; sum of squares error: 0.022888; squared error on held-out <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1000</mml:mn></mml:mrow></mml:math></inline-formula> ms point: 0.0043414. Well-coded model (with minimum error near <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:math></inline-formula>): WLS error: 11.3172; sum of squares error: 0.0016302; squared error on held-out <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1000</mml:mn></mml:mrow></mml:math></inline-formula> ms point: 0.0011631. BIC score: Delta BICÂ =Â BIC(direct model all items WLS) - BIC(coded model all items WLS): 11.4039, in favor of the well-coded model.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig3-figsupp1-v2"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22225.008</object-id><label>Figure 3âfigure supplement 2.</label><caption><title>Cross-validated comparison of the direct and well-coded storage models after leaving out <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:mrow></mml:math></inline-formula>s datapoints.</title><p>The A) direct and B) well-coded storage models are fit to the data, excluding the datapoints at time (<inline-formula><mml:math id="inf95"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:mrow></mml:math></inline-formula><bold>s</bold>). This is a leave-one-out or jackknife cross-validation procedure. The well-coded model predicts the withheld datapoints with smaller error than the uncoded/direct coding model. Direct model: WLS error: 79.2137; sum of squares error: 0.015975; squared error on held-out <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">2000</mml:mn></mml:mrow></mml:math></inline-formula> ms point: 0.010418. Well-coded model (with minimum error near <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">5</mml:mn></mml:mrow></mml:math></inline-formula>): WLS error: 2.9575; sum of squares error: 0.0007505; squared error on held-out <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">2000</mml:mn></mml:mrow></mml:math></inline-formula> ms point: 0.00083856. BIC scores: Delta BICÂ =Â BIC(direct model all items WLS) - BIC(coded model all items WLS): 32.4666, in favor of well-coded model.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig3-figsupp2-v2"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22225.009</object-id><label>Figure 3âfigure supplement 3.</label><caption><title>Comparison of models after removal of the shortest (100 ms) delay time-point under the argument that it represents a different memory process (iconic memory).</title><p>The <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1000</mml:mn></mml:mrow></mml:math></inline-formula> ms point is now used as the baseline level to analyze the time degradation of stored memory, instead of the <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">100</mml:mn></mml:mrow></mml:math></inline-formula> ms point, which is deleted altogether from the analysis. The argument for this analysis is that <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi mathsize="125%">T</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">100</mml:mn></mml:mrow></mml:math></inline-formula>ms might overlap with the process of iconic memory and should not be used in a comparison across the longer-latency short-term memory interval datapoints. The (A) direct model and (B) well-coded model, where (C) fit-quality plateaus to a nearly asymptotic constant with increasing <inline-formula><mml:math id="inf102"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula> (but the asymptotic value is nearly achieved by <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:math></inline-formula>). Direct model: WLS error: 37.317; sum of squares error: 0.0080949. Well-coded model (no minimum in error in interior of range; asymptotic decay of error with <inline-formula><mml:math id="inf104"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula>, with near-asymptotic value reached by <inline-formula><mml:math id="inf105"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:math></inline-formula>; here, we use <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">100</mml:mn></mml:mrow></mml:math></inline-formula>, but similar results including BIC scores for <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:math></inline-formula>: WLS error: 12.493; sum of squares error: 0.0019871. Delta BICÂ =Â BIC(direct model all items WLS) - BIC(coded model all items WLS): 24.8239, in favor of the well-coded model.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig3-figsupp3-v2"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22225.010</object-id><label>Figure 3âfigure supplement 4.</label><caption><title>Redefining item numbers as <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">[</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1â4â8â12</mml:mn><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (instead of <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi mathsize="125%">K</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mrow><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">[</mml:mo><mml:mn mathsize="125%" mathvariant="normal">1â2â4â6</mml:mn><mml:mo mathvariant="normal" maxsize="125%" minsize="125%">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) to take into account the memorization of item color in addition to orientation.</title><p>1. Fits with direct storage model and (B) well-coded model. For the well-coded model, fit quality reaches a minimum around NÂ =Â 10. Direct model: WLS error: 80.4649; sum of squares error: 0.016218. Well-coded model (with minimum error near <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:math></inline-formula>): WLS error: 12.4617; sum of squares error: 0.0016035. Delta BICÂ =Â BIC(direct model all items WLS) - BIC(coded model all items WLS): 68.0032 in favor of the coded model.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig3-figsupp4-v2"/></fig></fig-group><p>Does the direct storage model fail mostly because its dependence on time and item number are linear, while the data exhibits some nonlinear effects at the largest delays? On the contrary, direct storage fails to fit the data even at short delays when the performance curves are essentially linear (see the systematic underestimation of squared error by the model over <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi/><mml:mo>â¤</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> second delays in the 4- and 6-item curves). If anything, the slight sub-linearity in the 6-item curve at longer delays tends to bring it closer to the other curves and thus to the model, thus its effect is to slightly reduce the discrepancy between the data and fits from direct storage theory.</p><p>One view of the results, obtained by selecting model parameters to best match the 6-item curve, is that direct storage theory predicts an insufficiently strong <italic>improvement</italic> in performance with decreasing item number, <xref ref-type="fig" rid="fig3">Figure 3B</xref> (<inline-formula><mml:math id="inf112"><mml:mi>p</mml:mi></mml:math></inline-formula>-values for direct-storage model when fit to the 6-item responses: <inline-formula><mml:math id="inf113"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for 1 item; <inline-formula><mml:math id="inf114"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for 2 items; <inline-formula><mml:math id="inf115"><mml:mrow><mml:mn>0.76</mml:mn><mml:mo>,</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for 4 items; <inline-formula><mml:math id="inf116"><mml:mrow><mml:mn>0.22</mml:mn><mml:mo>,</mml:mo><mml:mn>0.39</mml:mn><mml:mo>,</mml:mo><mml:mn>0.38</mml:mn></mml:mrow></mml:math></inline-formula> for 6, excluding the <inline-formula><mml:math id="inf117"><mml:mn>100</mml:mn></mml:math></inline-formula> ms delay time-point; the <inline-formula><mml:math id="inf118"><mml:mi>p</mml:mi></mml:math></inline-formula>-values for the 1- and 2-item curves strongly suggest rejection of the model).</p></sec><sec id="s2-5"><title>Information-theoretic bound on memory performance with well-coded storage</title><p>Even if information storage in persistent activity networks is a central component of short-term memory, describing the storage step is not a sufficient account of memory. This fact is widely appreciated in memory psychophysics, where it has been observed that variations in attention, motivation, and other factors also affect memory performance (<xref ref-type="bibr" rid="bib4">Atkinson and Shiffrin, 1968</xref>; <xref ref-type="bibr" rid="bib47">Matsukura et al., 2007</xref>). Here we propose that, even discounting these complex factors, direct storage of a set of continuous variables into persistent activity networks with the same total dimension of stable states lacks generality as a model of memory because it does not consider how pre-encoding of information could affect its subsequent degradation, <xref ref-type="fig" rid="fig3">Figure 3CâE</xref>. This omission could help account for the mismatch between predictions from direct storage and human behavior, <xref ref-type="fig" rid="fig3">Figure 3AâB</xref>.</p><p>Storing information in noisy persistent activity networks means that after a delay there will be some information loss, as described above. Mathematically, information storage in a noisy medium is equivalent to passing the information through a noisy information channel. To allow for high-fidelity communication through a noisy channel, it is necessary to first appropriately encode the signal, <xref ref-type="fig" rid="fig3">Figure 3F</xref>. Encoding for error control involves the addition of appropriate forms of redundancy tailored to the channel noise. As shown by Shannon (<xref ref-type="bibr" rid="bib64">Shannon, 1948</xref>), very different levels of accuracy can be achieved with different forms of encoding for the same amount of coding redundancy and channel noise. Thus, predictions for memory performance after good encoding may differ substantially from the predictions from direct storage even though the underlying storage networks (channels) are identical.</p><p>Thus, a more general theory of information storage for short-term memory in the brain would consider the effects of arbitrary encoder-decoder pairs that sandwich the noisy storage stage, <xref ref-type="fig" rid="fig3">Figure 3G</xref>. In such a three-stage model, information to be stored is first passed to an encoder, which performs all necessary encoding. Encoding strategies may include source coding or compression of the data as well as, critically, channel coding â the addition of redundancy tailored to the noise in the channel so that, subject to constraints on how much redundancy can be added, the downstream effects of channel noise are minimized (<xref ref-type="bibr" rid="bib64">Shannon, 1948</xref>). The coded information is stored in persistent activity networks, <xref ref-type="fig" rid="fig3">Figure 3H</xref>. Finally, the information is accessed by a decoder or readout, <xref ref-type="fig" rid="fig3">Figure 3G</xref>. Here, we derive a bound on the best performance that can be achieved by any coding or decoding strategy, if the storage step involves graded persistent activity.</p><p>The encoder transforms the <inline-formula><mml:math id="inf119"><mml:mi>K</mml:mi></mml:math></inline-formula>-dimensional input variable into an <inline-formula><mml:math id="inf120"><mml:mi>N</mml:mi></mml:math></inline-formula> dimensional codeword, to be stored in a bank of storage networks with an <inline-formula><mml:math id="inf121"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional manifold of persistent activity states (in the form of <inline-formula><mml:math id="inf122"><mml:mi>N</mml:mi></mml:math></inline-formula> networks with a 1-dimensional manifold each, or 1 network with an <inline-formula><mml:math id="inf123"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional manifold, or something in between). To equalize resource use for the persistent activity networks in both direct storage and coded storage models of memory, the <inline-formula><mml:math id="inf124"><mml:mi>N</mml:mi></mml:math></inline-formula> stored states have a diffusivity <inline-formula><mml:math id="inf125"><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:math></inline-formula> each, in contrast to the diffusivity of <inline-formula><mml:math id="inf126"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> each for <inline-formula><mml:math id="inf127"><mml:mi>K</mml:mi></mml:math></inline-formula> states (compare <xref ref-type="fig" rid="fig3">Figure 3DâE and and GâH</xref>). The storage step is equivalent to passage of information through additive Gaussian information channels, with variance proportional to the storage duration <inline-formula><mml:math id="inf128"><mml:mi>T</mml:mi></mml:math></inline-formula> and to the diffusivity. The decoder error-corrects the output of the storage stage and inverts the code to provide an estimate of the stored variable. (For more details, see MaterialsÂ andÂ methods and Appendix.)</p><p>We can use information theory to derive the <italic>minimum achievable</italic> recall error over all possible encoder-decoder structures, for the given statistics of the variable to be remembered and the noise in the storage information channels. In particular, we use <italic>joint source-channel coding</italic> theory to first consider at what rate information can be conveyed through a noisy channel for a given level of noise and coding redundancy, then obtain the minimal achievable distortion (recall error) for that information rate (see Materials and Appendix). We obtain the following lower-bound on the recall error:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="script">ð</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This result is the <italic>theoretical lower bound</italic> on MSE achievable by any system that passes information through a noisy channel with the specified statistics: a Gaussian additive channel noise of zero mean and variance <inline-formula><mml:math id="inf129"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> per channel use, a codeword of dimension <inline-formula><mml:math id="inf130"><mml:mi>N</mml:mi></mml:math></inline-formula>, and a variable to be transmitted (stored) of dimension <inline-formula><mml:math id="inf131"><mml:mi>K</mml:mi></mml:math></inline-formula>, with entries that lie in the range <inline-formula><mml:math id="inf132"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. The bound becomes tight asymptotically (for large <inline-formula><mml:math id="inf133"><mml:mi>N</mml:mi></mml:math></inline-formula>), but for small <inline-formula><mml:math id="inf134"><mml:mi>N</mml:mi></mml:math></inline-formula> it remains a strict lower-bound. Although the potential for decoding errors is reduced at smaller <inline-formula><mml:math id="inf135"><mml:mi>N</mml:mi></mml:math></inline-formula>, the qualitative dependence of performance on item number and delay should remain the same (Appendix and (<xref ref-type="bibr" rid="bib58">Polyanskiy et al., 2010</xref>) ). The bound is derived by dividing the total resources (defined here, as in the direct storage case, as the ratio <inline-formula><mml:math id="inf136"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula>) evenly across all stored items (details in Appendix), similar to a âcontinuous resourceâ conception of memory. The same theoretical treatment will admit different resource allocations, for instance, one could split the resources into a fixed number of pieces and allocate those to a (sub)set of the presented items, more similar to the âdiscrete slotsâ model.</p><p>A heuristic derivation of the result above can be obtained by first noting that the capacity of a Gaussian channel with a given signal-to-noise ratio (<inline-formula><mml:math id="inf137"><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) is <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>u</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The summed capacity of <inline-formula><mml:math id="inf139"><mml:mi>N</mml:mi></mml:math></inline-formula> channels, spread across the <inline-formula><mml:math id="inf140"><mml:mi>K</mml:mi></mml:math></inline-formula> items of the stored variable, produces <inline-formula><mml:math id="inf141"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mpadded width="+5pt"><mml:mi>r</mml:mi></mml:mpadded><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mfrac><mml:mo>â¢</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>u</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The variance of a scalar within the unit interval represented by <inline-formula><mml:math id="inf142"><mml:mi>I</mml:mi></mml:math></inline-formula> bits of information is bounded below by <inline-formula><mml:math id="inf143"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>. Inserting <inline-formula><mml:math id="inf144"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mpadded width="+5pt"><mml:mi>r</mml:mi></mml:mpadded><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> into the variance expression and <inline-formula><mml:math id="inf145"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> into <inline-formula><mml:math id="inf146"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>u</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, yields <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> , up to scaling prefactors. The Appendix provides more rigorous arguments that the bound we derive is indeed the best that can theoretically be achieved.</p><p><xref ref-type="disp-formula" rid="equ2">Equation 2</xref> exhibits some characteristic features, including, first, a joint dependence on the number of stored items and the storage duration. According to this expression, the time-course of memory decay depends on the number of items. This effect arises because items compete for the same limited memory resources and when an item is allocated fewer resources it is more susceptible to the effects of noise over time. Second, the scaling with item number is qualitatively different than the scaling with storage duration: Increasing the number of stored items degrades performance much more steeply than increasing the storage interval, because item number is in the exponent. For a single memorized feature or item, the decline in accuracy with storage interval duration is predicted to be weak. On the other hand, increasing the number of memorized items while keeping the storage duration fixed should lead to a rapid deterioration in memory accuracy.</p><p>We next consider whether the performance of an optimal encoder (given this lower bound) can be distinguished from the direct storage model based on human performance data. The two predictions differ in their dependence upon the number of independent storage channels or networks, <inline-formula><mml:math id="inf147"><mml:mi>N</mml:mi></mml:math></inline-formula>, which we do not know how to control in human behavior. Equally important, since <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> provides a theoretical limit on performance, it is of interest to learn whether human behavior approximates the limit, and where it might deviate from it.</p></sec><sec id="s2-6"><title>Comparison of theoretical bound with human performance</title><p>In comparing the psychophysical data to the theoretical bound on short-term memory performance, there are two unknown parameters, <inline-formula><mml:math id="inf148"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> (the inverse diffusivity in each persistent activity network) and <inline-formula><mml:math id="inf149"><mml:mi>N</mml:mi></mml:math></inline-formula> (the number of such networks), both of which scale linearly with the neural resource of neuron number. The product of these parameters corresponds to total neural resource exactly as in the direct storage case. We fit <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> to human performance data, assuming as in the direct storage model that the total neural resource is fixed across all item numbers and delay durations, and setting the 100 ms delay values of the theoretical curves to their empirical values.</p><p>The resulting best fit between theory and human behavior is excellent (<xref ref-type="fig" rid="fig4">Figure 4E</xref>; <inline-formula><mml:math id="inf150"><mml:mi>p</mml:mi></mml:math></inline-formula> values that the data means may occur by sampling from the model, excluding the <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms time-points: <inline-formula><mml:math id="inf152"><mml:mrow><mml:mn>0.99</mml:mn><mml:mo>,</mml:mo><mml:mn>0.07</mml:mn><mml:mo>,</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula> for 1 item; <inline-formula><mml:math id="inf153"><mml:mrow><mml:mn>0.46</mml:mn><mml:mo>,</mml:mo><mml:mn>0.07</mml:mn><mml:mo>,</mml:mo><mml:mn>0.60</mml:mn></mml:mrow></mml:math></inline-formula> for 2 items; <inline-formula><mml:math id="inf154"><mml:mrow><mml:mn>0.54</mml:mn><mml:mo>,</mml:mo><mml:mn>0.24</mml:mn><mml:mo>,</mml:mo><mml:mn>0.43</mml:mn></mml:mrow></mml:math></inline-formula> for 4 items; <inline-formula><mml:math id="inf155"><mml:mrow><mml:mn>0.89</mml:mn><mml:mo>,</mml:mo><mml:mn>0.38</mml:mn><mml:mo>,</mml:mo><mml:mn>0.32</mml:mn></mml:mrow></mml:math></inline-formula> for 6; all values are larger than 0.05, most much more so. These <inline-formula><mml:math id="inf156"><mml:mi>p</mml:mi></mml:math></inline-formula> values indicate a significantly better fit to data than obtained with the direct storage model).</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.22225.011</object-id><label>Figure 4.</label><caption><title>Multiplicity of reasonable parametric solutions for the well-coded storage model, with <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">5</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">-</mml:mo><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">-</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:math></inline-formula> networks providing the best fits to human performance.</title><p>(<bold>A</bold>) The weighted least-squares error (colorbar indicating size of error on right) of the well-coded model fit to psychophysics data as a function of the two fit parameters, <inline-formula><mml:math id="inf158"><mml:mi class="ltx_font_mathcaligraphic" mathsize="125%">ð</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf159"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula>. The deep blue valley running near the diagonal of the parameter space constitutes a set of reasonable fits to the data. (<bold>B</bold>) Three fits to the data using parameters along the valley, sampled at <inline-formula><mml:math id="inf160"><mml:mrow><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">5</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">,</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:mrow><mml:mo mathsize="88%" mathvariant="normal" stretchy="false">,</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf161"><mml:mn mathsize="125%" mathvariant="normal">100</mml:mn></mml:math></inline-formula>. These three parameter sets are indicated by white circles in (<bold>A</bold>). (<bold>C</bold>) Blue curve: the weighted least-squares error in the fit between data and theory along the bottom of the valley seen in (<bold>A</bold>). Gray curve: the total resource use for the corresponding points along the valley.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22225.012</object-id><label>Figure 4âfigure supplement 1.</label><caption><title>Performance of individual subjects and fits to well-coded storage model.</title><p>The responses of individual subjects are also well-fit by the functional form of the theoretically obtained bound on memory performance. <italic>Top two panels:</italic> the quality of fit (weighted squared error of fit) as a function of the two parameters of the theory (dark blueÂ =Â best fit; dark redÂ =Â worst fit), for each of the 10 subjects in the study. <italic>Middle two panels:</italic> The quality of fit along the valley defined by the dark blue area in the top two panels, plotted as a function of <inline-formula><mml:math id="inf162"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula>, for each of the 10 subjects. Most subjects have an optimal value of <inline-formula><mml:math id="inf163"><mml:mrow><mml:mn mathsize="125%" mathvariant="normal">4</mml:mn><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">â¤</mml:mo><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">â¤</mml:mo><mml:mn mathsize="125%" mathvariant="normal">10</mml:mn></mml:mrow></mml:math></inline-formula>. The two outliers (subjects 7 and 8, with <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi mathsize="125%">N</mml:mi><mml:mo mathsize="125%" mathvariant="normal" stretchy="false">=</mml:mo><mml:mn mathsize="125%" mathvariant="normal">20</mml:mn></mml:mrow></mml:math></inline-formula>, respectively, have rather flat fit quality along the entire valley, as a function of <inline-formula><mml:math id="inf166"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula>, and thus other values of <inline-formula><mml:math id="inf167"><mml:mi mathsize="125%">N</mml:mi></mml:math></inline-formula> produce very similar fit quality.) <italic>Bottom two panels:</italic> Individual subject performance (circles: mean-squared error averaged across trials; error bars are the across-trial SEM) as a function of storage interval duration, for different item numbers (1, 2, 4 and 6 items, black, blue, cyan and green, respectively). Solid curves: fits from the theoretical bound on performance (minimum weighted squared error).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig4-figsupp1-v2"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.22225.013</object-id><label>Figure 4âfigure supplement 2.</label><caption><title>Fits of individual subject performance to direct storage model with hypothesis comparison score between direct and well-coded storage models.</title><p>The responses of individual subjects fit with weighted least-squares to the direct storage model. Weights are equal to the empirical standard error (SEM). The Bayesian Information Criterion-based hypothesis comparison score between the well-coded storage model and the direct storage model for individual subjects (<inline-formula><mml:math id="inf168"><mml:mrow><mml:mi mathsize="125%" mathvariant="normal">Î</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">â¢</mml:mo><mml:mi mathsize="125%">B</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">â¢</mml:mo><mml:mi mathsize="125%">I</mml:mi><mml:mo mathsize="80%" mathvariant="normal" stretchy="false">â¢</mml:mo><mml:mi mathsize="125%">C</mml:mi></mml:mrow></mml:math></inline-formula>) is indicated at the top of each subplot.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-22225-fig4-figsupp2-v2"/></fig></fig-group><p>If we penalize the well-coded storage model for its extra parameter compared to direct storage (<inline-formula><mml:math id="inf169"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf170"><mml:mi>N</mml:mi></mml:math></inline-formula>, versus the single parameter <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> for the direct storage model) through the Bayesian Information Criterion (BIC), a likelihood-based hypothesis comparison test (that more stringently penalizes model parameters than the AIC or Aikike Information Criterion), the evidence remains very strongly in favor of the well-coded memory storage model compared to direct storage (<inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mn>99</mml:mn><mml:mo>â«</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, where 10 is the cutoff for âvery strongâ support) (<xref ref-type="bibr" rid="bib39">Kass and Raftery, 1995</xref>). In fact, according to the BIC, the discrepancy in the quality of fit to the data between the models is so great that the increased parameter cost of the well-coded memory model barely perturbs the evidence in its favor. Some more statistical controls by jackknife cross-validation of the two models (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2</xref>), exclusion of the <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms point on the grounds that it might represent iconic memory recall rather than short-term memory (<xref ref-type="fig" rid="fig3s3">Figure 3âfigure supplement 3</xref>), and redefinition of the number of items in memory to take into account the colors and orientations of the objects are given in the Appendix (<xref ref-type="fig" rid="fig3s4">Figure 3âfigure supplement 4</xref>); the results are qualitatively unchanged, and also do not result in large quantitative deviations in the extracted parameters (discussed below).</p><p>The two-dimensional parameter space for fitting the theory to the data contains a one-dimensional manifold of reasonable solutions, <xref ref-type="fig" rid="fig4">Figure 4A</xref> (dark blue valley), most of which provide better fits to the data than the direct storage model. Some of these different fits to the data are shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. At large values of <inline-formula><mml:math id="inf174"><mml:mi>N</mml:mi></mml:math></inline-formula>, the manifold is roughly a hyperbola in <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, suggesting that the logarithms of the two neural resource parameters can roughly trade off with each other; indeed, the total resource use in the one-dimensional solution valley is roughly constant at large <inline-formula><mml:math id="inf177"><mml:mi>N</mml:mi></mml:math></inline-formula>, <xref ref-type="fig" rid="fig4">Figure 4C</xref> (gray curves). However, at smaller <inline-formula><mml:math id="inf178"><mml:mi>N</mml:mi></mml:math></inline-formula>, the resource use drops with increasing <inline-formula><mml:math id="inf179"><mml:mi>N</mml:mi></mml:math></inline-formula>. The fits are not equally good along the valley of reasonable solutions, and the best fit lies near <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> independent networks or channels (for jackknife cross-validation fits, see <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2</xref>, the best fits for the coded model can be closer to <inline-formula><mml:math id="inf181"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>; thus, the figure obtained for the number of memory networks should be taken as an order-of-magnitude estimate rather than an exact value). Resource use in the valley declines with increasing <inline-formula><mml:math id="inf182"><mml:mi>N</mml:mi></mml:math></inline-formula> to its asymptotic constant value (thus larger <inline-formula><mml:math id="inf183"><mml:mi>N</mml:mi></mml:math></inline-formula> would yield bigger representational efficiencies); however, by <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, resource use is already close to its final asymptotic value, thus the gains of increasing the number of separate memory networks beyond <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>â</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> diminish. The theory also provides good fits to individual subject performance for all ten subjects, using parameter values within a factor of 10 (and usually much less than a factor of 10) of each other (see Appendix).</p></sec><sec id="s2-7"><title>Comparison of neural resource use in direct and well-coded storage models of memory</title><p>Finally, we compare the neural resources required for storage in the direct storage model (best-fit) compared to the well-coded storage model. We quantify the neural resources required for well-coded storage as the product of the number of networks <inline-formula><mml:math id="inf186"><mml:mi>N</mml:mi></mml:math></inline-formula> with the inverse diffusive coefficient <inline-formula><mml:math id="inf187"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula>. This is proportional to the number of neurons required to implement storage. To replicate human behavior, coded storage requires resources totaling <inline-formula><mml:math id="inf188"><mml:mrow><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math></inline-formula> (in units of seconds) for <inline-formula><mml:math id="inf189"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf190"><mml:mrow><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mn>22</mml:mn></mml:mrow></mml:math></inline-formula> (s) for <inline-formula><mml:math id="inf191"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, corresponding to the parameter settings for the fits in <xref ref-type="fig" rid="fig4">Figures 4C</xref> and 5B (center), respectively. By contrast, uncoded storage requires a 40-fold increase in <inline-formula><mml:math id="inf192"><mml:mi>N</mml:mi></mml:math></inline-formula> or a 40-fold decrease in the diffusive growth rate in squared error, <inline-formula><mml:math id="inf193"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula>, per network (or a corresponding increase in the product, <inline-formula><mml:math id="inf194"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula>), because <inline-formula><mml:math id="inf195"><mml:mrow><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mn>1215</mml:mn></mml:mrow></mml:math></inline-formula> (s) under direct storage, to produce the best-fit result of <xref ref-type="fig" rid="fig3">Figure 3A</xref>. Thus, well-coded storage requires substantially fewer resources in the persistent activity networks for similar performance (assuming best fits of each produce similar performance). Equivalently, a memory system with good encoding can achieve substantially better performance with the same total storage resources, than if information were directly stored in persistent activity networks.</p><p>This result on the disparity in resource use between uncoded and coded information storage is an illustration of the power of strong error-correcting codes. Confronted with the prospect of imperfect information channels, finitely many resources, and the need to store or transmit information faithfully, one may take two different paths.</p><p>The first option is to split the total resources into <inline-formula><mml:math id="inf196"><mml:mi>K</mml:mi></mml:math></inline-formula> storage bins, into which the <inline-formula><mml:math id="inf197"><mml:mi>K</mml:mi></mml:math></inline-formula> variables are stored; when there are more variables, there are more bins and each variable receives a smaller bin. The other is to store <inline-formula><mml:math id="inf198"><mml:mi>N</mml:mi></mml:math></inline-formula> quantities in <inline-formula><mml:math id="inf199"><mml:mi>N</mml:mi></mml:math></inline-formula> bins regardless of <inline-formula><mml:math id="inf200"><mml:mi>K</mml:mi></mml:math></inline-formula>, by splitting each of the <inline-formula><mml:math id="inf201"><mml:mi>K</mml:mi></mml:math></inline-formula> variables into <inline-formula><mml:math id="inf202"><mml:mi>N</mml:mi></mml:math></inline-formula> pieces and assigning a piece from each of the different variables to one bin; when there are more variables, each variable gets a smaller piece of the bin. In the former approach, which is similar to the direct storage scenario, increasing <inline-formula><mml:math id="inf203"><mml:mi>N</mml:mi></mml:math></inline-formula> would lead to improvements in the fidelity of each of the <inline-formula><mml:math id="inf204"><mml:mi>K</mml:mi></mml:math></inline-formula> channels, <xref ref-type="fig" rid="fig4">Figure 4D</xref>. In the latter approach, which is the strong coding strategy, increasing <inline-formula><mml:math id="inf205"><mml:mi>N</mml:mi></mml:math></inline-formula> would increase the number of channels while keeping their fidelity fixed, <xref ref-type="fig" rid="fig4">Figure 4B</xref>. The latter ultimately yields a more efficient use of the same total resources in terms of the final quality of performance, especially for larger values of <inline-formula><mml:math id="inf206"><mml:mi>N</mml:mi></mml:math></inline-formula>, at least without considering the cost of the encoding and decoding steps.</p><p>If we hold the total resource <inline-formula><mml:math id="inf207"><mml:mrow><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> fixed, the lowest achievable MSE (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref> ) in the well-coded memory model is reached for maximally large <inline-formula><mml:math id="inf208"><mml:mi>N</mml:mi></mml:math></inline-formula> and thus maximally large <inline-formula><mml:math id="inf209"><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:math></inline-formula>. However, human memory performance appears to be best-fit by <inline-formula><mml:math id="inf210"><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¼</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>. It is not clear, if our model does capture the basic architecture of the human memory system, why the memory system might operate in a regime of relatively small <inline-formula><mml:math id="inf211"><mml:mi>N</mml:mi></mml:math></inline-formula>. First, note that for increasing <inline-formula><mml:math id="inf212"><mml:mi>N</mml:mi></mml:math></inline-formula>, the total resource cost by <inline-formula><mml:math id="inf213"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> is already down to within 10<inline-formula><mml:math id="inf214"><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:math></inline-formula> of the minimum resource cost reached at much larger <inline-formula><mml:math id="inf215"><mml:mi>N</mml:mi></mml:math></inline-formula>. Second, note that the theory is derived under the âdiffusiveâ memory storage assumption: that within a storage network, information loss is diffusive. Thus, the assumption implicitly made while varying the parameter <inline-formula><mml:math id="inf216"><mml:mi>N</mml:mi></mml:math></inline-formula> in <xref ref-type="fig" rid="fig4">Figure 4C</xref> is that as the number of networks (<inline-formula><mml:math id="inf217"><mml:mi>N</mml:mi></mml:math></inline-formula>) is increased, the diffusivity <inline-formula><mml:math id="inf218"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> per network will simply increase in proportion to keep <inline-formula><mml:math id="inf219"><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> fixed. However, the dynamics of persistent activity networks do not remain purely diffusive once the resource per network drops below a certain level: a new kind of non-diffusive error can start to become important (Schwab DJ &amp; Fiete I (in preparation)). In this regime, the effective diffusivity in the network can grow much faster than the inverse network size. The non-diffusive errors produce large, non-local errors (which may be consistent with âpure guessingâ or âsudden deathâ errors sometimes reported in memory psychophysics [<xref ref-type="bibr" rid="bib88">Zhang and Luck, 2009</xref>]). It is possible that the memory networks operate in a regime where each channel (memory network) is allocated enough resources to mostly avoid non-diffusive errors, and this limits the number of networks.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Key contributions</title><p>We have provided a fundamental lower-bound on the error of recall in short-term memory as a function of item number and storage duration, if information is stored in graded persistent activity networks (our noisy channels). This bound on performance with an underlying graded persistent activity mechanism provides a reference point for comparison with human performance regardless of whether the brain employs strong encoding and decoding processes in its memory systems. The comparison can yield insights into the strategies the brain does employ.</p><p>Next, we used empirical data from analog measurements of memory error as a function of both temporal delay and the number of stored items. Using results from the theory of diffusion on continuous attractor manifolds in neural networks, we derived an expression for memory performance if the memorized variables were stored directly in graded persistent activity networks. The resulting predictions did not match human performance. The mismatch invites further investigation into whether and how direct-storage models can be modified to account for real memory performance.</p><p>Finally, we found that the bound from theory provided an (unexpectedly) good match to human performance, <xref ref-type="fig" rid="fig4">Figure 4</xref>. We are not privy to the actual values of the parameters <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the brain and it is possible the brain uses a value of, to take an arbitrary example, <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi/><mml:mo>â</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>Ã</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to achieve a performance reached with <inline-formula><mml:math id="inf222"><mml:mi>N</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> , which would be (quantitatively) âsuboptimalâ. Nevertheless, the possibility that the brain might perform qualitatively according to the functional form of the theoretical bound is highly nontrivial: As we have seen, the addition of appropriate encoding and decoding systems can reduce the degradation in accuracy from scaling polynomially (<inline-formula><mml:math id="inf223"><mml:mrow><mml:mi/><mml:mo>â¼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) in the number of neurons, as in direct storage, to scaling exponentially (<inline-formula><mml:math id="inf224"><mml:mrow><mml:mi/><mml:mo>â¼</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for some <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>Î±</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). This is a startling possibility that requires more rigorous examination in future work.</p></sec><sec id="s3-2"><title>Are neural representations consistent with exponentially strong codes?</title><p>Typical population codes for analog variables, as presently understood, exhibit linear gains in performance with <inline-formula><mml:math id="inf226"><mml:mi>N</mml:mi></mml:math></inline-formula>; such codes involve neurons with single-bump or ramp-like tuning curves that are offset or scaled copies of one another. For related reasons, persistent activity networks with such tuning curves also exhibit linear gains in memory performance with <inline-formula><mml:math id="inf227"><mml:mi>N</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib19">Burak and Fiete, 2012</xref>). These âclassical population codesâ are ubiquitous in the sensory and motor peripheries as well as some cognitive areas. So far, the only example of an analog neural code known in principle to be capable of exponential scaling with <inline-formula><mml:math id="inf228"><mml:mi>N</mml:mi></mml:math></inline-formula> is the periodic, multi-scale code for location in grid cells of the mammalian entorhinal cortex (<xref ref-type="bibr" rid="bib35">Hafting et al., 2005</xref>; <xref ref-type="bibr" rid="bib71">Sreenivasan and Fiete, 2011</xref>; <xref ref-type="bibr" rid="bib46">Mathis et al., 2012</xref>) : with this code, animals can represent an exponentially large set of distinct locations at a fixed local spatial resolution using linearly many neurons (<xref ref-type="bibr" rid="bib28">Fiete et al., 2008</xref>; <xref ref-type="bibr" rid="bib71">Sreenivasan and Fiete, 2011</xref>).</p><p>A literal analogy with grid cells would imply that all such codes should look periodic as a function of the represented variable, with a range of periods. A more general view is that the exponential capacity of the grid cell code results from two related features: First, no one group of grid cells with a common spatial tuning period carries full information about the coded variable (the spatial location of the animal) â location cannot be uniquely specified by the spatially periodic group response even in the absence of any noise. Second, the partial location information in different groups is independent because of the distinct spatial periods across groups (<xref ref-type="bibr" rid="bib71">Sreenivasan and Fiete, 2011</xref>). In this more general view, strong codes need not be periodic, but there should be multiple populations that encode different, independent âpartsâ of the same variable, which would be manifest as different sub-populations with diverse tuning profiles, and mixed selectivity to multiple variables.</p><p>It remains to be seen whether neural representations for short-term visual memory are consistent with strong codes. Intriguingly, neural responses for short-term memory are diverse and do not exhibit tuning that is as simple or uniform as typical for classical population codes (<xref ref-type="bibr" rid="bib51">Miller et al., 1996</xref>; <xref ref-type="bibr" rid="bib32">Fuster and Alexander, 1971</xref>; <xref ref-type="bibr" rid="bib62">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="bib80">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib30">Funahashi, 2006</xref>; <xref ref-type="bibr" rid="bib33">Fuster and Jervey, 1981</xref>; <xref ref-type="bibr" rid="bib61">Rigotti et al., 2013</xref>). An interesting prediction of the well-coded model, amenable to experimental testing, is that the representation within a memory channel must be in an optimized format, and that this format is not necessarily the same format that information was initially presented in. The brain would have to perform a transformation from stimulus-space into a well-coded form, and one might expect to observe this transition of the representation at encoding. (See, e.g., recent works (<xref ref-type="bibr" rid="bib54">Murray et al., 2017</xref>; <xref ref-type="bibr" rid="bib70">Spaak et al., 2017</xref>), which show the existence of complex and heterogeneous dynamic transformations in primate prefrontal cortex during working memory tasks.) The less orthogonal the original stimulus space is to noise during storage and the more optimized the code for storage to resist degradation, the more different the mnemonic code will be from the sample-evoked signal. Studies that attempt to decode a stimulus from delay-period neural or BOLD activity on the basis of tuning curves obtained from the stimulus-evoked period are well-suited to test this question (<xref ref-type="bibr" rid="bib85">Zarahn et al., 1999</xref>; <xref ref-type="bibr" rid="bib24">Courtney et al., 1997</xref>; <xref ref-type="bibr" rid="bib57">Pessoa et al., 2002</xref>; <xref ref-type="bibr" rid="bib37">Jha and McCarthy, 2000</xref>; <xref ref-type="bibr" rid="bib51">Miller et al., 1996</xref>; <xref ref-type="bibr" rid="bib5">Baeg et al., 2003</xref>; <xref ref-type="bibr" rid="bib49">Meyers et al., 2008</xref>; <xref ref-type="bibr" rid="bib72">Stokes et al., 2013</xref>) : If it is possible to use early stimulus-evoked responses to accurately decode the stimulus over the delay-period (<xref ref-type="bibr" rid="bib85">Zarahn et al., 1999</xref>; <xref ref-type="bibr" rid="bib24">Courtney et al., 1997</xref>; <xref ref-type="bibr" rid="bib57">Pessoa et al., 2002</xref>; <xref ref-type="bibr" rid="bib37">Jha and McCarthy, 2000</xref>; <xref ref-type="bibr" rid="bib51">Miller et al., 1996</xref>), it would suggest that information is not re-coded for noise resistance. On the other hand, a representation that is reshaped during the delay period relative to the stimulus-evoked response (<xref ref-type="bibr" rid="bib5">Baeg et al., 2003</xref>; <xref ref-type="bibr" rid="bib49">Meyers et al., 2008</xref>; <xref ref-type="bibr" rid="bib72">Stokes et al., 2013</xref>) might support the possibility of re-coding for storage.</p><p>On the other hand, the encoding and decoding steps for strong codes add considerable complexity to the storage task, and it is unclear whether these steps can be performed efficiently so that the efficiencies of these codes are not nullified by their costs. In light of our current results, it will be interesting to further probe with neurophysiological tools whether storage for short-term visual memory is consistent with strong neural codes. With psychophysics, it will be important to compare human performance and the information-theoretic bound in greater detail. On the theoretical side, studying the decoding complexity of exponential neural codes is a topic of ongoing work (<xref ref-type="bibr" rid="bib29">Fiete et al., 2014</xref>; <xref ref-type="bibr" rid="bib21">Chaudhuri and Fiete, 2015</xref>), where we find that non-sparse codes made up of a product of many constraints on small subsets of the codewords might be amenable to strong error correction through simple neural dynamics.</p></sec><sec id="s3-3"><title>Relationship to existing work and questions for the future</title><p>Compared to other information-theoretic considerations of memory (<xref ref-type="bibr" rid="bib16">Brady et al., 2009</xref>; <xref ref-type="bibr" rid="bib66">Sims et al., 2012</xref>), the distinguishing feature of our approach is our focus on neuron- or circuit-level noise and the fundamental limits such noise will impose on persistence.</p><p>Our theoretical framework permits the incorporation of many additional elements: Variable allocation of resources during stimulus presentation based on task complexity, perceived importance, attention, and information loading rate, may all be incorporated into the present framework. This can be achieved by modeling <inline-formula><mml:math id="inf229"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf230"><mml:mi>N</mml:mi></mml:math></inline-formula> as dependent functions (e.g. as done in [<xref ref-type="bibr" rid="bib77">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib66">Sims et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Elmore et al., 2011</xref>]) rather than independent parameters, and by exploiting the flexibility allowed by our model in uneven resource allocation across items in the display (Materials and methods).</p><p>The memory psychophysics literature contains evidence of more complex memory effects, including a type of response called âsudden deathâ or pure guessing (<xref ref-type="bibr" rid="bib88">Zhang and Luck, 2009</xref>; <xref ref-type="bibr" rid="bib3">Anderson et al., 2011</xref>). These responses are characterized by not being localized around the true value of the cued variable, and contribute a uniform or pedestal component to the response distribution. Other studies show that these apparent pedestals may not be a separate phenomenon and can, at least in some cases, be modeled by a simple growth in the variance over a bounded (circular) variable of a unimodal response distribution that remains centered at the cue location (<xref ref-type="bibr" rid="bib77">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib12">Bays, 2014</xref>; <xref ref-type="bibr" rid="bib44">Ma et al., 2014</xref>). In our framework, good encoding ensures that for noise below a threshold, the decoder can recover an improved estimate of the stored variable; however, strong codes exhibit sharp threshold behavior as the noise in the channel is varied smoothly. Once the noise per channel grows beyond the threshold, so-called catastrophic or threshold errors will occur, and the errors will become non-local: this phenomenon will look like sudden death in the memory report. In this sense, an optimal coding and decoding framework operating on top of continuously diffusing states in memory networks is consistent with the existence of sudden death or pure guessing-like responses, even without a distinct underlying mechanistic process in the memory networks themselves. We note, however, that the fits to the data shown here were all in the below-threshold regime.</p><p>Another complex effect in memory psychophysics is misbinding, in which one or more of the multiple features (color, orientation, size, etc.) of an item are mistakenly associated with those from another item. This work should be viewed as a model of single-feature memory. Very recently, there have been attempts to model misbinding (<xref ref-type="bibr" rid="bib48">Matthey et al., 2015</xref>). It may be possible to extend the present model in the direction of (<xref ref-type="bibr" rid="bib48">Matthey et al., 2015</xref>) by imagining the memory networks to be multi-dimensional attractors encoding multiple features of an item.</p><p>It will be important to understand whether in the direct coding model, modifications with plausible biological interpretations can lead to significantly better agreement with the data. From a purely curve-fitting perspective, the model requires stronger-than-linear improvement in recall accuracy with declining item number, and one might thus convert the combined resource parameter <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> into a function that varies inversely with <inline-formula><mml:math id="inf232"><mml:mi>K</mml:mi></mml:math></inline-formula>. This step would result in a better fit, but would correspond in the direct storage model to an <italic>increased</italic> allocation of total memory resources when the task involves <italic>fewer</italic> items, an implausible modification. Alternatively, if multiple items are stored within a single persistent activity network, collision effects can limit performance for larger item numbers (<xref ref-type="bibr" rid="bib81">Wei et al., 2012</xref>), but a quantitative result on performance as a function of delay time and item number remain to be worked out. Further examination of the types of data we have considered here, with respect to predictions that would result from a memory model dependent on direct storage of variables into persistent activity network(s), should help further the goal of linking short-term memory performance with neural network models of persistent activity.</p><p>Finally, note that our results stem from considering a specific hypothesis about the neural substrates of short-term memory (that memory is stored in a continuum of persistent activity states) and from the assumption that forgetting in short-term memory is undesirable but neural resources required to maintain information have a cost. It will also be interesting to consider the possibility of information storage in discrete rather than graded persistent activity states, with appropriate discretization of analog information before storage. Such storage networks will yield different bounds on memory performance than derived here (<xref ref-type="bibr" rid="bib40">Koulakov et al., 2002</xref>; <xref ref-type="bibr" rid="bib34">Goldman et al., 2003</xref>; <xref ref-type="bibr" rid="bib29">Fiete et al., 2014</xref>), which should include the existence of small analog errors arising from discretization at the encoding stage, with little degradation over time because of the resistance of discrete states to noise. Also of great interest is to obtain predictions about degradation of short-term memory in activity-silent mechanisms such as synaptic facilitation (<xref ref-type="bibr" rid="bib6">Barak and Tsodyks, 2014</xref>; <xref ref-type="bibr" rid="bib50">Mi et al., 2017</xref>; <xref ref-type="bibr" rid="bib73">Stokes, 2015</xref>; <xref ref-type="bibr" rid="bib43">Lundqvist et al., 2016</xref>). A distinct alternate perspective on the limited persistence of short-term memory is that forgetting is a design feature that continually clears the memory buffer for future use and that limited memory allows for optimal search and computation that favors generalization instead of overfitting (<xref ref-type="bibr" rid="bib26">Cowan, 2001</xref>). In this view, neural noise and resource constraints are not bottlenecks and there may be little imperative to optimize neural codes for greater persistence and capacity. To this end, it will be interesting to consider predictions from a theory in which limited memory is a feature, against the predictions we have presented here from the perspective that the neural system must work to avoid forgetting.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Human psychophysics experiments</title><p>Ten neurologically normal subjects (age range <inline-formula><mml:math id="inf233"><mml:mn>19</mml:mn></mml:math></inline-formula>-<inline-formula><mml:math id="inf234"><mml:mn>35</mml:mn></mml:math></inline-formula> yr) participated in the experiment after giving informed consent. All subjects reported normal or corrected-to-normal visual acuity. Stimuli were presented at a viewing distance of <inline-formula><mml:math id="inf235"><mml:mn>60</mml:mn></mml:math></inline-formula> cm on a <inline-formula><mml:math id="inf236"><mml:mn>21</mml:mn></mml:math></inline-formula>â CRT monitor. Each trial began with the presentation of a central fixation cross (white, <inline-formula><mml:math id="inf237"><mml:msup><mml:mpadded width="+1.7pt"><mml:mn>0.8</mml:mn></mml:mpadded><mml:mo>â</mml:mo></mml:msup></mml:math></inline-formula> diameter) for <inline-formula><mml:math id="inf238"><mml:mn>500</mml:mn></mml:math></inline-formula> milliseconds, followed by a memory array consisted of <inline-formula><mml:math id="inf239"><mml:mn>1</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf240"><mml:mn>2</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf241"><mml:mn>4</mml:mn></mml:math></inline-formula>, or <inline-formula><mml:math id="inf242"><mml:mn>6</mml:mn></mml:math></inline-formula> oriented bars (<inline-formula><mml:math id="inf243"><mml:msup><mml:mpadded width="+1.7pt"><mml:mn>2</mml:mn></mml:mpadded><mml:mo>â</mml:mo></mml:msup><mml:mo>Ã</mml:mo><mml:msup><mml:mpadded width="+1.7pt"><mml:mn>0.3</mml:mn></mml:mpadded><mml:mo>â</mml:mo></mml:msup></mml:math></inline-formula> of visual angle) presented on a grey background on an imaginary circle (radius <inline-formula><mml:math id="inf244"><mml:msup><mml:mpadded width="+1.7pt"><mml:mn>4.4</mml:mn></mml:mpadded><mml:mo>â</mml:mo></mml:msup></mml:math></inline-formula>) around fixation with equal inter-item distances (centre to centre). The colors of the bars in each trial were randomly selected out of eight easily-distinguishable colors. The stimulus display was followed by a blank delay of <inline-formula><mml:math id="inf245"><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf246"><mml:mn>3</mml:mn></mml:math></inline-formula> seconds and at the end of each sequence, recall for one of the items was tested by displaying a âprobeâ bar of the same color with a random orientation. Subjects were instructed to rotate the probe using a response dial (Logitech Intl. SA) to match the remembered orientation of the item of the same color in the sequence - henceforth termed the target. Each of the participants performed between <inline-formula><mml:math id="inf247"><mml:mn>11</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf248"><mml:mn>15</mml:mn></mml:math></inline-formula> blocks of <inline-formula><mml:math id="inf249"><mml:mn>80</mml:mn></mml:math></inline-formula> trials. Each block consisted of <inline-formula><mml:math id="inf250"><mml:mn>20</mml:mn></mml:math></inline-formula> trials for each of the <inline-formula><mml:math id="inf251"><mml:mn>4</mml:mn></mml:math></inline-formula> possible item numbers, consisting of <inline-formula><mml:math id="inf252"><mml:mn>5</mml:mn></mml:math></inline-formula> trials for each delay duration.</p></sec><sec id="s4-2"><title>Overview of theoretical framework and key steps</title><sec id="s4-2-1"><title>Channel coding and channel rate</title><p>Consider transmitting information about <inline-formula><mml:math id="inf253"><mml:mi>K</mml:mi></mml:math></inline-formula> scalar variables in the form of codewords of power 1 (i.e., <inline-formula><mml:math id="inf254"><mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf255"><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is the average power allocated to encode item <inline-formula><mml:math id="inf256"><mml:mi>k</mml:mi></mml:math></inline-formula>, with the average taken over <inline-formula><mml:math id="inf257"><mml:mi>N</mml:mi></mml:math></inline-formula> different channel uses, so that the average power actually used is <inline-formula><mml:math id="inf258"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>â¤</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The number of channel uses, <inline-formula><mml:math id="inf259"><mml:mi>N</mml:mi></mml:math></inline-formula>, is equivalent in our memory framework to the number of parallel memory channels, each of which introduces a Gaussian white noise of variance <inline-formula><mml:math id="inf260"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. The rate of growth of variance of the variable stored in persistent activity networks, <inline-formula><mml:math id="inf261"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula>, is derived in <xref ref-type="bibr" rid="bib19">Burak and Fiete (2012)</xref>; here, when we refer to this diffusivity, it is in dimensionless units where the variable is normalized by its range.</p><p>The information throughput (i.e., the information rate per channel use, also known as channel rate) for such channels is bounded by (see Appendix for details):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>â¡</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>â</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>â¤</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>â</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Â where <inline-formula><mml:math id="inf262"><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:math></inline-formula> refers to any subset of the the <inline-formula><mml:math id="inf263"><mml:mi>K</mml:mi></mml:math></inline-formula> items, <inline-formula><mml:math id="inf264"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> defines an entire region of information rates that are achievable: the total encoding power or the total channel rate, or both, may be allocated to a single item, or distributed across multiple items. Thus, the expression of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> is compatible with interpretations of memory as either a continuous or a discrete resource (<xref ref-type="bibr" rid="bib77">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib87">Zhang and Luck, 2008</xref>). (E.g., setting <inline-formula><mml:math id="inf265"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for any <inline-formula><mml:math id="inf266"><mml:mrow><mml:mi>k</mml:mi><mml:mo>â¥</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, would correspond to a <inline-formula><mml:math id="inf267"><mml:mn>4</mml:mn></mml:math></inline-formula>-slot conceptualization of short-term memory. Distributing <inline-formula><mml:math id="inf268"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for any variable number <inline-formula><mml:math id="inf269"><mml:mi>K</mml:mi></mml:math></inline-formula> of statistically similar items, would more closely describe a continuous resource model.) For both conceptualizations, this framework would allow us to consider, if the experiment setup warranted, different allocations of power <inline-formula><mml:math id="inf270"><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and information rates across the encoded items.</p><p>For the delayed orientation matching task considered here, all presented items have equal complexity and <italic>a priori</italic> importance, so the relevant case is <inline-formula><mml:math id="inf271"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf272"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, together with equal-rate allocation, <inline-formula><mml:math id="inf273"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, resulting in the following bound on per-item or per-feature information throughput in the noisy channel (see Appendix for more detail):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>â¤</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Next we consider how this bound on information rate in turn constrains the reconstruction error of the source variable (i.e., the <inline-formula><mml:math id="inf274"><mml:mi>K</mml:mi></mml:math></inline-formula>-variable vector to be memorized, <inline-formula><mml:math id="inf275"><mml:mover accent="true"><mml:mi>Ï</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover></mml:math></inline-formula>).</p></sec><sec id="s4-2-2"><title>Source coding and rate-distortion theory</title><p>At a source coder that compresses a source variable, rate-distortion theory relates the source rate to the distortion in reconstructing the source, at least for specific source distributions and specific error (distortion) metrics. For instance, if the source variables are each drawn uniformly from the interval <inline-formula><mml:math id="inf276"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, then the mean-squared error in reconstructing the source, <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, is related to the source rate <inline-formula><mml:math id="inf278"><mml:mi>R</mml:mi></mml:math></inline-formula> through the rate-distortion function (see Appendix):<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>â¤</mml:mo><mml:mi>R</mml:mi><mml:mo>â¤</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>12</mml:mn><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-2-3"><title>Joint source-channel coding</title><p>If the source rate is set to equal the maximal channel rate of <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, then use the expression of <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> from rate-distortion theory, we obtain the predicted bound on distortion in the source variable after source coding and channel transmission. This predicted distortion bound is given in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. In general problems of information transmission through an noisy channel, it is not necessarily jointly optimal to separately derive the optimal channel rate and the optimal distortion for a given source rate, and then to set the source rate to equal the maximal channel rate; the total distortion of the source passed through the channel need not be lower-bounded by the resulting expression. However, in our case of interest the two-step procedure described above, deriving first the channel capacity then inserting the capacity into the rate-distortion equation, yields a tight bound on distortion for the memory framework.</p><p>This concludes the basic derivation, in outline form, of the main theoretical result of the manuscript. The Supplementary Information supplies more steps and detail.</p></sec></sec><sec id="s4-3"><title>Fitting of theory to data</title><p>In all fits of theory to data (for direct and well-coded storage), we assume that recall error at the shortest storage interval of 100 ms reflects <italic>baseline errors</italic> unrelated to the temporal loss of recall accuracy from noisy storage that is the focus of the present work. Under the assumption that this early (âinitialâ) error is independent of the additional errors accrued over the storage period, it is appropriate to treat the baseline (<inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms) MSE as an additive contribution to the rest of the MSE (the variance of the sum of independent random variables is the sum of their variances). For this reason, we are justified in treating the <inline-formula><mml:math id="inf280"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms errors as given by the data and setting these points as the initial offsets of the theory curves, which go on to explain the temporal (item-dependent) degradation of information placed in noisy storage.</p><p>The curves are fit by minimizing the summed weighted squared error of the theoretical prediction in fitting the subject-averaged performance data over all item numbers and storage durations. The theoretical predictions are given by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> for direct storage and <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> for well-coded storage. The weights in the weighted least-squares are the inverse SEMs for each (item, storage duration) pair. The parameters of the fit are <inline-formula><mml:math id="inf281"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> (direct storage model) or <inline-formula><mml:math id="inf282"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf283"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> (well-coded model). The parameter value selected is common across all item numbers and storage durations. The <inline-formula><mml:math id="inf284"><mml:mi>p</mml:mi></mml:math></inline-formula> values given in the main paper quantify how likely the data means are to have been based on samples from a Gaussian distribution centered on the theoretical prediction.</p></sec><sec id="s4-4"><title>Model comparison with the bayesian information criterion</title><p>The Bayesian Information Criterion (BIC) is a likelihood-based method for model comparison, with a penalty term that takes into account the number of parameters used in the candidate models. BIC is a Bayesian model comparison method, as discussed in <xref ref-type="bibr" rid="bib39">Kass and Raftery (1995)</xref></p><p>Given data <inline-formula><mml:math id="inf285"><mml:mi>x</mml:mi></mml:math></inline-formula> that are (assumed to be) drawn from a distribution in the exponential family and a model <inline-formula><mml:math id="inf286"><mml:mrow><mml:mi>M</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with associated parameters <inline-formula><mml:math id="inf287"><mml:mover accent="true"><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover></mml:math></inline-formula> (<inline-formula><mml:math id="inf288"><mml:mover accent="true"><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover></mml:math></inline-formula> is a vector of <inline-formula><mml:math id="inf289"><mml:mi>k</mml:mi></mml:math></inline-formula> parameters), the BIC is given by:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>L</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf290"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of observations, and <inline-formula><mml:math id="inf291"><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is the likelihood of the model (with parameters <inline-formula><mml:math id="inf292"><mml:mover accent="true"><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover></mml:math></inline-formula> selected by maximum likelihood). The smaller the BIC, the better the model. The more positive the difference<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>between a pair of models <inline-formula><mml:math id="inf293"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf294"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (with associated parameters <inline-formula><mml:math id="inf295"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Î¸</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively, possibly of different dimensions <inline-formula><mml:math id="inf296"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â </mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), the stronger the evidence for <inline-formula><mml:math id="inf297"><mml:msub><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>To obtain the BIC for the direct and coded models, the model distributions are taken to be Gaussians whose means (for each item and delay) are given by the theoretical results of <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>, respectively, and whose variance is given by the empirically measured data variance across trials and subjects, computed separately per item and delay. We used the parameters <inline-formula><mml:math id="inf298"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>2.28</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for the well-coded storage model, and <inline-formula><mml:math id="inf299"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>3.24</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the direct storage model, to obtain <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>172.67</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The empirical response variance is computed over each trial for each subjects, for a total of <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>660</mml:mn></mml:mrow></mml:math></inline-formula> observations for each <inline-formula><mml:math id="inf302"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> or (delay interval, item number) pair. The number of parameters is <inline-formula><mml:math id="inf303"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for direct storage and <inline-formula><mml:math id="inf304"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> for well-coded storage. Setting the parameter numbers to <inline-formula><mml:math id="inf305"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf306"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to take into account the 4 values of response errors at the shortest delay at <inline-formula><mml:math id="inf307"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms does not change the <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> score because the score is dominated by the likelihood term, so that these changes in the parameter penalty term have negligible effect.</p></sec></sec></body><back><sec id="s6" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Writingâoriginal draft, Writingâreview and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Writingâreview and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Writingâreview and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Writingâreview and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Software, Writingâoriginal draft, Writingâreview and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study reported here conform to the Declaration of Helsinki and all procedures were approved by the ethics committee of the National Hospital for Neurology and Neurosurgery (NHNN) prior to the study commencing. Research Ethics Committee number (ERC) 04/Q0406/60. Personal information about individuals was password protected and saved in compliance to the Data Protection Act 1998 (DPA).</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.22225.014</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-22225-transrepform-v2.pdf"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aksay</surname> <given-names>E</given-names></name><name><surname>Gamkrelidze</surname> <given-names>G</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Baker</surname> <given-names>R</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>In vivo intracellular recording and perturbation of persistent activity in a neural integrator</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>184</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1038/84023</pub-id><pub-id pub-id-type="pmid">11175880</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amit</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Modeling brain function: The world of attractor neural networks</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>DE</given-names></name><name><surname>Vogel</surname> <given-names>EK</given-names></name><name><surname>Awh</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Precision in visual working memory reaches a stable plateau when individual item limits are exceeded</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1128</fpage><lpage>1138</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4125-10.2011</pub-id><pub-id pub-id-type="pmid">21248137</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atkinson</surname> <given-names>R</given-names></name><name><surname>Shiffrin</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Human memory: A proposed system and its control processes</article-title><source>The Psychology of Learning and Motivation</source><volume>2</volume><fpage>89</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1016/S0079-7421(08)60422-3</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baeg</surname> <given-names>EH</given-names></name><name><surname>Kim</surname> <given-names>YB</given-names></name><name><surname>Huh</surname> <given-names>K</given-names></name><name><surname>Mook-Jung</surname> <given-names>I</given-names></name><name><surname>Kim</surname> <given-names>HT</given-names></name><name><surname>Jung</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dynamics of population code for working memory in the prefrontal cortex</article-title><source>Neuron</source><volume>40</volume><fpage>177</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00597-X</pub-id><pub-id pub-id-type="pmid">14527442</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Tsodyks</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Working models of working memory</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>20</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.10.008</pub-id><pub-id pub-id-type="pmid">24709596</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrouillet</surname> <given-names>P</given-names></name><name><surname>De Paepe</surname> <given-names>A</given-names></name><name><surname>Langerock</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Time causes forgetting from working memory</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>19</volume><fpage>87</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.3758/s13423-011-0192-8</pub-id><pub-id pub-id-type="pmid">22184034</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrouillet</surname> <given-names>P</given-names></name><name><surname>Gavens</surname> <given-names>N</given-names></name><name><surname>Vergauwe</surname> <given-names>E</given-names></name><name><surname>Gaillard</surname> <given-names>V</given-names></name><name><surname>Camos</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Working memory span development: a time-based resource-sharing model account</article-title><source>Developmental Psychology</source><volume>45</volume><fpage>477</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1037/a0014615</pub-id><pub-id pub-id-type="pmid">19271832</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrouillet</surname> <given-names>P</given-names></name><name><surname>Portrat</surname> <given-names>S</given-names></name><name><surname>Vergauwe</surname> <given-names>E</given-names></name><name><surname>Diependaele</surname> <given-names>K</given-names></name><name><surname>Camos</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Further evidence for temporal decay in working memory: reply to Lewandowsky and Oberauer (2009)</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>37</volume><fpage>1302</fpage><lpage>1317</lpage><pub-id pub-id-type="doi">10.1037/a0022933</pub-id><pub-id pub-id-type="pmid">21895395</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname> <given-names>PM</given-names></name><name><surname>Gorgoraptis</surname> <given-names>N</given-names></name><name><surname>Wee</surname> <given-names>N</given-names></name><name><surname>Marshall</surname> <given-names>L</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Temporal dynamics of encoding, storage, and reallocation of visual working memory</article-title><source>Journal of Vision</source><volume>11</volume><fpage>6</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1167/11.10.6</pub-id><pub-id pub-id-type="pmid">21911739</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname> <given-names>PM</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic shifts of limited working memory resources in human vision</article-title><source>Science</source><volume>321</volume><fpage>851</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1126/science.1158023</pub-id><pub-id pub-id-type="pmid">18687968</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Noise in neural populations accounts for errors in working memory</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>3632</fpage><lpage>3645</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3204-13.2014</pub-id><pub-id pub-id-type="pmid">24599462</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yishai</surname> <given-names>R</given-names></name><name><surname>Bar-Or</surname> <given-names>RL</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Theory of orientation tuning in visual cortex</article-title><source>PNAS</source><volume>92</volume><fpage>3844</fpage><lpage>3848</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.9.3844</pub-id><pub-id pub-id-type="pmid">7731993</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blair</surname> <given-names>HT</given-names></name><name><surname>Sharp</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Anticipatory head direction signals in anterior thalamus: evidence for a thalamocortical circuit that integrates angular head motion to compute head direction</article-title><source>Journal of Neuroscience</source><volume>15</volume><fpage>6260</fpage><lpage>6270</lpage><pub-id pub-id-type="pmid">7666208</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boucheny</surname> <given-names>C</given-names></name><name><surname>Brunel</surname> <given-names>N</given-names></name><name><surname>Arleo</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A continuous attractor network model without recurrent excitation: maintenance and integration in the head direction cell system</article-title><source>Journal of Computational Neuroscience</source><volume>18</volume><fpage>205</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1007/s10827-005-6559-y</pub-id><pub-id pub-id-type="pmid">15714270</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname> <given-names>TF</given-names></name><name><surname>Konkle</surname> <given-names>T</given-names></name><name><surname>Alvarez</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Compression in visual working memory: using statistical regularities to form more efficient memory representations</article-title><source>Journal of Experimental Psychology: General</source><volume>138</volume><fpage>487</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1037/a0016797</pub-id><pub-id pub-id-type="pmid">19883132</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brody</surname> <given-names>CD</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Basic mechanisms for graded persistent activity: discrete attractors, continuous attractors, and dynamic representations</article-title><source>Current Opinion in Neurobiology</source><volume>13</volume><fpage>204</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(03)00050-3</pub-id><pub-id pub-id-type="pmid">12744975</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname> <given-names>Y</given-names></name><name><surname>Fiete</surname> <given-names>IR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate path integration in continuous attractor network models of grid cells</article-title><source>PLoS Computational Biology</source><volume>5</volume><elocation-id>e1000291</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000291</pub-id><pub-id pub-id-type="pmid">19229307</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname> <given-names>Y</given-names></name><name><surname>Fiete</surname> <given-names>IR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fundamental limits on persistent activity in networks of noisy neurons</article-title><source>PNAS</source><volume>109</volume><fpage>17645</fpage><lpage>17650</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117386109</pub-id><pub-id pub-id-type="pmid">23047704</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campoy</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Evidence for decay in verbal short-term memory: a commentary on Berman, Jonides, and Lewis (2009)</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>38</volume><fpage>1129</fpage><lpage>1136</lpage><pub-id pub-id-type="doi">10.1037/a0026934</pub-id><pub-id pub-id-type="pmid">22746956</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chaudhuri</surname> <given-names>R</given-names></name><name><surname>Fiete</surname> <given-names>IR</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Using expander codes to construct Hopfield networks with exponential capacity</chapter-title><source>CoSyNe Meeting Abstract II-78</source><publisher-loc>Salt Lake City, UT, USA</publisher-loc></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Compte</surname> <given-names>A</given-names></name><name><surname>Brunel</surname> <given-names>N</given-names></name><name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>910</fpage><lpage>923</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.9.910</pub-id><pub-id pub-id-type="pmid">10982751</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conway</surname> <given-names>AR</given-names></name><name><surname>Kane</surname> <given-names>MJ</given-names></name><name><surname>Engle</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Working memory capacity and its relation to general intelligence</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>547</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2003.10.005</pub-id><pub-id pub-id-type="pmid">14643371</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Courtney</surname> <given-names>SM</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Keil</surname> <given-names>K</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Transient and sustained activity in a distributed neural system for human working memory</article-title><source>Nature</source><volume>386</volume><fpage>608</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1038/386608a0</pub-id><pub-id pub-id-type="pmid">9121584</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cover</surname> <given-names>T</given-names></name><name><surname>Thomas</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1991">1991</year><source>Elements of Information Theory</source><publisher-name>John Wiley and Sons, Inc</publisher-name><pub-id pub-id-type="doi">10.1002/0471200611</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The magical number 4 in short-term memory: a reconsideration of mental storage capacity</article-title><source>Behavioral and Brain Sciences</source><volume>24</volume><fpage>87</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1017/S0140525X01003922</pub-id><pub-id pub-id-type="pmid">11515286</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elmore</surname> <given-names>LC</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Magnotti</surname> <given-names>JF</given-names></name><name><surname>Leising</surname> <given-names>KJ</given-names></name><name><surname>Passaro</surname> <given-names>AD</given-names></name><name><surname>Katz</surname> <given-names>JS</given-names></name><name><surname>Wright</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual short-term memory compared in rhesus monkeys and humans</article-title><source>Current Biology</source><volume>21</volume><fpage>975</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.04.031</pub-id><pub-id pub-id-type="pmid">21596568</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname> <given-names>IR</given-names></name><name><surname>Burak</surname> <given-names>Y</given-names></name><name><surname>Brookings</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>What grid cells convey about rat location</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>6858</fpage><lpage>6871</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5684-07.2008</pub-id><pub-id pub-id-type="pmid">18596161</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fiete</surname> <given-names>IR</given-names></name><name><surname>Schwab</surname> <given-names>DS</given-names></name><name><surname>Tran</surname> <given-names>NM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A binary Hopfield network with information rate and applications to grid cell decoding</article-title><conf-name><italic>Proceedings of the 2nd Workshop on Biological Distributed Algorithms</italic></conf-name><publisher-loc>Austin, TX, USA</publisher-loc></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funahashi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Prefrontal cortex and working memory processes</article-title><source>Neuroscience</source><volume>139</volume><fpage>251</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2005.07.003</pub-id><pub-id pub-id-type="pmid">16325345</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fung</surname> <given-names>CC</given-names></name><name><surname>Wong</surname> <given-names>KY</given-names></name><name><surname>Wu</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A moving bump in a continuous manifold: a comprehensive study of the tracking dynamics of continuous attractor neural networks</article-title><source>Neural Computation</source><volume>22</volume><fpage>752</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1162/neco.2009.07-08-824</pub-id><pub-id pub-id-type="pmid">19922292</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname> <given-names>JM</given-names></name><name><surname>Alexander</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Neuron activity related to short-term memory</article-title><source>Science</source><volume>173</volume><fpage>652</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1126/science.173.3997.652</pub-id><pub-id pub-id-type="pmid">4998337</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname> <given-names>JM</given-names></name><name><surname>Jervey</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Inferotemporal neurons distinguish and retain behaviorally relevant features of visual stimuli</article-title><source>Science</source><volume>212</volume><fpage>952</fpage><lpage>955</lpage><pub-id pub-id-type="doi">10.1126/science.7233192</pub-id><pub-id pub-id-type="pmid">7233192</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname> <given-names>MS</given-names></name><name><surname>Levine</surname> <given-names>JH</given-names></name><name><surname>Major</surname> <given-names>G</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Robust persistent neural activity in a model integrator with multiple hysteretic dendrites per neuron</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1185</fpage><lpage>1195</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg095</pub-id><pub-id pub-id-type="pmid">14576210</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname> <given-names>T</given-names></name><name><surname>Fyhn</surname> <given-names>M</given-names></name><name><surname>Molden</surname> <given-names>S</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname> <given-names>SA</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id><pub-id pub-id-type="pmid">19225460</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname> <given-names>AP</given-names></name><name><surname>McCarthy</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Prefrontal Activity during Delayed-response Tasks Requiring Response Selection and Preparation</article-title><source>Proceedings of Cognitive Neuroscience Society</source></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonides</surname> <given-names>J</given-names></name><name><surname>Lewis</surname> <given-names>RL</given-names></name><name><surname>Nee</surname> <given-names>DE</given-names></name><name><surname>Lustig</surname> <given-names>CA</given-names></name><name><surname>Berman</surname> <given-names>MG</given-names></name><name><surname>Moore</surname> <given-names>KS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The mind and brain of short-term memory</article-title><source>Annual Review of Psychology</source><volume>59</volume><fpage>193</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.59.103006.093615</pub-id><pub-id pub-id-type="pmid">17854286</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname> <given-names>RE</given-names></name><name><surname>Raftery</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayes Factors</article-title><source>Journal of the American Statistical Association</source><volume>90</volume><fpage>773</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koulakov</surname> <given-names>AA</given-names></name><name><surname>Raghavachari</surname> <given-names>S</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name><name><surname>Lisman</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Model for a robust neural integrator</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>775</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1038/nn893</pub-id><pub-id pub-id-type="pmid">12134153</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewandowsky</surname> <given-names>S</given-names></name><name><surname>Oberauer</surname> <given-names>K</given-names></name><name><surname>Brown</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>No temporal decay in verbal short-term memory</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>120</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.12.003</pub-id><pub-id pub-id-type="pmid">19223224</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Vogel</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The capacity of visual working memory for features and conjunctions</article-title><source>Nature</source><volume>390</volume><fpage>279</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1038/36846</pub-id><pub-id pub-id-type="pmid">9384378</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundqvist</surname> <given-names>M</given-names></name><name><surname>Rose</surname> <given-names>J</given-names></name><name><surname>Herman</surname> <given-names>P</given-names></name><name><surname>Brincat</surname> <given-names>SL</given-names></name><name><surname>Buschman</surname> <given-names>TJ</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Gamma and Beta Bursts Underlie Working Memory</article-title><source>Neuron</source><volume>90</volume><fpage>152</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.028</pub-id><pub-id pub-id-type="pmid">26996084</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name><name><surname>Bays</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Changing concepts of working memory</article-title><source>Nature neuroscience</source><volume>17</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1038/nn.3655</pub-id><pub-id pub-id-type="pmid">24569831</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname> <given-names>DJC</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Information Theory, Inference &amp; Learning Algorithms</source><publisher-loc>New York</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Herz</surname> <given-names>AV</given-names></name><name><surname>Stemmler</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Resolution of nested neuronal representations can be exponential in the number of neurons</article-title><source>Physical Review Letters</source><volume>109</volume><elocation-id>018103</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.109.018103</pub-id><pub-id pub-id-type="pmid">23031134</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsukura</surname> <given-names>M</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name><name><surname>Vecera</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Attention effects during visual short-term memory maintenance: Protection or prioritization?</article-title><source>Perception &amp; Psychophysics</source><volume>69</volume><fpage>1422</fpage><lpage>1434</lpage><pub-id pub-id-type="doi">10.3758/BF03192957</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthey</surname> <given-names>L</given-names></name><name><surname>Bays</surname> <given-names>PM</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A probabilistic palimpsest model of visual short-term memory</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004003</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004003</pub-id><pub-id pub-id-type="pmid">25611204</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic population coding of category information in inferior temporal and prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>1407</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1152/jn.90248.2008</pub-id><pub-id pub-id-type="pmid">18562555</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mi</surname> <given-names>Y</given-names></name><name><surname>Katkov</surname> <given-names>M</given-names></name><name><surname>Tsodyks</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Synaptic Correlates of Working Memory Capacity</article-title><source>Neuron</source><volume>93</volume><fpage>323</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.004</pub-id><pub-id pub-id-type="pmid">28041884</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Erickson</surname> <given-names>CA</given-names></name><name><surname>Desimone</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neural mechanisms of visual working memory in prefrontal cortex of the macaque</article-title><source>Journal of Neuroscience</source><volume>16</volume><fpage>5154</fpage><lpage>5167</lpage><pub-id pub-id-type="pmid">8756444</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="1956">1956</year><article-title>The magical number seven plus or minus two: some limits on our capacity for processing information</article-title><source>Psychological Review</source><volume>63</volume><fpage>81</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1037/h0043158</pub-id><pub-id pub-id-type="pmid">13310704</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname> <given-names>G</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Tsodyks</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Synaptic theory of working memory</article-title><source>Science</source><volume>319</volume><fpage>1543</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1126/science.1150769</pub-id><pub-id pub-id-type="pmid">18339943</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>JD</given-names></name><name><surname>Bernacchia</surname> <given-names>A</given-names></name><name><surname>Roy</surname> <given-names>NA</given-names></name><name><surname>Constantinidis</surname> <given-names>C</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex</article-title><source>PNAS</source><volume>114</volume><fpage>394</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1073/pnas.1619449114</pub-id><pub-id pub-id-type="pmid">28028221</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pertzov</surname> <given-names>Y</given-names></name><name><surname>Bays</surname> <given-names>PM</given-names></name><name><surname>Joseph</surname> <given-names>S</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rapid forgetting prevented by retrospective attention cues</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>39</volume><fpage>1224</fpage><lpage>1231</lpage><pub-id pub-id-type="doi">10.1037/a0030947</pub-id><pub-id pub-id-type="pmid">23244045</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pertzov</surname> <given-names>Y</given-names></name><name><surname>Manohar</surname> <given-names>S</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Rapid forgetting results from competition over time between items in visual working memory</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>43</volume><fpage>528</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1037/xlm0000328</pub-id><pub-id pub-id-type="pmid">27668485</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessoa</surname> <given-names>L</given-names></name><name><surname>Gutierrez</surname> <given-names>E</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name><name><surname>Ungerleider</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neural correlates of visual working memory: fMRI amplitude predicts task performance</article-title><source>Neuron</source><volume>35</volume><fpage>975</fpage><lpage>987</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00817-6</pub-id><pub-id pub-id-type="pmid">12372290</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polyanskiy</surname> <given-names>Y</given-names></name><name><surname>Poor</surname> <given-names>HV</given-names></name><name><surname>Verdu</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Channel coding rate in the finite blocklength regime</article-title><source>IEEE Transactions on Information Theory</source><volume>56</volume><fpage>2307</fpage><lpage>2359</lpage><pub-id pub-id-type="doi">10.1109/TIT.2010.2043769</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raginsky</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>On the information capacity of gaussian channels under small peak power constraints</article-title><source>IEEE</source><pub-id pub-id-type="doi">10.1109/ALLERTON.2008.4797569</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricker</surname> <given-names>TJ</given-names></name><name><surname>Cowan</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Differences between presentation methods in working memory procedures: a matter of working memory consolidation</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>40</volume><fpage>417</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1037/a0034301</pub-id><pub-id pub-id-type="pmid">24059859</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname> <given-names>M</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Warden</surname> <given-names>MR</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name><name><surname>HernÃ¡ndez</surname> <given-names>A</given-names></name><name><surname>Lemus</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title><source>Nature</source><volume>399</volume><fpage>470</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1038/20939</pub-id><pub-id pub-id-type="pmid">10365959</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Noise, neural codes and cortical organization</article-title><source>Current Opinion in Neurobiology</source><volume>4</volume><fpage>569</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(94)90059-0</pub-id><pub-id pub-id-type="pmid">7812147</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A Mathematical Theory of Communication</article-title><source>Bell System Technical Journal</source><volume>27</volume><fpage>379</fpage><fpage>623</fpage><lpage>423</lpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Coding theorems for a discrete source with a fidelity criterion</article-title><source>Institute of Radio Engineers, International Convention Record, part 4</source><volume>7</volume><fpage>142</fpage><lpage>163</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sims</surname> <given-names>CR</given-names></name><name><surname>Jacobs</surname> <given-names>RA</given-names></name><name><surname>Knill</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An ideal observer analysis of visual working memory</article-title><source>Psychological Review</source><volume>119</volume><fpage>807</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1037/a0029856</pub-id><pub-id pub-id-type="pmid">22946744</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>EE</given-names></name><name><surname>Jonides</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neuroimaging analyses of human working memory</article-title><source>PNAS</source><volume>95</volume><fpage>12061</fpage><lpage>12068</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.20.12061</pub-id><pub-id pub-id-type="pmid">9751790</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>JG</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The information capacity of amplitude- and variance-constrained sclar gaussian channels</article-title><source>Information and Control</source><volume>18</volume><fpage>203</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1016/S0019-9958(71)90346-9</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Softky</surname> <given-names>WR</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The highly irregular firing of cortical cells is inconsistent with temporal integration of random EPSPs</article-title><source>Journal of Neuroscience</source><volume>13</volume><fpage>334</fpage><lpage>350</lpage><pub-id pub-id-type="pmid">8423479</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaak</surname> <given-names>E</given-names></name><name><surname>Watanabe</surname> <given-names>K</given-names></name><name><surname>Funahashi</surname> <given-names>S</given-names></name><name><surname>Stokes</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stable and Dynamic Coding for Working Memory in Primate Prefrontal Cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6503</fpage><lpage>6516</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3364-16.2017</pub-id><pub-id pub-id-type="pmid">28559375</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sreenivasan</surname> <given-names>S</given-names></name><name><surname>Fiete</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Grid cells generate an analog error-correcting code for singularly precise neural computation</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1330</fpage><lpage>1337</lpage><pub-id pub-id-type="doi">10.1038/nn.2901</pub-id><pub-id pub-id-type="pmid">21909090</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>MG</given-names></name><name><surname>Kusunoki</surname> <given-names>M</given-names></name><name><surname>Sigala</surname> <given-names>N</given-names></name><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>'Activity-silent' working memory in prefrontal cortex: a dynamic coding framework</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>394</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.05.004</pub-id><pub-id pub-id-type="pmid">26051384</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>SupÃ¨r</surname> <given-names>H</given-names></name><name><surname>Spekreijse</surname> <given-names>H</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A neural correlate of working memory in the monkey primary visual cortex</article-title><source>Science</source><volume>293</volume><fpage>120</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1126/science.1060496</pub-id><pub-id pub-id-type="pmid">11441187</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname> <given-names>H</given-names></name><name><surname>Kung Yao</surname></name> <name><surname>Yao</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Evaluation of rate-distortion functions for a class of independent identically distributed sources under an absolute-magnitude criterion</article-title><source>IEEE Transactions on Information Theory</source><volume>21</volume><fpage>59</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1109/TIT.1975.1055335</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Head direction cells and the neurophysiological basis for a sense of direction</article-title><source>Progress in Neurobiology</source><volume>55</volume><fpage>225</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/S0301-0082(98)00004-5</pub-id><pub-id pub-id-type="pmid">9643555</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname> <given-names>R</given-names></name><name><surname>Shin</surname> <given-names>H</given-names></name><name><surname>Chou</surname> <given-names>WC</given-names></name><name><surname>George</surname> <given-names>R</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Variability in encoding precision accounts for visual short-term memory limitations</article-title><source>PNAS</source><volume>109</volume><fpage>8780</fpage><lpage>8785</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117465109</pub-id><pub-id pub-id-type="pmid">22582168</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vembu</surname> <given-names>S</given-names></name><name><surname>Verdu</surname> <given-names>S</given-names></name><name><surname>Steinberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The source-channel separation theorem revisited</article-title><source>IEEE Transactions on Information Theory</source><volume>41</volume><fpage>44</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1109/18.370119</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Viterbi</surname> <given-names>AJ</given-names></name><name><surname>Omura</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="1979">1979</year><source>Principles of digital communication and coding</source><publisher-name>McGraw-Hill</publisher-name></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01868-3</pub-id><pub-id pub-id-type="pmid">11476885</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname> <given-names>Z</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Wang</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>From distributed resources to limited slots in multiple-item working memory: a spiking network model with normalization</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>11228</fpage><lpage>11240</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0735-12.2012</pub-id><pub-id pub-id-type="pmid">22895707</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilken</surname> <given-names>P</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A detection theory account of change detection</article-title><source>Journal of Vision</source><volume>4</volume><fpage>11</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1167/4.12.11</pub-id><pub-id pub-id-type="pmid">15669916</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname> <given-names>K</given-names></name><name><surname>Nykamp</surname> <given-names>DQ</given-names></name><name><surname>Constantinidis</surname> <given-names>C</given-names></name><name><surname>Compte</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1038/nn.3645</pub-id><pub-id pub-id-type="pmid">24487232</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>S</given-names></name><name><surname>Hamaguchi</surname> <given-names>K</given-names></name><name><surname>Amari</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamics and computation of continuous attractors</article-title><source>Neural Computation</source><volume>20</volume><fpage>994</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.10-06-378</pub-id><pub-id pub-id-type="pmid">18085986</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zarahn</surname> <given-names>E</given-names></name><name><surname>Aguirre</surname> <given-names>GK</given-names></name><name><surname>D'Esposito</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Temporal isolation of the neural correlates of spatial mnemonic processing with fMRI</article-title><source>Cognitive Brain Research</source><volume>7</volume><fpage>255</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(98)00029-9</pub-id><pub-id pub-id-type="pmid">9838152</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title><source>Journal of Neuroscience</source><volume>16</volume><fpage>2112</fpage><lpage>2126</lpage><pub-id pub-id-type="pmid">8604055</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Discrete fixed-resolution representations in visual working memory</article-title><source>Nature</source><volume>453</volume><fpage>233</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1038/nature06860</pub-id><pub-id pub-id-type="pmid">18385672</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Luck</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Sudden death and gradual decay in visual working memory</article-title><source>Psychological Science</source><volume>20</volume><fpage>423</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02322.x</pub-id><pub-id pub-id-type="pmid">19320861</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix</title><sec id="s7" sec-type="appendix"><title>Joint source-channel coding and memory: justification and main results</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.22225.015</object-id><sec id="s7-1"><title>Noisy information channels as a component of short-term memory systems</title><p>Noisy information channels have traditionally been used to model communication systems: in satellite or cell-phone communications, the transmitted information is degraded during passage from one point to another (<xref ref-type="bibr" rid="bib65">Shannon, 1959</xref>; <xref ref-type="bibr" rid="bib80">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>). Such transmission and degradation over space is referred to as a channel use. However, noisy channels are apt descriptors of any system in which information is put in to be accessed at a different place or a different time, with loss occurring in-between (<xref ref-type="bibr" rid="bib65">Shannon, 1959</xref>; <xref ref-type="bibr" rid="bib80">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>). Thus, hard drives are channels, with the main channel noise being the probability of random bit flips (from high-energy cosmic rays). Similarly, neural short-term memory systems store information and are subject to unavoidable loss because of the stochasticity of neural spiking and synaptic activation. In this sense, noise-induced loss in persistent activity networks is like passing the stored information through a noisy channel.</p></sec><sec id="s7-2"><title>Channel coding</title><p>In channel coding, a message is first encoded to add redundancy, then transmitted through the noisy channel, and finally decoded at the decoder. Here, we establish the terminology and basic results from Shannonâs noisy channel coding theory (<xref ref-type="bibr" rid="bib65">Shannon, 1959</xref>; <xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>), which are used in the main paper.</p><p>First, consider a task that involves storing or communicating a simple message, <inline-formula><mml:math id="inf309"><mml:mi>q</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf310"><mml:mi>q</mml:mi></mml:math></inline-formula> is a uniformly distributed index taking one of <inline-formula><mml:math id="inf311"><mml:mi>Q</mml:mi></mml:math></inline-formula> values: <inline-formula><mml:math id="inf312"><mml:mrow><mml:mi>q</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The message <inline-formula><mml:math id="inf313"><mml:mi>q</mml:mi></mml:math></inline-formula> is encoded according to a deterministic vector function (an encoding function), to generate the <inline-formula><mml:math id="inf314"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector <inline-formula><mml:math id="inf315"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig1">Figure 1</xref>. This is the <italic>channel-coding</italic> step. The codeword <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is redundant, is sent through the noisy channel, which produces an output <inline-formula><mml:math id="inf317"><mml:mi mathvariant="bold">ð²</mml:mi></mml:math></inline-formula> according to some conditional distribution <inline-formula><mml:math id="inf318"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð²</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf319"><mml:mi mathvariant="bold">ð²</mml:mi></mml:math></inline-formula> is an <inline-formula><mml:math id="inf320"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector; the channel is specified by the distribution <inline-formula><mml:math id="inf321"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð²</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). In a memoryless channel (no feedback from the decoder at the end of the channel back to the encoder at the mouth of the channel), the channel obeys<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð²</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where all distributions <inline-formula><mml:math id="inf322"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent an identical distribution that defines the channel (<xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>). In this setup, transmission of the scalar source variable <inline-formula><mml:math id="inf323"><mml:mi>q</mml:mi></mml:math></inline-formula> involves <inline-formula><mml:math id="inf324"><mml:mi>N</mml:mi></mml:math></inline-formula> independent channel uses.</p><p>The decoder constructs a mapping <inline-formula><mml:math id="inf325"><mml:mrow><mml:mi mathvariant="bold">ð²</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, to make an estimate <inline-formula><mml:math id="inf326"><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> of the received message from the channel outputs <inline-formula><mml:math id="inf327"><mml:mi mathvariant="bold">ð²</mml:mi></mml:math></inline-formula>. If <inline-formula><mml:math id="inf328"><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>â </mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula>, the decoder has made an error. The error probability is the probability that <inline-formula><mml:math id="inf329"><mml:mi>q</mml:mi></mml:math></inline-formula> is decoded incorrectly, averaged over all <inline-formula><mml:math id="inf330"><mml:mi>q</mml:mi></mml:math></inline-formula>. This scenario, in which <inline-formula><mml:math id="inf331"><mml:mi>q</mml:mi></mml:math></inline-formula>, which is a single number (and represents one of the messages to be communicated) and the decoder receives a single number (observation) from each channel use, is referred to as point-to-point communication (<xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>).</p><p>If the decoder can correctly decode <inline-formula><mml:math id="inf332"><mml:mi>q</mml:mi></mml:math></inline-formula>, the channel communication rate (also known as the rate per channel use), which quantifies how many information bits (about <inline-formula><mml:math id="inf333"><mml:mi>q</mml:mi></mml:math></inline-formula>) are transmitted per entry of the coded message <inline-formula><mml:math id="inf334"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula>, is given by <inline-formula><mml:math id="inf335"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Shannon showed in his noisy channel coding theorem (<xref ref-type="bibr" rid="bib65">Shannon, 1959</xref>; <xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>) that for any channel, in the limit <inline-formula><mml:math id="inf336"><mml:mrow><mml:mi>N</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="normal">â</mml:mi></mml:mrow></mml:math></inline-formula>, it is possible in principle to communicate error-free through the channel at any rate up to the <italic>channel capacity</italic> <inline-formula><mml:math id="inf337"><mml:mi>C</mml:mi></mml:math></inline-formula>, defined by:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>â¡</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:mo>â¢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mrow><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">ð²</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For specific channels, it is possible to explicitly compute the channel capacity in terms of interesting parameters of the channel model and encoder; below, we will state such results for our channels of interest, for subsequent use in our theoretical analysis.</p><sec id="s7-2-1"><title>Point-to-point Gaussian channel with a power constraint</title><p>For a scalar quantity transmitted over an additive Gaussian white noise channel of variance <inline-formula><mml:math id="inf338"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, with an average power constraint <inline-formula><mml:math id="inf339"><mml:mi>P</mml:mi></mml:math></inline-formula> for representing the codewords (i.e., <inline-formula><mml:math id="inf340"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>â¤</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>), the <italic>channel capacity</italic> , or maximum rate at which information can be transmitted without error, is given by (<xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>) :<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s7-2-2"><title>Gaussian multiple-access channel</title><p>Next, suppose the message is itself multi-dimensional (of dimension <inline-formula><mml:math id="inf341"><mml:mi>K</mml:mi></mml:math></inline-formula>), so that the message is <inline-formula><mml:math id="inf342"><mml:mrow><mml:mi mathvariant="bold">ðª</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (In a memory task, these <inline-formula><mml:math id="inf343"><mml:mi>K</mml:mi></mml:math></inline-formula> variables may correspond to different features of one item, or one feature each of multiple items, or some distribution of features and items. All features of all items are simply considered as elements of the message, appropriately ordered.)</p><p>The general framework for such a scenario is the multiple-access channel (MAC). In a MAC, separate encoders each encode one message element <inline-formula><mml:math id="inf344"><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> (<inline-formula><mml:math id="inf345"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), as an <inline-formula><mml:math id="inf346"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional codeword <inline-formula><mml:math id="inf347"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The full message <inline-formula><mml:math id="inf348"><mml:mi mathvariant="bold">ðª</mml:mi></mml:math></inline-formula> is thus represented by a set of <inline-formula><mml:math id="inf349"><mml:mi>K</mml:mi></mml:math></inline-formula> different <inline-formula><mml:math id="inf350"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional codewords, <inline-formula><mml:math id="inf351"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ðª</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The power of each encoder is limited to <inline-formula><mml:math id="inf352"><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> with a constraint on the summed power (we assume <inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¤</mml:mo><mml:mn>1.</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> The encoded outputs are transmitted through a channel with a single receiver at the end.</p><p>As before, we consider the channel to be Gaussian. In this Gaussian MAC model, the channel output <inline-formula><mml:math id="inf354"><mml:mi mathvariant="bold">ð²</mml:mi></mml:math></inline-formula> is a single <inline-formula><mml:math id="inf355"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector, like the output in the point-to-point communication case (<xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>). The MAC channel is defined by the distribution <inline-formula><mml:math id="inf356"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð²</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð²</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For a Gaussian MAC, <inline-formula><mml:math id="inf357"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">ð²</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">ð</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a Gaussian distribution with mean equal to <inline-formula><mml:math id="inf358"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and variance equal to the noise variance. The decoder is tasked with reconstructing all <inline-formula><mml:math id="inf359"><mml:mi>K</mml:mi></mml:math></inline-formula> elements of <inline-formula><mml:math id="inf360"><mml:mi mathvariant="bold">ðª</mml:mi></mml:math></inline-formula> from the <inline-formula><mml:math id="inf361"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional <inline-formula><mml:math id="inf362"><mml:mi mathvariant="bold">ð²</mml:mi></mml:math></inline-formula>.</p><p>The probability of error is defined as the average probability of error across all <inline-formula><mml:math id="inf363"><mml:mi>K</mml:mi></mml:math></inline-formula> entries of the message. The fundamental limit on information transmission over the MAC is not a single number, but a region in a <inline-formula><mml:math id="inf364"><mml:mi>K</mml:mi></mml:math></inline-formula>-dimensional space: It is possible to allocate power and thus rates differentially to different entries of the message <inline-formula><mml:math id="inf365"><mml:mi mathvariant="bold">ðª</mml:mi></mml:math></inline-formula>, and information capacity varies based on allocation. Through Shannonâs channel coding theorem, the region of achievable information rates for the Gaussian MAC with noise variance <inline-formula><mml:math id="inf366"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is given by:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:msup><mml:mo>â¤</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>â</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf367"><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:math></inline-formula> refers to any subset of <inline-formula><mml:math id="inf368"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, and we represent the summed rate for a given <inline-formula><mml:math id="inf369"><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id="inf370"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>â</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð®</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. In memory tasks, we assume the total power constraint is constant, regardless of the number of items, and <inline-formula><mml:math id="inf371"><mml:mi>K</mml:mi></mml:math></inline-formula> corresponds to the number of items. Thus, power allocation per item will generally vary (decrease) with item number.</p><p>To summarize, we have a fundamental limit on information transmission rates in a Gaussian multiple-access channel as described above.</p></sec><sec id="s7-2-3"><title>Capacity of a Gaussian MAC with equal per-item rate equals point-to-point channel capacity</title><p>The summed information rate through a Gaussian MAC channel is maximized when the per-item rate is equal across items. Moreover, at this equal-rate per-item point, the Gaussian MAC model corresponds directly to a point-to-point Gaussian (AWGN) channel coding model, where the channel input has an average power constraint <inline-formula><mml:math id="inf372"><mml:mi>P</mml:mi></mml:math></inline-formula>, which is set to <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf374"><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is the power constraint on the channel input of the <inline-formula><mml:math id="inf375"><mml:mi>k</mml:mi></mml:math></inline-formula>-th encoder of the original Gaussian MAC model. In this equivalent AWGN model, a single encoder is responsible for transmitting all of the <inline-formula><mml:math id="inf376"><mml:mi>K</mml:mi></mml:math></inline-formula> message elements, by dividing the point-to-point channel capacity equally among the message elements. The maximum information rate in a point-to-point AWGN channel is <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and therefore the information rate per item, if the rate is divided evenly over all <inline-formula><mml:math id="inf378"><mml:mi>K</mml:mi></mml:math></inline-formula> items, is <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This capacity can be achieved by setting the inputs for the AWGN point-to-point channel to be the <inline-formula><mml:math id="inf380"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector <inline-formula><mml:math id="inf381"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula>, with <inline-formula><mml:math id="inf382"><mml:mrow><mml:mi mathvariant="bold">ð±</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf383"><mml:mrow><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the set of <inline-formula><mml:math id="inf384"><mml:mi>K</mml:mi></mml:math></inline-formula> vectors of length <inline-formula><mml:math id="inf385"><mml:mi>N</mml:mi></mml:math></inline-formula> generated from the encoders of the Gaussian MAC. The <inline-formula><mml:math id="inf386"><mml:mi>i</mml:mi></mml:math></inline-formula>th component <inline-formula><mml:math id="inf387"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of <inline-formula><mml:math id="inf388"><mml:mi mathvariant="bold">ð±</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="inf389"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf390"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="inf391"><mml:mi>i</mml:mi></mml:math></inline-formula>th element of the vector <inline-formula><mml:math id="inf392"><mml:msup><mml:mi mathvariant="bold">ð±</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> which encodes the message element <inline-formula><mml:math id="inf393"><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula>, and therefore <inline-formula><mml:math id="inf394"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> contains information about all components of the message (joint representation of message elements).</p><p>Comparing the expression for the Gaussian MAC information rate with the capacity result from the corresponding point-to-point Gaussian channel, <inline-formula><mml:math id="inf395"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, it is clear that the summed rate of the equal-rate per-item Gaussian MAC can achieve the same (optimal) information rate per item as the point-to-point AWGN channel.</p><p><xref ref-type="fig" rid="fig4">Figure 4B</xref> of our main manuscript may be viewed as depicting the AWGN point-to-point channel, with a scalar input <inline-formula><mml:math id="inf396"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to each of the <inline-formula><mml:math id="inf397"><mml:mi>N</mml:mi></mml:math></inline-formula> memory networks (AWGN channels). It is interesting to note that both the AWGN channel and Gaussian MAC models suggest that the brain might encode distinct items independently but then store them jointly.</p></sec><sec id="s7-2-4"><title>Point-to-point communication through a Gaussian channel with a peak amplitude constraint</title><p>Suppose the codewords are amplitude-limited, rather than collectively power-limited, so that each element <inline-formula><mml:math id="inf398"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>â¤</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> for some amplitude <inline-formula><mml:math id="inf399"><mml:mi>A</mml:mi></mml:math></inline-formula>. If we are considering each entry of the codeword as being stored in a persistent activity network, then the maximal range of each codeword entry is constrained, rather than just the average power across entries. In this sense, amplitude-constrained channels may be more apt descriptors than power-constrained channels.</p><p>For comparison with the capacity of a Gaussian channel with a power constraint <inline-formula><mml:math id="inf400"><mml:mi>P</mml:mi></mml:math></inline-formula>, we set without loss of generality <inline-formula><mml:math id="inf401"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>P</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. Then, for a scalar quantity transmitted with this amplitude constraint over an additive Gaussian white noise channel of variance <inline-formula><mml:math id="inf402"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, the <italic>channel capacity</italic> is similar to that of the power-constrained Gaussian channel, but with the cost of a modest multiplicative pre-factor <inline-formula><mml:math id="inf403"><mml:mi>c</mml:mi></mml:math></inline-formula> that is smaller than, but close to size 1 (<xref ref-type="bibr" rid="bib69">Softky and Koch, 1993</xref>; <xref ref-type="bibr" rid="bib59">Raginsky, 2008</xref>):<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>â¢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>If the SNR (<inline-formula><mml:math id="inf404"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>) is such that <inline-formula><mml:math id="inf405"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">R</mml:mi></mml:msqrt><mml:mo>&lt;</mml:mo><mml:mn>1.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, then <inline-formula><mml:math id="inf406"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib59">Raginsky, 2008</xref>). Therefore, channel capacity of the amplitude-constrained Gaussian channel can be 80% or more of the channel capacity of the corresponding power-constrained Gaussian channel. In any case, the power-constrained Gaussian channel capacity expression is a good upper bound on the capacity of the amplitude-constrained version of that channel.</p></sec></sec><sec id="s7-3"><title>Joint source-channel coding</title><p>In memory experiments, it is not possible to directly measure information throughput in the internal storage networks. Rather, a related quantity that can be measured, and is thus the quantity of interest, is the accuracy of recall. In this section, we describe how the general bound on information throughput in the storage networks â derived in the previous section â can be used to strictly upper-bound the accuracy of recall in a specific class of memory tasks.</p><p>Consider a task that involves storing or communicating a variable <inline-formula><mml:math id="inf407"><mml:mi>Ï</mml:mi></mml:math></inline-formula>. This variable is known as the information source. The information source may be analog or discrete, and uniform or not. To remove redundancies in the source distribution or to possibly even further compress the inputs (at the loss of information), the source may be passed through a source-coding step. (For instance, the real interval <inline-formula><mml:math id="inf408"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> can be compressed through binary quantization into one bit by assigning the subinterval <inline-formula><mml:math id="inf409"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> to the point <inline-formula><mml:math id="inf410"><mml:mn>0</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="inf411"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf412"><mml:mn>1</mml:mn></mml:math></inline-formula>, at the expense of precision.) The output of the source coder is known as the <italic>message</italic> , which was the assumed input to the noisy channel in the sections discussed above. The message is a uniformly distributed index <inline-formula><mml:math id="inf413"><mml:mi>q</mml:mi></mml:math></inline-formula>, taking one of <inline-formula><mml:math id="inf414"><mml:mi>Q</mml:mi></mml:math></inline-formula> values, <inline-formula><mml:math id="inf415"><mml:mrow><mml:mi>q</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">â¯</mml:mi><mml:mo>,</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The source rate is the number of bits allocated per source symbol, or <inline-formula><mml:math id="inf416"><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For discrete, memoryless point-to-point Gaussian channels, Shannonâs separation theorem (<xref ref-type="bibr" rid="bib65">Shannon, 1959</xref>; <xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>) holds, which means that to obtain minimal distortion of a source variable that must be communicated through a noisy channel, it is optimal to separately compute the channel information rate, then set the source rate to equal the channel rate. Rate-distortion theory from source coding will then specify the lower bound on distortion with this scheme. Because the separation theorem holds for the point-to-point AWGN channel considered above, and because the point-to-point AWGN rate equals the maximal summed MAC rate, we can apply the separation theorem to our memory framework and then use rate-distortion theory to compute the lower bound on distortion.</p><p>To minimize distortion according to the separation theorem, we therefore set the source rate <inline-formula><mml:math id="inf417"><mml:mrow><mml:msub><mml:mi>log</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to equal the maximum number of bits that may be transmitted error-free over the channel. With this choice, all messages are transmitted without error in the channel. Then, we apply rate-distortion theory to determine the minimum distortion achievable for the allocated source rate. For a given source rate allocation, the distortion depends on several factors: the statistics of the source (e.g. whether it is uniform, Gaussian, etc.), the source coding scheme, and on the distortion measure (e.g. mean absolute error (an L-1 norm), mean squared error (an L-2 norm), or another metric that quantifies the difference between the true source and its estimate). Closed-form expressions for minimum achievable distortion do not exist for arbitrary sources and distortion metrics, but crucially, there are some useful bounds on specific distortion measures including the mean squared error, which is our focus.</p><sec id="s7-3-1"><title>Mean squared error (MSE) distortion</title><p>For arbitrary source distributions, the relationship between source rate (<inline-formula><mml:math id="inf418"><mml:mi>R</mml:mi></mml:math></inline-formula> bits per source symbol) and minimum MSE distortion (<inline-formula><mml:math id="inf419"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) at that rate, is given by:<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â¤</mml:mo><mml:mi>R</mml:mi><mml:mo>â¤</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msubsup><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf420"><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the differential entropy of the source, <inline-formula><mml:math id="inf421"><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the variance of the source, and <inline-formula><mml:math id="inf422"><mml:mi>log</mml:mi></mml:math></inline-formula> is in base-2. The inequality on the right is saturated (becomes an equality) for a Gaussian source (<xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>). The inequality on the left is the Shannon Lower Bound (<xref ref-type="bibr" rid="bib66">Sims et al., 2012</xref>) on MSE distortion for arbitrary memoryless sources, and it, too, is saturated for a Gaussian source (<xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>).</p><p>Specializing the above expression to a uniform source over the interval <inline-formula><mml:math id="inf423"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, we have <inline-formula><mml:math id="inf424"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf425"><mml:mrow><mml:msubsup><mml:mi>Ï</mml:mi><mml:mi>Ï</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, we obtain<disp-formula id="equ14"><label>(13)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>â¤</mml:mo><mml:mi>R</mml:mi><mml:mo>â¤</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>12</mml:mn><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Inverting the inequalities above to obtain bounds on the MSE distortion, we have<disp-formula id="equ15"><label>(14)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:msup><mml:mo>â¤</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â¤</mml:mo><mml:mfrac><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>12</mml:mn></mml:mfrac><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that the upper and lower bounds are identical in form â proportional to <inline-formula><mml:math id="inf426"><mml:mrow><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>â¢</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> â up to a constant prefactor that lies between <inline-formula><mml:math id="inf427"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>12</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. Thus, the lower bound on distortion is given by<disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is an unknown constant of size about <inline-formula><mml:math id="inf429"><mml:mn>1</mml:mn></mml:math></inline-formula>, somewhere in the range <inline-formula><mml:math id="inf430"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>12</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>Now, we set the information rate <inline-formula><mml:math id="inf431"><mml:mi>R</mml:mi></mml:math></inline-formula> for the source (bits per source symbol) in the equation above, to match the the maximum rate for error-free transmission in the noisy storage information channel. The maximum number of bits that can be stored error-free is <inline-formula><mml:math id="inf432"><mml:mi>N</mml:mi></mml:math></inline-formula> times the channel capacity given in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> , because <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> represents the information capacity for each channel use, and each of the <inline-formula><mml:math id="inf433"><mml:mi>N</mml:mi></mml:math></inline-formula> storage networks represents one channel use. Thus, we have <inline-formula><mml:math id="inf434"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf435"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is given in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> , and the minimum MSE distortion is:<disp-formula id="equ17"><label>(16)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="script">ð</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Because we are interested in the lower-bound on error, we set <inline-formula><mml:math id="inf436"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to the lower bound of its range, <inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, so that we obtain the expression given in the main paper (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref> ):<disp-formula id="equ18"><label>(17)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="script">ð</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Indeed, any other choice of <inline-formula><mml:math id="inf438"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> within its range <inline-formula><mml:math id="inf439"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mn>12</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> does not qualitatively affect our subsequent results in the main paper.</p><p>To summarize, we derived the bound given in <xref ref-type="disp-formula" rid="equ17">Equation 16</xref> by separately combining two different bounds - the lower-bound on achievable distortion at a source for a given source rate and the upper-bound on information throughput in a noisy information channel. This combination of the two separate bounds, where each bound did not take into account the statistics of the other process (the source bound was computed independently of the channel and the channel independently of the source), is in general sub-optimal. It is tight (optimal) in this case only because the uniform source and Gaussian channel obey the conditions of Shannonâs separation theorem, also known as the joint source-channel coding theorem (<xref ref-type="bibr" rid="bib25">Cover and Thomas, 1991</xref>; <xref ref-type="bibr" rid="bib80">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib45">MacKay, 2002</xref>; <xref ref-type="bibr" rid="bib65">Shannon, 1959</xref>; <xref ref-type="bibr" rid="bib79">Viterbi and Omura, 1979</xref>).</p></sec><sec id="s7-3-2"><title>Bound on recall accuracy for amplitude-constrained channels</title><p>As noted in Section 2 of the Appendix, the power-constrained channel capacity is an upper bound for the amplitude-constrained channel capacity (amplitude <inline-formula><mml:math id="inf440"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mi>P</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>). It follows that the lower-bound on distortion for power-constrained channels, <xref ref-type="disp-formula" rid="equ17">Equation 16</xref> , is a lower-bound on the amplitude-constrained channel. Further, because the channel capacity of an amplitude-constrained Gaussian channel is of the same form as the capacity of a power-constrained Gaussian channel, with a prefactor <inline-formula><mml:math id="inf441"><mml:mi>c</mml:mi></mml:math></inline-formula> that is close to 1, we easily see that the specific expression for MSE distortion is modified to be:<disp-formula id="equ19"><label>(18)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="script">ð</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mi>c</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Because <inline-formula><mml:math id="inf442"><mml:mi>N</mml:mi></mml:math></inline-formula> is a free parameter of the theory, we may simply renormalize <inline-formula><mml:math id="inf443"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> to equal <inline-formula><mml:math id="inf444"><mml:mi>N</mml:mi></mml:math></inline-formula>. Thus, the theoretical prediction obtained for a power-constrained channel is the same in functional form as that for an amplitude-constrained channel.</p><p>In comparing the theoretical prediction against the predictions of direct storage in persistent activity networks, however, we should take into account the factor <inline-formula><mml:math id="inf445"><mml:mi>c</mml:mi></mml:math></inline-formula>, noting that to produce an effective value of <inline-formula><mml:math id="inf446"><mml:mi>N</mml:mi></mml:math></inline-formula> requires <inline-formula><mml:math id="inf447"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> many networks, which is greater than <inline-formula><mml:math id="inf448"><mml:mi>N</mml:mi></mml:math></inline-formula> because <inline-formula><mml:math id="inf449"><mml:mrow><mml:mi>c</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s7-3-3"><title>Non-asymptotic considerations</title><p>Many of the numerical fits in the paper involve values of <inline-formula><mml:math id="inf450"><mml:mi>N</mml:mi></mml:math></inline-formula> that are not large: <inline-formula><mml:math id="inf451"><mml:mi>N</mml:mi></mml:math></inline-formula> is of order 10. When transmitting information with smaller <inline-formula><mml:math id="inf452"><mml:mi>N</mml:mi></mml:math></inline-formula>, the error-free information rate is lower (<xref ref-type="bibr" rid="bib58">Polyanskiy et al., 2010</xref>), or conversely, if transmitting at rates close to capacity with smaller numbers of channel uses (<inline-formula><mml:math id="inf453"><mml:mi>N</mml:mi></mml:math></inline-formula>) there can be decoding errors. In deriving our bound on distortion from joint-source channel coding theory, we inserted the asymptotic value of information rate (the capacity) into the rate-distortion function and assumed that information transmission at that rate would be error-free. If errors occur, the resulting distortion will be higher. It is important to note that, even far from the asymptotic limit in <inline-formula><mml:math id="inf454"><mml:mi>N</mml:mi></mml:math></inline-formula>, the derived lower-bound on distortion in <xref ref-type="disp-formula" rid="equ17">Equation 16</xref> remains a strict lower-bound; non-asymptotic effects can raise the overall error, not lower it.</p><p>Nevertheless, it is of interest to consider how distortion may be modified for values of <inline-formula><mml:math id="inf455"><mml:mi>N</mml:mi></mml:math></inline-formula> that are not asymptotically large. One would write the total non-asymptotic MSE distortion (<inline-formula><mml:math id="inf456"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow><mml:mrow><mml:mo>â¼</mml:mo><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>) as the sum of terms:<disp-formula id="equ20"><label>(19)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow><mml:mrow><mml:mo>â¼</mml:mo><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf457"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the error-free distortion bound derived above, <inline-formula><mml:math id="inf458"><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> is the probability of error in the non-asymptotic regime, and <inline-formula><mml:math id="inf459"><mml:msub><mml:mi>D</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> is the distortion in case of error. If an error resulted in total loss of information about the transmitted (coded) variable, <inline-formula><mml:math id="inf460"><mml:msub><mml:mi>D</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula> would scale as <inline-formula><mml:math id="inf461"><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, independent of <inline-formula><mml:math id="inf462"><mml:mi>N</mml:mi></mml:math></inline-formula> or other parameters in the problem. The only dependence on <inline-formula><mml:math id="inf463"><mml:mi>N</mml:mi></mml:math></inline-formula> would then enter through the probability of error, <inline-formula><mml:math id="inf464"><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>. The probability of error vanishes exponentially with <inline-formula><mml:math id="inf465"><mml:mi>N</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib58">Polyanskiy et al., 2010</xref>), and can be small even for relatively small values of <inline-formula><mml:math id="inf466"><mml:mi>N</mml:mi></mml:math></inline-formula>. The second term is in practice a small contributor to the MSE. Alternatively, one can ask how small <inline-formula><mml:math id="inf467"><mml:mi>N</mml:mi></mml:math></inline-formula> can be and at how far below the asymptotic capacity to enable information transmission at or below a given error rate. Analytical and numerical results in <xref ref-type="bibr" rid="bib58">Polyanskiy et al. (2010)</xref> show that at SNR values lower than the estimated SNR in the memory system model (<inline-formula><mml:math id="inf468"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mn>2.2</mml:mn></mml:mrow></mml:math></inline-formula> dB at <inline-formula><mml:math id="inf469"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> sec and <inline-formula><mml:math id="inf470"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> dB at <inline-formula><mml:math id="inf471"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> sec; while Figure 6 in (<xref ref-type="bibr" rid="bib58">Polyanskiy et al., 2010</xref>) has <inline-formula><mml:math id="inf472"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> dB and <inline-formula><mml:math id="inf473"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>), it is possible to remain within a factor of <inline-formula><mml:math id="inf474"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> of the asymptotic information capacity with <inline-formula><mml:math id="inf475"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>. Thus, the non-asymptotic expectation is that the information transmission rate should be scaled down from the asymptotically achievable information rate (the capacity) by some factor <inline-formula><mml:math id="inf476"><mml:mi>c</mml:mi></mml:math></inline-formula> (in this case, <inline-formula><mml:math id="inf477"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¼</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>). Thus, through <xref ref-type="disp-formula" rid="equ16">Equation 15</xref> , we see that the bound on distortion will remain the same as in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> of the main manuscript, with the replacement of <inline-formula><mml:math id="inf478"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> in the exponent by <inline-formula><mml:math id="inf479"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>â¢</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>. In other words, the previous values of the fit parameter <inline-formula><mml:math id="inf480"><mml:mi>N</mml:mi></mml:math></inline-formula> in the fits would actually correspond to <inline-formula><mml:math id="inf481"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. Thus, it actually takes <inline-formula><mml:math id="inf482"><mml:mi>c</mml:mi></mml:math></inline-formula> times more resources (where <inline-formula><mml:math id="inf483"><mml:mi>c</mml:mi></mml:math></inline-formula> scales slowly with <inline-formula><mml:math id="inf484"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>) to achieve a given level of performance non-asymptotically as asymptotically.</p><p>To summarize, the bound on distortion given in <xref ref-type="disp-formula" rid="equ17">Equation 16</xref> is still a strict lower-bound on distortion in the regime where <inline-formula><mml:math id="inf485"><mml:mi>N</mml:mi></mml:math></inline-formula> is not asymptotically large; moreover, the functional form of the bound can remain largely the same in the non-asymptotic regime because the error probability is small for modest <inline-formula><mml:math id="inf486"><mml:mi>N</mml:mi></mml:math></inline-formula>. In addition, it is possible to achieve a given low error probability at a fixed SNR by simply decreasing the information rate, which increases distortion in a way that is effectively the same as increasing the value of the free parameter <inline-formula><mml:math id="inf487"><mml:mi>N</mml:mi></mml:math></inline-formula>.</p></sec></sec></boxed-text></sec><sec id="s8" sec-type="appendix"><title>Direct (uncoded) storage in persistent activity networks</title><boxed-text><p>Modeling short-term memory as direct storage of variables in persistent activity networks, produces results that are inconsistent with the data, as shown in the main paper. To obtain predictions for persistence and capacity through direct storage in persistent activity networks, first consider storing a single circular orientation variable, for a single bar in the delayed orientation matching task, as a bump in one ring network (<xref ref-type="bibr" rid="bib13">Ben-Yishai et al., 1995</xref>; <xref ref-type="bibr" rid="bib2">Amit, 1992</xref>; <xref ref-type="bibr" rid="bib86">Zhang, 1996</xref>). The ring network would have neurons from all the <inline-formula><mml:math id="inf488"><mml:mi>N</mml:mi></mml:math></inline-formula> storage networks in our short-term memory system pooled together, thus the network is <inline-formula><mml:math id="inf489"><mml:mi>N</mml:mi></mml:math></inline-formula> times larger. The mean squared error of a variable stored in a continuous attractor neural network with stochastic neural spiking grows linearly with the storage interval <inline-formula><mml:math id="inf490"><mml:mi>T</mml:mi></mml:math></inline-formula> over short intervals (with âshortâ defined as all intervals before the root-mean squared error has grown to be an appreciable fraction of the range of the variable, <inline-formula><mml:math id="inf491"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi>Ï</mml:mi></mml:mrow></mml:math></inline-formula>). Let <inline-formula><mml:math id="inf492"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi></mml:mrow></mml:math></inline-formula> be the coded variable, with <inline-formula><mml:math id="inf493"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If the rate of growth of error in the individual storage networks of the main paper is <inline-formula><mml:math id="inf494"><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> (recall that <inline-formula><mml:math id="inf495"><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>/</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf496"><mml:mi>D</mml:mi></mml:math></inline-formula> is coefficient of diffusion (<xref ref-type="bibr" rid="bib19">Burak and Fiete, 2012</xref>); thus, the quantity <inline-formula><mml:math id="inf497"><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:math></inline-formula> describes the rate at which the stored variable drifts away from its initial value, normalized by the squared range of the variable, per unit power of the representation; alternatively, we may think of the total representional power as being normalized to 1 in all cases), then the rate of growth of squared error in the single ring network is <inline-formula><mml:math id="inf498"><mml:mrow><mml:mrow><mml:mpadded lspace="3.3pt" width="+3.3pt"><mml:mn>2</mml:mn></mml:mpadded><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib19">Burak and Fiete, 2012</xref>). The factor of <inline-formula><mml:math id="inf499"><mml:mi>N</mml:mi></mml:math></inline-formula> enters because if all other quantities are held fixed, the diffusion coefficient in continuous attractor memory networks is inversely proportional to network size. Thus, the squared error in the variable at short times <inline-formula><mml:math id="inf500"><mml:mi>T</mml:mi></mml:math></inline-formula> is given by <inline-formula><mml:math id="inf501"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">â¨</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">â©</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In other words, we have<disp-formula id="equ21"><label>(20)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="script">ð</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Next, consider storing <inline-formula><mml:math id="inf502"><mml:mi>K</mml:mi></mml:math></inline-formula> scalar variables, with each component ranging in <inline-formula><mml:math id="inf503"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, and represented in one of <inline-formula><mml:math id="inf504"><mml:mi>K</mml:mi></mml:math></inline-formula> different small networks, constructed from the single storage network above. Thus, its size is <inline-formula><mml:math id="inf505"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> of the above. Relative to <xref ref-type="disp-formula" rid="equ21">Equation 20</xref> above, we therefore have<disp-formula id="equ22"><label>(21)</label><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="script">ð</mml:mi></mml:mrow><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In other words, for memory systems involving direct storage in persistent activity networks without special encoding, we expect the squared error to grow linearly with <inline-formula><mml:math id="inf506"><mml:mi>K</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf507"><mml:mi>T</mml:mi></mml:math></inline-formula>. The prediction of uncoded storage in persistent activity networks can be compared directly with the prediction from encoded storage (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), because they involve the same parameters and the same resource use in the memory networks. While adding a proper encoding stage can reduce storage errors exponentially in <inline-formula><mml:math id="inf508"><mml:mi>N</mml:mi></mml:math></inline-formula>, uncoded storage results in decreases with <inline-formula><mml:math id="inf509"><mml:mi>N</mml:mi></mml:math></inline-formula> that are merely polynomial (more specifically, scaling as <inline-formula><mml:math id="inf510"><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>).</p><p>Finally, one may consider directly storing the <inline-formula><mml:math id="inf511"><mml:mi>K</mml:mi></mml:math></inline-formula>-dimensional variable in a single persistent activity network that is a <inline-formula><mml:math id="inf512"><mml:mi>K</mml:mi></mml:math></inline-formula>-dimensional ring network (a <inline-formula><mml:math id="inf513"><mml:mi>K</mml:mi></mml:math></inline-formula>-torus). In this situation, the neurons have to be arranged so the number of neurons per linear dimension of the network scales as <inline-formula><mml:math id="inf514"><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. Thus, the rate of growth of squared error along each dimension of the network scales as <inline-formula><mml:math id="inf515"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and we have<disp-formula id="equ23"><label>(22)</label><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mstyle mathsize="0.5em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">Î¦</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="script">ð</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This scaling with <inline-formula><mml:math id="inf516"><mml:mi>T</mml:mi></mml:math></inline-formula> remains linear, while the improvement in squared error with <inline-formula><mml:math id="inf517"><mml:mi>N</mml:mi></mml:math></inline-formula> is weaker than the scaling in <xref ref-type="disp-formula" rid="equ22">Equation 21</xref> , which in turn is weaker than the scaling in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> , and consequently produces worse fits to the data than does <xref ref-type="disp-formula" rid="equ22">Equation 21</xref> . Therefore, we have chosen to contrast the better of two scenarios of direct (uncoded) storage, <xref ref-type="disp-formula" rid="equ22">Equation 21</xref> , against the predictions of the theory of short-term memory proposed in this work.</p><sec id="s8-1"><title>Comparison of direct storage against coded storage in power- or amplitude-constrained channels</title><p>In the main text, we compared not only how the predictions of coded versus direct storage compare with each other as a function of <inline-formula><mml:math id="inf518"><mml:mi>T</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf519"><mml:mi>K</mml:mi></mml:math></inline-formula>, but also compared total resource use to achieve a given performance with the two different models of storage. In the latter comparison, we derive the total neural resource, <inline-formula><mml:math id="inf520"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula>, required in the two schemes. We report that direct storage requires a <inline-formula><mml:math id="inf521"><mml:mrow><mml:mi/><mml:mo>â¼</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula>-fold larger <inline-formula><mml:math id="inf522"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> than coded storage, basing our results on the expression for coded storage in power-constrained channels. As noted in Section 3 of the Appendix, the effective <inline-formula><mml:math id="inf523"><mml:mi>N</mml:mi></mml:math></inline-formula> for an amplitude-constrained channel, which might be a more apt constraint for persistent activity networks with bounded ranges, is actually <inline-formula><mml:math id="inf524"><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf525"><mml:mi>c</mml:mi></mml:math></inline-formula> is a prefactor close to but smaller than 1, that represents the fractional loss in channel capacity incurred by enforcing an amplitude rather than power constraint. As described in (<xref ref-type="bibr" rid="bib59">Raginsky, 2008</xref>) (see also related work in (<xref ref-type="bibr" rid="bib69">Softky and Koch, 1993</xref>) ), the cost of replacing a power constraint by an amplitude constraint is modest, with <inline-formula><mml:math id="inf526"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for an appropriate regime of channel SNR (this is the regime of SNR for our fits to the data). Thus, even with an amplitude constraint for the coded memory scenario, direct storage would require a <inline-formula><mml:math id="inf527"><mml:mrow><mml:mi/><mml:mo>â¼</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula>-fold larger <inline-formula><mml:math id="inf528"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec></boxed-text></sec><sec id="s9" sec-type="appendix"><title>Performance of individual subjects and comparison with theory</title><boxed-text><p>Here, we supply the data from individual subjects, as well as fits of the theory of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and the direct storage model 1 to their performance.</p><p>The individual subject responses and the fits of the well-coded storage model are shown in <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>. We first plot the quality-of-fit or energy surface of the fits of the well-coded model to the individual subject data (top two rows in <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>), as the two parameters of the model are varied. These individual-subject solution spaces look qualitatively similar to the across-subject aggregates reported in the main manuscript. All subjects exhibit a 1D manifold of âgoodâ parameter settings, along which the model provides a reasonable match to the data. The quality of fit along the 1D manifold (valley) is shown in the next two rows of <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>; based on the local minima of these curves, we infer the optimal settings of <inline-formula><mml:math id="inf529"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf530"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>â¢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ð</mml:mi></mml:mrow></mml:math></inline-formula> for each subject. The differences between individuals emerges in that the best <inline-formula><mml:math id="inf531"><mml:mi>N</mml:mi></mml:math></inline-formula> values range between 2 and 20, and that for most subjects, the best values range between 4 and 11. Subjects with deviations in the optimal <inline-formula><mml:math id="inf532"><mml:mi>N</mml:mi></mml:math></inline-formula> from this narrower range have essentially flat valleys between <inline-formula><mml:math id="inf533"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf534"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>), and thus the choice of <inline-formula><mml:math id="inf535"><mml:mi>N</mml:mi></mml:math></inline-formula> is not strongly constrained.</p><p>The minimum fit errors are necessarily larger than the minimum fit errors for the across-subject averaged data, because of the higher variability of individual subject data (fewer trials per subject than total trials across subjects). Nevertheless, the normalized squared errors of the fits can be quite low, and the theory provides good fits to the psychophysics data for the individual subjects.</p><p>We also fit the individual subject data to the direct storage models, to be able to compare the predictions from the two models, <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>. We then compute the Bayesian Information Criterion score for both the direct storage model and the well-coded storage model, and report the <inline-formula><mml:math id="inf536"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi><mml:mo>â¢</mml:mo><mml:mi>I</mml:mi><mml:mo>â¢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> score for hypothesis comparison, <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>. Positive (negative) <inline-formula><mml:math id="inf537"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi><mml:mo>â¢</mml:mo><mml:mi>I</mml:mi><mml:mo>â¢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> scores indicate support for the well-coded (direct) storage model, and an absolute value of 10 or greater indicates very strong support. Note that the <inline-formula><mml:math id="inf538"><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi><mml:mo>â¢</mml:mo><mml:mi>I</mml:mi><mml:mo>â¢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> scores for the individual subjects are much smaller in magnitude than the aggregate scores for all pooled data in the main manuscript, because the data set for individual subjects is smaller and has less statistical strength. Nevertheless, there is very strong support (<inline-formula><mml:math id="inf539"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi><mml:mo>â¢</mml:mo><mml:mi>I</mml:mi><mml:mo>â¢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>) for the well-coded model in 4 out of 10 subjects, close to strong support for direct storage in 2 out of 10 subjects (<inline-formula><mml:math id="inf540"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi><mml:mo>â¢</mml:mo><mml:mi>I</mml:mi><mml:mo>â¢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>), positive support for direct storage in 2 subjects, and essentially insignificant support (<inline-formula><mml:math id="inf541"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Î</mml:mi><mml:mo>â¢</mml:mo><mml:mi>B</mml:mi><mml:mo>â¢</mml:mo><mml:mi>I</mml:mi><mml:mo>â¢</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>) in 2 remaining subjects.</p></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.22225.016</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role>Reviewing Editor</role><aff><institution>New York University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Fundamental bound on the persistence and capacity of short-term memory stored as graded persistent activity&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and David Van Essen as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Tim Buschman (Reviewer #1); John D Murray (Reviewer #2); Brad Postle (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The manuscript presents an information-theoretic computational model of STM that suggests an intriguing new way that information may be coded in working memory. The theoretical framework developed here constitutes an important advance in linking neural circuit mechanisms to testable psychophysical behavior (here, working memory precision as a function of duration and load). The quantitative fit of the model to human behavior is compelling and bolsters the relevance of the theoretical advances.</p><p>The reviewers were all in agreement about the potential impact of this work presented. But all also agreed that further discussion of the proposed models and its implications should be added to more thoroughly place this work in the broader context of the field. Some specific suggestions are made below. Furthermore, there several detailed questions regarding aspects of the model that should also be addressed. I have edited and appended the revisions that are essential to include in a revision below.</p><p>Please address the following in a revision:</p><p>1) One reviewer noted that 'it takes too long to get to the point in the manuscript at which the reader knows, well, what the main point of the paper will be. It's not in the title, not in the Abstract, and, indeed, not clearly articulated until subsection âInformation-theoretic bound on memory performance with well-coded storageâ of the manuscript.' The first part of the manuscript is taken up with a lengthy exposition of why and how direct storage models are unsatisfactory. For a general-interest journal, one would want the central idea to be clearly articulated in one of the first paragraphs in the paper (not to mention in the Abstract), then the demonstration that direct storage models are insufficient to be dispatched within a few short paragraphs. Perhaps some of this could be accomplished in part by moving some of the text and analyses to figure legends? As things stand, the figures with their minimalist legends are inscrutable. One idea would be to display the panel from <xref ref-type="fig" rid="fig4">Figure 4E</xref> side-by-side with 3A and B, to permit a side-by-side comparison of the different approaches. Indeed, <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref> could be merged, together with much of the text between them.</p><p>2) A second major absence from the Introduction, which will raise concerns by many familiar with the current literature, is near absence of any consideration of the growing number of suggestions that STM might be accomplished by mechanisms other than sustained activity. To name just a few, there's a recent TICS paper by Stokes that is explicitly devoted to this idea, there are several theoretical accounts by Tsodyks, Barack, and colleagues (nicely summarized in a recent Current Opinion review), and there's the nonlinear dynamical systems model of Lundqvist and colleagues, recently illustrated with data from Miller's group.</p><p>If some variant of these &quot;activity-silent&quot; accounts is correct, are the ideas presented in this manuscript irrelevant, or are there principles from the present theory that would apply? Additionally/alternatively, are there principles from the present theory that might apply to sustained activity supporting a behavior other than STM?</p><p>3) Some of the writing contains incomplete or misleading assertions. For example, the idea that there are constraints on the amount of time that information can be held in STM ignores the fact that a classically held hallmark of STM is precisely that it is not sensitive to the passage of time, per se. (Two examples are from Keppel and Underwood, and many demonstrations of prolonged retention of information in STM in anterograde amnesic patients.) Indeed, puzzlingly, one of the papers cited by the authors to substantiate their assertion is entitled &quot;No temporal decay in verbal short-term memory.&quot;</p><p>4) The manuscript makes not contact with the growing literature of multivariate analyses of data from STM tasks, from nonhuman and human electrophysiology, and from human fMRI. Some of these studies show the ability to decode the contents of STM from delay-period activity with decoders trained on sample-evoked signal. Others suggest that the neural code may be dynamic, with minimal if any cross-temporal generalization (i.e., &quot;off-diagonal&quot; decoding). How does the proposed theory relate to this empirical literature? Without reference to these broader literatures, the present manuscript might be more suitable for a more specialized computational journal.</p><p>5) The authors argue that the currently accepted model of working memory predicts a linear increase in mean-squared error (MSE) over time and load (MSE ~ (load)*(time)). In contrast, they find a sub-linear increase in MSE with time (<xref ref-type="fig" rid="fig3">Figure 3A and 3B</xref>). This sub-linearity is well fit by the well-coded model. However, some of this non-linearity could be due to other, less-capacity-limited, forms of memory at very short time delays. For example, iconic memory, thought to have an extremely high capacity, is likely still available at 100 ms (some might argue for longer). This could lead to a reduction in the MSE at the lowest time delays. Ideally the authors would control for this using masking stimuli. Alternatively, the authors could control for this by excluding the very short delays from the analysis (possibly increasing the maximum memory delay if needed for fits).</p><p>6) As with many working memory paradigms, it is not entirely clear how to define the working memory load in the current task. It seems subjects must remember multiple pieces of information per memorandum (e.g. both color and orientation) in all cases except for the single item. This would suggest memory load is actually 1, 4, 8, and 12. Does this non-linearity account for the poor fit of the linear &quot;direct coding&quot; model? It seems like it might not, given the poor fit in <xref ref-type="fig" rid="fig3">Figure 3B</xref> but it would still be worth testing the two models with different values for memory load. Similarly, recent work has suggested some degree of independence of working memory load across the two visual hemifields. Again, this would suggest only the balanced displays can be directly compared (e.g. 2, 4, and 6 items). Does the well-coded model still provide a better fit If the analysis is restricted to these three conditions?</p><p>7) The authors appropriately use BIC to perform model comparison. However, these model comparison criterion often penalize parameters to different degrees. Did the authors also find the well-coded model generalized to a withheld dataset better than the direct coding model?</p><p>8) Recent work has debated whether errors during working memory are due, in part, to guessing or not (e.g. Luck, Awh, Vogel, Bays, etc). In fact, Steve Luck argues for no increase in variance with load (or time?), instead only an increase in guess rate. If fitting a circular Gaussian to the distribution do the authors find an increase in variance or an increase in baseline (or both)? Related to this, it isn't clear to me how the pure 'sudden-death' framework matches with the diffusivity arguments made here. It seems that perhaps the well-coded model could explain the existence of complete failures to remember if the signal diffuses too much, but the model would still argue for some diffusion of memory over time. This doesn't seem consistent with the current model. I know the authors attempt to address this in the Discussion section of the current manuscript but I would encourage the authors to clarify their position.</p><p>9) This study uses the co-authors' human psychophysical data from Pertzov et al., 2016 Journal of Experimental Psychology. That study decomposed errors into three sources: (<xref ref-type="bibr" rid="bib1">1</xref>) noisy representation; (<xref ref-type="bibr" rid="bib2">2</xref>) mis-binding or non-target responses; and (<xref ref-type="bibr" rid="bib3">3</xref>) random guessing. They reported that all three of these components increased with higher load and with longer delays. How does these prior findings relate to the present study? Are these different sources subsumed by the present model? Or are these important features that the present model (in the diffusive regime) does not account for? Does the present model produce only the first type of errors? The Authors mention that in another regime of the model, non-diffusive errors can produce pure guessing errors. Can the model speak to the mechanisms of mis-binding errors? Please include discussion of this point.</p><p>10) Regarding the implications for neural representations: The Authors discuss that one prediction of the model would be signatures of exponentially strong codes in neural representations. As I understand it, one way this could be implemented is that each of the N memory networks has a different spatial period for its periodic coding, as in the case of grid cells. The other feature of the present model is that for multi-item working memory, a memory network contains signals for all of the K items. It would be helpful if the Authors can clarify what the implications on neural representations are for this feature of distributed multi-item coding. Does this imply that single neurons would show mixed selectivity for multiple items? Please include discussion of this point.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.22225.017</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Please address the following in a revision:</p><p>1) One reviewer noted that 'it takes too long to get to the point in the manuscript at which the reader knows, well, what the main point of the paper will be. It's not in the title, not in the Abstract, and, indeed, not clearly articulated until subsection âInformation-theoretic bound on memory performance with well-coded storageâ of the manuscript.' The first part of the manuscript is taken up with a lengthy exposition of why and how direct storage models are unsatisfactory. For a general-interest journal, one would want the central idea to be clearly articulated in one of the first paragraphs in the paper (not to mention in the Abstract), then the demonstration that direct storage models are insufficient to be dispatched within a few short paragraphs. Perhaps some of this could be accomplished in part by moving some of the text and analyses to figure legends? As things stand, the figures with their minimalist legends are inscrutable. One idea would be to display the panel from <xref ref-type="fig" rid="fig4">Figure 4E</xref> side-by-side with 3A and B, to permit a side-by-side comparison of the different approaches. Indeed, <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref> could be merged, together with much of the text between them.</p></disp-quote><p>We have now edited the Abstract and Introduction to convey what the manuscript is about much earlier in the text. Please see the new introductory paragraph: &quot;In the present work, we make the following contributions: 1) Generate psychophysics predictions for information degradation as a function of delay period and number of stored items, if information is stored directly, without recoding, in persistent activity neural networks of a given size over given time interval; 2) Generate psychophysics predictions (though the use of joint source-channel coding theory) for a model that assumes information is restructured by encoding and decoding stages before and after storage in persistent activity neural networks; 3) Compare these models to new analog measurements \cite{Pertzov16} of human memory performance on an analog task as the demands on both maintenance duration and capacity are varied.&quot;</p><p>Please note that the early results of the manuscript are to establish the theoretical predictions for direct storage in persistent activity networks. To our knowledge, these predictions about degradation as a function of time and item number with direct storage have not been made explicit before, and so are one part of our results (if they had been made before it would have been easy to shorten this section and replace it with a citation). It is equally important to state the framework, formalism (including resource use parameters, etc.), and results for the direct storage model in the main results for comparison with the framework and parameters of the well-coced model, so that it is clear that we are making a fair comparison.</p><p>The figure captions are fairly long, and in merging plots as well as clarifying the captions as suggested, they have become slightly longer. Thus, moving more of the text of the results to the figure captions is not ideal. We have edited and shortened the direct storage Results section, but have not eviscerated it as we feel it is an integral part of our main result. As suggested, we have also merged <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>, to make a direct comparison between the different models easier for the reader.</p><disp-quote content-type="editor-comment"><p>2) A second major absence from the Introduction, which will raise concerns by many familiar with the current literature, is near absence of any consideration of the growing number of suggestions that STM might be accomplished by mechanisms other than sustained activity. To name just a few, there's a recent TICS paper by Stokes that is explicitly devoted to this idea, there are several theoretical accounts by Tsodyks, Barack, and colleagues (nicely summarized in a recent Current Opinion review), and there's the nonlinear dynamical systems model of Lundqvist and colleagues, recently illustrated with data from Miller's group.</p><p>If some variant of these &quot;activity-silent&quot; accounts is correct, are the ideas presented in this manuscript irrelevant, or are there principles from the present theory that would apply? Additionally/alternatively, are there principles from the present theory that might apply to sustained activity supporting a behavior other than STM?</p></disp-quote><p>We thank the reviewers very much for this comment. Indeed, we did not explicitly discuss activity-silent accounts of STM in our Introduction or Discussion (other than providing a reference to Mongillo and Tsodyks 2008, a model of how synaptic facilitation can aid in the robustness of short term memory). Given recent experimental and modeling results in this direction -- they are starting to form a compelling alternate STM mechanism to persistent activity mechanisms -- it is important to mention these accounts.</p><p>We have added a brief stand-alone passage in the Introduction, stating that our current work is complementary to efforts to explain STM in terms of synaptic facilitation/activity-silent mechanisms. In this passage, we cite the work of Mi, Katkov and Tsodyks, 2016: Barak and Tsodyks, 2014; Stokes, 2015 and Lundqvist et al., 2016.</p><p>With respect to the question about whether our model would apply to activity-silent mechanisms: In citing the model of Mongillo and Tsodyks, 2008 in our earlier manuscript, we had considered the possibility of synaptic facilitation as a source of a longer cellular time-constant to serve as the basis of STM, but we viewed that model as another persistent activity model, with activity supporting facilitation and facilitation supporting elevated activity. The facilitation process lent a slower intrinsic time-constant to the persistent activity feedback loop, thus providing a more robust/less fine-tuned way to generate persistent activity. Such a model would be subject to the same diffusion/drift problems as persistent activity models, qualitatively speaking (but quantitatively with lower noise or slower diffusion time-constant), and thus subject to similar degradation as considered in our present work.</p><p>The newer models cited in the paragraph may exhibit different dynamics, and be subject to different types of noise, in which case the general principle of restructuring of information to improve memory would still be true but the functional form of error versus number of items and <italic>N</italic> could be somewhat different. However, if the synaptic facilitation states in these models were subject to a Gaussian drift (e.g. if the facilitation states are analog-valued and some biophysical noise-process drives a random walk through the set of possible states even in the absence of neural activity), then they too could be could be treated as a bank of information channels with Gaussian noise and potentially our theory would extend to these, but with different parameters.</p><p>Since there are not yet good models of noise in the synaptic facilitation variable, for instance, and the effects of such noise on collective network memory states, we cannot directly yet compute a theoretical bound on memory performance for these mechanisms. However, that is definitely a future interest; with more theoretical work on modeling sources of noise in the activity-silent mechanisms, it will be possible to apply a similar theoretical framework to obtain bounds on memory performance with and without good encoding.</p><disp-quote content-type="editor-comment"><p>3) Some of the writing contains incomplete or misleading assertions. For example, the idea that there are constraints on the amount of time that information can be held in STM ignores the fact that a classically held hallmark of STM is precisely that it is not sensitive to the passage of time, per se. (Two examples are from Keppel and Underwood, and many demonstrations of prolonged retention of information in STM in anterograde amnesic patients.) Indeed, puzzlingly, one of the papers cited by the authors to substantiate their assertion is entitled &quot;No temporal decay in verbal short-term memory.&quot;</p></disp-quote><p>Indeed, as the reviewers note, early studies have emphasized the temporal robustness of STM, and compared to âiconicâ memory, STM is much less susceptible to forgetting. Consistent with this, our experimental results clearly demonstrate that single items are remembered with very little degradation over time, and the effects of increasing item number are stronger than the effects of increasing delay on memory performance.</p><p>However, there is performance degradation over time, especially for more items. We do not ourselves model pure temporal decay as a mechanism for memory loss, so it was not our intention to convey this in the Introduction. The source of confusion was our phrasing and references. We are now more careful in making a distinction between performance degradation over time versus the possible mechanisms for such degradation (which could include noise or interference or, less likely according to the literature, pure temporal decay mechanisms), please see our edits.</p><disp-quote content-type="editor-comment"><p>4) The manuscript makes not contact with the growing literature of multivariate analyses of data from STM tasks, from nonhuman and human electrophysiology, and from human fMRI. Some of these studies show the ability to decode the contents of STM from delay-period activity with decoders trained on sample-evoked signal. Others suggest that the neural code may be dynamic, with minimal if any cross-temporal generalization (i.e., &quot;off-diagonal&quot; decoding). How does the proposed theory relate to this empirical literature? Without reference to these broader literatures, the present manuscript might be more suitable for a more specialized computational journal.</p></disp-quote><p>Our formalism indicates that the representation within a memory channel must be in an optimised format, and that this format is not necessarily the same format that information was initially presented in. According to the information-theoretic view, the brain must perform a transformation from stimulus-space into an optimally coded form, and one might expect to observe this transition of the representation at encoding. The less optimal the original stimulus space, the more different the mnemonic code will likely be from the sample-evoked signal.</p><p>This insight by the reviewer constitutes a potential key prediction of the model, that in domains that are already combinatorially structured, neural representations should remain similar throughout the delay period, whereas in domains amenable to compression at encoding, neural codes during the delay will appear dynamic or at least different from the stimulus-evoked signal. We now include a discussion of this point in the manuscript (Discussion section, paragraph beginning &quot;It remains to be seen whether neural representations for short-term visual memory are consistent â¦.&quot;), also citing papers in the literature that show variously show either stable, conserved coding during delay or varying, different states during delay.</p><disp-quote content-type="editor-comment"><p>5) The authors argue that the currently accepted model of working memory predicts a linear increase in mean-squared error (MSE) over time and load (MSE ~ (load)*(time)). In contrast, they find a sub-linear increase in MSE with time (<xref ref-type="fig" rid="fig3">Figure 3A and 3B</xref>). This sub-linearity is well fit by the well-coded model. However, some of this non-linearity could be due to other, less-capacity-limited, forms of memory at very short time delays. For example, iconic memory, thought to have an extremely high capacity, is likely still available at 100 ms (some might argue for longer). This could lead to a reduction in the MSE at the lowest time delays. Ideally the authors would control for this using masking stimuli. Alternatively, the authors could control for this by excluding the very short delays from the analysis (possibly increasing the maximum memory delay if needed for fits).</p></disp-quote><p>Thank you for this comment. Please note that the real problem with the direct storage (linear) model is not so much that the function in time is linear, as that even the average slopes of the different-item number curves versus time are not fit by the slopes in the linear model: That is, if we fit the 1-item versus time data, then the predicted slope of the 6-item versus time slope prediction is far lower than the average slope of the actual data.</p><p>This can be seen in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. If we attempt to fit all the curves simultaneously as well as possible, again the slopes of the fits in time are far from the mean slopes of the curves, leaving aside the question of sub-linearity.</p><p>If we understand, the reviewer is suggesting the following scenario: Consider some process that has linear degradation of information in time (e.g. like direct storage of information into persistent activity networks). Add to this model the assumption that the 100 ms time-point is due to iconic memory. After excluding this 100 ms point, the uncoded model might provide a much better fit than it has so far, and it might also be more competitive with the coded model.</p><p>We now perform this analysis, and find that the uncoded model still fails to simultaneously fit the 1- and 6- item versus time data, and remains a substantially poorer fit than the coded model fit to the same data. The result does not change these qualitative comparisons.</p><disp-quote content-type="editor-comment"><p>6) As with many working memory paradigms, it is not entirely clear how to define the working memory load in the current task. It seems subjects must remember multiple pieces of information per memorandum (e.g. both color and orientation) in all cases except for the single item. This would suggest memory load is actually 1, 4, 8, and 12. Does this non-linearity account for the poor fit of the linear &quot;direct coding&quot; model? It seems like it might not, given the poor fit in <xref ref-type="fig" rid="fig3">Figure 3B</xref> but it would still be worth testing the two models with different values for memory load. Similarly, recent work has suggested some degree of independence of working memory load across the two visual hemifields. Again, this would suggest only the balanced displays can be directly compared (e.g. 2, 4, and 6 items). Does the well-coded model still provide a better fit If the analysis is restricted to these three conditions?</p></disp-quote><p>This is an excellent suggestion. We have now redefined the item numbers from (1, 2, 4, 6) to (1,4, 8, 12) and re-done the fits. We find that our qualitative conclusions remain unchanged.</p><disp-quote content-type="editor-comment"><p>7) The authors appropriately use BIC to perform model comparison. However, these model comparison criterion often penalize parameters to different degrees. Did the authors also find the well-coded model generalized to a withheld dataset better than the direct coding model?</p></disp-quote><p>Thank you for another good question. To address this, we redid the analysis by excluding one time-point across all item-number curves, then asked how well the curves obtained from fitting the other time-points predicted the error for the held-out data-point. We repeated this for another time-point. This is like a leave-one-out or jackknife cross-validation procedure. We find that the well-coded model predicts the withheld datapoints with smaller error than the uncoded/direct coding model.</p><disp-quote content-type="editor-comment"><p>8) Recent work has debated whether errors during working memory are due, in part, to guessing or not (e.g. Luck, Awh, Vogel, Bays, etc). In fact, Steve Luck argues for no increase in variance with load (or time?), instead only an increase in guess rate. If fitting a circular Gaussian to the distribution do the authors find an increase in variance or an increase in baseline (or both)? Related to this, it isn't clear to me how the pure 'sudden-death' framework matches with the diffusivity arguments made here. It seems that perhaps the well-coded model could explain the existence of complete failures to remember if the signal diffuses too much, but the model would still argue for some diffusion of memory over time. This doesn't seem consistent with the current model. I know the authors attempt to address this in the Discussion section of the current manuscript but I would encourage the authors to clarify their position.</p></disp-quote><p>Thank you for the opportunity to clarify. The direct storage model, which involves only diffusion, does not include a nonlinear &quot;sudden-death&quot; process. Instead, the error of recall will simply grow, continuously and monotonically, over time; it's still possible in this model that a noisy, discrete-in-time experiment will result in the appearance of a sudden-death event where there really is only continuous degradation in the underlying system (e.g. beyond some threshold of memory degradation, noise in the report or observation will make the memory appear to be &quot;gone&quot;). On the other hand, if information is stored in a well-coded way, according to some good error-correcting code, we would expect inherently sharp threshold behavior: such codes display a characteristic level of noise below which they can effectively suppress most error, and above which they are guaranteed to fail, and then their errors are large. Thus, the model would predict a relatively small accumulation of error over some interval, followed by a super-linear increase in squared error. We now clarify this point in the manuscript.</p><disp-quote content-type="editor-comment"><p>9) This study uses the co-authors' human psychophysical data from Pertzov et al., 2016 Journal of Experimental Psychology. That study decomposed errors into three sources: (1) noisy representation; (2) mis-binding or non-target responses; and (3) random guessing. They reported that all three of these components increased with higher load and with longer delays. How does these prior findings relate to the present study? Are these different sources subsumed by the present model? Or are these important features that the present model (in the diffusive regime) does not account for? Does the present model produce only the first type of errors? The Authors mention that in another regime of the model, non-diffusive errors can produce pure guessing errors. Can the model speak to the mechanisms of mis-binding errors? Please include discussion of this point.</p></disp-quote><p>Re. Misbinding: The current model does not address this source of error. We now clarify this fact in the text. Our model in its present form is presented a single feature dimension, in this case orientation, and thus it does not consider the binding problem/binding errors. Note that our model could in principle be extended to take into account the joint storage of pairs or more of features per item, by representing those features as part of a higher-dimensional continuous attractor network, as in the joint population code model considered by Matthey, Bays and Dayan (2015); this is certainly of future interest of us (but of course out of scope of the current work). We have now added a note about this point in the Discussion.</p><p>Re. sudden death: we have now clarified in Discussion how sudden death can be consistent with our framework: &quot;In our framework, good encoding ensures that for noise below a threshold, the decoder can recover an improved estimate of the stored variable; however, strong codes exhibit sharp threshold behavior as the noise in the channel is varied smoothly. [â¦]We note, however, that the fits to the data shown here were all in the below-threshold regime.&quot;</p><disp-quote content-type="editor-comment"><p>10) Regarding the implications for neural representations: The Authors discuss that one prediction of the model would be signatures of exponentially strong codes in neural representations. As I understand it, one way this could be implemented is that each of the N memory networks has a different spatial period for its periodic coding, as in the case of grid cells. The other feature of the present model is that for multi-item working memory, a memory network contains signals for all of the K items. It would be helpful if the Authors can clarify what the implications on neural representations are for this feature of distributed multi-item coding. Does this imply that single neurons would show mixed selectivity for multiple items? Please include discussion of this point.</p></disp-quote><p>Good question. It is difficult to imagine any scenario involving non-mixed selectivity for items in a strong-coding scheme, thus indeed mixed selectivity would be a prediction of such a such a scheme. We already had a longer discussion of the question of tuning curves for strong codes under the heading &quot;Are neural representations consistent with exponentially strong codes?&quot; in the Discussion. We have now added a comment about mixed selectivity there.</p></body></sub-article></article>