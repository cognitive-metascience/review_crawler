<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">40224</article-id><article-id pub-id-type="doi">10.7554/eLife.40224</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Bayesian analysis of retinotopic maps</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-115280"><name><surname>Benson</surname><given-names>Noah C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2365-8265</contrib-id><email>nben@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-39589"><name><surname>Winawer</surname><given-names>Jonathan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7475-5586</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology</institution><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Center for Neural Sciences</institution><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schira</surname><given-names>Mark</given-names></name><role>Reviewing Editor</role><aff><institution>University of Wollongong</institution><country>Australia</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>06</day><month>12</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e40224</elocation-id><history><date date-type="received" iso-8601-date="2018-07-18"><day>18</day><month>07</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-11-29"><day>29</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Benson and Winawer</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Benson and Winawer</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-40224-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.40224.001</object-id><p>Human visual cortex is organized into multiple retinotopic maps. Characterizing the arrangement of these maps on the cortical surface is essential to many visual neuroscience studies. Typically, maps are obtained by voxel-wise analysis of fMRI data. This method, while useful, maps only a portion of the visual field and is limited by measurement noise and subjective assessment of boundaries. We developed a novel Bayesian mapping approach which combines observation–a subject’s retinotopic measurements from small amounts of fMRI time–with a prior–a learned retinotopic atlas. This process automatically draws areal boundaries, corrects discontinuities in the measured maps, and predicts validation data more accurately than an atlas alone or independent datasets alone. This new method can be used to improve the accuracy of retinotopic mapping, to analyze large fMRI datasets automatically, and to quantify differences in map properties as a function of health, development and natural variation between individuals.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>vision</kwd><kwd>vision science</kwd><kwd>visual neuroscience</kwd><kwd>retinotopy</kwd><kwd>retinotopic map</kwd><kwd>occipital cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01 EY027964</award-id><principal-award-recipient><name><surname>Benson</surname><given-names>Noah C</given-names></name><name><surname>Winawer</surname><given-names>Jonathan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01 EY027401</award-id><principal-award-recipient><name><surname>Benson</surname><given-names>Noah C</given-names></name><name><surname>Winawer</surname><given-names>Jonathan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01 MH111417</award-id><principal-award-recipient><name><surname>Benson</surname><given-names>Noah C</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R00-EY022116</award-id><principal-award-recipient><name><surname>Winawer</surname><given-names>Jonathan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel Bayesian method of modeling retinotopic maps is more accurate than traditional voxel-wise methods and can be used to automatically derive high-quality maps.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Visual responses in a substantial part of the human brain are organized into retinotopic maps, in which nearby positions on the brain represent adjacent locations in the image. Accurate measurement of these maps using functional magnetic resonance imaging (fMRI) is essential to a wide range of neuroscience and clinical applications (<xref ref-type="bibr" rid="bib59">Wandell and Winawer, 2011</xref>), in which they often provide a basis to compare measurements across individuals, groups, tasks, stimuli, and laboratories. In particular, maps are employed to study homology between species (<xref ref-type="bibr" rid="bib47">Sereno and Tootell, 2005</xref>), cortical plasticity (<xref ref-type="bibr" rid="bib58">Wandell and Smirnakis, 2009</xref>), individual variation in cortical function (<xref ref-type="bibr" rid="bib13">Dougherty et al., 2003</xref>; <xref ref-type="bibr" rid="bib27">Harvey and Dumoulin, 2011</xref>), and development (<xref ref-type="bibr" rid="bib54">Van Essen, 1997</xref>; <xref ref-type="bibr" rid="bib11">Conner et al., 2004</xref>). Many studies of cortical visual function in human, whether in motion (<xref ref-type="bibr" rid="bib33">Huk et al., 2001</xref>), color (<xref ref-type="bibr" rid="bib17">Engel et al., 1997a</xref>), object recognition (<xref ref-type="bibr" rid="bib26">Grill-Spector et al., 1998</xref>), or attention (<xref ref-type="bibr" rid="bib39">Martínez et al., 1999</xref>), include retinotopic mapping as a first step. Finally, basic properties of the maps themselves, such as the cortical magnification function (mm of cortex per degree of visual field), can be used to understand visual performance (<xref ref-type="bibr" rid="bib16">Duncan and Boynton, 2003</xref>).</p><p>Despite their broad importance to neuroscience research, no method currently exists to fit a retinotopic map to a subject’s cortical surface based on measurement, without human intervention. Rather, most retinotopic analyses of fMRI data use a voxel-wise approach. The general method is (1) to measure responses to mapping stimuli, (2) to derive retinotopic coordinates for each voxel or surface vertex by analyzing traveling waves (<xref ref-type="bibr" rid="bib46">Sereno et al., 1995</xref>; <xref ref-type="bibr" rid="bib18">Engel et al., 1997b</xref>) or by solving a population receptive field (pRF) model (<xref ref-type="bibr" rid="bib15">Dumoulin and Wandell, 2008</xref>) for each voxel, and (3) to identify areal boundaries by visual inspection. Aside from requiring significant time and effort, the maps that result from this process retain many common sources of error including distortion of the BOLD signal due to partial voluming (<xref ref-type="bibr" rid="bib14">Dukart and Bertolino, 2014</xref>), vessel artifacts (<xref ref-type="bibr" rid="bib61">Winawer et al., 2010</xref>), other sources of physiological noise, and model fitting biases (<xref ref-type="bibr" rid="bib9">Binda et al., 2013</xref>). Due to the various sources of noise, the measured maps have discontinuities and often systematically miss portions of the visual field, such as the vertical meridian (<xref ref-type="bibr" rid="bib49">Silver et al., 2005</xref>; <xref ref-type="bibr" rid="bib36">Larsson and Heeger, 2006</xref>; <xref ref-type="bibr" rid="bib52">Swisher et al., 2007</xref>; <xref ref-type="bibr" rid="bib4">Arcaro et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Mackey et al., 2017</xref>). Further, the measured maps are limited by the available stimulus field of view in the scanner, often as little as 6–12° of eccentricity, and have difficulty measuring the foveal representation (<xref ref-type="bibr" rid="bib44">Schira et al., 2009</xref>), the portion of the maps most important for many visual tasks including reading and object recognition (<xref ref-type="bibr" rid="bib38">Malach et al., 2002</xref>). These many shortcomings of the traditional retinotopic mapping process derive from the fact that it is organized around optimizing the explanatory power of the retinotopy solutions from individual voxels, rather than that of the entire visual field or cortical area. As a consequence, it yields maps that are neither smooth nor complete—nor grounded in any context of how the visual field is warped onto the cortical surface. Lacking these data, the comparison of maps between subjects is difficult, and precise quantitative examination of individual differences is impossible. We refer to retinotopic maps predicted using voxel-wise methods as being derived from ‘Data Alone’ (<xref ref-type="fig" rid="fig1">Figure 1</xref>) because the pRF parameters of the individual voxels come from empirical measurements but are not contextualized in a model of retinotopic maps.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.002</object-id><label>Figure 1.</label><caption><title>We compare three different ways to predict a subject’s retinotopic maps.</title><p>The first method is to perform a retinotopic mapping experiment. The fMRI measurements are converted to retinotopic coordinates by a voxel-wise model and projected to the cortical surface. Although a model is used to identify the coordinates for each vertex or voxel, we call this ‘Data Alone’ because no spatial template of retinotopy is used. The second method is to apply a retinotopic atlas to an anatomical scan (typically a T1-weighted MRI) based on the brain’s pattern of sulcal curvature. This is called ‘Anatomy Alone’ because no functional MRI is measured for the individual. The third method combines the former two methods via Bayesian inference, using the brain’s anatomical structure as a prior constraint on the retinotopic maps while using the functional MRI data as an observation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig1-v2.tif"/></fig><p>An alternative to voxel-wise modeling of fMRI data is to build a retinotopic atlas—a computational model of the mapping between visual field position and cortical structure. Atlases are typically fit to a group-average description of function on the cortical surface after inter-subject cortical surface co-registration (<xref ref-type="bibr" rid="bib12">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib19">Fischl et al., 1999a</xref>). An example group-average description of retinotopy from the Human Connectome Project (<xref ref-type="bibr" rid="bib53">Uğurbil et al., 2013</xref>; <xref ref-type="bibr" rid="bib56">Van Essen et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Benson et al., 2018</xref>) and the corresponding atlas description are shown in <xref ref-type="fig" rid="fig2">Figure 2A and B</xref>. Such descriptions are useful despite large inter-subject variation because co-registration of the surface anatomies between subjects improves the inter-subject alignment of cortical function as well. For example, the surface area of V1 can vary by 2- to 3-fold across healthy adults (<xref ref-type="bibr" rid="bib13">Dougherty et al., 2003</xref>; <xref ref-type="bibr" rid="bib51">Stensaas et al., 1974</xref>; <xref ref-type="bibr" rid="bib3">Andrews et al., 1997</xref>), yet atlases of the mean anatomical locations (<xref ref-type="bibr" rid="bib60">Wang et al., 2015</xref>) and the mean functional organization (<xref ref-type="bibr" rid="bib6">Benson et al., 2012</xref>; <xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>) of V1 can predict the functional organization of left-out subjects with high accuracy. The atlas, after being fit to training data, is applied to an individual anatomical MR image without functional data via anatomical alignment of the image to the atlas followed by interpolation (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). These atlases solve two of the problems of voxel-wise retinotopic maps: they represent the entire visual field and are smooth, but they are limited by the quality of the anatomical alignment and provide only a description of the mean—they cannot capture the idiosyncrasies of the maps in an individual subject because they assume that once a correspondence is found between the sulcal pattern in two subjects’ visual cortices, the function will match. Thus, if one were interested in individual variation in cortical topography <italic>after</italic> anatomical registration, this method is uninformative: it assumes the answer is 0. Accordingly, we refer to retinotopic maps predicted by atlases as being derived from ‘Anatomy Alone’ (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.003</object-id><label>Figure 2.</label><caption><title>The retinotopic prior and its use in predicting retinotopic maps.</title><p>(<bold>A</bold>) The retinotopic prior is based on the Human Connectome Project (HCP) group-average retinotopic maps (<xref ref-type="bibr" rid="bib8">Benson et al., 2018</xref>), shown here on an orthographic projection of the V1-V3 region. OP indicates the occipital pole, and CaS indicates the Calcarine sulcus. Projections are identical in each row throughout. (<bold>B</bold>) The retinotopic prior was designed to resemble the HCP group-average retinotopy, and was further warped to minimize differences between the two according to the methods described by <xref ref-type="bibr" rid="bib7">Benson et al. (2014)</xref>. (<bold>C</bold>) The measured (‘Data Alone’) retinotopic maps of subject S1201, all scans combined. Comparison of rows <italic>B</italic> and <italic>C</italic> demonstrates that the use of the retinotopic prior to predict the retinotopic maps of an individual subject results in a reasonable prediction. (<bold>D</bold>) Combining the retinotopic prior with the observed retinotopic maps from an individual subject yields Bayesian inferred maps.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig2-v2.tif"/></fig><p>In this paper, we present a solution to the problems of both atlas- and voxel-based retinotopic maps. We hypothesize that a Bayesian model of retinotopic maps, combining data (retinotopic voxel- or vertex-wise measurements) with a prior (a full-field atlas derived from the anatomy), will eliminate many of the issues with retinotopic mapping described above by optimizing the description of cortical retinotopic maps in the context of the full visual field and the corresponding cortex (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). We propose that such methods can describe cortical retinotopic maps in individual subjects more accurately than an atlas alone or measurements alone. These hypotheses are motivated by two factors. First, previous work employing functional data to supplement global anatomical alignments between subjects has found an increase in the overlap of functional ROIs drawn from independent localizers (<xref ref-type="bibr" rid="bib22">Frost and Goebel, 2013</xref>). Thus, even when subjects are aligned anatomically, appreciable and systematic differences in the structure-function relationship remain. Allowing the measurement from an individual subject to inform the alignment will, in part, capture these individual differences. Secondly, we believe that the basic form of the atlas (the prior) is sufficiently accurate that incorporating it will result in a more accurate estimate of the retinotopic map than the measurements alone.</p><p>The method we employ is a Bayesian maximum-likelihood optimization that describes the retinotopic maps in striate and extra striate cortex with previously infeasible precision. Unlike previous work on functional alignment (<xref ref-type="bibr" rid="bib28">Haxby et al., 2011</xref>), we perform alignment between each individual subject’s retinotopic parameters and a model of retinotopy described on the (anatomically-aligned) group-average cortical surface. This optimization builds on previous work using iterative approaches to fit and interpolate smooth retinotopic maps in individual subjects (<xref ref-type="bibr" rid="bib13">Dougherty et al., 2003</xref>) by incorporating an explicit prior in the place of human intervention and adopting an explicitly Bayesian formulation.</p><p>We publish with this paper a tool capable of implementing the method we describe as well as all source code employed. We use these tools to characterize retinotopic maps from several subjects in terms of the precise warping from visual field to visual cortex. Using these characterizations, we are able to quantify the extent to which variations in retinotopic organization are due to anatomical differences versus differences in the structure-function relationship. We show that, in fact, these two sources of variability—differences in structure and differences in the structure-to-function mapping—are roughly equal and orthogonal across subjects. This means that after warping individual cortical surfaces to bring the anatomies into registration, an additional warping, equal in size, is needed to bring the functional maps into alignment, thereby demonstrating substantial variability in an early, sensory region of the human brain.</p></sec><sec id="s2" sec-type="results|discussion"><title>Results and discussion</title><p>Retinotopic mapping experiments were performed on eight subjects using fMRI. Twelve individual retinotopy scans were performed on each subject then combined into 21 ‘training’ datasets and 1 ‘validation’ dataset for cross-validation as well as one full dataset of all scans for detailed analysis (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>; see also Materials and methods). Predicted maps were then generated using the training datasets and compared to the validation dataset. The training and validation datasets are largely independent in that they are derived from separate scans; however, some dependency remains in that the different scans were obtained from the same scanning session, so that they share anything common to the session (viewing conditions, scanner hardware, etc.). We compared three methods for predicting retinotopic maps (<xref ref-type="fig" rid="fig1">Figure 1</xref>): (1) using the training data alone as a prediction (the ‘observed’ maps); (2) using the subject’s anatomy alone to apply an anatomically-defined template of retinotopy (the ‘prior’ maps); and (3) combining data with anatomy (observation with prior) using Bayesian inference (the ‘inferred’ maps). We then leverage the differences between these methods to characterize the pattern of individual differences in retinotopic maps across subjects. The prior map (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), used for methods 2 and 3, was derived from fitting a template to a high-quality dataset derived from 181 subjects in the Human Connectome Project (<xref ref-type="bibr" rid="bib53">Uğurbil et al., 2013</xref>; <xref ref-type="bibr" rid="bib56">Van Essen et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Benson et al., 2018</xref>) (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><sec id="s2-1"><title>Bayesian inference has the advantages of an anatomical atlas while also respecting individual differences</title><p>The inferred retinotopic maps, predicted by Bayesian inference, provide high-quality descriptions of the full retinotopic topology for each subject’s V1-V3 regions. These maps can be produced even in the absence of observed retinotopic measurements (i.e., using the prior alone), or by combination with retinotopic data. In this latter case, the inferred maps have all the advantages of the retinotopic prior (topologically smooth maps, predictions beyond the stimulus aperture, etc.), while also accounting for idiosyncrasies in individual maps. Three examples of maps that demonstrate this advantage are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. The first two columns show maps in which, relative to the validation maps, the predictions made from data alone have highly curved iso-eccentricity contours. These contours reflect noise rather than true curvature in the iso-eccentricity contours, as shown by the validation data. For these two columns, the predictions from the prior alone have iso-eccentric contours that are too smooth (as compared to the validation data). The correct lines appear to lie between the training data and the prior. Hence when data is combined with the prior (<xref ref-type="fig" rid="fig3">Figure 3</xref>, third column) the iso-eccentric contours resemble those of the validation dataset. The third row of <xref ref-type="fig" rid="fig3">Figure 3</xref> shows an instance in which, even lacking a coherent polar angle reversal to define the ventral V1/V2 boundary near the fovea in the predictions made from data alone, combination of the data with prior more accurately predicts that boundary in the validation dataset than the prior alone.</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.004</object-id><label>Figure 3.</label><caption><title>Inferred retinotopic maps accurately predict features of validation retinotopy.</title><p>Twelve close-up plots of the retinotopic maps of three hemispheres are shown with predictions made from Data alone, Prior alone, or Data +Prior. The right two columns show the validation dataset with the right column indicating the context of the close-up patches. The first three columns show different methods of predicting the retinotopic maps, as in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Approximate iso-eccentricity or iso-angular contour lines for the validation dataset have been draw in white on all close-up plots. Black contour lines show the same approximate contour lines for the three prediction methods. Flattened projections of cortex were created using an orthographic projection (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2A</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig3-v2.tif"/></fig><p>In constructing the Bayesian-inferred retinotopic map for a single hemisphere, we perform two deformations, detailed in <xref ref-type="fig" rid="fig4">Figure 4</xref> (see also Materials and methods): (1) we first deform that hemisphere’s inflated surface to register it to an average anatomical atlas using FreeSurfer (leftmost arrow in <xref ref-type="fig" rid="fig4">Figure 4C</xref>); and (2) we then further deform the surface to register it to the retinotopic prior, based on the hemisphere’s retinotopic measurements (second arrow in <xref ref-type="fig" rid="fig4">Figure 4C</xref>). These steps together account for the individual differences in the organization of the subjects’ retinotopic maps. In step 1, we account for structural differences across subjects—the deformation that occurs for this registration is unique for each subject. This is where prior work ended (<xref ref-type="bibr" rid="bib6">Benson et al., 2012</xref>; <xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>). In step 2, we account for the differences in the relationship between structure and function <italic>across</italic> subjects. Although it is possible that deformations in step two partly compensate for imperfections in the first two registrations, we propose that, to a first approximation, the deformations applied in the last step indicate meaningful individual differences in the structure-function relationship.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.005</object-id><label>Figure 4.</label><caption><title>Deriving retinotopic predictions.</title><p>Three methods of predicting retinotopic maps (as in <xref ref-type="fig" rid="fig1">Figure 1</xref>) for an example subject. (<bold>A</bold>) Predicted retinotopic maps based on training data alone are found by solving the pRF models for each voxel and projecting them to the cortical surface. The training data (left) and prediction (right) are identical. (<bold>B</bold>) To predict a retinotopic map using the prior alone, the subject’s cortical surface is aligned to FreeSurfer’s <italic>fsaverage</italic> anatomical atlas (represented by rectilinear checkerboards), bringing the subject’s anatomy into alignment with the anatomically-based prior, which is represented by iso-eccentricity contour lines in the figure (see also <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2C</xref>). The model of retinotopy is then used to predict the retinotopic parameters of the vertices based on their anatomically-aligned positions. After the predictions have been made, the cortical surface is relaxed. Maps are shown as checkerboards in order to demonstrate the warping (insets show original data and curvature). (<bold>C</bold>) Bayesian inference of the retinotopic maps of the subject are made by combining retinotopic data with the retinotopic prior. This is achieved by first aligning the subject’s vertices with the <italic>fsaverage</italic> anatomical atlas (as in <bold>B</bold>) then further warping the vertices to bring them into alignment with the data-driven model of retinotopy (shown as iso-eccentricity contour lines). The warping was performed by minimizing a potential function that penalized both the deviation from from the prior (second column) as well as deviations between the retinotopic observations and the retinotopic model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig4-v2.tif"/></fig></sec><sec id="s2-2"><title>Individual differences in the V1-V3 structure-function relationship across subjects are substantial</title><p>The Bayesian model fitting allows us to parcellate two sources of variation between individuals: differences in surface anatomy (sulcal patterns) and differences in structure-to-function mapping, that is how retinotopic features map onto the surface anatomy. These two sources of variation map approximately to the two deformations in our atlas fitting: differences in surface anatomy are reflected in the deformation used for the surface alignment, and differences in structure-to-function mapping are reflected in the deformation for retinotopic alignment (the Bayesian fit). Note that in our implementation, both alignments are achieved by warping the individual subject’s vertices, the former to minimize error in surface curvature, the latter to minimize errors in retinotopic measures. (After the process is complete, the alignments can be reversed, thereby bringing the retinotopic predictions back into the native anatomical space.) Because both the anatomical alignment and retinotopic alignment are computed as changes in the position of surface vertices, it is straightforward to compare the two processes.</p><p>There are some subjects for whom there are large differences between the retinotopic atlas defined by the prior and the atlas defined by the Bayesian fit (<xref ref-type="fig" rid="fig5">Figure 5A and B</xref>). In this example subject, the iso-eccentricity lines in the Bayesian atlas are substantially more compressed along the posterior-anterior axis compared to the anatomical atlas, and the iso-angle lines in V2/V3 are more dorsal compared to the anatomical atlas. Where there are discrepancies, the Bayesian inferred map is more accurate. For example, the polar angles and the eccentricities in the validation data are approximately constant where the Bayesian map predicts iso-angle and iso-eccentricity lines, but not where the prior map predicts them (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). This indicates that even after anatomical alignment, the retinotopy in this subject differs systematically from the prior. For some other subjects, the two atlases are in closer agreement such that the prior alone is a good fit to the retinotopic data (<xref ref-type="fig" rid="fig5">Figure 5C</xref>).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.006</object-id><label>Figure 5.</label><caption><title>Comparison of inferred and prior maps.</title><p>(<bold>A</bold>) A subject whose maps were poorly predicted by the retinotopic prior and thus required major deformation (S1205, RH, dataset 9). (<bold>B</bold>) To illustrate the differences between the Prior Alone (black lines in <italic>A</italic>) and the combination of Data +Prior (white lines in <italic>A</italic>), traces of the polar angle (top) and eccentricity (bottom) values beneath the lines indicated by arrows are shown. The eccentricities traced by the iso-angle lines and the polar angles traced by the iso-eccentricity lines of the Bayesian-inferred maps more closely match the angles/eccentricities of their associated trace lines than do the polar angles/eccentricities beneath the lines of the Prior alone (<bold>C</bold>) A subject whose retinotopic maps were well-predicted by the prior and thus required relatively minor deformation during the Bayesian inference step (subject S1202, LH, dataset 17). In both <bold>A</bold> and <bold>C</bold>, black lines show the retinotopic prior and white lines show the maps inferred by Bayesian inference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig5-v2.tif"/></fig><p>To quantify the two types of deformations, we calculated the mean 3 × 3 distance matrix between (1) a vertex’s native position, (2) its position in the anatomical alignment (<italic>fsaverage</italic> position), and (3) its ‘retinotopic position’ after alignment to the retinotopic prior. All vertices in the V1-V3 region within 12° of eccentricity, as predicted by the Bayesian inference on the full dataset (all retinotopy scans combined), were used. We then performed 2D metric embedding to determine the mean deformation steps and the mean angle between them (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Overall, the mean deformation distance across vertices is 3.3° ± 0.6° (<italic>µ</italic> ± <italic>σ</italic> across subjects) of the cortical sphere for the anatomical alignment and 3.4° ± 0.4° for the retinotopic alignment. The mean angle between these deformations is 83.0° ± 9.6° (note that this last measurement is in terms of degrees of rotational angle rather than degrees of the cortical sphere). The anatomical alignment corresponds to variation in the surface topology and is accounted for in anatomical atlases (<xref ref-type="bibr" rid="bib6">Benson et al., 2012</xref>; <xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>); the retinotopic alignment corresponds to variation in the structure-function mapping and is accounted for in the Bayesian model. An additional summary measurement of these deformations, the root-mean-square deviation (RMSD) distances across vertices near the occipital pole in a particular retinotopy dataset, provides a metric of the total warping applied during each step of the alignment process for each subject. A summary of this measurement, as well as various other summary statistics is provided in <xref ref-type="table" rid="table1">Table 1</xref>.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.007</object-id><label>Figure 6.</label><caption><title>Individual differences between subjects in the structure-function relationship are substantial.</title><p>(<bold>A</bold>) The mean deformation vectors, used to warp a surface vertex from its Native to its Anatomical (<italic>fsaverage</italic>-aligned) position and from its Anatomical to its Retinotopic position, are shown relative to each other. The wedges plotted beneath the mean arrows indicate ±1 standard deviation of the angle across subjects while the shaded regions at the end of the wedges indicate ±1 standard deviation of the lengths of the vectors. Note that because registration steps are always performed on a subject's inflated spherical hemispheres, these distances were calculated in terms of degrees of the cortical sphere and are not directly equivalent to mm of cortex. (<bold>B</bold>) The alignment of the V1-V3 region to the retinotopic prior increases the standard deviation of the surface curvature across subjects, suggesting that retinotopic alignment is not simply an improvement on FreeSurfer’s curvature-based alignment. Histograms show the probability density of the across-subject standard deviation of curvature values for all vertices in the V1-V3 region with a Bayesian-inferred eccentricity between 0 and 12°. (<bold>C</bold>) Bayesian-inferred iso-eccentricity lines and V1/V2/V3 boundaries plotted for all subjects simultaneously on the <italic>fsaverage</italic> spherical atlas. Lines are plotted with an opacity of 1/2 to visualize overlap. The left two plots and the right two plots share identical lines but have different colors. Iso-eccentricity lines are colored in magenta (1.5°), yellow (3°), and cyan (6°). Iso-angle lines are plotted in blue (upper vertical meridian), green (horizontal meridian), and red (lower vertical meridian).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig6-v2.tif"/></fig><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.008</object-id><label>Table 1.</label><caption><title>Summary statistics for each subject.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Subject</th><th valign="top">Hemisphere</th><th valign="top">V1 area (mm<sup>3</sup>)<sup>*</sup></th><th valign="top">V1 volume (mm<sup>3</sup>)<sup>*</sup></th><th valign="top">Anatomical RMSD<sup>†</sup></th><th valign="top">Retinotopic RMSD<sup>†</sup></th></tr></thead><tbody><tr><td valign="top">S1201</td><td valign="top">RH</td><td valign="top">1308</td><td valign="top">3733</td><td valign="top">2.88</td><td valign="top">3.15</td></tr><tr><td valign="top">S1201</td><td valign="top">LH</td><td valign="top">1315</td><td valign="top">4133</td><td valign="top">1.82</td><td valign="top">2.47</td></tr><tr><td valign="top">S1202</td><td valign="top">RH</td><td valign="top">2024</td><td valign="top">3706</td><td valign="top">1.21</td><td valign="top">2.73</td></tr><tr><td valign="top">S1202</td><td valign="top">LH</td><td valign="top">2085</td><td valign="top">4199</td><td valign="top">1.28</td><td valign="top">2.65</td></tr><tr><td valign="top">S1203</td><td valign="top">RH</td><td valign="top">1574</td><td valign="top">3152</td><td valign="top">2.13</td><td valign="top">3.27</td></tr><tr><td valign="top">S1203</td><td valign="top">LH</td><td valign="top">1489</td><td valign="top">2941</td><td valign="top">2.06</td><td valign="top">3.77</td></tr><tr><td valign="top">S1204</td><td valign="top">RH</td><td valign="top">1906</td><td valign="top">3325</td><td valign="top">2.29</td><td valign="top">3.00</td></tr><tr><td valign="top">S1204</td><td valign="top">LH</td><td valign="top">1645</td><td valign="top">3015</td><td valign="top">2.18</td><td valign="top">3.10</td></tr><tr><td valign="top">S1205</td><td valign="top">RH</td><td valign="top">1995</td><td valign="top">3926</td><td valign="top">1.99</td><td valign="top">2.91</td></tr><tr><td valign="top">S1205</td><td valign="top">LH</td><td valign="top">1884</td><td valign="top">3372</td><td valign="top">1.76</td><td valign="top">3.31</td></tr><tr><td valign="top">S1206</td><td valign="top">RH</td><td valign="top">1647</td><td valign="top">3116</td><td valign="top">2.12</td><td valign="top">2.73</td></tr><tr><td valign="top">S1206</td><td valign="top">LH</td><td valign="top">1632</td><td valign="top">2692</td><td valign="top">1.63</td><td valign="top">3.22</td></tr><tr><td valign="top">S1207</td><td valign="top">RH</td><td valign="top">1648</td><td valign="top">3402</td><td valign="top">1.84</td><td valign="top">2.41</td></tr><tr><td valign="top">S1207</td><td valign="top">LH</td><td valign="top">1421</td><td valign="top">2764</td><td valign="top">1.74</td><td valign="top">3.15</td></tr><tr><td valign="top">S1208</td><td valign="top">RH</td><td valign="top">1712</td><td valign="top">3509</td><td valign="top">1.50</td><td valign="top">2.58</td></tr><tr><td valign="top">S1208</td><td valign="top">LH</td><td valign="top">1494</td><td valign="top">3083</td><td valign="top">1.89</td><td valign="top">3.08</td></tr></tbody></table><table-wrap-foot><fn><p><sup>*</sup> The V1 boundary was determined from the Bayesian-inferred map constructed by combining the retinotopic prior with the full retinotopy dataset.</p><p>† Units of the RMSD values are degrees of the cortical sphere; these are approximately equivalent to mm, but exact measurements in mm are distorted during inflation of the surface. ‘Anatomical’ RMSD refers to the deviation between the subject’s native anatomical sphere and the <italic>fsaverage</italic>-aligned sphere while ‘Retinotopic’ RMSD refers to the deviation between the <italic>fsaverage</italic>-aligned sphere and the retinotopically aligned sphere. The RMSD values were averaged over all vertices within the inner 12° of eccentricity of the V1-V3 region. Use of a larger patch of cortex (e.g., the flattened map projections in <xref ref-type="fig" rid="fig4">Figure 4A</xref>) does not qualitatively change the relationship between anatomical and retinotopic RMSD values.</p></fn></table-wrap-foot></table-wrap><p>The retinotopic deformation distances were not significantly different than the the anatomical deformation distances, which were still substantial; this is true whether one calculates the mean deformation distance over the entire patch of cortex immediately around the occipital pole (shown in the maps in <xref ref-type="fig" rid="fig3">Figure 3</xref>) or over only the vertices that are predicted to be in V1-V3; <xref ref-type="fig" rid="fig6">Figure 6</xref> is calculated over the latter of these two ROIs. Note that if the retinotopic deformation distances had been much larger, the prior anatomical atlases would have been less accurate. Had they been close to 0, then the anatomical atlas alone would have been as accurate as the Bayesian model.</p><p>We interpret the warping performed to align retinotopic data to the anatomical prior (the retinotopic alignment) as evidence of individual differences in the way in which retinotopy maps to sulcal topology. An alternate possibility is that the retinotopic alignment corrects for incomplete or incorrect warping performed by FreeSurfer during alignment of each subject’s sulcal topology to that of the <italic>fsaverage</italic> hemispheres (the anatomical alignment). We have found FreeSurfer to be a well-vetted and reliable tool for functional alignment; however, no anatomical alignment process is optimal, and more improvements to alignment algorithms, such as those being pursued using the HCP database (<xref ref-type="bibr" rid="bib24">Glasser et al., 2016</xref>), may reduce the length of our retinotopic alignment step. (Note, additionally, that though we use the <italic>fsaverage</italic> here, our tools are compatible with other possible alignments.) It is thus possible that we have overestimated the amount of functional variance remaining across subjects after anatomical alignment. Two observations suggest that the functional variance due to imperfect anatomical alignment is small, however. First, the angle between the alignments is roughly orthogonal, meaning that there is very little movement along the axis of the first (structural) alignment during the second (retinotopic) alignment. Had the first alignment been in the correct direction but too conservative, then we would have expected the retinotopic alignment to be in the same (or similar) direction, rather than orthogonal. Second, if the retinotopic alignment served to correct an incomplete or incorrect anatomical warping, then anatomical metrics such as curvature would become more uniform across subjects after retinotopic alignment compared to after only anatomical alignment. <xref ref-type="fig" rid="fig6">Figure 6B</xref> demonstrates that this explanation is unlikely by showing the distribution across surface vertices of the standard deviation of curvature across subjects. When curvature is compared across subjects without retinotopic or anatomical alignment (‘Native’ alignment in <xref ref-type="fig" rid="fig6">Figure 6B</xref>) the standard deviation is quite high. When subjects are compared after anatomical alignment, the standard deviations are much lower. After further alignment to the anatomical prior of retinotopy (Retinotopic alignment), the standard deviation of curvature across subjects is between these two extrema. This suggests that the retinotopic alignment is sacrificing some amount of structural uniformity across subjects in order to accommodate the individual differences in subjects’ structure-to-function mapping, and is consistent with our interpretation that there are substantial individual differences in the mapping between retinotopy and surface topology.</p><p>The large individual differences that remain, even after structural co-registration (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), point to the importance of using at least some individual subject functional data when inferring the maps, rather than assuming the atlas (prior) is correct. The specific nature of these deformations, and whether, for example, they fall into a few basic patterns, is an important question about the natural variation of individual brains. Our new method, combined with large datasets such as the HCP retinotopy data set (<xref ref-type="bibr" rid="bib8">Benson et al., 2018</xref>) and new alignment tools such as MSMAll (<xref ref-type="bibr" rid="bib43">Robinson et al., 2014</xref>), could be used to address this question.</p></sec><sec id="s2-3"><title>The inferred maps make highly accurate predictions with very little data</title><p>To quantify the accuracy of our Bayesian-inferred retinotopic maps, and to compare the accuracy against other predictions, we used a cross-validation scheme such that predictions from data alone, the prior alone, or via Bayesian inference were compared against independent validation datasets (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). The validation datasets were derived from 6 of the 12 scans; the predictions from data alone and from Bayesian inference were derived from training datasets, which were comprised of various combinations of 1–6 independent scans (between 3.2 and 19.2 min of data). The predictions from the prior alone did not use training data.</p><p>To compare methods of predicting retinotopic maps (<xref ref-type="fig" rid="fig1">Figure 1</xref>), vertices of interest were identified using the maps inferred from the validation dataset. All vertices from the inner 12° of eccentricity of these maps were compared to their counterparts in the predicted maps. Note that the inferred maps from the validation dataset were used only to identify the vertices included in the comparison; the retinotopic coordinates from the validation datasets themselves were taken as the ‘gold-standard’ measurements. In computing prediction accuracy, we weighted the vertices by the fraction of variance explained for each vertex’s pRF solution in the validation dataset. For the three types of training datasets (prior alone, data alone, Bayesian inference), we assume that each vertex makes a prediction regardless of the variance explained. To prevent errors at high eccentricity from dominating the error metric, we calculated the scaled error for a vertex to be the angular distance in the visual field between its retinotopic coordinates in the predicted map and the validation dataset, divided by the eccentricity in the validation dataset. <xref ref-type="fig" rid="fig7">Figure 7</xref> shows the scaled mean squared error (MSE) for the various predicted maps in terms of the amount of time spent collecting retinotopic data for the map.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.009</object-id><label>Figure 7.</label><caption><title>Comparison of prediction errors for three methods of predicting retinotopic maps.</title><p>Errors are shown in terms of the number of minutes spent collecting retinotopic mapping scans (<italic>x</italic>-axis). The <italic>y</italic>-axis gives the mean squared eccentricity-scaled error. Each plotted point represents a different number of minutes in the scanner, with error bars plotting ±1 standard error across subjects. For short scan times, errors are significantly higher for predictions made with the data alone than for those made using Bayesian inference. An offset has been added to the <italic>x</italic>-values of each of the black (−0.25) and red (+0.25) points to facilitate viewing.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig7-v2.tif"/></fig><p>The maps predicted via Bayesian inference were highly accurate irrespective of the amount of data used to inform the fits (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Between those inferred maps informed by 3.2 min of scan time (one scan) and 19.2 min (six scans), the scaled MSE of the prediction remains in the range of ~0.4–0.5. These scaled errors are larger near the fovea because the denominator used for scaling the error metric (i.e., the eccentricity) could be very small; when the range is limited to 3 to 12 deg, the MSE is much lower,~0.20–0.26. Expressed separately in units of polar angle and eccentricity, the mean absolute polar angle error from a Bayesian map derived from a single 3.2 min scan is 25° ± 11° and the mean absolute eccentricity error is 0.76° ± 0.34° (<italic>µ</italic> ± <italic>σ</italic> across 96 datasets). For the prior alone, these errors are substantially higher: 34° ± 12° for polar angle and 1.3° ± 0.17° for eccentricity. Note that these errors are approximately 3 × higher than those reported for previous versions of the anatomical prior (11° for polar angle and 0.37° for eccentricity) (<xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>); however, these discrepancies are due to differences in the metric used, the amount of data collected, the thresholding applied, and the use of smoothing. Some of these factors we cannot reproduce exactly (amount of data) or have deliberately abandoned (smoothing), but by using the same metric (median absolute error across all vertices) and similar thresholding (1.25°&lt;predicted eccentricity&lt;8.75°), we obtain errors very close to those previously reported: 5.9° of polar angle and 0.46° of eccentricity. In contrast to the inferred maps for which the accuracy is largely independent of scan time, the accuracy of the predictions from data alone was highly influenced by scan time. The scaled MSE of the maps predicted from the training datasets alone for the same range of scan times ranged from ~2.2 (3.2 minutes of training data) to ~0.3 (19.2 minutes of training data). With more than ~11 min of scan time, the predictions made from the training datasets alone have a slightly lower scaled MSE than those made from Bayesian inference; although, notably, the improvements in scaled MSE for more than 11 min of scan time are small.</p><p>The prediction accuracy from the prior alone (0 min. scan time) was generally intermediate in accuracy between the predictions from the Bayesian model and the data alone. Predictions derived from the anatomically-defined atlas alone are more accurate than predictions from 3.2 min of scanning and only slightly less accurate than predictions from training datasets derived from 6.4 min of scanning; this is in agreement with previous analyses of prediction error versus measurement error in retinotopic mapping experiments (<xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>). The predictions using the prior alone were universally less accurate than the Bayesian predictions (<xref ref-type="fig" rid="fig7">Figure 7</xref>, cyan)—for all subjects and all training datasets, the combination of prior with data improved prediction accuracy compared to the anatomical prior alone. These data demonstrate that the application of our new method to a small amount of retinotopic mapping data yields a higher quality retinotopic map than can be derived from other sources alone, with the possible exception of data derived from a long retinotopic mapping session.</p><p>The fact that the increased prediction accuracy from the Bayesian maps is almost independent of the amount of scan time used for the observations suggests that much of the individual variability is captured by a low dimensional warping from the template, which can be inferred from a modest amount of data. This hypothesis is further supported by visual inspection of different datasets, such as in <xref ref-type="fig" rid="fig8">Figure 8A</xref>. Although the amount of noise in the maps clearly varies between the validation dataset (19.2 min. scan time, left column), training dataset 1 (3.2 min., second column), and training dataset 10 (6.4 min., third column), the signal is clear enough that a human expert would likely draw similar boundary lines for each map; our method does as well. Importantly, some warpings are not permitted by the fitting algorithm. For example, the topology of the template is a hard constraint, such that vertices cannot pass through one another. This puts an upper limit on the accuracy of the template: the best predictors of left out data might require a change in topology, which is not permitted. We discuss the significance of these issues in the section subsequent section, 'What is ground truth’.</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.010</object-id><label>Figure 8.</label><caption><title>Systematic errors in training and validation datasets.</title><p>(<bold>A</bold>) Many small inconsistencies in the retinotopic maps are duplicated in both the validation dataset and the training datasets but not in the maps predicted by Bayesian inference. Maps for three example hemispheres are shown with validation datasets as well as training dataset 1, training dataset 10, and the Bayesian-inferred maps from dataset 20. Ellipses highlight blips of noise in the validation maps that are unlikely to represent the true underlying map, but that are correlated with the training maps. Such blips are significantly different in the inferred and validation maps, likely inflating the error of the inferred maps. Black lines show the V1-V3 boundaries in the Bayesian-inferred maps. (<bold>B</bold>) Discontinuity errors. If the validation data is used to project the disks shown in the visual field in the middle panel to the cortical surface, the resulting map is messy and contains a number of inconsistencies due to measurement error. While the Bayesian inferred map may contain errors of its own, it will always predict a topologically smooth retinotopic map with respect to the topology of the visual field.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig8-v2.tif"/></fig><p>Another significant advantage of the method is that it eliminates the need for human intervention in the process of delineating retinotopic maps and visual areas. In most studies that require retinotopic mapping data, one or more experimenters hand-label the visual area boundaries. While human raters are better able to understand atypical retinotopic boundaries than our method, they are nonetheless subject to inter-rater disagreement and human error. Furthermore, although expert human raters have a much more nuanced prior about retinotopic map organization than our method, and thus may sometimes draw boundaries better than our method, our method at least makes its prior explicit and quantifiable, and, thus, comparable and replicable across studies.</p></sec><sec id="s2-4"><title>What is ground truth?</title><p>The motivation for a Bayesian approach to retinotopic mapping can be found most clearly in the measured retinotopic maps themselves. In all of our measured retinotopic maps, there are numerous systematic imperfections (<xref ref-type="fig" rid="fig8">Figure 8A</xref>), and the literature contains many reports of similar errors (<xref ref-type="bibr" rid="bib61">Winawer et al., 2010</xref>; <xref ref-type="bibr" rid="bib42">Press et al., 2001</xref>; <xref ref-type="bibr" rid="bib23">Gardner, 2010</xref>; <xref ref-type="bibr" rid="bib10">Boubela et al., 2015</xref>). These imperfections can arise from a variety of sources, including partial voluming (<xref ref-type="bibr" rid="bib32">Huettel et al., 2014</xref>), negative BOLD (<xref ref-type="bibr" rid="bib48">Shmuel et al., 2002</xref>), and large, draining veins. Imperfections due to blood vessel artifacts can have effects over large distances (<xref ref-type="bibr" rid="bib61">Winawer et al., 2010</xref>), and most perniciously, they may lead to large and reliable responses that nonetheless differ from the local neuronal activity in the voxel (<xref ref-type="bibr" rid="bib10">Boubela et al., 2015</xref>). Such artifacts can be difficult to track down and are often not eliminated by typical methods of cleaning up maps such as smoothing, thresholding, or simply collecting larger datasets.</p><p>Although with large datasets (&gt;19 min of scan time), the prediction accuracy for the validation dataset is highest using the data alone rather than the data and prior, we believe that even in these cases the combination of data + prior is probably closest to ground truth. We defined accuracy operationally as the difference from the validation set, as this provides a single set of independent measures that can be used to assess the accuracy of all three types of models. The validation dataset is defined by at least as many scans as any of the training datasets, and hence is our best measurement. However, the validation dataset is not ground truth, as it is subject to errors from systematic and random measurement noise. The inferred maps, unlike the maps from the validation and training datasets alone, produce a topologically smooth transformation into the visual field: they represent the complete visual field with no holes, redundancies, or discontinuities. Hence, an enclosed region in the visual field will project to an enclosed region in the inferred map on the cortical surface. This is not the case for the maps predicted by the data alone without the use of an atlas (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). We consider this difference to be an advantage of the inferred maps, since it is generally accepted that the cortical surface of V1 is a topological map of the visual field.</p><p>In short, since we do not correct for all of these potential sources of systematic error, we consider our estimates of error from the Bayesian-inferred maps to be conservative, and the estimate from the data-to-data predictions to be liberal.</p></sec><sec id="s2-5"><title>The Bayesian model accurately predicts visual field positions not included in the training data</title><p>One important advantage of using the method of Bayesian inference outlined in this paper is that it provides predictions beyond the extent of the stimulus aperture in the retinotopy experiment. These peripheral predictions extend to 90° of eccentricity, even though the data used to derive the prior was based on stimuli that only extended to ~8° of eccentricity. Hence, it is important to ask whether the model makes accurate predictions in the periphery. We demonstrate this in two ways. First, when our registration algorithm is run using only a subset of the eccentricity range (e.g., only data within the first 3° or the first 6° of eccentricity), the predicted maps remain accurate to 12° of eccentricity (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). Second, we compared wide-field retinotopy data, collected out to 48° of eccentricity from subject S1201 to the Bayesian-inferred map predictions made using our data with a 12°-aperture (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). We find that in both cases, our method is highly accurate despite lacking training data for peripheral measurements (though note that in the latter case, the extrapolation was only tested on one subject; in principle, subjects with poorer data quality or unstable fixation could result in less accurate extrapolation). Because the extrapolations to untested eccentricities are generally accurate, we conclude that even if prediction accuracy within the measured regions were similar for the Bayesian model and the data-to-data predictions, the Bayesian model is advantageous because it includes predictions for regions of the visual field beyond training data.</p><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.011</object-id><label>Figure 9.</label><caption><title>The Bayesian-inferred maps accurately predict eccentricity beyond the range of the stimulus.</title><p>(<bold>A</bold>) In order to examine how accurately the retinotopic maps predicted using Bayesian inference describe the retinotopic arrangement outside of the range of the stimulus used to construct them we constructed maps from all datasets using only the inner 3° or 6° of eccentricity then compared the predictions to the full validation dataset. Eccentricity is well predicted out to 12° regardless of the eccentricity range used to construct the predicted map, indicating that our inferred maps are likely accurate beyond the range of the stimulus. In addition, we compared the wide-field retinotopic mapping data from subject S1201 to the inferred retinotopic maps (<bold>B</bold>) and the anatomical prior (<bold>C</bold>) using only the 12° stimulus; the inferred eccentricity is shown in terms of the validation eccentricity. The highest errors appear in the fovea (&lt;3°), while predictions made by the inferred maps are most accurate in the periphery, indicating that eccentricity may be well-predicted far beyond the range of the stimulus (out to 48° of eccentricity in this case). Predictions of peripheral data are slightly less accurate when made by the prior than by the inferred maps, which suggests that the extrapolation is improved by the Bayesian inference.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig9-v2.tif"/></fig></sec><sec id="s2-6"><title>The Bayesian inferred maps accurately reproduce systematic properties of the visual field maps</title><p>Another aspect in which our work here extends previous methods is the addition of the pRF size to the retinotopic quantities predicted by the model in the inferred maps—previous work predicted only the pRF centers (<xref ref-type="bibr" rid="bib6">Benson et al., 2012</xref>; <xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>). Here, we predict the pRF sizes for the vertices based on the eccentricity inferred from the Bayesian map and the assumed linear relationship between eccentricity and pRF size. The inferred pRF size of a vertex is the best linear fit to the measured pRF size versus the vertex’s inferred eccentricity (<xref ref-type="fig" rid="fig10">Figure 10A</xref>). While an approximately linear relationship is reasonable given the literature, the absolute scale is likely dependent on variety of measurement factors such as voxel size, stimulus spatial frequency, and subject fixation (<xref ref-type="bibr" rid="bib1">Alvarez et al., 2015</xref>). Hence, we do not attempt to infer the slope or intercept based on prior measurements.</p><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.012</object-id><label>Figure 10.</label><caption><title>Aggregate pRF size and cortical magnification measurements are in agreement with previous literature.</title><p>(<bold>A</bold>) PRF sizes by eccentricity are shown for V1, V2, and V3, as calculated from the full datasets; shaded regions show standard errors across subjects. (<bold>B</bold>) Cortical magnification is shown in terms of eccentricity for V1-V3, as calculated from the full datasets. Again, the shaded regions show standard errors across subjects. The dashed black line shows the equation for cortical magnification provided by <xref ref-type="bibr" rid="bib31">Horton and Hoyt, 1991</xref>. (<bold>C</bold>) Cortical magnification as calculated using the pRF coordinates inferred by the Bayesian inference. Note that in both <italic>A</italic> and <italic>B</italic>, eccentricity refers to measured eccentricity while in <italic>C</italic>, eccentricity refers to Bayesian-inferred eccentricity. (<bold>D</bold>) The difference between the cortical magnification predicted by <xref ref-type="bibr" rid="bib31">Horton and Hoyt, 1991</xref> and the cortical magnification of the (<bold>D</bold>) measured and (<bold>E</bold>) inferred maps; the data are the same as in <italic>B</italic> and <italic>C</italic>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-40224-fig10-v2.tif"/></fig><p>Another metric inversely related to pRF size is the cortical magnification, usually measured in terms of mm<sup>2</sup> of cortex representing one degree<sup>2</sup> of the visual field. We summarize these measurements in <xref ref-type="fig" rid="fig10">Figure 10B and C</xref>. Our measurements of cortical magnification are broadly in agreement with previous work by <xref ref-type="bibr" rid="bib31">Horton and Hoyt, 1991</xref>, shown by the dotted black line panels <italic>B-E</italic> of <xref ref-type="fig" rid="fig10">Figure 10</xref>. The cortical magnification of the inferred maps is quite similar to that of the observed retinotopic maps. In both cases, V1 has slightly lower cortical magnification than V2 and V3 near the fovea, but higher magnification in the periphery. This difference is slightly exaggerated in the inferred maps relative to the observed maps; although this difference is slight and is in agreement with previous examinations of cortical magnification (<xref ref-type="bibr" rid="bib44">Schira et al., 2009</xref>); however, note that in our maps, the crossover between V1 cortical magnification and V2/V3 cortical magnification occurs at a higher eccentricity (~3°) than previously reported (~0.7–1°). This is emphasized in <xref ref-type="fig" rid="fig10">Figure 10D and E</xref>, which shows the curves from <xref ref-type="fig" rid="fig10">Figure 10C and D</xref> in terms of their difference from the prediction of <xref ref-type="bibr" rid="bib31">Horton and Hoyt, 1991</xref> (the black dashed line in panels B-E). Note that in the inferred maps, although the cortical magnification in V1 is lower than V2 below 3° of eccentricity, the difference between them is small between 1.5° and 3°.</p></sec><sec id="s2-7"><title>The retinotopic prior and Bayesian-inferred maps include 12 visual areas</title><p>Previous research on the retinotopic organization of visual cortex used a model of V1, V2, and V3 retinotopy described by <xref ref-type="bibr" rid="bib45">Schira et al. (2010)</xref> to produce a template of retinotopy that included only those visual areas. This ‘banded double-sech’ model accurately describes the anisotropic magnification of the visual field on the cortical surface, particularly near the fovea. However, we have observed, particularly in individual data, that retinotopic data from outside the V1-V3 region described by the Schira model has a large impact on the quality of the inferred map. Accordingly, in creating our retinotopic prior, we constructed a new model that includes nine additional visually active regions: hV4, VO1, VO2, V3a, V3b, LO1, LO2, TO1, and TO2. This model employed a new method of constructing 2D models of retinotopy that was specifically designed to accommodate the distortions caused by anatomical alignment, inflation, and flattening of the cortical surface. The new method is much simpler to extend to many more visual field maps, as it does not rely on an analytic description of the flattened (2D) retinotopic maps, which are only available for V1-V3. Rather, it requires as input a cortical map image on which estimates of the visual area boundaries have been drawn manually and labeled as either a foveal boundary, a peripheral boundary, an upper vertical meridian, a lower vertical meridian, or a horizontal meridian. A minimization technique is then used to fill in the retinotopic coordinates between the drawn boundaries (see Models of Retinotopy in Materials and methods). The new retinotopic prior, including all new areas can be seen in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>. Although we consider the addition of 9 retinotopic areas to be an important development, we consider these areas preliminary and do not analyze them in detail here. One reason for this is that the organizations of many of these areas remain under dispute. Additionally, the responses to our stimuli in these areas is of a considerably lower quality than in V1-V3; thus even were we to analyze the accuracy of the predictions in these ares, our validation dataset would be a particularly poor standard. We do, however, include these areas in the predictions made by the Bayesian inference method so that interested researchers may analyze or use them. These areas are included in the data provided with this paper, and predictions of these areas are included when using the tools provided in the Data Availability Statement.</p></sec><sec id="s2-8"><title>Making the Bayesian inference explicit</title><p>Our method has the advantage of allowing the retinotopic atlas to act as a prior constraint on new observed data. This is a Bayesian model in the general sense of combining a prior belief with a measurement in order to make an inference. The computation can be formulated in an explicit Bayesian framework. We define a hypothesis <italic>H</italic> to be a particular warping of the cortical surface, and we define the evidence <italic>E</italic> to be a particular set of retinotopic measurements. We then convert the cost functions from <xref ref-type="table" rid="table2">Table 2</xref> into probabilities by assuming an exponential relationship. Hence, the prior probability of <italic>H</italic> is defined in terms of the deviation from the retinotopic prior: <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the likelihood of the evidence under a given hypothesis, <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, is defined in terms of the fit between the retinotopic model and the retinotopic measurements: <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. During registration, we seek the hypothesis <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that maximizes the posterior probability <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Because <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a constant, we can ignore it and instead maximize the function given in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, which is equivalent to minimizing <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This operation is performed during registration. Thus, to derive our cost function from Bayes’ rule, we write:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>∝</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>∝</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>∝</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>∝</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The explicit Bayesian formulation above clarifies several features of our model. First, the prior probability distributions assumed for vertex lengths are the same for all vertices (rows 1, 5, and six in <xref ref-type="table" rid="table2">Table 2</xref>; P(H) in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). If we had ground truth maps for a large population, we could, in principle, derive edge-specific probability distributions for <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, and convert these to edge-edge-specific cost functions (<xref ref-type="table" rid="table1">Table 1</xref>) for the minimization process. We can get a sense of how these distributions might differ across occipital cortex by visualizing the warp fields from our data set (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). These fields show that our registration process causes some vertices to move much more than others, at least in our small subject pool (n = 8). These warp fields are not sufficient to derive edge-specific priors because the number of subjects is small, and because we do not know the end-points of the registration reflect the ground truth maps. The use of a large dataset, such as the 181 HCP subjects (<xref ref-type="bibr" rid="bib8">Benson et al., 2018</xref>) might be helpful in future work to derive edge-specific priors. A further challenge to incorporating realistic priors would be to capture the dependencies across edges in the prior distribution (the joint probability distribution, which would be a function of thousands of variables, one per edge, imposing an enormous computational burden).</p><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.40224.013</object-id><label>Table 2.</label><caption><title>Components of the registration potential function</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">Term</th><th valign="top">Description</th><th valign="top">Form</th></tr></thead><tbody><tr><td>1</td><td><p><inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>E</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></td><td>Penalizes changes in the <break/>distances between <break/>neighboring vertices in the <break/>mesh.</td><td><p><inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></p></td></tr><tr><td>2</td><td><inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>ϑ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>Θ</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>Penalizes the changes in t <break/>he angles of the triangles <break/>in the mesh.</td><td><inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td>3</td><td><p><inline-formula><mml:math id="inf12"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>P</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></td><td>Penalizes any change in <break/>the positions of the <break/>vertices on the perimeter <break/>of the map.</td><td><inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td>4</td><td><p><inline-formula><mml:math id="inf14"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>φ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>Φ</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></td><td>Decreases as a retinotopic <break/>vertex <italic>u</italic> approaches its <break/>anchor-point <bold><italic>y</italic></bold> in the <break/>retinotopy model.</td><td><inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:mtext> </mml:mtext><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td>5</td><td><p><inline-formula><mml:math id="inf16"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>E</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></td><td>Harmonic component of <break/>the edge-length deviation <break/>penalty <inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>E</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</td><td><inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td>6</td><td><p><inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>E</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p></td><td>Infinite-well component of <break/>the edge-length deviation <break/>penalty <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td><td><inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td>7</td><td><inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>ϑ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>Θ</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>Harmonic component of <break/>the angle deviation <break/>penalty <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.<inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td>8</td><td><inline-formula><mml:math id="inf26"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>ϑ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>Θ</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>Infinite-well component of <break/>the angle deviation <break/>penalty <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</td><td><inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>A second feature of our method made explicit by the Bayesian formulation is that the prior probability distributions are 0 for solutions that violate the atlas topology. This assumption is implemented implicitly in the cost function, which rises to infinity as the length of an edge approaches 0 or the angle between edges approaches 0. This aspect of the cost function prevents vertices or edges from crossing, thus preserving topology. Because we assume that the cost function is the negative logarithm of the prior probability distribution, the infinite cost indicates an assumed probability of 0. If ground truth data contradicted this assumption (that is, if there were ground truth maps which violated the topology of the model), the prior probability distributions and corresponding cost functions could be changed accordingly.</p><p>A third feature of the method is that the likelihood functions depend on the data quality. In <xref ref-type="table" rid="table1">Table 1</xref>, line 4, the weighting of each vertex (w) is proportional to the variance explained by the pRF model. PRF solutions with high variance explained lead to a higher cost when the atlas vertex is far from the corresponding data point. This part of the cost function shows up in the likelihood, F<italic><sub>φ</sub></italic>(x), in the Bayesian formulation (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). The interpretation is that there is a low likelihood of observing a high-variance-explained pRF solution in a location far from the template solution. A more realistic likelihood calculation (but one that is beyond the scope of our current knowledge and computational resources) would require a noise model that allowed one to compute how likely a pattern of pRF solutions was given a hypothesized map.</p></sec><sec id="s2-9"><title>Individual differences in structure-function relationship</title><p>An important question in human neuroscience is the degree to which different brains, when brought into anatomical registration, share the same functional mapping. There is no single, agreed-upon method to register the brains of different individuals, but a general finding is that cortical function shows better inter-observer agreement when the brains are aligned based on sulcal topology (surface registration) rather than volume registration (<xref ref-type="bibr" rid="bib60">Wang et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Van Essen et al., 1998</xref>). Here, using surface registration, we find that substantial individual differences in functional mapping remains, for example as evidenced by the amount of additional warping needed to align individual brains to retinotopic measurements (<xref ref-type="fig" rid="fig6">Figure 6</xref>). These results are consistent with studies showing differences in structural and functional alignment of primate and human area MT (<xref ref-type="bibr" rid="bib35">Large et al., 2016</xref>). They are also consistent with studies combining structural and functional alignment in the absence of a model or template of the underlying function (<xref ref-type="bibr" rid="bib22">Frost and Goebel, 2013</xref>; <xref ref-type="bibr" rid="bib28">Haxby et al., 2011</xref>). Such studies show that the extra warping driven by functional alignment leads to better predictions of functional responses in cross-validated data.</p><p>The anatomical atlas, described previously (<xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>), was adapted into the first step of our method (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) and is equivalent to the prediction using the retinotopic prior alone that we present here (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2B</xref>). Consistent with previous results, we find that the prior alone produces reasonably good predictions for the retinotopic maps of most subjects (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Additionally, maps predicted using the prior alone contain many of the advantages described here such as topological smoothness, complete coverage of the visual field, and prediction of peripheral retinotopy. However, the idiosyncrasies of individual subjects’ retinotopic maps are often not well predicted by the retinotopic prior alone (<xref ref-type="fig" rid="fig3">Figures 3</xref>, <xref ref-type="fig" rid="fig5">5A and B</xref>). By combining both the retinotopic prior and a small amount of measured data, we are able to produce higher-quality predictions that not only share these advantages but also improve the prediction accuracy of the maps beyond that of the measurement error (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p></sec><sec id="s2-10"><title>Limitations and biases of the inferred maps</title><p>We have shown in this paper that the application of Bayesian inference to retinotopic mapping data can yield substantial rewards for basic researchers interested in quantitative retinotopic analysis. However, a number of questions about the scope and limitations of the method remain. For one, it is unclear how our method, developed using data from a small subset of the population (eight subjects), would cope with subjects whose retinotopic organizations are much different than those that are typically assumed in vision science. Such edge-case subjects could include members of clinical populations, such as individuals lacking an optic chiasm (<xref ref-type="bibr" rid="bib30">Hoffmann et al., 2012</xref>; <xref ref-type="bibr" rid="bib5">Bao et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Olman et al., 2018</xref>), or healthy individuals whose retinotopic boundaries are merely unusual (<xref ref-type="bibr" rid="bib57">Van Essen and Glasser, 2018</xref>). We consider how our Bayesian models perform for edge-cases by fitting the models to two subjects from the Human Connectome Project whose retinotopic maps were recently noted for their peculiarity by <xref ref-type="bibr" rid="bib57">Van Essen and Glasser (2018)</xref>. The retinotopic maps for these subjects as well as the inferred iso-angular and iso-eccentricity lines in V1-V3 are shown in <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>. Both of these subjects have atypical polar angle organization in their left hemisphere dorsal V2 and V3 maps. Regarding the limitations of the method as applied to subjects such as these, two things are clear: first, the method is unable to reproduce the precise topology of the subjects’ unusual dorsal maps, and, second, it is nonetheless capturing most of the maps accurately. In particular, the inferred eccentricity maps are highly accurate despite the mismatched polar angle maps. The polar angle maps cannot be accurately captured by the Bayesian model, because the Bayesian model assumes a prior probability of zero to any solutions that differ topologically from the template, such as these.</p><p>In the case of more extreme departures from typical retinotopic organization, such as the overlapping maps observed in achiasmic patients, we cannot be certain that our method would yield coherent inferences. However, we note that this problem is not unique to our method; in fact, generic voxel-wise pRF models also fail for subjects with highly unusual retinotopic organizations such as achiasma. Typically, these model failures are identified, and updates to the model are proposed to account for the relevant conditions. In the case of achiasmic patients, the usual assumption that a pRF can be described as a single Gaussian fails, and new models were developed with two spatially displaced Gaussians (<xref ref-type="bibr" rid="bib30">Hoffmann et al., 2012</xref>). In the case of our method, one might use an alternate formulation of our retinotopic prior in order to better model the maps of the subjects shown in <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref> or subjects from a particular clinical population.</p><p>A separate but equally critical question about the method we present is whether it encapsulates any systematic biases about retinotopic organization. Given the field’s imperfect knowledge about precise retinotopic organization across individuals, this can be a difficult question to answer; however we note a number of features and assumptions along these lines. With respect to the retinotopic prior, one critical assumptions that was employed during its creation involves the structure of the polar angle reversals at map boundaries (e.g. the V1/V2 boundary or the V3/hV4 boundary). Our prior assumes that the polar angle at these boundaries lies on the vertical meridians (or horizontal meridians in the case of the V2/V3 boundaries). In the group-average retinotopic maps from the 181 HCP subjects (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>), however, it is clear that many polar angle reversals occur several degrees away from the vertical (or horizontal) meridian, differing from the prior. In these cases, our Bayesian maps differ systematically from the data. Because the Bayesian computation allows the vertex positions to change but does not allow the retinotopic quantities to change, all retinotopic locations contained in the prior are assigned to some cortical location in the Bayesian map, differing from the observed validation data, which is often missing representations of the vertical meridians. In addition, ‘notches’ of missing representation of the lower vertical meridian can be seen in several polar angle maps (for example, in<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3A</xref>, black arrows). In both cases—maps that do not quite reach the vertical meridian and maps that have large notches along the boundaries—it is not yet known whether these properties reflect unusual features of the underlying neuronal maps or limits of the fMRI acquisition or analysis.</p></sec><sec id="s2-11"><title>Method availability and usage</title><p>The method described in this paper has been made publicly available in a number of ways in order to enable easy use by other researchers. The Bayesian inference method itself is implemented as part of a free software library called Neuropythy; we have publicly archived the version of this library used in the preparation of this manuscript at DOI:<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/1312983#.XEWVx8_7TOQ">10.5281/zenodo.1312983</ext-link>. Additionally, we have created a universally executable Docker image and have publicly archived it at DOI:<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/1313859#.XEWV4M_7TOQ">10.5281/zenodo.1313859</ext-link>. Detailed instructions for the use of both the library and the universally executable Docker image are available at the Open Science Foundation repository associated with this paper (<ext-link ext-link-type="uri" xlink:href="https://osf.io/knb5g/">https://osf.io/knb5g/</ext-link>). In brief, the method may be run with only a few inputs: a FreeSurfer directory for the subject (which can be generated from an anatomical image), and a set of files containing the measured retinotopic parameters for the subject’s cortical surface. The outputs produced are a similar set of files describing the inferred retinotopic parameters of the subject’s cortical surface. Runtime on a contemporary desktop computer is less than an hour per subject. Detailed instructions on how to use the tools documented in this paper are included in the Open Science Foundation website mentioned above.</p></sec></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><sec id="s3-1"><title>Scientific transparency</title><p>All source code and notebooks as well as all anonymized data employed in this Methods section and the preparation of this manuscript have been made publicly available at the Open Science Foundation: <ext-link ext-link-type="uri" xlink:href="https://osf.io/knb5g/.">https://osf.io/knb5g/.</ext-link> Version 0.6.0 and later of the Neuropythy library can automatic download these data and interpret them into Python data structures.</p></sec><sec id="s3-2"><title>Subjects</title><p>This study was approved by the New York University Institutional Review Board, and all subjects provided written consent. A total of eight subjects (4 female, mean age 31, range 26–46) participated in the experiment. All scan protocols are described below.</p></sec><sec id="s3-3"><title>Magnetic resonance imaging</title><p>All MRI data were collected at the New York University Center for Brain Imaging using a 3T Siemens Prisma scanner. Data were acquired with a 64-channel phased array receive coil. High resolution whole-brain anatomical T1-weighted images (1 mm<sup>3</sup> isotropic voxels) were acquired from each subject for registration and segmentation using a 3D rapid gradient echo sequence (MPRAGE). BOLD fMRI data were collected using a T2*-sensitive echo planar imaging pulse sequence (1 s TR; 30 ms echo time; 75° flip angle; 2.0 × 2.0 × 2.0 mm<sup>3</sup> isotropic voxels, multiband acceleration 6). Two additional scans were collected with reversed phase-encoded blips, resulting in spatial distortions in opposite directions. These scans were used to estimate and correct for spatial distortions in the EPI runs using a method similar to (<xref ref-type="bibr" rid="bib2">Andersson et al., 2003</xref>) as implemented in FSL (<xref ref-type="bibr" rid="bib50">Smith et al., 2004</xref>).</p><p>Anatomical images were processed using the FreeSurfer image analysis suite, which is freely available online (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>) (<xref ref-type="bibr" rid="bib12">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib19">Fischl et al., 1999a</xref>; <xref ref-type="bibr" rid="bib20">Fischl et al., 1999b</xref>; <xref ref-type="bibr" rid="bib21">Fischl and Dale, 2000</xref>). Subject brains were inflated and aligned to FreeSurfer’s anatomical <italic>fsaverage</italic> atlas.</p></sec><sec id="s3-4"><title>Stimulus protocols</title><p>Each subject participated in 12 retinotopic mapping scans using the same stimulus employed in the Human Connectome Project (<xref ref-type="bibr" rid="bib8">Benson et al., 2018</xref>). Briefly, bar apertures on a uniform gray background swept gradually across the visual field at four evenly-spaced orientations while the subject maintained fixation. Bar apertures contained a grayscale pink noise background with randomly placed objects, faces, words, and scenes. All stimuli were presented within a circular aperture extending to 12.4° of eccentricity. The bars were a constant width (1.5°) at all eccentricities. Subjects performed a task in which they were required to attend to the fixation dot and indicate when its color changed.</p><p>The 12 scans were split into several subsets and analyzed as independent datasets. Six of the scans (two of each bar width) were allocated to the subject’s <italic>validation dataset</italic>, while the remaining three scans were used for 21 training datasets: 6 datasets with one scan each, 5 datasets with two scans each, 4 datasets with three scans each, 3 datasets with four scans each, 2 datasets with five scans each, and one dataset with all six training scans. Additionally, all 12 scans were included in a <italic>full dataset</italic> which was used for all analyses not related to the accuracy of the Bayesian inference method.</p><p>Additionally, one previously published retinotopy dataset for with a wide field of view (48° of eccentricity) was re-analyzed (<xref ref-type="bibr" rid="bib59">Wandell and Winawer, 2011</xref>) (their <xref ref-type="fig" rid="fig3">Figure 3</xref>). The subject for this dataset was also included in the newly acquired data, S1201. The wide-field-of-view dataset was used as a further validation set for models derived from the newly acquired data for S1201, as it enabled us to test the accuracy of model predictions in the far periphery from models derived from data with limited eccentricity.</p></sec><sec id="s3-5"><title>FMRI processing</title><p>Spatial distortions due to inhomogeneities in the magnetic field were corrected using in-house software from NYU’s Center for Brain Imaging (<ext-link ext-link-type="uri" xlink:href="http://cbi.nyu.edu/software">http://cbi.nyu.edu/software</ext-link>). The data were then motion-corrected by co-registering all volumes of all scans to the first volume of the first scan in the session. The fMRI slices were co-registered to the whole brain T1-weighted anatomy, and the time series resampled via trilinear interpolation to the 1 mm<sup>3</sup> voxels within the cortical ribbon (gray matter). Finally, the time series were averaged for each voxel across all scans with the same stimulus within a given dataset.</p></sec><sec id="s3-6"><title>PRF solutions</title><p>Retinotopic maps were produced by solving linear, circularly symmetric population receptive field (pRF) models for each voxel using Vistasoft, as described previously (<xref ref-type="bibr" rid="bib15">Dumoulin and Wandell, 2008</xref>). pRF models were solved using a two-stage coarse-to-fine approach on the time series in the 1 mm<sup>3</sup> gray matter voxels. The first stage of the model fit was a grid fit, solved on time series that were temporally decimated (2×), spatially blurred on the cortical surface using a discrete heat kernel (approximately equal to a Gaussian kernel of 5 mm width at half height), and subsampled by a factor of 2. The decimation and blurring helps to find an approximate solution that is robust to local minima. The parameters obtained from the grid fit were interpolated to all gray matter voxels and used as seeds for the subsequent nonlinear optimization. Finally, the pRF parameters were projected from the volume to the cortical surface vertices for white, mid-gray, and pial surfaces using nearest-neighbor interpolation; values were then averaged across the three layers using a weighted-mean in which the fraction of BOLD signal variance explained by the pRF model was used as a weight. All vertices with a pRF variance explained fraction less than 0.1 were ignored.</p></sec><sec id="s3-7"><title>Models of retinotopy</title><p>To generate our initial models of the retinotopic maps, we begin by hand-drawing boundaries for 12 retinotopic maps. These boundaries need only be drawn once for a single group-average retinotopic map. The boundaries are projected onto the cortical surface, and the retinotopic coordinates for each vertex on the surface are deduced via a minimization procedure. This minimization is motivated by two principles: (1) the retinotopic fields (polar angle and eccentricity) should be as orthogonal to each other as possible and (2) the retinotopic fields should be as smooth as possible. To this end, the minimization routine simultaneously maximizes both the smoothness of the retinotopic fields between vertices connected by edges in the mesh as well as the overall orthogonality between the polar angle field and the eccentricity field. The hand-drawn boundary values are held constant during the minimization. <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> gives the function <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that is minimized, where <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϱ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> represent vectors of the polar angle and eccentricity values, respectively; <bold><inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></bold> represents the set of edges between vertices in the mesh; and <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the matrix of vertex coordinates (i.e., <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the coordinate vector of the vertex <italic>u</italic>). During minimization the values of the polar angle and eccentricity vectors are scaled such that both fields ranged from −1 to 1 (e.g., polar angle boundary values of 0° and 180° were assigned values of −1 and 1, respectively) so that the fields could be evaluated easily for orthogonality; after the minimization, the polar angle was linearly rescaled back to the range 0°−180° while eccentricity was rescaled so as to have an exponential distribution that best fit the group-average. We employed this model generation routine using boundaries drawn over the group-average data (see Anatomically-defined Atlas of Retinotopy: Group Average, below) as well as the <xref ref-type="bibr" rid="bib60">Wang et al. (2015)</xref> atlas and the <xref ref-type="bibr" rid="bib29">Hinds et al. (2008)</xref> V1 boundary as rough guidelines.<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⋅</mml:mo><mml:mi>ρ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>A full description of the model, including how it can be projected onto an <italic>fsaverage</italic> spherical surface or an individual subject’s <italic>fsaverage</italic>-aligned spherical surface, is provided in the open source Neuropythy library (<ext-link ext-link-type="uri" xlink:href="https://github.com/noahbenson/neuropythy">https://github.com/noahbenson/neuropythy</ext-link>).</p></sec><sec id="s3-8"><title>Anatomically-defined atlas of retinotopy</title><p>Construction of the anatomically-defined atlas of retinotopy is summarized in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>. Previous work employed a mass-spring-damper system combined with a nonlinear gradient-descent minimization in order to register group-average retinotopic data, averaged on FreeSurfer’s <italic>fsaverage_sym</italic> hemisphere (<xref ref-type="bibr" rid="bib25">Greve et al., 2013</xref>), with a model of V1, V2, and V3 retinotopy (<xref ref-type="bibr" rid="bib45">Schira et al., 2010</xref>). In this paper, we modify this technique slightly to bring it more in line with previous established methods such as those used by FreeSurfer for surface-based anatomical registration (<xref ref-type="bibr" rid="bib12">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib19">Fischl et al., 1999a</xref>). In brief, retinotopy is measured in a group of subjects via fMRI; the subjects’ cortical meshes are aligned to the <italic>fsaverage</italic> surface via FreeSurfer’s surface registration; the retinotopic coordinates are then averaged across subjects at each vertex on a single atlas of the cortical surface; a 2D atlas of retinotopy is then placed on this cortical surface; and finally, the cortical surface is warped to match the retinotopic atlas as best as possible given constraints on the warping. Each of these steps is described in more detail below.</p><p><italic>Group-average Data.</italic> Group-average retinotopic maps (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2B</xref>) were obtained from 181 subjects whose data were published and made freely available as part of the Human Connectome Project (<xref ref-type="bibr" rid="bib8">Benson et al., 2018</xref>). The resulting group-average retinotopic maps are shown in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2B</xref>.</p><p><italic>Cortical Map Projection.</italic> The cortical surfaces of the <italic>fsaverage</italic> left and right hemispheres, on which the group-average data were constructed, were inflated both to a smooth hemisphere (FreeSurfer’s ‘inflated’ surface) as well as to a sphere (FreeSurfer’s ‘sphere’ surface); the vertices on the spherical surfaces were then flattened to 2D maps using an orthographic map projection. Precise parameters of this projection and the source code used to generate it are included in the Data Availability Statement. We refer to the 2D vertex coordinates in this resulting map as the ‘initial vertex coordinates’ because they precede the warping of the vertex coordinates that occurs during registration.</p><p><italic>Registration.</italic> The initial vertex coordinates of the map projections described above were warped in order to bring the polar angle and eccentricity measurements of the vertices into alignment with the 2D model’s predictions of retinotopy while maintaining topological constraints: that is preventing triangles in the triangle mesh representing the 2D cortical map from inverting and penalizing excessive stretching or compression of the map. This process was achieved by minimizing a potential function defined in terms of the edges of the triangle mesh, the angles of the triangle mesh, and the positions of the vertices with polar angle and eccentricity measurements above the weight threshold (see <italic>Group-Average Data</italic>, above). <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> gives this potential function, <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is further broken down into four components detailed in <xref ref-type="table" rid="table2">Table 2</xref>. Fundamentally, the potential function <italic>F</italic> is a sum of two kinds of penalties: penalties for deviations from the reference mesh and penalties for mismatches between the vertices with retinotopic coordinates and their positions in the retinotopic model. In the case of the former, the reference mesh is gi <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the potential of the deviations are defined by <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The latter is described by <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>φ</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In these functions, <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the <italic>n</italic> × 2 matrix of the 2D-coordinates of each vertex while <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mo>×</mml:mo><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the same coordinates in the reference mesh; <bold>E</bold> represents the set of undirected edges (represented as <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> pairs such that <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are not both in <bold>E</bold>) in the reference mesh; <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the set of angle triples (<italic>a</italic>, <italic>b</italic>, <italic>c</italic>) such that the angle is between edge <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and edge <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <bold>P</bold> is the set of vertices that lie on the perimeter of the 2D map projection; <italic>r<bold><sub>x</sub></bold></italic>(<italic>u</italic>,<italic>v</italic>) is the Euclidean distance between vectors <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>u</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; and <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the counter-clockwise angle between vectors <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>c</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the set of anchors defined by the retinotopic model in which each anchor is represented by a tuple <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <italic>w</italic> is the weight of the anchor, <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the vertex drawn to the anchor, <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the standard deviation of the anchor’s Gaussian potential well, and <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><bold><italic>y</italic></bold> is the 2D point to which the anchor is attached; the constants <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> are the minimum and maximum allowable edge lengths, respectively.<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>φ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The term of the potential function devoted to the retinotopic model is given in <italic>F<sub>φ</sub></italic> (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>; <xref ref-type="table" rid="table2">Table 2</xref>). This potential term is a set of inverted-Gaussian potential wells called anchors. Each anchor represents the attraction of a single vertex <italic>u</italic>, with measured polar angle <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, eccentricity <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϱ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and weight <italic>w</italic>, to a 2D point <bold>y</bold>, at which the retinotopic model predicts a polar angle value of <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and an eccentricity value of <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϱ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note that each visual area represents every point <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϱ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> in the visual field, there are multiple anchors per vertex with retinotopic data. In fact, the retinotopic model used in this paper defines nine maps in addition to the V1-V3 maps (see Model of Retinotopy, above), bringing the total number of anchors per retinotopic vertex to 12. The additional areas are intended partly to prevent vertices immediately outside of V1-V3 from being drawn incorrectly into the V1-V3 section of the model and are not analyzed in detail in this paper. Each anchor additionally defines a parameter σ; this value is the width (standard deviation) of the anchor’s Gaussian potential well; σ is defined as the minimum distance from the given anchor to any other anchor to which <italic>u</italic> is also attracted; this value was given a maximum value of 20ε where ε is the mean edge-length in the projected map.</p><p>The potential function was minimized using a gradient descent algorithm sensitive to the singularities in the terms <italic>G<sub>e</sub></italic>, and <italic>G<sub>θ</sub></italic> (<xref ref-type="table" rid="table2">Table 2</xref>); whenever the singularity is accidentally crossed, the minimizer backtracks and chooses a smaller step-size. This approach prevents the inversion (from counter-clockwise ordering to clockwise ordering) of any triangle in the mesh, as such an inversion would require the minimization trajectory to pass through a singularity at the point where α = 0 or α = π. The source code used to minimize the potential function as well as specifications of the gradients of each term is provided in the open-source library included with the Neuropythy and Neurotica libraries (<ext-link ext-link-type="uri" xlink:href="https://github.com/noahbenson/nben">https://github.com/noahbenson/nben</ext-link>).</p><p>Minimization was run for at least 2500 steps in which the step-size was constrained such that the displacement of each vertex in each step was at most 1/50<sup>th</sup> of the average edge-length in the map projection. A small amount of exponentially distributed random noise was added to the gradient at each step with the constraint that the gradient direction at each vertex be conserved; this noise did not affect the minimum obtained by the search but did speedup convergence significantly (see associated libraries for further details). Convergence was generally observed within 1000–2000 steps. The set of vertex coordinates that resulted from this minimization brings the retinotopic measurements associated with the vertices in V1, V2, and V3 referred to as the <italic>registered vertex coordinates</italic>.</p><p><italic>Prediction.</italic> The registered vertex coordinates, once obtained, give the alignment of the subject’s cortical surface to the model of retinotopy; accordingly, a prediction of any vertex’s associated pRF and visual area label can be derived by comparing the the vertex’s registered coordinates with the model. Every vertex whose registered coordinates fall within the model’s V1 boundary, for example, is labeled as part of V1. Because only the vertex coordinates, and not the vertex identities, are changed during the registration process, there is no need to invert the registration: visual area label, polar angle, and eccentricity values assigned to each vertex apply as readily to the vertices whether they are visualized in the registered vertex coordinates or in the coordinates that define the subject’s white-matter surface, for example. The retinotopic map predictions for the group-average data is shown in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2D</xref> (left column).</p><p>Because the group-average retinotopic data were used in the registration, the predicted map that results provides a reasonable estimate of any subject’s expected retinotopic map, as shown previously (<xref ref-type="bibr" rid="bib6">Benson et al., 2012</xref>; <xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>); although the predicted map does not account for further individual differences in the structure to function relationship, as we show in this paper. Additionally, because the predicted map from the group-averaged data is defined on the <italic>fsaverage</italic> subject’s cortical surfaces, a retinotopic map prediction for any new subject, for whom retinotopic mapping measurements may not be available, can be easily obtained: one can use FreeSurfer to align the new subject’s cortical surface with the <italic>fsaverage</italic> subject’s surface (anatomical structure alignment) then to project the retinotopic maps from the <italic>fsaverage</italic> subject to the new subject based on the anatomical similarity between them. Because of this, we refer to this group-average retinotopic prediction as the <italic>anatomically-defined atlas of retinotopy.</italic> This atlas is used as <italic>the prior</italic> for the Bayesian model fit, described below. The atlas is similar but not identical to one presented previously (<xref ref-type="bibr" rid="bib7">Benson et al., 2014</xref>).</p></sec><sec id="s3-9"><title>Bayesian retinotopic maps</title><p>The anatomically-defined atlas of retinotopy, while providing a good prediction for most subjects’ individual retinotopic maps, nonetheless does not account for individual differences in the mapping between anatomical location and retinotopic coordinates. Accordingly, predicted retinotopic maps for individual subjects were refined starting from the anatomically-defined atlas of retinotopy using a similar method as was used to generate the atlas originally; this process is detailed in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. For each subject, their cortical surface was aligned based on anatomical structure to the <italic>fsaverage</italic> subject’s cortical surface using FreeSurfer, then their retinotopic data were projected to a map using the identical map projection described above in the section on the anatomically-defined atlas of retinotopy. Note that, in this case, the anatomical alignment to the <italic>fsaverage</italic> subject serves to make the map projections as similar as possible between individual subjects and the anatomically-defined atlas of retinotopy. If we were not interested in incorporating information obtained from the anatomically-defined atlas of retinotopy (which represents a prior belief of retinotopic organization based on group-average data), this step would not be necessary.</p><p>The individual subject’s projected map is then arranged according to the registered vertex coordinates from the anatomically-defined atlas of retinotopy; this step reflects the prior belief that the group-average registration to the retinotopy model is generally accurate for an individual subject when that subject’s anatomical structure has been aligned to the <italic>fsaverage</italic> subject’s. Critically, none of the steps taken so far in processing the individual subject’s data relies on any measurements of retinotopy that might be associated with that subject. Rather, these steps have relied only on anatomical structure. If, for a subject, no retinotopic measurements are made, then there is no data with which to modify this prior belief; accordingly, the prediction of retinotopy for that subject would be identical to the prediction of retinotopy contained in the anatomically-defined atlas. In other words, without observation, the prior remains the prediction.</p><p>The next step registers the individually measured retinotopy data to the anatomically-defined atlas. Before registration, the individual subject’s data is resampled onto a uniform triangular mesh, and each vertex whose retinotopic measurements are above threshold are given a weight, <italic>w</italic>, based on the variance explained, <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, of its pRF model solution. The mesh is resampled to the same uniform triangle mesh used as the initial vertex coordinates in the registration of the anatomically-defined atlas of retinotopy in order to speedup registration. Triangles that are tightly pinched (i.e., triangles with internal angles near 0 or π) can drastically slow the registration progress by forcing the minimizer to frequently backtrack steps; resampling makes such behavior much less likely during the initial minimization. Aside from the weight, other parameters tracked by the potential field, including anchors parameters used by the function <italic>F<sub>φ</sub></italic>(<bold><italic>x</italic></bold>), are obtained identically as with the anatomically-defined atlas of retinotopy. These anchors inherit the weight of the vertex to which they apply, but are reduced when the field sign of the triangles adjacent to the vertex does not match the field sign of the visual area to which it is tied by the anchor or when the pRF size predicted by the model does not match that of the vertex’s measured pRF. Details regarding the weights on anchors are provided in the neuropythy library.</p><p>For each training dataset of each subject, minimization was run for 2500 steps using the same protocol that was used with the anatomically-defined atlas of retinotopy. Retinotopic map prediction, based on the positions of the registered vertex coordinates in the retinotopy model, were also computed identically to those in the anatomically-defined atlas. Identical minimization and prediction methods were run for each test dataset as well, but these results were not used to measure the accuracy or effectiveness of the prediction methods.</p></sec><sec id="s3-10"><title>Cortical magnification</title><p>Cortical magnification was calculated using both the observed retinotopic maps and the inferred maps that were produced by combining each subject’s full retinotopy dataset with the retinotopic prior. This combination of data should, in theory, produce the highest-quality retinotopic map predictions of which we are capable (see Results and Discussion). Cortical magnification was calculated by first projecting all vertices in a single visual area (such as V1) into the visual field based on their pRF centers. The cortical magnification of a particular polar angle and eccentricity is then the total white vertex surface-area (as calculated by FreeSurfer) of all pRF centers within a disk of some radius <italic>α</italic>, divided by the area of the disk (<italic>πα</italic><sup>2</sup>). For an eccentricity <italic>ρ</italic>, we used a radius <italic>α = ρ/3.</italic></p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The research was supported by NIH grants R00-EY022116, R01-MH111417, R01- EY027964, and R01-EY027401. The authors declare no competing financial interests.</p><p>All figures and plots were produced using the Neurotica library (<ext-link ext-link-type="uri" xlink:href="https://github.com/noahbenson/neurotica">https://github.com/noahbenson/neurotica</ext-link>) for Mathematica 11 (<xref ref-type="bibr" rid="bib62">Wolfram Research, 2018</xref>). Registrations and interpolations were performed with the assistance of the nibabel (DOI:10.5281/zenodo.1287921), numpy (<xref ref-type="bibr" rid="bib40">Oliphant, 2006</xref>), and scipy (<xref ref-type="bibr" rid="bib34">Jones, 2001</xref>) libraries for Python.</p></ack><sec id="s4" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Investigation, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: This study was conducted with the approval of the New York University Institutional Review Board (IRB-FY2016-363) and in accordance with the Declaration of Helsinki. Informed consent was obtained for all subjects.</p></fn></fn-group></sec><sec id="s5" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><object-id pub-id-type="doi">10.7554/eLife.40224.014</object-id><label>Supplementary file 1.</label><caption><title>Cross-validation schema.</title><p>To evaluate the accuracy of the predictions of retinotopic maps, we employ a cross-validation schema. Each subject’s 12 retinotopic mapping scans were divided into one large set of validation data as well as 21 smaller sets of training data. An additional dataset of all 12 scans was used for analysis of retinotopic properties not linked to evaluation of the quality of the predicted maps.</p></caption><media mime-subtype="pdf" mimetype="application" xlink:href="elife-40224-supp1-v2.pdf"/></supplementary-material><supplementary-material id="supp2"><object-id pub-id-type="doi">10.7554/eLife.40224.015</object-id><label>Supplementary file 2.</label><caption><title>Deriving the anatomically-defined atlas of retinotopy (the prior).</title><p>(A) The group-average polar angle (top) and eccentricity (bottom) maps. The cortical surface is inflated to a sphere then flattened to a map. (B) The model of retinotopy shown with polar angle plotted on the left and eccentricity plotted on the right hemispheres. (C) The retinotopic prior is constructed from the group-average data using an updated version of the method described by <xref ref-type="bibr" rid="bib7">Benson et al. (2014)</xref>. Note that while only eccentricity is shown, polar angle and eccentricity are registered simultaneously. The checkerboard underlay illustrates the anatomical warping. (D) There is approximate agreement between the boundaries of visual areas V1, V2, and V3 as defined by two atlases. The Wang et al. maximum probability atlas (2015) and the retinotopic template defined here have similar boundaries. The template extends from 0° to 90° eccentricity, whereas the Wang et al atlas is limited to the field of view of their experiments (14°), hence the template maps are larger. (E) Because there is a topological isomorphism between the cortical surface, the left and right hemifields, and the model of retinotopy, the three representations have exact one-to-one correspondences.</p></caption><media mime-subtype="pdf" mimetype="application" xlink:href="elife-40224-supp2-v2.pdf"/></supplementary-material><supplementary-material id="supp3"><object-id pub-id-type="doi">10.7554/eLife.40224.016</object-id><label>Supplementary file 3.</label><caption><title>The retinotopic prior.</title><p>(A) The 181-subject group-average retinotopic maps from the Human Connectome Project 7T Retinotopy Dataset are shown. These maps were used to construct the prior. Black arrows in the left-most plots indicate ‘notches’ of the V3 representation of the upper and lower vertical meridians that are absent in the group-average data. (B) The retinotopic prior is shown from 0 to 12° of eccentricity with boundary lines between areas. All 12 retinotopic areas included in the prior are shown.</p></caption><media mime-subtype="pdf" mimetype="application" xlink:href="elife-40224-supp3-v2.pdf"/></supplementary-material><supplementary-material id="supp4"><object-id pub-id-type="doi">10.7554/eLife.40224.017</object-id><label>Supplementary file 4.</label><caption><title>Warp fields in the V1-V3 region across all subjects.</title><p>The warp fields are calculated using the individual vertex deviations during the registration process (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The top row shows the mean vertex deformation across all subjects while the bottom three rows show the first three principal components of the deviations. The brightness of the arrows is based on their relative lengths. Note that because the top row shows the mean warp-field across subjects, the exact direction of the arrows is significant; however, in the bottom three rows, the principal component axes are shown, so the inversion of the arrows is equivalent to the plotted arrows.</p></caption><media mime-subtype="pdf" mimetype="application" xlink:href="elife-40224-supp4-v2.pdf"/></supplementary-material><supplementary-material id="supp5"><object-id pub-id-type="doi">10.7554/eLife.40224.018</object-id><label>Supplementary file 5.</label><caption><title>Retinotopic maps for subjects</title><p>(A) 198653 and (B) 644246 from the Human Connectome Project. These two subjects have unusual retinotopic organization in the polar angle maps of their left hemispheres (A) and on both hemispheres (B); this organization is not accounted for my our model of retinotopy and thus provides an example of how our Bayesian inference method performs when provided with atypical retinotopic maps. In the polar angle maps (top), black lines indicate V1/V2/V3 boundaries. In the eccentricity maps (bottom), black lines show the outer V3 boundaries and the 0.5°, 1°, 2°, 4° and 8° iso-eccentricity curves. Black arrows indicate the sites of atypical retinotopic organization.</p></caption><media mime-subtype="pdf" mimetype="application" xlink:href="elife-40224-supp5-v2.pdf"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.40224.019</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-40224-transrepform-v2.pdf"/></supplementary-material><sec id="s6" sec-type="data-availability"><title>Data availability</title><p>All data generated or analyzed in this study have been made public on an Open Science Foundation website: <ext-link ext-link-type="uri" xlink:href="https://osf.io/knb5g/">https://osf.io/knb5g/</ext-link>. Preprocessed MRI data as well as analyses and source code for reproducing figures and performing additional analyses can be found on the Open Science Foundation website <ext-link ext-link-type="uri" xlink:href="https://osf.io/knb5g/">https://osf.io/knb5g/</ext-link>. Performing Bayesian inference using your own retinotopic maps. To perform Bayesian inference on a FreeSurfer subject, one can use the neuropythy Python library (<ext-link ext-link-type="uri" xlink:href="https://github.com/noahbenson/neuropythy">https://github.com/noahbenson/neuropythy</ext-link>). For convenience, this library has also been packaged into a Docker container that is freely available on Docker Hub (<ext-link ext-link-type="uri" xlink:href="https://hub.docker.com/r/nben/neuropythy">https://hub.docker.com/r/nben/neuropythy</ext-link>). The following command will provide an explanation of how to use the Docker: '&gt; docker run -it --rm nben/neuropythy:v0.5.0 register_retinotopy --help'. Detailed instructions on how to use the tools documented in this paper are included in the Open Science Foundation website mentioned above.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Bayesian Models of Human Retinotopic Organization</data-title><source>Open Science Framework</source><pub-id assigning-authority="other" pub-id-type="archive" xlink:href="https://osf.io/knb5g/">osf.io/knb5g/</pub-id></element-citation></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarez</surname> <given-names>I</given-names></name><name><surname>de Haas</surname> <given-names>B</given-names></name><name><surname>Clark</surname> <given-names>CA</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name><name><surname>Schwarzkopf</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Comparing different stimulus configurations for population receptive field mapping in human fMRI</article-title><source>Frontiers in Human Neuroscience</source><volume>9</volume><elocation-id>96</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00096</pub-id><pub-id pub-id-type="pmid">25750620</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname> <given-names>JL</given-names></name><name><surname>Skare</surname> <given-names>S</given-names></name><name><surname>Ashburner</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title><source>NeuroImage</source><volume>20</volume><fpage>870</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00336-7</pub-id><pub-id pub-id-type="pmid">14568458</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews</surname> <given-names>TJ</given-names></name><name><surname>Halpern</surname> <given-names>SD</given-names></name><name><surname>Purves</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Correlated size variations in human visual cortex, lateral geniculate nucleus, and optic tract</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>2859</fpage><lpage>2868</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-08-02859.1997</pub-id><pub-id pub-id-type="pmid">9092607</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>McMains</surname> <given-names>SA</given-names></name><name><surname>Singer</surname> <given-names>BD</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Retinotopic organization of human ventral visual cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>10638</fpage><lpage>10652</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2807-09.2009</pub-id><pub-id pub-id-type="pmid">19710316</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname> <given-names>P</given-names></name><name><surname>Purington</surname> <given-names>CJ</given-names></name><name><surname>Tjan</surname> <given-names>BS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Using an achiasmic human visual system to quantify the relationship between the fMRI BOLD signal and neural response</article-title><source>eLife</source><volume>4</volume><elocation-id>e09600</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.09600</pub-id><pub-id pub-id-type="pmid">26613411</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname> <given-names>NC</given-names></name><name><surname>Butt</surname> <given-names>OH</given-names></name><name><surname>Datta</surname> <given-names>R</given-names></name><name><surname>Radoeva</surname> <given-names>PD</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Aguirre</surname> <given-names>GK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The retinotopic organization of striate cortex is well predicted by surface topology</article-title><source>Current Biology</source><volume>22</volume><fpage>2081</fpage><lpage>2085</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.09.014</pub-id><pub-id pub-id-type="pmid">23041195</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname> <given-names>NC</given-names></name><name><surname>Butt</surname> <given-names>OH</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Aguirre</surname> <given-names>GK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Correction of distortion in flattened representations of the cortical surface allows prediction of V1-V3 functional organization from anatomy</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003538</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003538</pub-id><pub-id pub-id-type="pmid">24676149</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Benson</surname> <given-names>NC</given-names></name><name><surname>Jamison</surname> <given-names>KW</given-names></name><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Vu</surname> <given-names>A</given-names></name><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Coalson</surname> <given-names>TS</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Kay</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Human Connectome Project 7 Tesla retinotopy dataset: Description and population receptive field analysis</article-title><source>Journal of Vision</source><pub-id pub-id-type="doi">10.1167/18.13.23</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binda</surname> <given-names>P</given-names></name><name><surname>Thomas</surname> <given-names>JM</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name><name><surname>Fine</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Minimizing biases in estimating the reorganization of human visual areas with BOLD retinotopic mapping</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/13.7.13</pub-id><pub-id pub-id-type="pmid">23788461</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boubela</surname> <given-names>RN</given-names></name><name><surname>Kalcher</surname> <given-names>K</given-names></name><name><surname>Huf</surname> <given-names>W</given-names></name><name><surname>Seidel</surname> <given-names>EM</given-names></name><name><surname>Derntl</surname> <given-names>B</given-names></name><name><surname>Pezawas</surname> <given-names>L</given-names></name><name><surname>Našel</surname> <given-names>C</given-names></name><name><surname>Moser</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>fMRI measurements of amygdala activation are confounded by stimulus correlated signal fluctuation in nearby veins draining distant brain regions</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>10499</elocation-id><pub-id pub-id-type="doi">10.1038/srep10499</pub-id><pub-id pub-id-type="pmid">25994551</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conner</surname> <given-names>IP</given-names></name><name><surname>Sharma</surname> <given-names>S</given-names></name><name><surname>Lemieux</surname> <given-names>SK</given-names></name><name><surname>Mendola</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Retinotopic organization in children measured with fMRI</article-title><source>Journal of Vision</source><volume>4</volume><fpage>10</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1167/4.6.10</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. I. Segmentation and surface reconstruction</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dougherty</surname> <given-names>RF</given-names></name><name><surname>Koch</surname> <given-names>VM</given-names></name><name><surname>Brewer</surname> <given-names>AA</given-names></name><name><surname>Fischer</surname> <given-names>B</given-names></name><name><surname>Modersitzki</surname> <given-names>J</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visual field representations and locations of visual areas V1/2/3 in human visual cortex</article-title><source>Journal of Vision</source><volume>3</volume><fpage>1</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1167/3.10.1</pub-id><pub-id pub-id-type="pmid">14640882</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dukart</surname> <given-names>J</given-names></name><name><surname>Bertolino</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>When structure affects function--the need for partial volume effect correction in functional and resting state magnetic resonance imaging studies</article-title><source>PLoS ONE</source><volume>9</volume><elocation-id>e114227</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0114227</pub-id><pub-id pub-id-type="pmid">25460595</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname> <given-names>RO</given-names></name><name><surname>Boynton</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical magnification within human primary visual cortex correlates with acuity thresholds</article-title><source>Neuron</source><volume>38</volume><fpage>659</fpage><lpage>671</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00265-4</pub-id><pub-id pub-id-type="pmid">12765616</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname> <given-names>S</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Wandell</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997a</year><article-title>Colour tuning in human visual cortex measured with functional magnetic resonance imaging</article-title><source>Nature</source><volume>388</volume><fpage>68</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/40398</pub-id><pub-id pub-id-type="pmid">9214503</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname> <given-names>SA</given-names></name><name><surname>Glover</surname> <given-names>GH</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="1997">1997b</year><article-title>Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title><source>Cerebral Cortex</source><volume>7</volume><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.2.181</pub-id><pub-id pub-id-type="pmid">9087826</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999a</year><article-title>Cortical surface-based analysis. II: Inflation, flattening, and a surface-based coordinate system</article-title><source>NeuroImage</source><volume>9</volume><fpage>195</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id><pub-id pub-id-type="pmid">9931269</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999b</year><article-title>High-resolution intersubject averaging and a coordinate system for the cortical surface</article-title><source>Human Brain Mapping</source><volume>8</volume><fpage>272</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:4&lt;272::AID-HBM10&gt;3.0.CO;2-4</pub-id><pub-id pub-id-type="pmid">10619420</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Measuring the thickness of the human cerebral cortex from magnetic resonance images</article-title><source>PNAS</source><volume>97</volume><fpage>11050</fpage><lpage>11055</lpage><pub-id pub-id-type="doi">10.1073/pnas.200033797</pub-id><pub-id pub-id-type="pmid">10984517</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frost</surname> <given-names>MA</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Functionally informed cortex based alignment: an integrated approach for whole-cortex macro-anatomical and ROI-based functional alignment</article-title><source>NeuroImage</source><volume>83</volume><fpage>1002</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.07.056</pub-id><pub-id pub-id-type="pmid">23899723</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Is cortical vasculature functionally organized?</article-title><source>NeuroImage</source><volume>49</volume><fpage>1953</fpage><lpage>1956</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.07.004</pub-id><pub-id pub-id-type="pmid">19596071</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Coalson</surname> <given-names>TS</given-names></name><name><surname>Robinson</surname> <given-names>EC</given-names></name><name><surname>Hacker</surname> <given-names>CD</given-names></name><name><surname>Harwell</surname> <given-names>J</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Andersson</surname> <given-names>J</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname> <given-names>DN</given-names></name><name><surname>Van der Haegen</surname> <given-names>L</given-names></name><name><surname>Cai</surname> <given-names>Q</given-names></name><name><surname>Stufflebeam</surname> <given-names>S</given-names></name><name><surname>Sabuncu</surname> <given-names>MR</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Brysbaert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A surface-based analysis of language lateralization and cortical asymmetry</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1477</fpage><lpage>1492</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00405</pub-id><pub-id pub-id-type="pmid">23701459</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname> <given-names>K</given-names></name><name><surname>Kushnir</surname> <given-names>T</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name><name><surname>Edelman</surname> <given-names>S</given-names></name><name><surname>Itzchak</surname> <given-names>Y</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A sequence of object-processing stages revealed by fMRI in the human occipital lobe</article-title><source>Human Brain Mapping</source><volume>6</volume><fpage>316</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1998)6:4&lt;316::AID-HBM9&gt;3.0.CO;2-6</pub-id><pub-id pub-id-type="pmid">9704268</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>BM</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The relationship between cortical magnification factor and population receptive field size in human visual cortex: constancies in cortical architecture</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>13604</fpage><lpage>13612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2572-11.2011</pub-id><pub-id pub-id-type="pmid">21940451</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Conroy</surname> <given-names>BR</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Hanke</surname> <given-names>M</given-names></name><name><surname>Ramadge</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A common, high-dimensional model of the representational space in human ventral temporal cortex</article-title><source>Neuron</source><volume>72</volume><fpage>404</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.026</pub-id><pub-id pub-id-type="pmid">22017997</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinds</surname> <given-names>OP</given-names></name><name><surname>Rajendran</surname> <given-names>N</given-names></name><name><surname>Polimeni</surname> <given-names>JR</given-names></name><name><surname>Augustinack</surname> <given-names>JC</given-names></name><name><surname>Wiggins</surname> <given-names>G</given-names></name><name><surname>Wald</surname> <given-names>LL</given-names></name><name><surname>Diana Rosas</surname> <given-names>H</given-names></name><name><surname>Potthast</surname> <given-names>A</given-names></name><name><surname>Schwartz</surname> <given-names>EL</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Accurate prediction of V1 location from cortical folds in a surface coordinate system</article-title><source>NeuroImage</source><volume>39</volume><fpage>1585</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.10.033</pub-id><pub-id pub-id-type="pmid">18055222</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffmann</surname> <given-names>MB</given-names></name><name><surname>Kaule</surname> <given-names>FR</given-names></name><name><surname>Levin</surname> <given-names>N</given-names></name><name><surname>Masuda</surname> <given-names>Y</given-names></name><name><surname>Kumar</surname> <given-names>A</given-names></name><name><surname>Gottlob</surname> <given-names>I</given-names></name><name><surname>Horiguchi</surname> <given-names>H</given-names></name><name><surname>Dougherty</surname> <given-names>RF</given-names></name><name><surname>Stadler</surname> <given-names>J</given-names></name><name><surname>Wolynski</surname> <given-names>B</given-names></name><name><surname>Speck</surname> <given-names>O</given-names></name><name><surname>Kanowski</surname> <given-names>M</given-names></name><name><surname>Liao</surname> <given-names>YJ</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Plasticity and stability of the visual system in human achiasma</article-title><source>Neuron</source><volume>75</volume><fpage>393</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.05.026</pub-id><pub-id pub-id-type="pmid">22884323</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horton</surname> <given-names>JC</given-names></name><name><surname>Hoyt</surname> <given-names>WF</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The representation of the visual field in human striate cortex. A revision of the classic Holmes map</article-title><source>Archives of Ophthalmology</source><volume>109</volume><fpage>816</fpage><lpage>824</lpage><pub-id pub-id-type="doi">10.1001/archopht.1991.01080060080030</pub-id><pub-id pub-id-type="pmid">2043069</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huettel</surname> <given-names>SA</given-names></name><name><surname>Song</surname> <given-names>AW</given-names></name><name><surname>McCarthy</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Functional Magnetic Resonance Imaging</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huk</surname> <given-names>AC</given-names></name><name><surname>Ress</surname> <given-names>D</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neuronal basis of the motion aftereffect reconsidered</article-title><source>Neuron</source><volume>32</volume><fpage>161</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00452-4</pub-id><pub-id pub-id-type="pmid">11604147</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2001">2001</year><data-title><italic>SciPy: Open Source Scientific Tools for Python</italic></data-title><source>scipy.org</source></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Large</surname> <given-names>I</given-names></name><name><surname>Bridge</surname> <given-names>H</given-names></name><name><surname>Ahmed</surname> <given-names>B</given-names></name><name><surname>Clare</surname> <given-names>S</given-names></name><name><surname>Kolasinski</surname> <given-names>J</given-names></name><name><surname>Lam</surname> <given-names>WW</given-names></name><name><surname>Miller</surname> <given-names>KL</given-names></name><name><surname>Dyrby</surname> <given-names>TB</given-names></name><name><surname>Parker</surname> <given-names>AJ</given-names></name><name><surname>Smith</surname> <given-names>JET</given-names></name><name><surname>Daubney</surname> <given-names>G</given-names></name><name><surname>Sallet</surname> <given-names>J</given-names></name><name><surname>Bell</surname> <given-names>AH</given-names></name><name><surname>Krug</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Individual Differences in the Alignment of Structural and Functional Markers of the V5/MT Complex in Primates</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3928</fpage><lpage>3944</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw180</pub-id><pub-id pub-id-type="pmid">27371764</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsson</surname> <given-names>J</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Two retinotopic visual areas in human lateral occipital cortex</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>13128</fpage><lpage>13142</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1657-06.2006</pub-id><pub-id pub-id-type="pmid">17182764</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackey</surname> <given-names>WE</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Curtis</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual field map clusters in human frontoparietal cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22974</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22974</pub-id><pub-id pub-id-type="pmid">28628004</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Levy</surname> <given-names>I</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The topography of high-order human object areas</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>176</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(02)01870-3</pub-id><pub-id pub-id-type="pmid">11912041</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martínez</surname> <given-names>A</given-names></name><name><surname>Anllo-Vento</surname> <given-names>L</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Frank</surname> <given-names>LR</given-names></name><name><surname>Buxton</surname> <given-names>RB</given-names></name><name><surname>Dubowitz</surname> <given-names>DJ</given-names></name><name><surname>Wong</surname> <given-names>EC</given-names></name><name><surname>Hinrichs</surname> <given-names>H</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Involvement of striate and extrastriate visual cortical areas in spatial attention</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>364</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1038/7274</pub-id><pub-id pub-id-type="pmid">10204544</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oliphant</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>A Guide to NumPy</source><publisher-loc>United states</publisher-loc><publisher-name>Trelgol Publishing</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olman</surname> <given-names>CA</given-names></name><name><surname>Bao</surname> <given-names>P</given-names></name><name><surname>Engel</surname> <given-names>SA</given-names></name><name><surname>Grant</surname> <given-names>AN</given-names></name><name><surname>Purington</surname> <given-names>C</given-names></name><name><surname>Qiu</surname> <given-names>C</given-names></name><name><surname>Schallmo</surname> <given-names>MP</given-names></name><name><surname>Tjan</surname> <given-names>BS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hemifield columns co-opt ocular dominance column structure in human achiasma</article-title><source>NeuroImage</source><volume>164</volume><fpage>59</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.12.063</pub-id><pub-id pub-id-type="pmid">28017921</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Press</surname> <given-names>WA</given-names></name><name><surname>Brewer</surname> <given-names>AA</given-names></name><name><surname>Dougherty</surname> <given-names>RF</given-names></name><name><surname>Wade</surname> <given-names>AR</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Visual areas and spatial summation in human visual cortex</article-title><source>Vision Research</source><volume>41</volume><fpage>1321</fpage><lpage>1332</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(01)00074-8</pub-id><pub-id pub-id-type="pmid">11322977</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname> <given-names>EC</given-names></name><name><surname>Jbabdi</surname> <given-names>S</given-names></name><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Andersson</surname> <given-names>J</given-names></name><name><surname>Burgess</surname> <given-names>GC</given-names></name><name><surname>Harms</surname> <given-names>MP</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>MSM: a new flexible framework for Multimodal Surface Matching</article-title><source>NeuroImage</source><volume>100</volume><fpage>414</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.05.069</pub-id><pub-id pub-id-type="pmid">24939340</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schira</surname> <given-names>MM</given-names></name><name><surname>Tyler</surname> <given-names>CW</given-names></name><name><surname>Breakspear</surname> <given-names>M</given-names></name><name><surname>Spehar</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The foveal confluence in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>9050</fpage><lpage>9058</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1760-09.2009</pub-id><pub-id pub-id-type="pmid">19605642</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schira</surname> <given-names>MM</given-names></name><name><surname>Tyler</surname> <given-names>CW</given-names></name><name><surname>Spehar</surname> <given-names>B</given-names></name><name><surname>Breakspear</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modeling magnification and anisotropy in the primate foveal confluence</article-title><source>PLoS Computational Biology</source><volume>6</volume><elocation-id>e1000651</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000651</pub-id><pub-id pub-id-type="pmid">20126528</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Reppas</surname> <given-names>JB</given-names></name><name><surname>Kwong</surname> <given-names>KK</given-names></name><name><surname>Belliveau</surname> <given-names>JW</given-names></name><name><surname>Brady</surname> <given-names>TJ</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title><source>Science</source><volume>268</volume><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1126/science.7754376</pub-id><pub-id pub-id-type="pmid">7754376</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>From monkeys to humans: what do we now know about brain homologies?</article-title><source>Current Opinion in Neurobiology</source><volume>15</volume><fpage>135</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2005.03.014</pub-id><pub-id pub-id-type="pmid">15831394</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shmuel</surname> <given-names>A</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Pfeuffer</surname> <given-names>J</given-names></name><name><surname>Van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Adriany</surname> <given-names>G</given-names></name><name><surname>Hu</surname> <given-names>X</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Sustained negative BOLD, blood flow and oxygen consumption response and its coupling to the positive response in the human brain</article-title><source>Neuron</source><volume>36</volume><fpage>1195</fpage><lpage>1210</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01061-9</pub-id><pub-id pub-id-type="pmid">12495632</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname> <given-names>MA</given-names></name><name><surname>Ress</surname> <given-names>D</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Topographic maps of visual spatial attention in human parietal cortex</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>1358</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1152/jn.01316.2004</pub-id><pub-id pub-id-type="pmid">15817643</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Johansen-Berg</surname> <given-names>H</given-names></name><name><surname>Bannister</surname> <given-names>PR</given-names></name><name><surname>De Luca</surname> <given-names>M</given-names></name><name><surname>Drobnjak</surname> <given-names>I</given-names></name><name><surname>Flitney</surname> <given-names>DE</given-names></name><name><surname>Niazy</surname> <given-names>RK</given-names></name><name><surname>Saunders</surname> <given-names>J</given-names></name><name><surname>Vickers</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>De Stefano</surname> <given-names>N</given-names></name><name><surname>Brady</surname> <given-names>JM</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stensaas</surname> <given-names>SS</given-names></name><name><surname>Eddington</surname> <given-names>DK</given-names></name><name><surname>Dobelle</surname> <given-names>WH</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>The topography and variability of the primary visual cortex in man</article-title><source>Journal of Neurosurgery</source><volume>40</volume><fpage>747</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.3171/jns.1974.40.6.0747</pub-id><pub-id pub-id-type="pmid">4826600</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swisher</surname> <given-names>JD</given-names></name><name><surname>Halko</surname> <given-names>MA</given-names></name><name><surname>Merabet</surname> <given-names>LB</given-names></name><name><surname>McMains</surname> <given-names>SA</given-names></name><name><surname>Somers</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual topography of human intraparietal sulcus</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>5326</fpage><lpage>5337</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0991-07.2007</pub-id><pub-id pub-id-type="pmid">17507555</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uğurbil</surname> <given-names>K</given-names></name><name><surname>Xu</surname> <given-names>J</given-names></name><name><surname>Auerbach</surname> <given-names>EJ</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Vu</surname> <given-names>AT</given-names></name><name><surname>Duarte-Carvajalino</surname> <given-names>JM</given-names></name><name><surname>Lenglet</surname> <given-names>C</given-names></name><name><surname>Wu</surname> <given-names>X</given-names></name><name><surname>Schmitter</surname> <given-names>S</given-names></name><name><surname>Van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Strupp</surname> <given-names>J</given-names></name><name><surname>Sapiro</surname> <given-names>G</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Wang</surname> <given-names>D</given-names></name><name><surname>Harel</surname> <given-names>N</given-names></name><name><surname>Garwood</surname> <given-names>M</given-names></name><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Feinberg</surname> <given-names>DA</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Miller</surname> <given-names>KL</given-names></name><name><surname>Sotiropoulos</surname> <given-names>SN</given-names></name><name><surname>Jbabdi</surname> <given-names>S</given-names></name><name><surname>Andersson</surname> <given-names>JL</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>Pushing spatial and temporal resolution for functional and diffusion MRI in the Human Connectome Project</article-title><source>NeuroImage</source><volume>80</volume><fpage>80</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.012</pub-id><pub-id pub-id-type="pmid">23702417</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A tension-based theory of morphogenesis and compact wiring in the central nervous system</article-title><source>Nature</source><volume>385</volume><fpage>313</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1038/385313a0</pub-id><pub-id pub-id-type="pmid">9002514</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Drury</surname> <given-names>HA</given-names></name><name><surname>Joshi</surname> <given-names>S</given-names></name><name><surname>Miller</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Functional and structural mapping of human cerebral cortex: solutions are in the surfaces</article-title><source>PNAS</source><volume>95</volume><fpage>788</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.3.788</pub-id><pub-id pub-id-type="pmid">9448242</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Barch</surname> <given-names>DM</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>The WU-Minn Human Connectome Project: an overview</article-title><source>NeuroImage</source><volume>80</volume><fpage>62</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id><pub-id pub-id-type="pmid">23684880</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Glasser</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Parcellating Cerebral Cortex: How Invasive Animal Studies Inform Noninvasive Mapmaking in Humans</article-title><source>Neuron</source><volume>99</volume><fpage>640</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.002</pub-id><pub-id pub-id-type="pmid">30138588</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Smirnakis</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Plasticity and stability of visual field maps in adult primary visual cortex</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>873</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.1038/nrn2741</pub-id><pub-id pub-id-type="pmid">19904279</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Imaging retinotopic maps in the human brain</article-title><source>Vision Research</source><volume>51</volume><fpage>718</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.08.004</pub-id><pub-id pub-id-type="pmid">20692278</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic Maps of Visual Topography in Human Cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Horiguchi</surname> <given-names>H</given-names></name><name><surname>Sayres</surname> <given-names>RA</given-names></name><name><surname>Amano</surname> <given-names>K</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mapping hV4 and ventral occipital cortex: the venous eclipse</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/10.5.1</pub-id><pub-id pub-id-type="pmid">20616143</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Wolfram Research</collab></person-group><year iso-8601-date="2018">2018</year><data-title><italic>Mathematica</italic></data-title><publisher-loc>Champaign, Illinois</publisher-loc><publisher-name>Wolfram Research, Inc</publisher-name></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.40224.023</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Schira</surname><given-names>Mark</given-names></name><role>Reviewing Editor</role><aff><institution>University of Wollongong</institution><country>Australia</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Bayesian Analysis of Retinotopic Maps&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Mark Schira as the Reviewing Editor and Reviewer #3, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: D. Samuel Schwarzkopf (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision combining the comments from all three reviewers to help you prepare a revised submission.</p><p>Summary:</p><p>The manuscript by Benson and Winaver introduces a very sophisticated toolbox that combines a prior with fMRI scanning to estimate the retinotopic layout of a large set of visual areas for an individual subject. This work is an extension of previous work from the first author. The authors previous work has proven very successful and useful, and there is little doubt that the current work will be very useful, too. These techniques could theoretically be very beneficial in situations where only minimal data can be collected (elderly, children, patients). Since these Bayesian maps also contain automatic delineation, they could be very useful for reducing experimenter biases in manual delineation, which is still the standard to the field. Of course, this comes with the caveat of reduced flexibility, a potential systematic bias and that the delineation of many areas beyond V3 remains controversial. The auto-delineation is only as good as the atlas used as prior – but even then, the atlas procedure would enhance the reproducibility of findings. In general, this method is solid, cutting edge, and provides a significant improvement over previous procedures.</p><p>Essential revisions:</p><p>The overall assessment of the manuscript from all three reviewers is quite positive, but they raise a set of concerns, as follows.</p><p>1) There were questions about how well the methods can deal with extreme individual variability. As it stands, the manuscript presents a detailed and informative analysis of variability in the anatomical and functional features of visual field maps. The ability to characterize the extent to which variations in retinotopic organization are due to anatomical differences versus differences in the structure-function relationship is remarkable. However, the discussion on how this method would allow the quantification of more extreme and unexpected departures from expected topographical organization should be expanded. There exist several reports showing how clinical conditions (for example: achiasma, hemi-hydranencephaly and albinism) can affect the topographic organization of areas in early visual cortex. It should be discussed what results the method would provide if presented with such anomalies, expected or unexpected. At the present time there is a brief suggestion in the introduction about clinical applications, but we believe it should be expanded in the Discussion.</p><p>2) On a related note, the Bayesian inference maps do a good job at revealing individual differences in the functional architecture of visual cortex. However, while the deviations from a validation data set are small for these Bayesian maps, it would be informative to understand under what situations this technique <italic>fails</italic>. It is unsurprising that the noisy “Data alone” or the “Anatomy alone” maps contain errors – but could the authors elaborate on any systematic errors the Bayesian method has? Do most errors originate in the prior or the empirical retinotopic maps?</p><p>3) In a proper Bayesian formulation, the effect of the evidence on the posterior should depend on both the strength of the evidence but also on the certainty of the prior, which would presumably vary with the visual area. Our understanding of the methods was that the prior was essentially weighted equally at all locations. However, the certainty of the prior maps is arguably better for V1 than the higher areas (this is more of a problem for the higher regions beyond V3 and the authors already acknowledge that this aspect is still experimental).</p><p>Moreover, there was some concern that, as more data are introduced the estimated maps do not seem to improve much further. We understand and appreciate the comments from the authors that they believe this as a strength rather than a bug, because even excessive amount of data might not hold the ground truth. However, it still is a concern: under Bayesian optimization, the uncertainty of the data estimate should continue to get smaller with increasing repetition. This does not seem to be the case, instead after a small amount of data the model 'refuses' to make any further compromises. Further discussion of this point might be useful.</p><p>4) The authors should be much clearer and more upfront what the novel contribution of this work is, as opposed to previous work. From our judgment the improvements are substantial, but the reader has to piece together what is based on previous work and what novel, hence a succinct rapport early on would be much appreciated. Along a similar line, we believe the manuscript would benefit from being made more concise.</p><p>5) The authors already discuss this to some degree, but of course there is no proper ground truth in these analyses. They used a large retinotopic data set as validation data, which seems entirely reasonable. But this has some implications for the individual differences in functional architecture that this analysis reveals: offsets between cortical folding structure and retinotopic organisation could be because of errors in alignment between the functional scans and the cortical sampling. Even a small offset could masquerade as a shift in visual field borders. Thus, some differences may reveal not individual differences in maps but in the idiosyncrasies in image alignment.</p><p>6) We were a bit confused about the pRF size data. Both the pattern and magnitude of results in the Bayesian inference map don't match the empirical pRF data very well. pRF sizes in the actual data are much greater and differ systematically between V1, V2, and V3. The Bayesian data have smaller pRFs (intercept at 0) and V1 and V2 are essentially indistinguishable. The authors should discuss those discrepancies and other issues that might affect pRF size, such as fixation stability and the size of the centre-surround configuration.</p><p>Moreover, the model predicts visual field positions not included in the training data and returns an estimate of pRF size. While the fit between the inferred and the validated eccentricity is remarkable, the plot on the wide-field data is based on a single participant. This limitation should be stressed in the text as a cautionary note. Extrapolation is a notoriously hard problem. The quality of the data plays a major role. A naïve participant, with not-so-stable fixation, and maybe moving in the scanner could result in a different extrapolation trend.</p><p>7) Inter-individual differences that remain after the first anatomical alignment step are characterised as variability in the structure vs. function relation, and they may or may not largely be that. However, no alignment process is optimal and some of the remaining variance is undoubtedly due to insufficiencies of the anatomical alignment process. The alignment process used is based on the nice work from Hinds et al., but one does wonder if the large dataset used may not allow improvements to this process – and if only through fine tuning of some parameters.</p><p>8) Figure 9 is very hard to decipher, it needs more space and thinner lines and different rendering of the error range. It also seems to suggest that the magnification curves of V2 and V3 cross that of V1 at around 1.5-3 degrees. Crossing of magnification curved was first shown by Schira et al. (2009) and explained by Schira et al. (2010). Your data seems to suggest the crossing is at higher eccentricities than the 0.7-1 degree reported by Schira et al., which is possible but deserves some discussion. Specifically, since measured and inferred maps disagree on this value.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.40224.024</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The overall assessment of the manuscript from all three reviewers is quite positive, but they raise a set of concerns, as follows.</p><p>1) There were questions about how well the methods can deal with extreme individual variability. As it stands, the manuscript presents a detailed and informative analysis of variability in the anatomical and functional features of visual field maps. The ability to characterize the extent to which variations in retinotopic organization are due to anatomical differences versus differences in the structure-function relationship is remarkable. However, the discussion on how this method would allow the quantification of more extreme and unexpected departures from expected topographical organization should be expanded. There exist several reports showing how clinical conditions (for example: achiasma, hemi-hydranencephaly and albinism) can affect the topographic organization of areas in early visual cortex. It should be discussed what results the method would provide if presented with such anomalies, expected or unexpected. At the present time there is a brief suggestion in the introduction about clinical applications, but we believe it should be expanded in the Discussion.</p></disp-quote><p>The reviewers are correct that the manuscript emphasized retinotopy measures and models for typical subjects, and did not address how the method would work for unusual or extreme cases. This is an excellent suggestion, and we have now added new data, analyses, and discussion on this issue. Specifically, there is a new section of the manuscript explicitly discussing unusual retinotopic maps and the Bayesian-inferred maps that our method finds when examining them. The new section is called, ‘Limitations and biases of the inferred maps.’ In brief, we used our method to infer the retinotopic maps of two subjects from the Human Connectome Project with unusual map organization. These unusual cases were noted and discussed by Van Essen and Glasser (2018) in a recent review, based on data from a manuscript we collaborated on with them, currently in press (Benson et al., 2018). Both of these subjects have unusual topologies in the functional organization of their left hemisphere dorsal V3 maps that are inconsistent with the retinotopic model we use and the rest of the field generally assumes. Although we find that the inferred maps for these subjects have a different topology than the subjects’ observed maps, the inferred maps appear to be reasonable attempts to explain the observed retinotopic organization with a model that does not topologically match the data.</p><p>These maps and model fits are shown in the new Supplementary file 5, and the expanded discussion of the limits and potential applications to clinical populations can be found in the main text (‘Limitations and biases of the inferred maps.’).</p><p><italic>2) On a related note, the Bayesian inference maps do a good job at revealing individual differences in the functional architecture of visual cortex. However, while the deviations from a validation data set are small for these Bayesian maps, it would be informative to understand under what situations this technique</italic> fails<italic>. It is unsurprising that the noisy “Data alone” or the “Anatomy alone” maps contain errors – but could the authors elaborate on any systematic errors the Bayesian method has? Do most errors originate in the prior or the empirical retinotopic maps?</italic> </p><p>How and why the Bayesian maps produce systematic errors is an important question, but it is difficult if not impossible to answer definitively due to a lack of ground truth maps. Nonetheless, the reviewers are correct that there are some systematic deviations between the maps predicted from our method and our best measurements (validation data) that likely reflect more than measurement noise. For example, our template assumes that the polar angle map at the boundary between ventral V3v and hV4 reverses at precisely the lower vertical meridian. However, in many subjects’ retinotopic maps, this reversal is several degrees away from the meridian, such that there are few to no pRF centers within a few degrees of vertical. Because our Bayesian maps warp the positions of the vertices but not the retinotopic values of the anatomically-defined template, and because this template contains a complete representation of the visual field (all the way to the vertical meridian in each map), the vertical meridian always remains in the inferred maps. This differs from the data, where the vertical meridian is often missing. Whether this is a bug or a feature depends on properties of ground truth maps, which are currently not perfectly known.</p><p>This example is discussed in the third paragraph of the new section of the paper on the limitations and biases of the method. See also the related response to point 1 above.</p><disp-quote content-type="editor-comment"><p>3) In a proper Bayesian formulation, the effect of the evidence on the posterior should depend on both the strength of the evidence but also on the certainty of the prior, which would presumably vary with the visual area. Our understanding of the methods was that the prior was essentially weighted equally at all locations. However, the certainty of the prior maps is arguably better for V1 than the higher areas (this is more of a problem for the higher regions beyond V3 and the authors already acknowledge that this aspect is still experimental).</p></disp-quote><p>We agree that the certainty of the prior maps could (and surely does) differ across cortical locations. We did not incorporate a variable level of certainty into our model. A more complete model would do so. This omission does not, however, indicate that our model is not a proper Bayesian formulation; rather it means that the prior probability distributions we assumed differ from the actual distributions. Consider the distance penalty. Our registration algorithm penalizes model solutions as the distance between neighboring vertices increases. The size of the penalty depends on the distance but not on <italic>which</italic> vertices are moved in the registration. This is akin to a prior probability distribution of edge lengths that is the same for all edges. This assumption is simple to implement and describe, though surely incorrect in detail. For example, some edges are unlikely to change much during registrations, and others are likely to change a lot. Relaxing this assumption would not require a conceptual change to the model, but would require an implementation change: an edge-specific cost function rather than the same cost function for all edges. Deriving independent probability distributions (and hence corresponding cost functions) for each edge would likely improve the model, but is currently beyond the scope of this paper.</p><p>We believe there is a value in simplified assumptions (such as identical probability distributions for all edges) even at the cost of accuracy. That said, in ongoing work, we are in fact trying to derive edge-specific distributions from a large retinotopy dataset (Benson et al., 2018).</p><p>We now discuss this issue in the three paragraphs beginning after Equation 1 in the section, ‘Making the Bayesian inference explicit.’ Moreover, we plot the mean warp field, and the first several principal components of the warp fields. These warp fields show how much (and in what direction) the vertices are shifted from the prior by the Bayesian method (Supplementary file 4). The results confirm the reviewers’ assumption: some vertices have high loadings on the PCs, and others do not, indicating that the Bayesian fits result in some vertices being shifted much more than others.</p><disp-quote content-type="editor-comment"><p>Moreover, there was some concern that, as more data are introduced the estimated maps do not seem to improve much further. We understand and appreciate the comments from the authors that they believe this as a strength rather than a bug, because even excessive amount of data might not hold the ground truth. However, it still is a concern: under Bayesian optimization, the uncertainty of the data estimate should continue to get smaller with increasing repetition. This does not seem to be the case, instead after a small amount of data the model 'refuses' to make any further compromises. Further discussion of this point might be useful.</p></disp-quote><p>It is true that the template solutions are relatively stable with a small amount of data. One could view this as a success or a failure. There are two reasons for the stability. One has to do with the prior probability distributions and one has to do with the likelihood.</p><p>The prior: The topology of the template is preserved by a hard rule. This is now discussed explicitly in terms of prior probability solutions. If a particular map configuration has a prior probability of 0, then this configuration will never be the Bayesian solution, no matter how precise the measurement. This puts an upper bound on how accurate the Bayesian models can be.</p><p>The likelihood: The likelihood in the Bayesian formulation does in fact depend on measurement precision. Specifically, the cost function grows with distance between a vertex and its anchor point, and with the variance explained by the pRF model (Equation 2, row 4, <italic>w</italic> term). As a result, vertices with higher quality data contribute more to the Bayesian fit. However, this growth is linear with respect to the coherence of the pRF model. Because we do not know the mapping between model coherence and the true likelihood, our weights are effectively chosen by heuristic.</p><p>We now make these points explicit in the text. See the next-to-last paragraph of the section, ‘The inferred maps make highly accurate predictions using very little data.’ as well as the section, ‘‘Making the Bayesian inference explicit.’ See also our response to query points 2 and 3.</p><disp-quote content-type="editor-comment"><p>4) The authors should be much clearer and more upfront what the novel contribution of this work is, as opposed to previous work. From our judgment the improvements are substantial, but the reader has to piece together what is based on previous work and what novel, hence a succinct rapport early on would be much appreciated. Along a similar line we believe the manuscript would benefit from being made more concise.</p></disp-quote><p>Agreed. We have reworked sections of the Introduction to make the novel contributions much clearer up-front and have cleaned up our writing throughout.</p><disp-quote content-type="editor-comment"><p>5) The authors already discuss this to some degree, but of course there is no proper ground truth in these analyses. They used a large retinotopic data set as validation data, which seems entirely reasonable. But this has some implications for the individual differences in functional architecture that this analysis reveals: offsets between cortical folding structure and retinotopic organisation could be because of errors in alignment between the functional scans and the cortical sampling. Even a small offset could masquerade as a shift in visual field borders. Thus, some differences may reveal not individual differences in maps but in the idiosyncrasies in image alignment.</p></disp-quote><p>We agree entirely that the image-alignment errors are a potential source of noise and should therefore temper our conclusions. We have added additional text discussing this issue. See updated text in the second-to-last paragraph of the section, ‘Individual differences in the V1-V3 structure-function relationship across subjects are substantial.’</p><disp-quote content-type="editor-comment"><p>6) We were a bit confused about the pRF size data. Both the pattern and magnitude of results in the Bayesian inference map don't match the empirical pRF data very well. pRF sizes in the actual data are much greater and differ systematically between V1, V2, and V3. The Bayesian data have smaller pRFs (intercept at 0) and V1 and V2 are essentially indistinguishable. The authors should discuss those discrepancies and other issues that might affect pRF size, such as fixation stability and the size of the centre-surround configuration.</p></disp-quote><p>In the previous version, we compared our pRF sizes to those from a prior paper (Kay et al., 2013), in which the V1 and V2 pRF sizes did not differ substantially.</p><p>We removed the comparison to the Kay et al., 2013 paper (Figure 10) and clarified the description of pRF size (see the section “The Bayesian inferred maps accurately reproduce systematic properties of the visual field maps”).</p><disp-quote content-type="editor-comment"><p>Moreover, the model predicts visual field positions not included in the training data and returns an estimate of pRF size. While the fit between the inferred and the validated eccentricity is remarkable, the plot on the wide-field data is based on a single participant. This limitation should be stressed in the text as a cautionary note. Extrapolation is a notoriously hard problem. The quality of the data plays a major role. A naïve participant, with not-so-stable fixation, and maybe moving in the scanner could result in a different extrapolation trend.</p></disp-quote><p>Agreed. We now note that one of the two panels on extrapolation of eccentricity comes from only one subject, and that the accuracy of extrapolation to untested eccentricities likely depends on the quality of the measured data and subject fixation (see the section “The Bayesian model accurately predicts visual field positions not included in the training data”.</p><disp-quote content-type="editor-comment"><p>7) Inter-individual differences that remain after the first anatomical alignment step are characterised as variability in the structure vs. function relation, and they may or may not largely be that. However, no alignment process is optimal and some of the remaining variance is undoubtedly due to insufficiencies of the anatomical alignment process. The alignment process used is based on the nice work from Hinds et al., but one does wonder if the large dataset used may not allow improvements to this process – and if only through fine tuning of some parameters.</p></disp-quote><p>Inter-subject cortical surface alignment remains a challenging problem and a source of uncertainty in our data. As we mention in the paper, it is conceivable that our method does little more than correct for insufficient cortical surface alignment; though we believe this not to be the complete story. In this paper we have chosen to employ FreeSurfer’s alignment tools due in part to their popularity in the community. This effectively makes it easier for our tools to be used to analyze existing datasets. We agree that large neuroimaging databases (e.g., the Human Connectome Project) engender a substantial opportunity to improve cortical alignment algorithms.</p><p>We have expanded our discussion of these ideas to the text (last two paragraphs of ‘Individual differences in the V1-V3 structure-function relationship across subjects are substantial.’).</p><disp-quote content-type="editor-comment"><p>8) Figure 9 is very hard to decipher, it needs more space and thinner lines and different rendering of the error range. It also seems to suggest that the magnification curves of V2 and V3 cross that of V1 at around 1.5-3 degrees. Crossing of magnification curved was first shown by Schira et al. (2009) and explained by Schira et al. (2010). Your data seems to suggest the crossing is at higher eccentricities than the 0.7-1 degree reported by Schira et al., which is possible but deserves some discussion. Specifically, since measured and inferred maps disagree on this value.</p></disp-quote><p>Agreed. Figure 9 (now Figure 10) has been cleaned up and replotted. The previous version of the figure used the standard error of the parameters rather than the standard error of the model fits themselves, and correcting this has made the plots more legible. We have additionally expanded our discussion of the cortical magnification regarding the crossover of cortical magnification of V1-V3.</p></body></sub-article></article>