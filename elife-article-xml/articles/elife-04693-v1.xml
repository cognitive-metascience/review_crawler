<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">04693</article-id><article-id pub-id-type="doi">10.7554/eLife.04693</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Role of visual and non-visual cues in constructing a rotation-invariant representation of heading in parietal cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-18534"><name><surname>Sunkara</surname><given-names>Adhira</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-11799"><name><surname>DeAngelis</surname><given-names>Gregory C</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-18789"><name><surname>Angelaki</surname><given-names>Dora E</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Biomedical Engineering</institution>, <institution>Washington University in St. Louis</institution>, <addr-line><named-content content-type="city">St. Louis</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Neuroscience</institution>, <institution>Baylor College of Medicine</institution>, <addr-line><named-content content-type="city">Houston</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Brain and Cognitive Sciences</institution>, <institution>University of Rochester</institution>, <addr-line><named-content content-type="city">Rochester</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Romo</surname><given-names>Ranulfo</given-names></name><role>Reviewing editor</role><aff><institution>Universidad Nacional Autonoma de Mexico</institution>, <country>Mexico</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>sunkara@bcm.edu</email> (AS);</corresp><corresp id="cor2"><email>angelaki@cabernet.cns.bcm.edu</email> (DEA)</corresp><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="pub" publication-format="electronic"><day>18</day><month>02</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e04693</elocation-id><history><date date-type="received"><day>10</day><month>09</month><year>2014</year></date><date date-type="accepted"><day>20</day><month>01</month><year>2015</year></date></history><permissions><copyright-statement>© 2015, Sunkara et al</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Sunkara et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-04693-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.04693.001</object-id><p>As we navigate through the world, eye and head movements add rotational velocity patterns to the retinal image. When such rotations accompany observer translation, the rotational velocity patterns must be discounted to accurately perceive heading. The conventional view holds that this computation requires efference copies of self-generated eye/head movements. Here we demonstrate that the brain implements an alternative solution in which retinal velocity patterns are themselves used to dissociate translations from rotations. These results reveal a novel role for visual cues in achieving a rotation-invariant representation of heading in the macaque ventral intraparietal area. Specifically, we show that the visual system utilizes both local motion parallax cues and global perspective distortions to estimate heading in the presence of rotations. These findings further suggest that the brain is capable of performing complex computations to infer eye movements and discount their sensory consequences based solely on visual cues.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.001">http://dx.doi.org/10.7554/eLife.04693.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.04693.002</object-id><title>eLife digest</title><p>When strolling along a path beside a busy street, we can look around without losing our stride. The things we see change as we walk forward, and our view also changes if we turn our head—for example, to look at a passing car. Nevertheless, we can still tell that we are walking in a straight-line because our brain is able to compute the direction in which we are heading by discounting the visual changes caused by rotating our head or eyes.</p><p>It remains unclear how the brain gets the information about head and eye movements that it would need to be able to do this. Many researchers had proposed that the brain estimates these rotations by using a copy of the neural signals that are sent to the muscles to move the eyes or head. However, it is possible that the brain can estimate head and eye rotations by directly analyzing the visual information from the eyes. One region of the brain that may contribute to this process is the ventral intraparietal area or ‘area VIP’ for short.</p><p>Sunkara et al. devised an experiment that can help distinguish the effects of visual cues from copies of neural signals sent to the muscles during eye rotations. This involved training monkeys to look at a 3D display of moving dots, which gives the impression of moving through space. Sunkara et al. then measured the electrical signals in area VIP either when the monkey moved its eyes (to follow a moving target), or when the display changed to give the monkey the same visual cues as if it had rotated its eyes, when in fact it had not.</p><p>Sunkara et al. found that the electrical signals recorded in area VIP when the monkey was given the illusion of rotating its eyes were similar to the signals recorded when the monkey actually rotated its eyes. This suggests that visual cues play an important role in correcting for the effects of eye rotations and correctly estimating the direction in which we are heading. Further research into the mechanisms behind this neural process could lead to new vision-based treatments for medical disorders that cause people to have balance problems. Similar research could also help to identify ways to improve navigation in automated vehicles, such as driverless cars.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.002">http://dx.doi.org/10.7554/eLife.04693.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>rhesus macaque</kwd><kwd>ventral intraparietal area</kwd><kwd>pursuit eye movement</kwd><kwd>self motion</kwd><kwd>optic flow</kwd><kwd>dynamic perspective</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute (NEI)</institution></institution-wrap></funding-source><award-id>NEI, R01-EY-017866</award-id><principal-award-recipient><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute (NEI)</institution></institution-wrap></funding-source><award-id>NEI, R01-EY-016178</award-id><principal-award-recipient><name><surname>DeAngelis</surname><given-names>Gregory C</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute (NEI)</institution></institution-wrap></funding-source><award-id>NEI CORE, EY-001319</award-id><principal-award-recipient><name><surname>DeAngelis</surname><given-names>Gregory C</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.0</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Visual information (optic flow) can be used to discount self-generated rotations and aid in the accurate representation of heading.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Retinal images of the environment are altered by self-generated rotations such as eye or head movements. In order to perceive the world accurately, the component of retinal patterns resulting from such rotations needs to be discounted by the visual system. How the brain achieves such a rotation-invariant visual representation of the world remains unclear. Visually guided navigation is an important context in which achieving rotation-invariance is critical for accurate behavior (<xref ref-type="bibr" rid="bib21">Gibson, 1950</xref>; <xref ref-type="bibr" rid="bib57">Warren and Saunders, 1995</xref>; <xref ref-type="bibr" rid="bib22">Grigo and Lappe, 1999</xref>). For example, while walking down a sidewalk and simultaneously looking at a passing car using eye or head rotations, the brain must discount the visual consequences of the self-generated rotations to estimate and maintain one's direction of translation (i.e., heading).</p><p>Self-motion results in retinal velocity patterns known as ‘optic flow’ (<xref ref-type="bibr" rid="bib21">Gibson, 1950</xref>). During translations, the resulting retinal pattern is generally an expansionary or contractionary radial flow field from which the point of zero velocity (Focus of Expansion, FOE) can be used to estimate heading (<xref ref-type="bibr" rid="bib51">Tanaka et al., 1986</xref>; <xref ref-type="bibr" rid="bib56">Warren et al., 1988</xref>; <xref ref-type="bibr" rid="bib14">Duffy and Wurtz, 1995</xref>; <xref ref-type="bibr" rid="bib5">Britten, 2008</xref>). However, eye or head rotations alter this flow pattern such that deciphering heading requires decomposing the resultant optic flow into translational and rotational components (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Psychophysical (<xref ref-type="bibr" rid="bib44">Royden et al., 1992</xref>; <xref ref-type="bibr" rid="bib43">Royden, 1994</xref>; <xref ref-type="bibr" rid="bib12">Crowell et al., 1998</xref>) and electrophysiological (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib39">Page and Duffy, 1999</xref>; <xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>) studies have often emphasized the role of non-visual signals, such as efference copies of self-generated eye/head movements, in discounting rotations to estimate heading. Such non-visual signals can represent several different sources of rotation, including eye-in-head (R<sub>EH</sub>), head-on-body (R<sub>HB</sub>), and body-in-world (R<sub>BW</sub>) movements (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Critically, retinal image motion is determined by the translation and rotation of the eye relative to the world (T<sub>EW</sub> and R<sub>EW</sub>, <xref ref-type="fig" rid="fig1">Figure 1B</xref>), such that extracting heading from optic flow requires compensating for the total rotation of the eye-in-world (where, R<sub>EW</sub> = R<sub>EH</sub> + R<sub>HB</sub> + R<sub>BW</sub>). Therefore, in general, multiple non-visual signals would need to be added to achieve a rotation-invariant estimate of heading, potentially compounding the noise that is associated with each signal (<xref ref-type="bibr" rid="bib20">Gellman and Fletcher, 1992</xref>; <xref ref-type="bibr" rid="bib36">Li and Matin, 1992</xref>; <xref ref-type="bibr" rid="bib12">Crowell et al., 1998</xref>).<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.04693.003</object-id><label>Figure 1.</label><caption><title>The problem of dissociating translations and rotations, and experimental approaches.</title><p>(<bold>A</bold>) Optic flow patterns during self-motion (shown as planar projections onto a flat image). Forward translations result in symmetric flow patterns (black vector fields) with a focus of expansion (FOE) indicating heading. When rotations are added to forward translations, the resultant optic flow pattern has an FOE shift in the direction of the added rotation (rightward rotation: red, leftward rotation: blue). (<bold>B</bold>) VIP receives both visual and non-visual signals that may be used to achieve rotation-invariant heading estimates. Visual optic flow signals contain information about translation and rotation of the eye in the world (T<sub>EW</sub>, R<sub>EW</sub>) whereas non-visual signals (efference copies) may contain information about rotation of eye-in-head (R<sub>EH</sub>), rotation of head-on-body (R<sub>HB</sub>), or rotation of body-in-world (R<sub>BW</sub>). (<bold>C</bold>) Visual stimuli simulating translations in eight directions spanning the entire horizontal plane were presented to the monkey. (<bold>D</bold>) Schematic showing the translation and rotation parameters in the simulated 3D cloud. Inset shows the trapezoidal velocity profile of translation and rotation during the course of a trial (1500 ms). (<bold>E</bold>) During the ‘Real pursuit (RP)’ condition, the optic flow stimulus on the screen simulated translation, while rotation was added by having the monkey smoothly pursue a visual target that moved leftward or rightward across the screen. During the ‘Simulated pursuit (SP)’ condition, the monkey fixated at the center of the display while optic flow simulated combinations of translation and eye rotation. During real and simulated pursuit, the optic flow patterns projected onto the monkey's retina were nearly identical.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.003">http://dx.doi.org/10.7554/eLife.04693.003</ext-link></p></caption><graphic xlink:href="elife-04693-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.04693.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Dependence of translational and rotational optic flow properties on viewing distance.</title><p>Translational optic flow vectors (left column) decrease in magnitude as the distance of the plane being viewed increases. Rotational optic flow (middle column), however, remains constant irrespective of the viewing distance. When these translation and rotation flow fields are added, the resultant FOE shift consequently varies with distance to the plane (right column). Therefore, in a 3D environment where objects are present at varying distance from the observer, no single FOE exists. For the stimulus parameters used in this study, the nearest depth plane of the simulated 3D cloud (25 cm) results in a 20° shift in FOE; at the screen depth of 35 cm, the shift is 33° and for any plane beyond 45 cm (45–125 cm), the FOE is undefined as the optic flow is dominated by rotations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.004">http://dx.doi.org/10.7554/eLife.04693.004</ext-link></p></caption><graphic xlink:href="elife-04693-fig1-figsupp1-v1.tif"/></fig></fig-group></p><p>Alternatively, rotation-invariance can theoretically be achieved exclusively through visual processing (<xref ref-type="bibr" rid="bib37">Longuet-Higgins and Prazdny, 1980</xref>; <xref ref-type="bibr" rid="bib41">Rieger and Lawton, 1985</xref>). If the brain can use optic flow to directly estimate and discount rotations of the eye-in-world (R<sub>EW</sub>), such mechanisms may provide a complementary and potentially more efficient way to decompose rotations and translations to achieve invariant heading perception. Psychophysical studies have provided evidence that visual cues may play a role in estimating heading in the presence of rotations (<xref ref-type="bibr" rid="bib22">Grigo and Lappe, 1999</xref>; <xref ref-type="bibr" rid="bib33">Li and Warren, 2000</xref>; <xref ref-type="bibr" rid="bib11">Crowell and Andersen, 2001</xref>; <xref ref-type="bibr" rid="bib34">Li and Warren, 2002</xref>, <xref ref-type="bibr" rid="bib35">2004</xref>; <xref ref-type="bibr" rid="bib45">Royden et al., 2006</xref>). However, electrophysiological evidence for the role of visual cues is ambiguous, in part because previous neurophysiological studies either did not include visual controls for eye rotation (<xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>), simulated rotations incorrectly (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>) or employed insufficient analysis methods (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>; <xref ref-type="bibr" rid="bib4">Bremmer et al., 2010</xref>; <xref ref-type="bibr" rid="bib26">Kaminiarz et al., 2014</xref>) (see ‘Discussion’).</p><p>We recorded neural activity from the macaque ventral intraparietal area (VIP) to evaluate the relative roles of visual and non-visual cues in computing heading in the presence of rotations. To elucidate the role of visual cues, we accurately simulated combinations of translations and rotations using visual stimuli containing a variety of cues present during natural self-motion. Our results provide novel evidence that (1) a subpopulation of VIP neurons utilizes visual cues to signal heading in a rotation-invariant fashion and (2) both local motion parallax and global perspective cues present in optic flow contribute to these computations. In addition, we find that visual and non-visual sources of rotation elicit similar responses in VIP, suggesting multi-sensory combination of both visual and non-visual cues in representing rotations. We further show that rotation-invariance is distinct from the reference frame used to represent heading, and provide additional support that heading representation in VIP is close to eye-centered (<xref ref-type="bibr" rid="bib8">Chen et al., 2013</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To investigate the effect of rotations on the visual heading tuning of VIP neurons, we presented visual stimuli simulating eight directions of translation in the horizontal plane (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) and two directions of rotation (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). To evaluate the relative roles of visual and non-visual cues, rotations were introduced in the form of either ‘real’ or ‘simulated’ pursuit eye movements. During real pursuit (RP, <xref ref-type="fig" rid="fig1">Figure 1E</xref>, left), the monkey smoothly tracked a target moving across the screen such that both visual and non-visual rotation cues were present. During simulated pursuit (SP, <xref ref-type="fig" rid="fig1">Figure 1E</xref>, right), the visual motion stimulus accurately simulated a combination of translation and eye rotation while the monkey fixated a stationary target at the center of the display (non-visual cues were absent). In order to provide a rich visual environment, the first experiment simulated self-motion through a 3D cloud of dots, a stimulus that contains both local motion parallax cues resulting from translation (<xref ref-type="bibr" rid="bib25">Helmholtz and Southall, 1924</xref>; <xref ref-type="bibr" rid="bib21">Gibson, 1950</xref>; <xref ref-type="bibr" rid="bib37">Longuet-Higgins and Prazdny, 1980</xref>; <xref ref-type="bibr" rid="bib31">Koenderink and van Doorn, 1987</xref>) and global perspective cues to rotation (<xref ref-type="bibr" rid="bib29">Koenderink and van Doorn, 1976</xref>; <xref ref-type="bibr" rid="bib22">Grigo and Lappe, 1999</xref>). To further explore the underpinnings of a retinal solution in achieving rotation-invariance, a second experiment used a fronto-parallel plane (FP) of dots, which eliminates the local motion parallax cues but retains global perspective cues to rotation.</p><sec id="s2-1"><title>Analysis of the effects of rotation on optic flow</title><p>When rotation and translation occur simultaneously, the resulting pattern of retinal velocity vectors can differ substantially from the radial optic flow patterns observed during pure translation. This change is often conceptualized as a shift in the focus of expansion (FOE) (<xref ref-type="bibr" rid="bib55">Warren and Hannon, 1990</xref>; <xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>). However, in a visual scene with depth structure, adding rotation results in different FOE shifts at different depths (<xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>). This is due to a key difference in the properties of optic flow resulting from translations and rotations—the magnitudes of translational optic flow vectors decrease with distance (depth), whereas rotational optic flow vectors are independent of depth (<xref ref-type="bibr" rid="bib37">Longuet-Higgins and Prazdny, 1980</xref>). Hence, for more distal points in a scene, rotations produce a larger FOE shift (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). For the translation and rotation parameters used in this study, the nearest plane in the 3D cloud (25 cm) results in an FOE shift of approximately 20°. However, for any plane farther than 45 cm, the resultant optic flow has an undefined FOE (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, top row). The simulated 3D cloud ranged from 25 cm to 125 cm, resulting in a large volume of the stimulus space having undefined FOE shifts. Since FOE shift is an ill-defined measure of the visual consequence of rotations, we simply refer to the net visual stimulation associated with simultaneous translation and rotation as the ‘resultant optic flow’.</p><p>Forward translations result in an expansionary flow field, for which adding a rightward rotation causes a rightward shift of the focus of expansion (for any given plane). On the other hand, backward translations produce a contractionary flow field and adding a rightward rotation results in a leftward shift in the focus of contraction (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). If a neuron signals heading regardless of the presence of rotations, then its tuning curves during real and simulated pursuit should be identical to the heading tuning curve measured during pure translation (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). For a neuron that instead represents the resultant optic flow rather than the translation component (heading), a transformation of the tuning curve is expected due to the added rotations. As a result of the opposite shifts expected for forward (expansionary flow field) and backward translations (contractionary flow field), the heading tuning curve of a neuron preferring forward headings would have a peak that shifts to the right and a trough that shifts to the left during rightward eye rotation; together, these effects cause a skewing of the tuning curve (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, red curve). For the same neuron, leftward eye rotation would cause the peak to shift to the left and the trough to shift to the right, thus having an opposite effect on the shape of the tuning curve (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, blue curve). Neurons that prefer lateral headings, which are common in VIP (<xref ref-type="bibr" rid="bib7">Chen et al., 2011</xref>), may in fact, show no shift in the peak. But, since opposite shifts are expected for forward and backward headings, the resulting tuning curve may exhibit substantial bandwidth changes (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.04693.005</object-id><label>Figure 2.</label><caption><title>Predicted transformations of heading tuning curves due to rotations.</title><p>(<bold>A</bold>) Forward and backward translations result in expansion and contraction flow fields, respectively (row 1). Adding rotation causes the FOE to shift in opposite directions for forward and backward translations (rows 2, 3). (<bold>B</bold>, <bold>C</bold>, <bold>D</bold>) Hypothetical heading tuning curves showing the predicted transformations due to rotations (rightward, red; leftward, blue). (<bold>B</bold>) Schematic illustration of rotation-invariant heading tuning curves. (<bold>C</bold>) Schematic representing a cell that responds to resultant optic flow (no rotation tolerance) with a heading preference of straight ahead (90°). Rightward rotation causes a rightward shift of the tuning curve for forward headings (around 90°), and a leftward shift for backward headings (around 270°). The opposite pattern holds for leftward rotations. Here, the net result of rotation is a skewing of the tuning curve. (<bold>D</bold>) Schematic tuning of a cell with a leftward heading preference (180°) and no rotation tolerance. In this case, the tuning bandwidth increases for leftward rotations and decreases for rightward rotations. The opposite bandwidth changes would be observed for a cell with a 0° heading preference (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.005">http://dx.doi.org/10.7554/eLife.04693.005</ext-link></p></caption><graphic xlink:href="elife-04693-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.04693.006</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Schematic showing tuning curve transformations for hypothetical neurons with different heading preferences.</title><p>For cells that prefer lateral headings (0°, 180°), rotations (rightward: red, leftward: blue) cause changes in tuning bandwidth. The expected change in bandwidth is opposite for cells preferring 0° and 180°. For cells preferring forward or backward motion, rotations cause opposite directions of shifts in the peak and trough of the tuning curve, thus changing the shape of the tuning curve.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.006">http://dx.doi.org/10.7554/eLife.04693.006</ext-link></p></caption><graphic xlink:href="elife-04693-fig2-figsupp1-v1.tif"/></fig></fig-group></p><p>Therefore, under the null hypothesis that neural responses are simply determined by the resultant optic flow, the expected effect of rotation on heading tuning is not simply a global shift of the tuning curve, as was assumed previously (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib39">Page and Duffy, 1999</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>; <xref ref-type="bibr" rid="bib4">Bremmer et al., 2010</xref>; <xref ref-type="bibr" rid="bib26">Kaminiarz et al., 2014</xref>). Further illustrations of the expected effects of rotation for hypothetical neurons with different heading preferences are shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. We designed our quantitative analysis of heading tuning curves specifically to account for these previously unrecognized complexities (see ‘Materials and methods’).</p></sec><sec id="s2-2"><title>Influence of visual and non-visual cues on heading representation in VIP</title><p>Heading tuning curves (translation only) can be compared to real pursuit (RP) and simulated pursuit (SP) tuning curves (translation + rotation) to evaluate whether a VIP neuron signals heading invariant to rotations (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), or whether it responds to the resultant optic flow (<xref ref-type="fig" rid="fig2">Figure 2C,D</xref>). <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows heading tuning curves for an example neuron during pure translation (black curve), as well as during rightward (red) and leftward (blue) rotations added in RP and SP conditions. The tuning curves in this example show only minor changes during RP indicating that the cell signals heading in a manner that is largely invariant to eye rotation, consistent with previous findings for real eye rotation (<xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>). Interestingly, the tuning curves of the same neuron during SP also change very little, showcasing the role of visual signals in compensating for rotation. Thus, rotation invariance in VIP that was previously attributed to non-visual signals (<xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>) might also be driven by visual cues.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.04693.007</object-id><label>Figure 3.</label><caption><title>Heading tuning curves from two example VIP neurons.</title><p>Five tuning curves were obtained per cell: one pure translation curve (black), two real pursuit (RP, left column) curves, and two simulated pursuit (SP, right column) curves (rightward rotation: red, leftward rotation: blue). Black horizontal line indicates baseline activity. Red and blue stars in the left column (RP) indicate responses during pursuit in darkness, and in the right column (SP) indicate responses to simulated eye rotation. (<bold>A</bold>) This neuron has largely rotation-invariant tuning curves in both RP and SP conditions (shifts not significantly different from 0, CI from bootstrap), and has significant rotation responses during both pursuit in darkness and simulated rotation (compared to baseline; Wilcoxon signed rank test p &lt; 0.05). (<bold>B</bold>) This example neuron shows significant bandwidth changes during SP (shifts &gt;0°, CI from bootstrap), similar to the prediction of <xref ref-type="fig" rid="fig2">Figure 2D</xref>. Of the rotation-only conditions, the cell only responds significantly during rightward pursuit in darkness (Wilcoxon signed-rank test p = 0.01).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.007">http://dx.doi.org/10.7554/eLife.04693.007</ext-link></p></caption><graphic xlink:href="elife-04693-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.04693.008</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Bandwidth changes observed in data.</title><p>Bandwidths of linearly interpolated tuning curves were calculated as the full width at half height (FWHH). The difference in FWHH between the pure translation and rotation-added tuning curves (reds: rightward, blues: leftward rotation) are plotted for cells with lateral heading preferences (since the largest bandwidth changes would occur for cells preferring lateral motion). The bandwidth changes observed are in the directions predicted by <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. The change in bandwidths at 0° and 180° headings are significantly different for both RP (Wilcoxon rank sum test; leftward, rightward: p &lt; 0.001) and SP (Wilcoxon rank sum test; leftward, rightward: p &lt; 0.001) conditions.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.008">http://dx.doi.org/10.7554/eLife.04693.008</ext-link></p></caption><graphic xlink:href="elife-04693-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.04693.009</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Heading tuning curves from two example VIP neurons that preferred forward headings.</title><p>Tuning curves are measured and represented as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>A</bold>) This neuron has tuning that is largely invariant to rotations during both real and simulated pursuit. (<bold>B</bold>) This example neuron shows significant shifts in the tuning peak during both RP and SP. Since this neuron prefers forward translations, the shifts observed are similar to the predictions made in <xref ref-type="fig" rid="fig2">Figure 2C</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.009">http://dx.doi.org/10.7554/eLife.04693.009</ext-link></p></caption><graphic xlink:href="elife-04693-fig3-figsupp2-v1.tif"/></fig></fig-group></p><p>Data for another example VIP neuron (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) reveal RP tuning curves that are also largely consistent in shape with the pure translation curve, but which have larger response amplitudes during leftward pursuit. During simulated pursuit, however, the tuning curves of this neuron show clear bandwidth changes. Thus, this second example neuron appears to rely more on non-visual cues to discount rotations. Note that this example neuron preferred lateral headings (leftward) and showed large bandwidth changes during SP, as predicted in the schematic illustration of <xref ref-type="fig" rid="fig2">Figure 2D</xref>. Such bandwidth changes were observed consistently among VIP neurons that preferred lateral translations; specifically, rightward rotations increased bandwidth for cells preferring rightward headings (∼0°) and decreased bandwidth for cells preferring leftward headings (∼180°), with the opposite pattern holding for leftward rotations (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). We find analogous results for cells that preferred forward/backward translations. Specifically, we find neurons with tuning curve peaks around forward/backward heading that are invariant to added rotations (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>), as well as neurons for which the tuning curve peaks shift with real and simulated pursuit (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>), as shown in the simulations in <xref ref-type="fig" rid="fig2">Figure 2C</xref>.</p><p>Because of these changes in tuning curve bandwidth or shape, analysis of the effects of rotation on heading tuning requires more complex and rigorous approaches (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) than the cross-correlation or rank-order methods used in previous studies (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>; <xref ref-type="bibr" rid="bib4">Bremmer et al., 2010</xref>; <xref ref-type="bibr" rid="bib26">Kaminiarz et al., 2014</xref>). It is also critical to distinguish between changes in response gain and changes in the shape (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>; see Discussion) of tuning curves, which our analysis allows because we sample the entire heading tuning curve (<xref ref-type="bibr" rid="bib38">Mullette-Gillman et al., 2009</xref>; <xref ref-type="bibr" rid="bib6">Chang and Snyder, 2010</xref>; <xref ref-type="bibr" rid="bib42">Rosenberg and Angelaki, 2014</xref>). As shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, the first step in the analysis involves normalizing each RP and SP tuning curve to match the dynamic range of the pure translation tuning curve. Following this transformation, the change in the shape of the RP and SP tuning curves can be measured without ambiguity. To account for the expected changes in bandwidth and skew, partial shifts of the tuning curve were measured separately for forward (0°:180°) and backward (180°:360°) headings. Thus, four shift values were obtained from each neuron for both real and simulated pursuit, corresponding to forward/backward headings and left/right rotation directions. These four values were averaged for each neuron to quantify the transformation in shape and obtain one shift metric for RP tuning curves and one for SP tuning curves (see ‘Materials and methods’, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>Results are summarized for the population of recorded neurons (n = 72; from two monkeys) in <xref ref-type="fig" rid="fig4">Figure 4</xref>. A shift of 0° implies that the neuronal representation of translation is invariant to rotation (i.e., the shape of heading tuning curves are highly similar, as in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). A positive shift indicates under-compensation for rotation, such that responses change in a manner consistent with the resultant optic flow. Negative shifts indicate that the tuning curve transformation was in the direction opposite to that expected based on the resultant optic flow. This can be interpreted as an over-compensation for rotation. As noted earlier, though the FOE shift for the nearest depth plane (25 cm) in our stimuli is 20°, a majority of the cloud volume (45–125 cm deep) is dominated by rotations, such that the resultant optic flow has undefined FOEs. This implies that neurons should show shifts that are generally much larger than 20° if they do not discount the rotations and merely represent the resultant optic flow.<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.04693.010</object-id><label>Figure 4.</label><caption><title>Scatterplot and marginal distributions of shifts measured during real pursuit (RP) and simulated pursuit (SP) using 3D cloud stimuli (n = 72 cells).</title><p>A shift of 0° indicates rotation-invariance. Positive and negative shifts indicate under-compensation and over-compensation for rotation, respectively. Grey shaded area corresponds to shifts &gt;20° (conservative estimate of shift for cells with no tolerance to rotations). Red data points correspond to the shifts associated with the example cells shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Error bars depict bootstrapped 95% confidence intervals (CI). Colored regions of marginal distributions indicate shifts ≤20°. Darker colors indicate shifts not significantly different from 0°. Uncolored histograms indicate shifts significantly &gt;20°. Diagonal histogram shows difference in RP and SP shifts for each neuron with a median of −6.0° indicating that for most cells SP shifts tended to be larger than RP shifts (significantly &lt;0°; Wilcoxon signed-rank test p = 0.02).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.010">http://dx.doi.org/10.7554/eLife.04693.010</ext-link></p></caption><graphic xlink:href="elife-04693-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.04693.011</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Method for analyzing tuning curve shifts.</title><p>(<bold>A</bold>) Example tuning curves of a single neuron for RP and SP conditions. Black–pure translation; red–rightward rotation; blue–leftward rotation. (<bold>B</bold>) First, the offset and gain of the RP/SP tuning curves are corrected to match the offset and gain values of the pure translation tuning curve. (<bold>C</bold>, <bold>D</bold>) To account for bandwidth changes in the shift calculations, the RP/SP tuning curves are split into halves corresponding to forward (0:180°) and backward (180:360°) headings. The pure translation tuning curve is then circularly shifted to minimize the sum squared error with each half of the leftward rotation (<bold>C</bold>) and rightward rotation tuning curves (<bold>D</bold>). This yields four shift values (shown in <bold>C</bold>, <bold>D</bold>) each, for SP and RP, which are averaged. (<bold>E</bold>) We simulated noisy neuronal tuning curves that have different response amplitudes, offsets and bandwidth/shape changes similar to real data (see ‘Materials and methods’). The expected shifts for the simulated data should correspond to 20°. The mean shifts (based on 10 repetitions) were not significantly different from 20° (t-test; p = 1), indicating that our analysis method correctly extracts tuning curve changes despite variations in shape, response amplitude, or baseline response.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.011">http://dx.doi.org/10.7554/eLife.04693.011</ext-link></p></caption><graphic xlink:href="elife-04693-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.04693.012</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Problems with previous approaches to measuring shifts in the absence of full tuning curve measurements.</title><p>Previous studies (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>; <xref ref-type="bibr" rid="bib47">Shenoy et al., 2002</xref>; <xref ref-type="bibr" rid="bib4">Bremmer et al., 2010</xref>; <xref ref-type="bibr" rid="bib26">Kaminiarz et al., 2014</xref>) evaluated heading tuning curves in a narrow range around straight ahead (white region around 90° in panel <bold>A</bold>). (<bold>A</bold>) Two hypothetical tuning curves with different bandwidths and amplitudes. (<bold>B</bold>) Correcting for the difference in response amplitudes reveals a clear difference in bandwidths, which may reflect a lack of compensation for rotation. The rank order of the responses (1–1, 2–2, 3–3) would be identical for the two curves, which previous methods would erroneously interpret as evidence for rotation-invariance (<xref ref-type="bibr" rid="bib4">Bremmer et al., 2010</xref>; <xref ref-type="bibr" rid="bib26">Kaminiarz et al., 2014</xref>). (<bold>C</bold>) Simulated tuning curves with peaks roughly near straight ahead. The underlying von Mises functions (to which Poisson noise was added) have peaks at headings of 80°, 100° and 120°, resulting in a simulated shift of 20°. (<bold>D</bold>) The cross-correlation function between the translation only (black) and rotation-added (red—leftward rotation, blue—rightward rotation) tuning curves. Reflecting the true shift of 20° that was introduced into the tuning curves (before noise was added), the cross-correlation functions peak near a lag of +20°. (<bold>E</bold>) Simulated tuning curves with peaks at 180° and different bandwidths. The difference in the width of the full tuning curves corresponded to a 20° shift (at half-height). (<bold>F</bold>) Cross-correlation functions for the simulated neurons with lateral heading preferences are quite flat and show no evidence of a peak near a lag of +20°. (<bold>G</bold>) Shifts from 10 sets of simulated tuning curves (with different noise samples) were measured using cross-correlation for heading preferences ranging from 0° to 180°. The gain and offsets of the tuning curves were randomized for each set. The mean shift (black markers) approaches the true shift (20°, dashed line) for tuning curves with heading preferences near 90°, but the mean shifts are grossly inaccurate for simulated neurons with lateral heading preferences. In contrast, <xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1E</xref> shows that our analysis method correctly estimates tuning curve shifts regardless of heading preference.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.012">http://dx.doi.org/10.7554/eLife.04693.012</ext-link></p></caption><graphic xlink:href="elife-04693-fig4-figsupp2-v1.tif"/></fig></fig-group></p><p>In the RP condition, 22/72 (30.6%) neurons showed shifts that were not significantly different from zero (bootstrap 95% CI); these cells can be considered to represent heading in a rotation-invariant fashion. For SP, 17/72 (23.6%) neurons had shifts that were not significantly different from zero, indicating that purely visual cues were sufficient to achieve rotation-invariance in these neurons. Only 13/72 (18.1%) neurons during RP and 19/72 (23.4%) neurons during SP showed shifts that were significantly greater than 20°, suggesting that only a minority of VIP neurons simply represent the resultant optic flow.</p><p>The median shift of the population during RP is 8.5°, which is significantly less than the 13.8° median shift observed during SP (Wilcoxon signed-rank test; p = 0.02), indicating greater tolerance to rotations in the presence of both non-visual and visual cues. However, both median shifts are significantly greater than 0° (Wilcoxon signed-rank test; p &lt; 0.001), and less than 20° (Wilcoxon signed-rank test; RP: p &lt; 0.001, SP: p = 0.005) suggesting that, on average, VIP neurons do not simply represent the resultant optic flow, but rather signal heading in a manner that is at least partially tolerant to rotations. Together, these findings indicate that VIP can signal heading in the presence of rotations using both visual and non-visual cues. Importantly, this tolerance to rotations is observed even when only visual cues are present (SP).</p></sec><sec id="s2-3"><title>Visual and non-visual rotation signals in VIP</title><p>The previous section shows that VIP neurons can use visual cues to signal heading in the presence of rotations, but it is unclear if the rotational component is also represented. During real pursuit, the rotation arises from a movement of the eye relative to the head. In this case, both non-visual and visual sources of information about the rotation are available. These two sources of information differ in that the non-visual source signals the rotation of the eye relative to the head (R<sub>EH</sub>) and the visual source signals the rotation of the eye relative to the world (R<sub>EW</sub>). Previous studies have shown that VIP receives efference copies of pursuit eye movements (<xref ref-type="bibr" rid="bib10">Colby et al., 1993</xref>; <xref ref-type="bibr" rid="bib15">Duhamel et al., 1997</xref>), reflecting an R<sub>EH</sub> signal. However, no previous studies have tested if VIP also carries an R<sub>EW</sub> signal based on visual rotation information present in optic flow.</p><p>To test whether neurons in VIP signal rotations based on both non-visual and visual cues, we analyzed data from interleaved rotation-only trials (leftward and rightward rotations) in which the monkey either pursued a target in darkness (non-visual R<sub>EH</sub> signal) or fixated centrally while the visual stimulus simulated a rotation (visual R<sub>EW</sub> signal) with the same velocity profile as pursuit in darkness. We found that about half of the rotation responses were significantly different from baseline activity during both real and simulated rotations (144 responses from 72 cells; 73/144, 50.7% during pursuit in darkness and 78/144, 54.2% during simulated rotation). Since we only tested horizontal (yaw axis) rotations at a single constant velocity, it is likely that more VIP neurons are responsive to rotation, but prefer different rotation velocities or axes of rotation.</p><p>In our experiments, the R<sub>EW</sub> signal is equivalent to the R<sub>EH</sub> signal since only eye rotations are considered. Therefore, similarity between the efference copy signal (R<sub>EH</sub>) and the neural responses to purely visual rotation stimuli (R<sub>EW</sub>) would suggest the presence of an integrated (visual and non-visual) R<sub>EW</sub> signal in VIP. We find that the baseline-subtracted responses to these two types of rotation stimuli are significantly correlated (rightward rotation: Spearman r = 0.50, p &lt; 0.001; leftward rotation: Spearman r = 0.39; p = 0.001), supporting the presence of a rotation signal derived from purely visual cues (R<sub>EW</sub>) in area VIP (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Furthermore, the difference in response between rightward and leftward rotations (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) shows that many VIP neurons exhibit direction-selective responses to rotation. We also find significant correlation between the differential responses (left—right rotation) during real and simulated rotation (Spearman r = 0.59; p &lt; 0.001). These results support the hypothesis of multi-sensory convergence of visual and non-visual cues to provide consistent rotation information, which may be critical for encoding rotations, in addition to achieving a rotation-invariant representation of heading.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.04693.013</object-id><label>Figure 5.</label><caption><title>Neural responses to pure rotation stimuli.</title><p>(<bold>A</bold>) Scatterplot and marginal distributions of baseline-subtracted rotation responses. The monkey either pursued a target across a dark screen (pursuit in darkness) or fixated centrally as rotation was simulated in the 3D dot cloud (simulated rotation). Filled marginal distributions indicate significant rotation responses compared to baseline (t-test, p ≤ 0.05). Red and blue symbols denote rightward and leftward rotations, respectively. (<bold>B</bold>) Scatterplot of differences between leftward and rightward rotation responses. Filled marginal distributions indicate significant differences between leftward and rightward rotation responses (t-test, p ≤ 0.05).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.013">http://dx.doi.org/10.7554/eLife.04693.013</ext-link></p></caption><graphic xlink:href="elife-04693-fig5-v1.tif"/></fig></p><p>It is important to note that, in general, retinal motion corresponding to R<sub>EW</sub> is a combination of R<sub>EH</sub>, rotation of the head-on-body (R<sub>HB</sub>), and body-in-world (R<sub>BW</sub>). And each of these different rotations will be accompanied by different efference copy (non-visual) signals. If VIP neurons represent R<sub>EW</sub> based on non-visual signals, then they would have to represent a combination of all efference copy signals: R<sub>EW</sub> = R<sub>EH</sub> + R<sub>HB</sub> + R<sub>BW</sub>. Although we cannot test this directly with our data, the correlations observed in <xref ref-type="fig" rid="fig5">Figure 5</xref> allow for the possibility that VIP neurons represent R<sub>EW</sub> based on both visual and non-visual cues.</p></sec><sec id="s2-4"><title>Role of perspective distortions in achieving rotation-invariance</title><p>Results from the 3D cloud experiment (<xref ref-type="fig" rid="fig4">Figure 4</xref>) demonstrate, for the first time at the neural level, a clear contribution of visual cues in achieving a rotation-tolerant representation of heading. To gain a deeper understanding of the visual mechanisms involved in dissociating translations and rotations, we investigated which optic flow properties are used by the visual system to infer self-motion from visual cues. <xref ref-type="bibr" rid="bib25">Helmholtz and Southall (1924)</xref> and <xref ref-type="bibr" rid="bib21">Gibson (1950)</xref> suggested that local motion parallax information plays an important role in deciphering self-motion based on the depth structure of a scene. In a 3D environment, two points can have similar retinal locations, but different depths. As illustrated in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, optic flow vectors resulting from observer translation are dependent on depth, producing different retinal velocities for points at different depths. This difference in velocity between nearby points at different depths gives rise to local motion parallax. Rotations, on the other hand, produce image motion that is not depth-dependent, and therefore lacking local motion parallax. As a result, for a rich 3D environment, computing the local difference between optic flow vectors corresponding to points at different depths allows the rotational component of optic flow to be subtracted away (<xref ref-type="bibr" rid="bib37">Longuet-Higgins and Prazdny, 1980</xref>; <xref ref-type="bibr" rid="bib41">Rieger and Lawton, 1985</xref>; <xref ref-type="bibr" rid="bib55">Warren and Hannon, 1990</xref>), and the singularity point of the resulting motion parallax field (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) corresponds to the observer's heading. This solution requires rich depth structure in the scene, which is not always present. For instance, walking through a dense forest provides robust local motion parallax cues, but walking towards a wall or through an open field, does not.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.04693.014</object-id><label>Figure 6.</label><caption><title>Role of dynamic perspective cues in signaling rotation-invariant heading.</title><p>(<bold>A</bold>) Optic flow fields during combined translation and rotation at two different depth planes have different FOE shifts. The dotted circle indicates true heading. Subtracting these flow fields yields a motion parallax field that eliminates the rotational component and the point of zero local motion parallax corresponds to the true heading. (<bold>B</bold>) Rotational optic flow can be decomposed into laminar flow and dynamic perspective cues. Dynamic perspective cues may signal eye rotations even in the absence of depth structure. (<bold>C</bold>) Scatterplot and marginal distributions of shifts measured using the fronto-parallel plane stimulus during real and simulated pursuit (n = 34 cells). Format as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Open and filled symbols denote data collected during binocular and monocular viewing, respectively. Errorbars denote bootstrapped 95% CIs. All filled histograms indicate shifts significantly &lt;37°. Dark colored histogram bins indicate cells with shifts not significantly different from 0°. Uncolored bars indicate shifts ≥37°.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.014">http://dx.doi.org/10.7554/eLife.04693.014</ext-link></p></caption><graphic xlink:href="elife-04693-fig6-v1.tif"/></fig></p><p>In addition to local motion parallax cues resulting from observer translation, optic flow also contains global components of motion that convey information about observer rotation. When a pure eye rotation is simulated using optic flow stimuli, the image contains distortions resulting from the changing orientation of the eye relative to the scene, that we term ‘dynamic perspective cues’ (see <xref ref-type="bibr" rid="bib27">Kim et al., 2014</xref> for more details). A correct simulation of rotational optic flow can thus be characterized as a combination of laminar flow and dynamic perspective cues (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Importantly, these cues are independent of the depth structure of the scene and are present in scenes having rich 3D structure as well as scenes consisting of a single plane. Theoretical studies have proposed that such cues may play an important role in estimating and discounting the rotational component of optic flow to estimate heading (<xref ref-type="bibr" rid="bib29">Koenderink and van Doorn, 1976</xref>, <xref ref-type="bibr" rid="bib30">1981</xref>; <xref ref-type="bibr" rid="bib22">Grigo and Lappe, 1999</xref>). A recent electrophysiological study in MT provides evidence that the visual system may be capable of using these dynamic perspective cues to disambiguate the sign of depth specified by motion parallax (<xref ref-type="bibr" rid="bib27">Kim et al., 2014</xref>).</p><p>To examine the role of dynamic perspective cues, we conducted a second set of experiments using a fronto-parallel (FP) plane of dots with zero disparity. These visual stimuli contain global perspective cues to rotation, as in the 3D cloud stimulus, but lack local motion parallax cues. For 11/34 neurons recorded, the stimulus was viewed binocularly; the remaining cells were recorded while the monkey viewed the stimulus monocularly with the eye contralateral to the recording hemisphere. In contrast to previous studies, which kept the simulated distance to a FP wall constant over the duration of a trial (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>), the simulated distance of the FP plane changed, in our stimuli, from 45 cm at the beginning to 18 cm at the end of the trial. This more accurately simulates the real world situation in which approaching a wall reduces its distance from the observer over time. As a result, the speed of the translation component of optic flow increased over time for forward translations as the distance to the wall decreased (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Since rotational optic flow is invariant to the distance from a wall (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), the resulting shift in FOE due to added rotations changed over time in our stimulus. During the middle 750 ms of a forward translation stimulus, real or simulated pursuit results in an average FOE shift of 37°. Hence, heading tuning shifts significantly smaller than 37° would provide evidence for the hypothesis that the visual system can use dynamic perspective cues to discount rotations.</p><p><xref ref-type="fig" rid="fig6">Figure 6C</xref> summarizes the shifts in heading tuning measured during presentation of the FP plane stimulus. The median shifts across the population for real pursuit (14.3°) and simulated pursuit (21.5°) were both significantly less than the 37° expected if there were no tolerance for rotations (Wilcoxon signed-rank test; p &lt; 0.005). The median values were also significantly different from each other (Wilcoxon signed-rank test; p = 0.03) and greater than 0° (Wilcoxon signed-rank test; p &lt; 0.001). Furthermore, 8/34 (23.5%) neurons during RP and 5/34 (14.7%) neurons during SP had shifts that were not significantly different from 0° (darker colors in <xref ref-type="fig" rid="fig6">Figure 6C</xref>), implying rotation-invariant heading responses. Only 6/34 (17.6%) neurons during RP and 12/34 (35.3%) neurons during SP showed shifts that were statistically greater than or not different from 37° (95% CI; see ‘Materials and methods’). These results indicate that, even in the absence of non-visual signals and 3D visual cues such as local motion parallax, a large sub-population of VIP neurons can use global perspective cues to at least partially mitigate the effect of rotations on heading tuning. Shifts measured during simulated pursuit in the 3D cloud experiments were significantly less than shifts measured using the FP plane (Wilcoxon rank sum test; p = 0.02). This implies that both local motion parallax cues arising from translations, and global features such as dynamic perspective cues arising from rotations play important roles in visually dissociating translations and rotations.</p></sec><sec id="s2-5"><title>Reference frames for representing heading</title><p>Since the eyes physically rotate during real pursuit, but the head does not, previous studies interpreted rotation-invariant heading tuning as evidence that VIP neurons represent self-motion in a head-centered reference frame (<xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>). In contrast, studies that measured heading tuning with the eye and head at different static positions have revealed an eye-centered reference frame for visual heading tuning in VIP (<xref ref-type="bibr" rid="bib8">Chen et al., 2013</xref>, <xref ref-type="bibr" rid="bib9">2014</xref>). On the surface, these results appear to be incompatible with each other. However, we posit that the issues of rotation-invariant heading tuning and reference frames are not necessarily linked. Indeed, we show below that VIP neurons can discount rotations without signaling translation direction in a head-centered reference frame.</p><p>The key to reconciling these issues is appreciating that, during eye pursuit, the eye-centered reference frame rotates relative to a subject's heading (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). As the eye rotates, the direction of translation remains constant in head-centered coordinates (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, dashed green lines). However, in the rotating eye-centered reference frame, the translation direction relative to the eye changes over time, such that the focus of expansion moves across the retina (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). This is true for both the RP and SP conditions. During RP, the eye physically moves and the FOE remains constant on the screen, whereas during SP, the eye remains stationary as the FOE drifts across the screen. Hence, the temporal change in the translation direction with respect to the retina is the same during both real and simulated pursuit.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.04693.015</object-id><label>Figure 7.</label><caption><title>Distinguishing reference frames from rotation invariance.</title><p>(<bold>A</bold>) Schematic of a rightward eye rotation while translating forward. As the eye position changes during smooth pursuit, the eye reference frame (ERF, black axes) rotates relative to the head (R<sub>EH</sub>) and the direction of translation in the world, T<sub>EW</sub>. Since the head is not rotating relative to the world, the head reference frame (HRF, green axes) remains constant with respect to the heading. (<bold>B</bold>) In retinal co-ordinates, the translation component of optic flow changes with eye position and results in a drifting FOE (x) across the retina. The translation direction represented by the FOE changes from right of straight ahead to left of straight ahead for rightward rotations. (<bold>C</bold>, <bold>D</bold>) Heading corresponding to the largest firing rate gradient was identified for each neuronal tuning curve and the temporal responses at that heading were evaluated. Dashed straight lines show the predicted population response slopes based on the assumption of an eye-centered reference frame. The population average of the normalized time course of firing rate is plotted for each condition type—translation only (grey), rightward rotation (red) and leftward rotation (blue) for real pursuit (<bold>C</bold>) and simulated pursuit (<bold>D</bold>). Shaded regions indicate standard errors. The significant positive and negative trends observed are consistent with a reference frame that is intermediate between eye- and head-centered, but closer to an eye-centered reference frame.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.015">http://dx.doi.org/10.7554/eLife.04693.015</ext-link></p></caption><graphic xlink:href="elife-04693-fig7-v1.tif"/></fig></p><p>In our experimental protocol, as well as that of previous studies (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>; <xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>), the average eye position during the translation-only, real pursuit and simulated pursuit conditions is the same (centered on the screen) over the duration of a trial. Therefore, the average eye position is the same as the average head position. As a result, time-averaged neural responses may provide insight into what signal is represented (heading or resultant optic flow), but not about whether these signals are represented in an eye- or head-centered reference frame. To evaluate reference frames, responses must be examined with the eye at different positions relative to the simulated heading. In our experiments, we can examine the temporal responses of neurons to study reference frames since the translation direction in eye coordinates changes over time. Accordingly, an eye-centered representation of heading would result in systematic temporal response variations due to the rotating reference frame, and these variations would be different for leftward and rightward rotations of the eye. In contrast, a head-centered representation would result in responses that are constant over time, and similar for rightward and leftward rotations during both real and simulated pursuit.</p><p>For a neuron representing heading in an eye-centered reference frame, a rightward eye rotation would result in an upward trend in firing rate over time for headings along the positive slope of the tuning curve. In contrast, a leftward eye rotation would result in a downward trend (<xref ref-type="fig" rid="fig7">Figure 7C,D</xref>, dashed lines). It is important to note that these trends are determined by the changing eye position and are independent of how tolerant the heading representation is to rotations (i.e., the extent of compensation). The degree of rotation compensation would result in a shift in the mean firing rate away from the pure translation responses (as discussed in previous sections), irrespective of the reference frame in which translations are represented. Therefore, neurons can represent translations invariant to rotations in either a head-centered or an eye-centered reference frame.</p><p>In order to evaluate the underlying reference frame for representing translations in area VIP, we examined the temporal changes in firing rate for each neuron over the same 750 ms epoch used in the rest of the analyses. If neurons signal heading in an eye-centered reference frame, the largest temporal variations in firing rate will occur at headings along the steepest portion of the tuning curve. Therefore, we identified the heading corresponding to the largest positive gradient for each tuning curve, and examined the temporal dynamics of responses for that direction. In order to determine the expected temporal changes in firing rate under the assumption of an eye-centered reference frame, the slope of the tuning curve at the heading corresponding to the largest gradient was calculated for each normalized tuning curve. The average predicted slopes for the population based on our data were ±0.41/s during real pursuit and ±0.4/s for simulated pursuit (dashed lines in <xref ref-type="fig" rid="fig7">Figure 7C, D</xref>).</p><p>These predictions, based on an eye-centered reference frame hypothesis, were compared to the average time course of normalized responses of the population of VIP neurons (see ‘Materials and methods’ for details). VIP population responses show trends in the directions predicted by an eye-centered reference frame, but are inconsistent with the expectation for a head-centered reference frame (red, blue curves in <xref ref-type="fig" rid="fig7">Figure 7C,D</xref>). The slopes observed in VIP responses correspond to an intermediate reference frame that lies closer to an eye-centered frame than a head-centered reference frame. Specifically, for real pursuit, average responses increased for rightward eye rotation (slope = 0.29/s, 95% CI = [0.21 0.37], linear regression) and decreased for leftward rotation (slope = −0.24/s, 95% CI = [−0.16–0.32]). These slopes are significantly different from 0 and ∼65% as steep as the predictions of the eye-centered reference frame, thus indicating an intermediate reference frame. Since the temporal response profile was essentially flat during the translation only condition (slope = 0.01/s, 95% CI = [−0.04 0.06]) and opposite trends are observed for rightward vs leftward rotations, these response patterns cannot be explained by other basic aspects of neural response dynamics, such as adaptation.</p><p>Interestingly, similar trends are also observed during simulated pursuit (rightward: slope = 0.28/s, 95% CI = [0.21 0.35]; leftward: slope = −0.26/s, 95% CI = [−0.17–0.35], linear regression), for which the eye does not physically rotate. These slopes are again about two-thirds as steep as expected based on the eye-centered reference frame hypothesis. Whereas previous studies have demonstrated a role of non-visual signals in estimating the position of the eye or head relative to the body (<xref ref-type="bibr" rid="bib48">Squatrito and Maioli, 1997</xref>; <xref ref-type="bibr" rid="bib32">Lewis et al., 1998</xref>; <xref ref-type="bibr" rid="bib28">Klier et al., 2005</xref>), these results suggest that visual signals in VIP carry information about changes in eye position even in the absence of efference copy signals. In other words, the temporal dynamics of an eye rotation may be inferred from the rotational components of optic flow and used to modulate neural responses during simulated pursuit. This further strengthens the functional role of visual signals in VIP for estimating rotational information and contributing to a rotation-invariant heading representation.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We evaluated how heading is represented in macaque area VIP in the presence of rotations, finding that a sub-population of VIP neurons represent heading in a rotation-invariant fashion while a majority of the population is at least partially tolerant to rotations. Importantly, rotation invariance can be achieved using both non-visual and purely visual cues. Previous neurophysiology literature emphasized the importance of non-visual cues, especially efference copy signals, but clear evidence for the role of visual cues has been missing, as discussed below. In contrast, our study provides novel evidence for the role of visual cues in discounting rotations and representing heading. Furthermore, we show that both local motion parallax and global dynamic perspective visual cues present in optic flow play a significant role in decomposing the components of self-motion. The importance of visual signals is reinforced by our finding that VIP neurons also carry rotation signals derived from purely visual cues. The significant correlation between visual and non-visual rotation responses is consistent with a multi-sensory representation of rotations. In addition, we resolve an important ambiguity in the literature between the concepts of tolerance to rotations and reference frames. Specifically, we examine the effect of a rotating eye reference frame on visual responses to show that rotation tolerance does not necessarily imply a head-centered reference frame. Our findings show conclusively that visual cues play a significant role in achieving rotation-invariant heading representations.</p><sec id="s3-1"><title>Importance of visual cues</title><p>It is important to recognize that the significance of visual cues in discounting rotation extends beyond eye pursuit to head-on-body (R<sub>HB</sub>) and body-in-world (R<sub>BW</sub>) rotations as well. The efference copy signals for each of these sources of rotation depend on the specific motor commands generating the movement. If we consider that eye, head, and body rotations are often generated simultaneously, multiple efference copy signals must be added together and subsequently discounted from the resultant optic flow to signal heading accurately. Each of these non-visual signals is associated with signal-dependent noise (<xref ref-type="bibr" rid="bib20">Gellman and Fletcher, 1992</xref>; <xref ref-type="bibr" rid="bib36">Li and Matin, 1992</xref>; <xref ref-type="bibr" rid="bib12">Crowell et al., 1998</xref>); thus, combining multiple, potentially independent, efference copy signals to estimate rotations may not always be an efficient solution for the brain. On the other hand, the information contained in visual cues is independent of the source of rotation and represents rotation of the eye relative to the world (R<sub>EW</sub>). The R<sub>EW</sub> information present in optic flow inherently reflects the sum of all the different sources of rotation (R<sub>EW</sub> = R<sub>EH</sub> + R<sub>HB</sub> + R<sub>BW</sub>) and thus provides direct information regarding the total rotation of the eyes during self-motion. Therefore, visual signals may have important advantages when the goal is to accurately estimate heading in the presence of self-generated rotations.</p><p>However, we also face situations in which visual information may be sparse, such as driving at night on an open road (limited visual range and depth structure), where non-visual signals may be crucial. As expected, given the brain's propensity towards multi-sensory integration, we find that both visual and non-visual signals contribute to discounting rotations to represent heading. Real pursuit shifts are smaller than simulated pursuit shifts, and both types of shifts are smaller for a dense 3D cloud than a fronto-parallel plane.</p><p>Given the variety of efference copy signals present in parietal cortex (<xref ref-type="bibr" rid="bib1">Andersen, 1997</xref>) and the correlation observed between the R<sub>EH</sub> (pursuit in darkness) and R<sub>EW</sub> (pure simulated rotation) responses in our data (<xref ref-type="fig" rid="fig5">Figure 5</xref>), we postulate that VIP contains an integrated representation of rotation that relies on both visual signals and efference copy inputs. However, to conclusively test these theories, experiments with multiple rotation velocities and directions as well as different sources of rotation (e.g., eye vs head pursuit) need to be conducted. How these visual rotation cues are combined with efference copy signals and other non-visual sensory cues to rotation (e.g., vestibular inputs) warrants further investigation.</p></sec><sec id="s3-2"><title>Comparison to previous behavioral studies</title><p>Several human psychophysical studies have assessed pursuit compensation during heading estimation based on visual and non-visual cues. However, owing to variations in experimental protocols, visual stimuli, and instructions given to the subjects, the results of these studies vary substantially. If we consider studies that used 3D cloud stimuli, we find that some studies report large errors in heading perception (the difference between reported heading and true heading) in the absence of efference copy signals (<xref ref-type="bibr" rid="bib44">Royden et al., 1992</xref>; <xref ref-type="bibr" rid="bib43">Royden, 1994</xref>; <xref ref-type="bibr" rid="bib2">Banks et al., 1996</xref>), whereas other studies report that subjects are able to accurately perceive their heading based on purely visual stimuli (<xref ref-type="bibr" rid="bib54">Warren and Hannon, 1988</xref>, <xref ref-type="bibr" rid="bib55">1990</xref>; <xref ref-type="bibr" rid="bib52">van den Berg and Brenner, 1994</xref>). In order to compare results across these studies, we calculated the degree of compensation as the difference between the error in heading perception and the shift in FOE based on the experimental parameters, normalized by the expected shift in FOE ([FOE shift-heading error]/FOE shift). The rotation compensation observed in these studies during simulated pursuit (only visual cues) ranged from 100% to 20% for a 3D cloud stimulus (based on the depth plane corresponding to the screen distance). Studies with smaller compensatory effects (<xref ref-type="bibr" rid="bib44">Royden et al., 1992</xref>; <xref ref-type="bibr" rid="bib43">Royden, 1994</xref>; <xref ref-type="bibr" rid="bib2">Banks et al., 1996</xref>) concluded that optic flow was insufficient for estimating translations in the presence of rotations. However, these studies used visual stimuli with a small field of view and a low density of dots in the 3D cloud, thus limiting the visual information available for estimating heading in the presence of rotations (<xref ref-type="bibr" rid="bib22">Grigo and Lappe, 1999</xref>). Despite these limitations in the visual stimuli, the compensatory effects were greater than 0. Moreover, other studies have shown that richer visual stimuli, including landmarks (<xref ref-type="bibr" rid="bib33">Li and Warren, 2000</xref>, <xref ref-type="bibr" rid="bib35">2004</xref>; <xref ref-type="bibr" rid="bib45">Royden et al., 2006</xref>) and larger fields of view (<xref ref-type="bibr" rid="bib52">van den Berg and Brenner, 1994</xref>; <xref ref-type="bibr" rid="bib22">Grigo and Lappe, 1999</xref>), resulted in larger compensatory effects based on purely visual cues.</p><p>In this study, using a 3D cloud stimulus, we observed a large and continuous range of compensatory effects, including a substantial subset of VIP neurons that compensated completely for rotations, as well as neurons that do not compensate at all or even over-compensate for rotations. Since the experimental parameters used in our study and the various behavioral papers are different, it is difficult to compare our results quantitatively with the published behavioral findings. However, the fact that we find moderate, but significant, compensation during simulated pursuit is broadly consistent with the psychophysical literature.</p><p>Furthermore, depending on how the population of VIP neurons is decoded, a substantial range of behavioral effects might be expected. For instance, if the rotation-invariant neurons are selectively decoded to estimate heading, it should be possible for VIP to support behavioral responses with compensation close to 100%. On the other hand, if all VIP neurons are decoded with equal weights, we would expect the behavioral errors to be comparable to the mean compensation observed in the neural population. It is also important to note that in many behavioral studies, subjects made small but significant errors even during real pursuit (<xref ref-type="bibr" rid="bib18">Freeman, 1999</xref>; <xref ref-type="bibr" rid="bib19">Freeman et al., 2000</xref>; <xref ref-type="bibr" rid="bib11">Crowell and Andersen, 2001</xref>), consistent with our finding that the average compensation among VIP neurons is not complete even when both visual and non-visual cues to rotation are available.</p><p>Some psychophysical studies attribute the errors observed during simulated pursuit to the misinterpretation of path-independent rotations (such as eye pursuit during straight translations) as motion along a curved path (<xref ref-type="bibr" rid="bib43">Royden, 1994</xref>; <xref ref-type="bibr" rid="bib45">Royden et al., 2006</xref>). In behavioral studies that eliminate this ambiguity through specific instruction to subjects, heading errors during simulated pursuit are reported to be largely reduced (<xref ref-type="bibr" rid="bib35">Li and Warren, 2004</xref>; <xref ref-type="bibr" rid="bib45">Royden et al., 2006</xref>). This provides further evidence that the visual system is indeed capable of estimating rotation-invariant heading based on purely visual stimuli. It is also possible that the range of compensation observed in our data could be a result of this perceptual ambiguity. To evaluate how the brain resolves this ambiguity, neurophysiological studies using both path-independent rotations and curved path stimuli are needed.</p></sec><sec id="s3-3"><title>Comparison with previous electrophysiological studies</title><p>Previous physiological studies emphasized the contribution of efference copy signals to achieving rotation invariance (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib39">Page and Duffy, 1999</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>; <xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>). However, these studies could not conclusively establish a contribution of visual rotation cues to heading tuning for various reasons. Some studies did not use a simulated pursuit condition and therefore could not disambiguate visual and non-visual contributions to the rotation-invariance of heading tuning they observed (<xref ref-type="bibr" rid="bib39">Page and Duffy, 1999</xref>; <xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>). On the other hand, <xref ref-type="bibr" rid="bib3">Bradley et al. (1996)</xref> and <xref ref-type="bibr" rid="bib46">Shenoy et al. (1999)</xref>; (<xref ref-type="bibr" rid="bib47">2002</xref>) included a simulated pursuit condition in their experiments, but the visual stimulus used to simulate pursuit was incorrect. To mimic pursuit, they simply added laminar flow to their fronto-parallel plane (i.e. no local motion parallax cues) optic flow stimuli, and thus their stimuli lacked the dynamic perspective cues necessary to accurately simulate eye rotations on a flat display. When rendering visual stimuli, dynamic perspective cues should be incorporated any time the eye changes orientation relative to the scene (<xref ref-type="bibr" rid="bib27">Kim et al., 2014</xref>).</p><p>If eye rotation is simulated (incorrectly) as laminar flow on a flat screen, then it should not be possible for neurons to exhibit rotation-tolerant heading tuning because the addition of laminar motion simply shifts the focus of expansion in the flow field, and does not provide any rotation cues. Indeed, <xref ref-type="bibr" rid="bib3">Bradley et al. (1996)</xref> found that MSTd neurons did not compensate for rotations when pursuit was simulated in this manner. In contrast, <xref ref-type="bibr" rid="bib46">Shenoy et al. (1999)</xref>; (<xref ref-type="bibr" rid="bib47">2002</xref>) reported that MSTd neurons show considerable tolerance to rotation when pursuit was simulated as laminar flow, despite the fact that little or no rotation tolerance was reported psychophysically by the same laboratory for simulated pursuit (<xref ref-type="bibr" rid="bib12">Crowell et al., 1998</xref>). Compared to <xref ref-type="bibr" rid="bib3">Bradley et al. (1996)</xref>, <xref ref-type="bibr" rid="bib47">Shenoy et al. (2002)</xref> used a smaller display size and yet observed larger compensatory effects. This finding contradicts theoretical and psychophysical studies that have established that a larger display size should improve pursuit compensation based on visual cues (<xref ref-type="bibr" rid="bib31">Koenderink and van Doorn, 1987</xref>; <xref ref-type="bibr" rid="bib22">Grigo and Lappe, 1999</xref>). We believe that the counter-intuitive results obtained by <xref ref-type="bibr" rid="bib46">Shenoy et al. (1999)</xref>; (<xref ref-type="bibr" rid="bib47">2002</xref>) stem from the fact that the boundary of their visual stimuli moved across the retina during real and simulated pursuit (but not during the fixation condition), and thus stimulated different regions of the visual field in and around the receptive field of a neuron over time. Such a moving image boundary defined only by the rotation velocity would not occur under natural conditions as a result of eye rotations. By changing the region of visual space that was stimulated over the course of a trial, <xref ref-type="bibr" rid="bib46">Shenoy et al. (1999)</xref>; (<xref ref-type="bibr" rid="bib47">2002</xref>) likely induced changes in the amplitude (response gain) or shape of heading tuning curves.</p><p><xref ref-type="bibr" rid="bib46">Shenoy et al. (1999)</xref>; (<xref ref-type="bibr" rid="bib47">2002</xref>) measured heading tuning over a narrow range (±32°) around straight ahead, and estimated shifts in tuning by cross-correlation analysis. While cross-correlation is invariant to gain changes, it only provides an accurate measure of tuning shifts if the tuning curve has a clear peak within the range of headings tested (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2C,D</xref>; see ‘Materials and methods’). In contrast, cells that prefer lateral headings generally have monotonic tuning curves around straight ahead (e.g., <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2E</xref>), and this generally yields rather flat cross-correlation functions with no clear peak (e.g., <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2F</xref>). As a result, cross-correlation analysis produces fairly accurate estimates of shifts for cells with heading preferences within the range of headings tested, but does not provide reliable shifts for neurons with monotonic tuning functions in that range (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2G</xref>).</p><p>These simulations suggest that the degree of rotation compensation reported previously (<xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>) may have been inaccurate for neurons with monotonic tuning around straight-forward, which are common in areas MSTd (<xref ref-type="bibr" rid="bib24">Gu et al., 2006</xref>, <xref ref-type="bibr" rid="bib23">2010</xref>) and VIP (<xref ref-type="bibr" rid="bib7">Chen et al., 2011</xref>). This may also help explain the partial rotation compensation observed by <xref ref-type="bibr" rid="bib46">Shenoy et al. (1999)</xref>; (<xref ref-type="bibr" rid="bib47">2002</xref>) in their (incorrect) simulated rotation condition, which contained no relevant visual cues that could be used to compensate for rotation. In contrast to the cross-correlation method, our method for measuring shifts works well for cells with all heading preferences (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E</xref>), and is robust to variations in the gain, offset and shape of tuning curves.</p><p>More recently, <xref ref-type="bibr" rid="bib4">Bremmer et al. (2010)</xref> and <xref ref-type="bibr" rid="bib26">Kaminiarz et al. (2014)</xref> reported that neurons in areas MSTd and VIP, respectively, show rotation-invariant heading tuning based solely on visual cues. However, these studies only measured neural responses to three headings (forward, 30° leftward, and 30° rightward), and defined rotation-tolerance based on a rank-ordering of heading responses across the different eye movement conditions. Since absolute firing rates were not considered, it is likely that shifts in tuning curves could go undetected by this method in the presence of gain fields or bandwidth changes. For instance, this analysis would report identical rank-order for the tuning curves shown in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A</xref>, and would erroneously classify them as rotation-invariant. In addition, the authors did not attempt to compare their results to the tuning shifts that would be expected if neurons do not compensate for rotation. Consider that, in their ground-plane stimuli (e.g., <xref ref-type="fig" rid="fig1">Figure 1</xref> of <xref ref-type="bibr" rid="bib26">Kaminiarz et al., 2014</xref>), rotation has a large effect on slow-speed optic flow vectors near the horizon, and high-speed foreground vectors are much less altered. For neurons with receptive fields below the horizontal meridian or those with responses dominated by high speeds, one might not expect the rank ordering of heading responses to change even if neurons do not compensate for rotation. Thus, the results of these studies are difficult to interpret.</p><p>By comparison with the above studies, we accurately simulated eye rotations such that correct 2D and 3D visual cues are present in the stimuli. We also measured full heading tuning curves and our analysis methods allowed us to disambiguate changes in response gain from shifts or shape changes in the tuning curve. By using a large display and maintaining the same area of retinal stimulation for all viewing conditions (see ‘Materials and methods’), we eliminated artifacts that likely confounded the results of some previous studies (<xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>). Therefore, we are confident that our findings in the simulated rotation condition reflect a true contribution of visual cues to the problem of dissociating translations and rotations.</p></sec><sec id="s3-4"><title>Implications for self-motion and navigation</title><p>In order to navigate through the environment and interact successfully with objects, it is imperative that we distinguish visual motion caused by self-generated movements from that caused by external events in the world (<xref ref-type="bibr" rid="bib40">Probst et al., 1984</xref>; <xref ref-type="bibr" rid="bib53">Wallach, 1987</xref>; <xref ref-type="bibr" rid="bib57">Warren and Saunders, 1995</xref>). For instance, the visual consequences of eye or head rotations need to be discounted in order to accurately perceive whether an object is stationary or moving in the world. The neuroscience literature has extensively studied and emphasized the contribution of efference copy signals to discounting self-generated movements in several sensory systems (<xref ref-type="bibr" rid="bib1">Andersen, 1997</xref>; <xref ref-type="bibr" rid="bib13">Cullen, 2004</xref>; <xref ref-type="bibr" rid="bib28">Klier et al., 2005</xref>). We have presented novel evidence for an alternative solution that is available to the visual system—using large-field visual motion cues to discount self-generated rotations. The ability of VIP neurons to represent heading during rotations, even in the absence of efference copy signals, suggests that visual mechanisms may make substantial contributions to a variety of neural computations that involve estimating and accounting for self-generated rotations.</p><p>The contribution of visual cues may be especially important in situations where efference copy signals are either unreliable or absent. For instance, driving along a winding path and looking in the direction of instantaneous heading does not result in any eye or head movements relative to the body (i.e., no efference copy signals). However, such curvilinear motion still introduces rotational components in the optic flow field and disrupts the FOE. In order to estimate such motion trajectories, the visual system would need to decompose self-motion into both translational and rotational components. This study suggests that such trajectory computations based purely on optic flow may be feasible. How the visual system may implement such computations warrants further research and may provide useful insights to neuroscientists as well as those in the fields of computer vision and robotic navigation.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects and surgery</title><p>Two adult rhesus monkeys (<italic>Macaca mulatta</italic>), weighing 8–10 kg, were chronically implanted with a circular molded, lightweight plastic ring for head restraint and a scleral coil for monitoring eye movements (see <xref ref-type="bibr" rid="bib24">Gu et al., 2006</xref>; <xref ref-type="bibr" rid="bib17">Fetsch et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Takahashi et al., 2007</xref> for more detail). Following recovery from surgery, the monkeys were trained to sit head restrained in a primate chair. They were subsequently trained using standard operant conditioning to fixate and pursue a small visual target for liquid rewards, as described below. All surgical materials and methods were approved by the Institutional Animal Care and Use Committees at Washington University and Baylor College of Medicine, and were in accordance with NIH guidelines.</p><p>The primate chair was affixed inside a field coil frame (CNC Engineering, Seattle, WA, USA) with a flat display screen in front. The sides and top of the coil frame were covered with a black enclosure that restricted the animals' view to the display screen. A three-chip DLP projector (Christie Digital Mirage 2000, Kitchener, Ontario, Canada) was used to rear-project images onto the 60 × 60 cm display screen located ∼30 cm in front of the monkey (thus subtending 90° × 90° of visual angle). Visual stimuli were generated by an OpenGL accelerator board (nVidia Quadro FX 3000G). The display had a pixel resolution of 1280 × 1024, 32-bit color depth, and was updated at the same rate as the movement trajectory (60 Hz). Behavioral control and data acquisition were accomplished by custom scripts (see <xref ref-type="supplementary-material" rid="SD1-data">Source code 1</xref>) written for use with the TEMPO system (Reflective Computing, St. Louis, MO, USA).</p></sec><sec id="s4-2"><title>Stimulus and Task</title><p>Visual stimuli were presented for a duration of 1500 ms during each trial and consisted of various combinations of eight heading directions in the horizontal plane (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) and two rotational directions (leftward and rightward). Translation and rotation velocities followed a trapezoidal profile in which the velocity was constant (translation: 24 cm/s, rotation: 17°/s) during the middle 750 ms (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) of the stimulus period.</p><p>The optic flow stimuli were generated using a 3D rendering engine (OpenGL) to accurately simulate combinations of observer translation and rotation. In the 3D cloud protocol, the virtual environment consisted of a cloud of dots that was 150 cm wide, 100 cm tall, 160 cm deep and had a density of 0.002 dots/cm<sup>3</sup>. The part of the cloud visible to the monkey was clipped in depth to range from 25 cm to 125 cm (relative to the observer) at all times. This clipping ensured that the same volume of dots was visible to the monkey over the duration of a trial as we simulated a translation of 27 cm through the cloud. The stimulus was rendered as a red-green anaglyph that the monkey viewed stereoscopically through red/green filters.</p><p>In the second experimental protocol, a fronto-parallel plane (FP) of dots was rendered with a density of 0.2 dots/cm<sup>2</sup>. The plane was rendered with zero binocular disparity and was viewed by the monkey either binocularly or monocularly, without any red/green filters. During the course of a trial (1500 ms), the 27 cm translation resulted in the simulated distance of the wall changing from 45 cm at the beginning, to 18 cm at the end. We simulated this change in wall distance to better replicate the real world situation of approaching a fronto-parallel wall. Apart from replacing the 3D cloud with a FP plane and the removal of binocular disparity in the stimuli, all other experimental parameters (such as velocity profiles, trial types, stimulus duration, etc) were the same as in the 3D cloud experiment.</p><p>During each session, the monkey's eye position was monitored online using the implanted scleral search coil. Only trials in which the monkey's eye remained within a pre-determined eye window (see below) were rewarded with a drop of juice. Trials were aborted if the eye position constraints set by the eye window were violated.</p><p>The experiment consisted of three main trial types: pure translation, translation + real eye pursuit (RP), and translation + simulated pursuit (SP). (i) For the pure translation condition, the monkey fixated a visual target at the center of the screen and maintained fixation within a 2° eye window while the optic flow stimuli were presented. Optic flow stimuli simulated eight headings within the horizontal plane, corresponding to all azimuth angles in 45° steps. The pure translation stimuli were rendered by translating the OpenGL camera along one of the eight headings with the velocity profile shown in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. (ii) For the real pursuit (RP) condition, the animal actively pursued a moving target while the same translational optic flow stimuli as above were presented on the display screen. A rightward rotation trial started when the fixation target appeared 9.5° to the left of center. Once the monkey fixated this target (within 1000 ms), it moved to the right following a trapezoidal velocity profile (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Analogously, leftward pursuit trials began with the target appearing on the right and moving leftward. The monkey was required to pursue the moving visual target and maintain gaze within a 4° eye window during the acceleration and deceleration periods (0:375 ms and 1125:1500 ms). During the middle 750 ms of the trial (constant velocity phase), the monkey was required to maintain gaze within a 2° window around the visual target. Importantly, the optic flow stimulus was windowed with a software rendered aperture that moved simultaneously with the pursuit target. Thus, the area of the retina being stimulated during the RP trials remained constant over time, eliminating potential confounds from moving the stimulus across the receptive field over time (see ‘Discussion’). (iii) For the simulated pursuit (SP) condition, optic flow stimuli accurately simulated combinations of the same eight headings with leftward or rightward rotations, while the monkey fixated at the center of the screen (2° window). These stimuli were rendered by translating and rotating the OpenGL camera with the same trapezoidal velocity profile of the moving target in the RP condition. This ensured that the retinal optic flow patterns in the RP and SP conditions were identical (assuming accurate pursuit in the RP condition). The area of retinal stimulation was also identical in the SP and RP conditions.</p><p>In addition to these main stimulus conditions, the experimental protocol also included three types of pure rotation conditions for both leftward and rightward directions: (i) eye pursuit over a black background (with the projector on), (ii) eye pursuit over a static field of dots, and (iii) pure rotational optic flow in a 3D cloud (simulated rotation-only). We also included a blank screen during visual fixation and a static field of dots during fixation to measure the spontaneous activity and baseline visual response of the neurons, respectively. Therefore, each block of trials (for both 3D cloud and FP protocols) consisted of 48 unique stimulus conditions: eight directions * (1 translation only +2 RP + 2 SP) + 8 controls.</p></sec><sec id="s4-3"><title>Electrophysiological recordings</title><p>To record from single neurons extracellularly, tungsten microelectrodes (FHC; tip diameter, 3 µm; impedance, 1–3 MΩ at 1 kHz) were inserted into the cortex through a transdural guide tube, using a hydraulic microdrive. Neural voltage signals were amplified, filtered (400–5000 Hz), discriminated (Plexon Systems), and displayed on SpikeSort software (Plexon systems). The times of occurrence of action potentials and all behavioral events were digitized and recorded with 1 ms resolution. Eye position was monitored online and recorded using the implanted scleral search coil. Raw neural signals were also digitized at a rate of 25 kHz using the Plexon system for off-line spike sorting.</p><p>VIP was first identified using MRI scans as described in detail in <xref ref-type="bibr" rid="bib7">Chen et al. (2011)</xref>. Electrode penetrations were then directed to the general area of gray matter around the medial tip of the intraparietal sulcus with the goal of characterizing the entire anterior-posterior extent of area VIP—typically defined as the intraparietal area with directionally selective visual responses (<xref ref-type="bibr" rid="bib10">Colby et al., 1993</xref>; <xref ref-type="bibr" rid="bib16">Duhamel et al., 1998</xref>). To determine direction selectivity, we presented a patch of drifting dots for which the size, position, and velocity could be manipulated manually with a computer mouse. We used this mapping procedure to characterize the presence or absence of strong visual drive as well as the direction and speed selectivity of multi-unit and single-unit activity. At each location along the anterior–posterior axis, we first identified the medial tip of the intraparietal sulcus and then moved laterally until there was no longer a directionally selective visual response in the multi-unit activity.</p><p>During each experimental session, we inserted a single microelectrode into the region of cortex identified as VIP. Single unit action potentials were then isolated online using a dual voltage-time window discriminator. Within the region of gray matter identified as VIP, we recorded from any neuron that showed robust visual responses during our search procedure. Once a single unit was isolated, we ran the 3D cloud protocol with all conditions randomly interleaved (72 neurons). Each stimulus was repeated at least four, and usually five, times. At the end of the 3D cloud protocol, if isolation of the neuron remained stable, we ran the fronto-parallel plane (FP) protocol for 4–5 repetitions (34 neurons). For the FP protocol, the red/green stereo glasses were either removed during the binocular viewing sessions (11/34), or replaced with an eye patch during the monocular viewing sessions (23/34), such that the eye ipsilateral to the recording hemisphere was occluded.</p></sec><sec id="s4-4"><title>Analyses</title><p>Analysis of spike data and statistical tests were performed using MATLAB (MathWorks). Tuning curves for the different stimulus conditions (translation only, RP, SP) were generated using the average firing rate of the cell (spikes/s) during the middle 750 ms of each successfully completed trial. This analysis window was chosen such that rotation/translation velocities were constant and the monkey was pursuing or fixating the visual target in the small 2<sup>°</sup> window. To determine the effect of rotations on neural responses, the translation only tuning curve was compared to the RP/SP tuning curves.</p><p>Previous studies (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib39">Page and Duffy, 1999</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>; <xref ref-type="bibr" rid="bib58">Zhang et al., 2004</xref>; <xref ref-type="bibr" rid="bib26">Kaminiarz et al., 2014</xref>) only measured tuning curves over a narrow range of headings around straight ahead. Without measuring the full tuning curve, it is very difficult to distinguish between gain fields and shifts in the tuning curves (<xref ref-type="bibr" rid="bib38">Mullette-Gillman et al., 2009</xref>; <xref ref-type="bibr" rid="bib6">Chang and Snyder, 2010</xref>; <xref ref-type="bibr" rid="bib42">Rosenberg and Angelaki, 2014</xref>). Furthermore, these previous studies assumed that rotations would cause a global shift of the tuning curve in the absence of pursuit compensation. However, as shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, rotations can change the shape of the tuning curve, including both skew and bandwidth changes. Therefore, we suspect that the cross-correlation methods or rank-ordering of responses used in previous studies are insufficient to characterize all of the changes in heading tuning due to rotations (see also <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><p>To account for these more complex changes in heading tuning curves, we developed a novel 3-step analysis procedure, as illustrated for an example cell in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. Step 1: we measured the minimum and maximum responses of the pure translation tuning curve. The lowest response (trough) and amplitude (maximum—minimum) of the RP/SP tuning curves were then matched to those of the pure translation curve by vertically shifting and scaling the responses, respectively. Step 2: because the predicted effects of rotation are opposite for forward and backward headings (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), RP and SP tuning curves were split into heading ranges of 0–180° and 180–360°. We tested whether each half of the tuning curve was significantly tuned using an ANOVA (p ≤ 0.05). All the tuning curves were then linearly interpolated to a resolution of 1°. Step 3: for half-curves that showed significant tuning, we performed a shift analysis as follows. The pure translation tuning curve was circularly shifted (in steps of 1°) to minimize the sum-squared error with each half of the RP/SP tuning curves. For neurons that were significantly tuned in all conditions and in both direction ranges, this analysis yielded four shift values for real pursuit and four shifts for simulated pursuit. In order to quantify the transformation of heading tuning due to rotations, the four shift values were averaged to arrive at one shift value for real pursuit and one shift for simulated pursuit for each cell.</p><p>The 95% confidence intervals (CIs) for the shifts plotted in <xref ref-type="fig" rid="fig4 fig6">Figures 4 and 6C</xref>, were calculated using a bootstrap analysis. Bootstrapped tuning curves for translation only, real pursuit, and simulated pursuit were generated by resampling responses with replacement. The same offset, gain and shift calculations were performed on each one of 300 bootstrapped tuning curves to produce a distribution of shifts for each neuron from which the 95% CI was calculated by the percentile method.</p><p>In order to test the efficacy of our analysis method, we simulated heading tuning curves using von Mises functions (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), with gain (A), preferred direction (φ), and width (k) as free parameters (<xref ref-type="bibr" rid="bib49">Swindale, 1998</xref>).<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>V</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To simulate the tuning curve transformations caused by adding rotational optic flow, a second shape parameter (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) and skew (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) terms were added to the von Mises functions as follows:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where, σ is the second shape parameter (such that slope of the function at half-height can vary independently of the width at half-height) and γ is the skew parameter (see <xref ref-type="bibr" rid="bib49">Swindale, (1998)</xref> for more details). The second shape parameter (σ) was manipulated to yield rotation-added tuning curves with bandwidth changes of 40° (20° on each half of the tuning curve) for cells preferring close to lateral translations ([340°:20°], [160°:200°]). For cells preferring all other headings (close to forward or backward translations), the skew parameter (γ) was manipulated to yield a 20° shift in the peaks of the rotation-added tuning curves.</p><p>Random gain values ranging from 0.66 to 1.33 were used to scale the rotation-added tuning curves and random offset values (0–40 spikes/s) were also added to the tuning curves corresponding to leftward and rightward rotations. Poisson random noise was added to all tuning curves (averaged over five simulated stimulus repetitions) and the curves were sampled at heading intervals of 45°, similar to the recorded data. Shifts were measured between the translation only and rotation-added curves using the partial shift analysis method described above. The mean shifts resulting from 10 sets of simulated tuning curves with heading preferences ranging from 0:360° are shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E</xref>. These simulations demonstrate that our method is capable of accurately measuring shifts in the presence of gain, offset and shape changes for neurons with a variety of heading preferences.</p><p>To compare our method with the cross-correlation method used in previous studies (<xref ref-type="bibr" rid="bib3">Bradley et al., 1996</xref>; <xref ref-type="bibr" rid="bib46">Shenoy et al., 1999</xref>, <xref ref-type="bibr" rid="bib47">2002</xref>), von Mises functions with Poisson noise were generated as described above (<xref ref-type="disp-formula" rid="equ1">Equations 1</xref>–<xref ref-type="disp-formula" rid="equ3">3</xref>), but were sampled and analyzed as described in those papers. Specifically, simulated tuning curves were generated by sampling the von Mises functions at headings in the range of ±32° around straight ahead, with 8° sampling intervals. To match the previous studies, the resulting data were then smoothed with a three-point moving average and interpolated using a spline function at 1° intervals (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2C,E</xref>). The rotation-added tuning curves were horizontally shifted in 1° increments relative to the translation-only curve and the maximum correlation coefficient between the curves was measured using the equation described in <xref ref-type="bibr" rid="bib46">Shenoy et al. (1999)</xref> (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2D,F</xref>). This analysis was repeated for 10 sets of simulated tuning curves (different random noise samples) for 10 different heading preferences in the range from 0:180° (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2G</xref>). Since this analysis was based only on the narrow heading range of ±32° around straight forward, we did not simulate neurons with backwards heading preferences in the range of 180–360° because such neurons would have little response in this heading range. In contrast with our analysis, this cross-correlation method resulted in unreliable tuning shifts for simulated neurons with heading preferences outside the narrow range of measured headings (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2G</xref>).</p><p>To test the rotating reference frame hypothesis (<xref ref-type="fig" rid="fig7">Figure 7</xref>), the gradient of firing rate was calculated at each heading on each measured tuning curve and the heading associated with the largest positive gradient was selected. The predicted slopes for an eye-centered reference frame were calculated as the average gradient for all the neurons for a given condition (dashed lines in <xref ref-type="fig" rid="fig7">Figure 7C,D</xref>). To test whether the temporal responses match this prediction, the time course of firing rate was measured at the heading associated with the largest positive gradient, for neurons recorded during the 3D cloud protocol. For sharply tuned neurons, it is possible that the true largest gradient lay between sampled headings. Hence, the measured largest gradient could be part of the peak or trough of the tuning curve. To account for such instances in the data, we excluded tuning curves for which the mean response at the largest gradient heading was not significantly different (t-test; p ≤ 0.05) from the responses of its immediate neighboring headings (29/360 total tuning curves from 72 cells). The time course of firing rate during each trial for the selected heading was calculated by convolving the spike events with a Gaussian kernel (σ = 25 ms). The temporal responses from all selected tuning curves were averaged by condition and used to calculate the mean and standard errors shown in <xref ref-type="fig" rid="fig7">Figure 7C,D</xref>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Jing Lin and Johnny Wen for programming assistance and Ari Rosenberg for comments on the manuscript. We would also like to thank Mandy Turner and Tammy Humbird for assistance with animal care.</p></ack><sec sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>AS, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>GCD, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>DEA, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All surgical and experimental procedures were approved by the Institutional Animal Care and Use Committees at Washington University (#20100230) and Baylor College of Medicine (#AN-5795) and were in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health.</p></fn></fn-group></sec><sec sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.04693.016</object-id><label>Source code 1.</label><caption><p>Custom scripts. Custom scripts written for use with the TEMPO system (Reflective Computing, St. Louis, MO). The scripts define experiment specific parameters and the control loop determining the structure of each individual trial.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04693.016">http://dx.doi.org/10.7554/eLife.04693.016</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-04693-code1-v1.zip"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year>1997</year><article-title>Neural mechanisms of visual motion perception in primates</article-title><source>Neuron</source><volume>18</volume><fpage>865</fpage><lpage>872</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80326-8</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Ehrlich</surname><given-names>SM</given-names></name><name><surname>Backus</surname><given-names>BT</given-names></name><name><surname>Crowell</surname><given-names>JA</given-names></name></person-group><year>1996</year><article-title>Estimating heading during real and simulated eye movements</article-title><source>Vision Research</source><volume>36</volume><fpage>431</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00122-0</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>DC</given-names></name><name><surname>Maxwell</surname><given-names>M</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year>1996</year><article-title>Mechanisms of heading perception in primate visual cortex</article-title><source>Science</source><volume>273</volume><fpage>1544</fpage><lpage>1547</lpage><pub-id pub-id-type="doi">10.1126/science.273.5281.1544</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Kubischik</surname><given-names>M</given-names></name><name><surname>Pekel</surname><given-names>M</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name></person-group><year>2010</year><article-title>Visual selectivity for heading in monkey area MST</article-title><source>Experimental Brain Research</source><volume>200</volume><fpage>51</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1990-3</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name></person-group><year>2008</year><article-title>Mechanisms of self-motion perception</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>389</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112953</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>SW</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name></person-group><year>2010</year><article-title>Idiosyncratic and systematic aspects of spatial representations in the macaque parietal cortex</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>107</volume><fpage>7951</fpage><lpage>7956</lpage><pub-id pub-id-type="doi">10.1073/pnas.0913209107</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year>2011</year><article-title>Representation of vestibular and visual cues to self-motion in ventral intraparietal cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>12036</fpage><lpage>12052</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0395-11.2011</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year>2013</year><article-title>Eye-centered representation of optic flow tuning in the ventral intraparietal area</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>18574</fpage><lpage>18582</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2837-13.2013</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year>2014</year><article-title>Eye-centered visual receptive fields in the ventral intraparietal area</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>353</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1152/jn.00057.2014</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year>1993</year><article-title>Ventral intraparietal area of the macaque: anatomic location and visual response properties</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>902</fpage><lpage>914</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowell</surname><given-names>JA</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year>2001</year><article-title>Pursuit compensation during self-motion</article-title><source>Perception</source><volume>30</volume><fpage>1465</fpage><lpage>1488</lpage><pub-id pub-id-type="doi">10.1068/p3271</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowell</surname><given-names>JA</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year>1998</year><article-title>Visual self-motion perception during head turns</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>732</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1038/3732</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cullen</surname><given-names>KE</given-names></name></person-group><year>2004</year><article-title>Sensory signals during active versus passive movement</article-title><source>Current Opinion in Neurobiology</source><volume>14</volume><fpage>698</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2004.10.002</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duffy</surname><given-names>CJ</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year>1995</year><article-title>Response of monkey MST neurons to optic flow stimuli with shifted centers of motion</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>5192</fpage><lpage>5208</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Ben Hamed</surname><given-names>S</given-names></name><name><surname>Graf</surname><given-names>W</given-names></name></person-group><year>1997</year><article-title>Spatial invariance of visual receptive fields in parietal cortex neurons</article-title><source>Nature</source><volume>389</volume><fpage>845</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1038/39865</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year>1998</year><article-title>Ventral intraparietal area of the macaque: congruent visual and somatic response properties</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>126</fpage><lpage>136</lpage></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Deangelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year>2007</year><article-title>Spatial reference frames of visual, vestibular, and multimodal heading signals in the dorsal subdivision of the medial superior temporal area</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>700</fpage><lpage>712</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3553-06.2007</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>TC</given-names></name></person-group><year>1999</year><article-title>Path perception and Filehne illusion compared: model and data</article-title><source>Vision Research</source><volume>39</volume><fpage>2659</fpage><lpage>2667</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(98)00293-4</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>TC</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Crowell</surname><given-names>JA</given-names></name></person-group><year>2000</year><article-title>Extraretinal and retinal amplitude and phase errors during Filehne illusion and path perception</article-title><source>Perception &amp; Psychophysics</source><volume>62</volume><fpage>900</fpage><lpage>909</lpage><pub-id pub-id-type="doi">10.3758/BF03212076</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gellman</surname><given-names>RS</given-names></name><name><surname>Fletcher</surname><given-names>WA</given-names></name></person-group><year>1992</year><article-title>Eye position signals in human saccadic processing</article-title><source>Experimental Brain Research</source><volume>89</volume><fpage>425</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1007/BF00228258</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>JJ</given-names></name></person-group><year>1950</year><article-title>The perception of visual surfaces</article-title><source>The American Journal of Psychology</source><volume>63</volume><fpage>367</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.2307/1418003</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grigo</surname><given-names>A</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name></person-group><year>1999</year><article-title>Dynamical use of different sources of information in heading judgments from retinal flow</article-title><source>Journal of the Optical Society of America A, Optics, Image Science, and Vision</source><volume>16</volume><fpage>2079</fpage><lpage>2091</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.16.002079</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>Adeyemo</surname><given-names>B</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year>2010</year><article-title>Decoding of MSTd population activity accounts for variations in the precision of heading perception</article-title><source>Neuron</source><volume>66</volume><fpage>596</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.026</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Watkins</surname><given-names>PV</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year>2006</year><article-title>Visual and nonvisual contributions to three-dimensional heading selectivity in the medial superior temporal area</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>73</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2356-05.2006</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Helmholtz</surname><given-names>HV</given-names></name><name><surname>Southall</surname><given-names>JPC</given-names></name></person-group><year>1924</year><source>Helmholtz's treatise on physiological optics</source><publisher-loc>Rochester, N.Y</publisher-loc><publisher-name>The Optical Society of America</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaminiarz</surname><given-names>A</given-names></name><name><surname>Schlack</surname><given-names>A</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name></person-group><year>2014</year><article-title>Visual selectivity for heading in the macaque ventral intraparietal area</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>2470</fpage><lpage>2480</lpage><pub-id pub-id-type="doi">10.1152/jn.00410.2014</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year>2014</year><article-title>A novel role for visual perspective cues in the neural computation of depth</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>129</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1038/nn.3889</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klier</surname><given-names>EM</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Hess</surname><given-names>BJ</given-names></name></person-group><year>2005</year><article-title>Roles of gravitational cues and efference copy signals in the rotational updating of memory saccades</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>468</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1152/jn.00700.2004</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname><given-names>JJ</given-names></name><name><surname>van Doorn</surname><given-names>AJ</given-names></name></person-group><year>1976</year><article-title>Local structure of movement parallax of the plane</article-title><source>Journal of the Optical Society of America</source><volume>66</volume><fpage>717</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1364/JOSA.66.000717</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname><given-names>JJ</given-names></name><name><surname>van Doorn</surname><given-names>AJ</given-names></name></person-group><year>1981</year><article-title>Exterospecific component of the motion parallax field</article-title><source>Journal of the Optical Society of America</source><volume>71</volume><fpage>953</fpage><lpage>957</lpage><pub-id pub-id-type="doi">10.1364/JOSA.71.000953</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname><given-names>JJ</given-names></name><name><surname>van Doorn</surname><given-names>AJ</given-names></name></person-group><year>1987</year><article-title>Facts on optic flow</article-title><source>Biological Cybernetics</source><volume>56</volume><fpage>247</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1007/BF00365219</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>RF</given-names></name><name><surname>Gaymard</surname><given-names>BM</given-names></name><name><surname>Tamargo</surname><given-names>RJ</given-names></name></person-group><year>1998</year><article-title>Efference copy provides the eye position information required for visually guided reaching</article-title><source>Journal of Neurophysiology</source><volume>80</volume><fpage>1605</fpage><lpage>1608</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Warren</surname><given-names>WH</given-names><suffix>Jr</suffix></name></person-group><year>2000</year><article-title>Perception of heading during rotation: sufficiency of dense motion parallax and reference objects</article-title><source>Vision Research</source><volume>40</volume><fpage>3873</fpage><lpage>3894</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(00)00196-6</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Warren</surname><given-names>WH</given-names><suffix>Jr</suffix></name></person-group><year>2002</year><article-title>Retinal flow is sufficient for steering during observer rotation</article-title><source>Psychological Science</source><volume>13</volume><fpage>485</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00486</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Warren</surname><given-names>WH</given-names><suffix>Jr</suffix></name></person-group><year>2004</year><article-title>Path perception during rotation: influence of instructions, depth range, and dot density</article-title><source>Vision Research</source><volume>44</volume><fpage>1879</fpage><lpage>1889</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.03.008</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Matin</surname><given-names>L</given-names></name></person-group><year>1992</year><article-title>Visual direction is corrected by a hybrid extraretinal eye position signal</article-title><source>Annals of the New York Academy of Sciences</source><volume>656</volume><fpage>865</fpage><lpage>867</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.1992.tb25277.x</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Longuet-Higgins</surname><given-names>HC</given-names></name><name><surname>Prazdny</surname><given-names>K</given-names></name></person-group><year>1980</year><article-title>The interpretation of a moving retinal image</article-title><source>Proceedings of the Royal Society of London Series B, Containing Papers of a Biological Character</source><volume>208</volume><fpage>385</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1098/rspb.1980.0057</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullette-Gillman</surname><given-names>OA</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><year>2009</year><article-title>Motor-related signals in the intraparietal cortex encode locations in a hybrid, rather than eye-centered reference frame</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>1761</fpage><lpage>1775</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn207</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname><given-names>WK</given-names></name><name><surname>Duffy</surname><given-names>CJ</given-names></name></person-group><year>1999</year><article-title>MST neuronal responses to heading direction during pursuit eye movements</article-title><source>Journal of Neurophysiology</source><volume>81</volume><fpage>596</fpage><lpage>610</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Probst</surname><given-names>T</given-names></name><name><surname>Krafczyk</surname><given-names>S</given-names></name><name><surname>Brandt</surname><given-names>T</given-names></name><name><surname>Wist</surname><given-names>ER</given-names></name></person-group><year>1984</year><article-title>Interaction between perceived self-motion and object-motion impairs vehicle guidance</article-title><source>Science</source><volume>225</volume><fpage>536</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1126/science.6740325</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rieger</surname><given-names>JH</given-names></name><name><surname>Lawton</surname><given-names>DT</given-names></name></person-group><year>1985</year><article-title>Processing differential image motion</article-title><source>Journal of the Optical Society of America A, Optics and Image Science</source><volume>2</volume><fpage>354</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.000354</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>A</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year>2014</year><article-title>Gravity influences the visual representation of object tilt in parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>14170</fpage><lpage>14180</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2030-14.2014</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname><given-names>CS</given-names></name></person-group><year>1994</year><article-title>Analysis of misperceived observer motion during simulated eye rotations</article-title><source>Vision Research</source><volume>34</volume><fpage>3215</fpage><lpage>3222</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)90085-X</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname><given-names>CS</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Crowell</surname><given-names>JA</given-names></name></person-group><year>1992</year><article-title>The perception of heading during eye movements</article-title><source>Nature</source><volume>360</volume><fpage>583</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/360583a0</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royden</surname><given-names>CS</given-names></name><name><surname>Cahill</surname><given-names>JM</given-names></name><name><surname>Conti</surname><given-names>DM</given-names></name></person-group><year>2006</year><article-title>Factors affecting curved versus straight path heading perception</article-title><source>Perception &amp; Psychophysics</source><volume>68</volume><fpage>184</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.3758/BF03193668</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Bradley</surname><given-names>DC</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year>1999</year><article-title>Influence of gaze rotation on the visual response of primate MSTd neurons</article-title><source>Journal of Neurophysiology</source><volume>81</volume><fpage>2764</fpage><lpage>2786</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Crowell</surname><given-names>JA</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year>2002</year><article-title>Pursuit speed compensation in cortical area MSTd</article-title><source>Journal of Neurophysiology</source><volume>88</volume><fpage>2630</fpage><lpage>2647</lpage><pub-id pub-id-type="doi">10.1152/jn.00002.2001</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squatrito</surname><given-names>S</given-names></name><name><surname>Maioli</surname><given-names>MG</given-names></name></person-group><year>1997</year><article-title>Encoding of smooth pursuit direction and eye position by neurons of area MSTd of macaque monkey</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>3847</fpage><lpage>3860</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swindale</surname><given-names>NV</given-names></name></person-group><year>1998</year><article-title>Orientation tuning curves: empirical description and estimation of parameters</article-title><source>Biological Cybernetics</source><volume>78</volume><fpage>45</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1007/s004220050411</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>K</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>May</surname><given-names>PJ</given-names></name><name><surname>Newlands</surname><given-names>SD</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year>2007</year><article-title>Multimodal coding of three-dimensional rotation and translation in area MSTd: comparison of visual and vestibular selectivity</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>9742</fpage><lpage>9756</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0817-07.2007</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Hikosaka</surname><given-names>K</given-names></name><name><surname>Saito</surname><given-names>H</given-names></name><name><surname>Yukie</surname><given-names>M</given-names></name><name><surname>Fukada</surname><given-names>Y</given-names></name><name><surname>Iwai</surname><given-names>E</given-names></name></person-group><year>1986</year><article-title>Analysis of local and wide-field movements in the superior temporal visual areas of the macaque monkey</article-title><source>The Journal of Neuroscience</source><volume>6</volume><fpage>134</fpage><lpage>144</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname><given-names>AV</given-names></name><name><surname>Brenner</surname><given-names>E</given-names></name></person-group><year>1994</year><article-title>Why two eyes are better than one for judgements of heading</article-title><source>Nature</source><volume>371</volume><fpage>700</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/371700a0</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallach</surname><given-names>H</given-names></name></person-group><year>1987</year><article-title>Perceiving a stable environment when one moves</article-title><source>Annual Review of Psychology</source><volume>38</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1146/annurev.ps.38.020187.000245</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names><suffix>Jr</suffix></name><name><surname>Hannon</surname><given-names>DJ</given-names></name></person-group><year>1988</year><article-title>Direction of self-motion is perceived from optical flow</article-title><source>Nature</source><volume>336</volume><fpage>162</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1038/336162a0</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names><suffix>Jr</suffix></name><name><surname>Hannon</surname><given-names>DJ</given-names></name></person-group><year>1990</year><article-title>Eye movements and optical flow</article-title><source>Journal of the Optical Society of America A, Optics and Image Science</source><volume>7</volume><fpage>160</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.7.000160</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names><suffix>Jr</suffix></name><name><surname>Morris</surname><given-names>MW</given-names></name><name><surname>Kalish</surname><given-names>M</given-names></name></person-group><year>1988</year><article-title>Perception of translational heading from optical flow</article-title><source>Journal of Experimental Psychology Human Perception and Performance</source><volume>14</volume><fpage>646</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.14.4.646</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names><suffix>Jr</suffix></name><name><surname>Saunders</surname><given-names>JA</given-names></name></person-group><year>1995</year><article-title>Perceiving heading in the presence of moving objects</article-title><source>Perception</source><volume>24</volume><fpage>315</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1068/p240315</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Heuer</surname><given-names>HW</given-names></name><name><surname>Britten</surname><given-names>KH</given-names></name></person-group><year>2004</year><article-title>Parietal area VIP neuronal responses to heading stimuli are encoded in head-centered coordinates</article-title><source>Neuron</source><volume>42</volume><fpage>993</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.06.008</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.04693.017</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Romo</surname><given-names>Ranulfo</given-names></name><role>Reviewing editor</role><aff><institution>Universidad Nacional Autonoma de Mexico</institution>, <country>Mexico</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Role of visual and non-visual cues in constructing a rotation-invariant representation of heading in parietal cortex” for consideration at <italic>eLife</italic>. Your article has been favorably evaluated by Eve Marder (Senior editor), a Reviewing editor, and three peer reviewers.</p><p>The Reviewing editor and the reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>Reviewer #1 recommends publication in its current form and has no substantive concerns.</p><p>Reviewer #2 had one major concern: The authors make a major compelling case that VIP neurons compensate, at least partially, for rotations, which could allow the monkey to accurately estimate heading in the presences of rotations induced by head or eye movements. The problem is that the degree to which the monkey makes use of these cues is not clear. In an ideal world, the authors could train monkeys to report estimated heading and demonstrate that the monkey compensates, even partially, for rotation induced changes in FOE when it estimates heading (e.g. heading estimates are still pretty good even if made during pursuit). However, this would not be a straightforward experiment to carry out because the changes in FOE induced by pursuit are small, and the monkey's estimates of heading would probably be noisy to begin with. Further, this would entail an entirely new set of experiments that would probably take years to carry out.</p><p>According to the reviewer, there two alternatives, as follows:</p><p>First, the authors could go into more detail about the psychophysical literature involving heading estimation during pursuit. There are some references to this in the current version of the paper, but the topic is not well-developed.</p><p>Second, the authors should further consider ideas about how signals in VIP should be combined to allow an accurate estimate of heading. This would naturally be speculative, but it could provide a very addition to the paper for the following reason: <xref ref-type="fig" rid="fig4">Figure 4</xref> makes clear that neurons vary continuously in the degree to which they are rotation invariant. Indeed, a good handful of neurons (third quadrant) shift in the wrong direction! Of neurons that do shift in the right direction, it is clear that invariance to rotation does not define a category of neurons specialized for this task. Given that rotation invariance is clearly a response feature that is continuously distributed, how might the neurons be combined to allow the kind of heading estimation that subjects actually make? Although the authors won't have behavioral data from these monkeys, they can at least speculate based on behavior in humans.</p><p>Reviewer#3: The main result is established thoroughly with appropriate analyses. Two of the subsidiary results are much less convincing and will need additional analyses. First, the claim that VIP can use dynamic perspective cues to detect rotation. She/he is not sure what distinction is here between 2D and 3D. It is true that authors simulate a frontoparallel plan. But it is not true that there is no motion parallax—under perspective projection, the angular distance between points will change with surface motion. How is this not motion parallax? It seems to she/he that the information in dynamic perspective is fundamentally the same as in the 3D cloud, but is much smaller in amplitude and smoother in space. But the distinction is quantitative. So the authors need a clearer statement about what exactly is different.</p><p>The other difficulty with this section is that the methods are hard to follow. In the Results, the authors say that the simulated distance of the FP plane changed, but not what the values were. Taken at face value, this would imply a change in the speed of all the dots, which complicates things. But perhaps the simulated translation speed was also changed so that to a first approximation speeds did not change, only the relationship between them? She/he does not understand the reason for doing this experiment with a sudden change at 750 ms. It would be much simpler to do the tuning curves with simulated pursuit in exactly the same way, just using 3D environment that happened to be at a FP plane. Why change everything else?</p><p>Second, the discussion of the eye and head centered reference frames is not compelling. Again, part of this is semantic and just needs to be laid out more carefully. In the condition with simulated pursuit, the eye stays stationary. So a truly eye centered co-ordinate frame would imply no changes. Instead, it seems that there is a co-ordinate frame centered on some virtual eye whose position is inferred from the image. But this process of inferring eye position can only infer changes in eye position, so it is hard to see how a center for this reference frame is defined.</p><p>A serious issue with the analysis is that the changes rates in <xref ref-type="fig" rid="fig7">Figure 7</xref> only show us that there is some change with eye position. They place no constraints on how closely the co-ordinates follow the eyes rather than the head. Given all the complications they stress elsewhere about the importance of considering the whole tuning curve appropriately, this analysis seems very dangerous. A much simpler approach, that is more robust by the authors' own arguments, would be to construct tuning curves for the first 300 ms of this epoch and the last 300 ms, and then apply the same metrics they develop elsewhere. Finally, the claim that the grey curves in <xref ref-type="fig" rid="fig7">Figure 7</xref> provide control to simple adaptation phenomena is very unclear, because the retinal stimulation s quite different in the pursuit case from the translation alone case. If the condition tested was forward translation (90 degrees), the pursuit stimuli would have substantial net lateral motion, whereas the translation only stimulus would not. So if adaptation primarily affected responses to lateral motion, it could produce exactly this pattern of results.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.04693.018</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Reviewer #2 had one major concern: The authors make a major compelling case that VIP neurons compensate, at least partially, for rotations, which could allow the monkey to accurately estimate heading in the presences of rotations induced by head or eye movements. The problem is that the degree to which the monkey makes use of these cues is not clear. In an ideal world, the authors could train monkeys to report estimated heading and demonstrate that the monkey compensates, even partially, for rotation induced changes in FOE when it estimates heading (e.g. heading estimates are still pretty good even if made during pursuit). However, this would not be a straightforward experiment to carry out because the changes in FOE induced by pursuit are small, and the monkey's estimates of heading would probably be noisy to begin with. Further, this would entail an entirely new set of experiments that would probably take years to carry out</italic>.</p><p><italic>According to the reviewer, there two alternatives, as follows</italic>:</p><p><italic>First, the authors could go into more detail about the psychophysical literature involving heading estimation during pursuit. There are some references to this in the current version of the paper, but the topic is not well-developed</italic>.</p><p><italic>Second, the authors should further consider ideas about how signals in VIP should be combined to allow an accurate estimate of heading. This would naturally be speculative, but it could provide a very addition to the paper for the following reason:</italic> <xref ref-type="fig" rid="fig4"><italic>Figure 4</italic></xref> <italic>makes clear that neurons vary continuously in the degree to which they are rotation invariant. Indeed, a good handful of neurons (third quadrant) shift in the wrong direction! Of neurons that do shift in the right direction, it is clear that invariance to rotation does not define a category of neurons specialized for this task. Given that rotation invariance is clearly a response feature that is continuously distributed, how might the neurons be combined to allow the kind of heading estimation that subjects actually make? Although the authors won't have behavioral data from these monkeys, they can at least speculate based on behavior in humans</italic>.</p><p>Thank you for these suggestions. We are planning to test this behavior in monkeys, but as the reviewer noted, this will be a major endeavor and therefore has to be beyond the scope of this study. However, we hope that the following additions and changes to the paper will address these questions sufficiently.</p><p>As per the reviewer’s suggestion, we have added an entire sub-section to the Discussion (‘Comparison to previous behavioral studies’) in which we review the psychophysical literature and compare our results to the existing literature. It is important to note that depending on the parameters of the visual stimuli, psychophysical studies from different laboratories report different amounts of rotation compensation. However, most studies report at least partial compensation during simulated pursuit, and the average compensation exhibited by VIP neurons lies within the range of the behavioral results. This supports our main result that visual cues play an important role in estimating heading during pursuit. In this new section, we also discuss a range of population decoding strategies that could explain the behavioral data based on the neural responses that we observe.</p><p><italic>Reviewer#3: The main result is established thoroughly with appropriate analyses. Two of the subsidiary results are much less convincing and will need additional analyses. First, the claim that VIP can use dynamic perspective cues to detect rotation. She/he is not sure what distinction is here between 2D and 3D. It is true that authors simulate a frontoparallel plan. But it is not true that there is no motion parallax</italic>—<italic>under perspective projection, the angular distance between points will change with surface motion. How is this not motion parallax? It seems to she/he that the information in dynamic perspective is fundamentally the same as in the 3D cloud, but is much smaller in amplitude and smoother in space. But the distinction is quantitative. So the authors need a clearer statement about what exactly is different</italic>.</p><p>The motion parallax we refer to in this paper is based on how the concept was first described by Helmholtz (1925) and <xref ref-type="bibr" rid="bib21">Gibson (1950)</xref>. If there are two points at different distances from the observer that project to the same retinal location, a translation of the observer results in the closer point having larger retinal velocity than the farther point (assuming the eyes remain still). Rotational optic flow, on the other hand, is invariant to depth. Motion parallax is defined here as the local difference in optic flow vectors between points in the scene that are at different depths but the same retinal location. This has now been clarified in the Results section. Hence, in the case of a frontoparallel plane, such local motion parallax does not exist. To make this distinction clear, we have also rephrased the terminology and now refer to these cues as ‘local motion parallax’ throughout the text.</p><p>Rotation of the eye causes global patterns of image motion that indicate that a rotation has occurred. When simulated on a display under planar projection, a pure rotation involves global perspective distortions of the image. The reviewer is therefore correct that such global flow patterns indicating rotation are similar during the 3D cloud and FP plane stimuli. Hence we have modified the text throughout to remove the previous distinction between 3D cues (motion parallax) and 2D cues (perspective distortion).</p><p>Importantly, while motion parallax results from observer translations, dynamic perspective cues are a result of eye rotations relative to the scene (under planar projection). This has been explained in detail in a recent paper by one of the co-authors, G.C. DeAngelis (Kim et al., 2014). We emphasize here that dynamic perspective cues are present during both the 3D cloud and frontoparallel plane stimuli. Therefore the specific difference between the 3D cloud and FP plane stimuli is the presence or absence of local motion parallax cues resulting from depth variations. We have added and modified the text to explicitly mention this (please see paragraphs sixteen and seventeen of the Results section).</p><p>These edits also include more precise definitions of these cues and clearer explanations of the sources of these cues (i.e., translation vs. rotation). We thank the reviewer for helping us to clarify these issues, which are important.</p><p><italic>The other difficulty with this section is that the methods are hard to follow. In the Results, the authors say that the simulated distance of the FP plane changed, but not what the values were. Taken at face value, this would imply a change in the speed of all the dots, which complicates things. But perhaps the simulated translation speed was also changed so that to a first approximation speeds did not change, only the relationship between them?</italic></p><p>We have revised the corresponding parts of the Methods to make it easier to follow (‘Stimulus and Task’). The reviewer is correct that the speed of the dots changes as the simulated wall gets closer to the observer. This is because in a real world situation, approaching the wall does in fact result in changes in optic flow speed over time. In contrast, previous studies simulated constant optic flow velocities over the duration of a trial. Such a stimulus corresponds to the impossible scenario in which an observer translates towards a wall, but the distance between the observer and the wall does not change. To us, it makes little sense to simulate this situation. Hence, in order to understand how the brain encodes self-motion, it was necessary that we include these more naturalistic features in our stimulus.</p><p>We now describe in more detail this feature of the stimulus in the Methods and explain the rationale and implications of such a stimulus in the Results.</p><p><italic>She/he does not understand the reason for doing this experiment with a sudden change at 750 ms. It would be much simpler to do the tuning curves with simulated pursuit in exactly the same way, just using 3D environment that happened to be at a FP plane. Why change everything else?</italic></p><p>There was clearly a misunderstanding here. There was no sudden change at 750ms in the FP plane stimulus. The FP plane stimuli were generated using the same 3D environment in which the 3D cloud was simply replaced by a fronto-parallel plane and the disparity was set to zero. The rest of the stimulus parameters, including the velocity profiles, were the same during the two visual scenes. We have revised the Methods to clarify this point.</p><p><italic>Second, the discussion of the eye and head centered reference frames is not compelling. Again, part of this is semantic and just needs to be laid out more carefully. In the condition with simulated pursuit, the eye stays stationary. So a truly eye centered co-ordinate frame would imply no changes. Instead, it seems that there is a co-ordinate frame centered on some virtual eye whose position is inferred from the image. But this process of inferring eye position can only infer changes in eye position, so it is hard to see how a center for this reference frame is defined</italic>.</p><p>The optic flow patterns are similar during both real and simulated pursuit and result in the FOE drifting across the retina over time as shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref>. Hence, in a truly eye-centered co-ordinate frame, we would expect the representation of translation to change over time even in the simulated pursuit conditions. We have clarified this in the Results section.</p><p>The reviewer is correct that drift of the FOE over time can be used to infer changes in eye position. We rephrased the relevant sentences in the Results to make this distinction clearer.</p><p><italic>A serious issue with the analysis is that the changes rates in</italic> <xref ref-type="fig" rid="fig7"><italic>Figure 7</italic></xref> <italic>only show us that there is some change with eye position. They place no constraints on how closely the co-ordinates follow the eyes rather than the head</italic>.</p><p>This is a very good point. We have addressed this issue by modifying <xref ref-type="fig" rid="fig7">Figure 7C, D</xref> and plotting the expected eye-centered response based on the average tuning properties of the cells (described in the Methods). We find that the slope of measured VIP population responses is significantly different from a slope of 0 and is about 65% of the expected slope based on an eye-centered response. Hence, we can now make a much clearer statement about the extent to which these temporal dynamics are consistent with an eye centered reference frame. The result suggests an intermediate reference frame, but closer to eye-centered than head-centered.</p><p><italic>Given all the complications they stress elsewhere about the importance of considering the whole tuning curve appropriately, this analysis seems very dangerous. A much simpler approach, that is more robust by the authors' own arguments, would be to construct tuning curves for the first 300 ms of this epoch and the last 300 ms, and then apply the same metrics they develop elsewhere</italic>.</p><p>The analysis method suggested by the reviewer would not address the question of how the firing rate changes as a result of changing eye position. Measuring the shifts in tuning only provides an indication of which signal is being represented—the translation component or the resultant optic flow. In other words, measuring shifts in tuning provides information about whether a cell compensates for rotations or not. Hence, the suggested analysis would only address how the cells compensate for rotations over time and not about the reference frame in which VIP neurons represent self-motion. In order to evaluate the reference frame, it is necessary to measure the temporal changes in firing, corresponding to changes in the heading relative to the eye. We have added text further clarifying this point (please see the Results section).</p><p><italic>Finally, the claim that the grey curves in</italic> <xref ref-type="fig" rid="fig7"><italic>Figure 7</italic></xref> <italic>provide control to simple adaptation phenomena is very unclear, because the retinal stimulation s quite different in the pursuit case from the translation alone case. If the condition tested was forward translation (90 degrees), the pursuit stimuli would have substantial net lateral motion, whereas the translation only stimulus would not. So if adaptation primarily affected responses to lateral motion, it could produce exactly this pattern of results</italic>.</p><p>It is unclear to us why adaptation would primarily affect responses to lateral motion. But even if this were true, we would expect that the firing rate would decrease over time irrespective of the direction of rotation. Hence, this hypothesis would not explain an increase in the firing rate over time for rightward rotations and a decrease for leftward rotations. Moreover, these opposite trends observed during rightward versus leftward pursuit are consistent with the direction expected based on an eye-reference frame hypothesis. We have modified the corresponding section of the Results to focus on the expected outcome of a difference in slope between rightward and leftward pursuit directions, rather than a flat line in the fixation condition. We think that this emphasis largely avoids the problem that the reviewer has raised.</p></body></sub-article></article>