<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">50732</article-id><article-id pub-id-type="doi">10.7554/eLife.50732</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Categorical representation from sound and sight in the ventral occipito-temporal cortex of sighted and blind</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-152648"><name><surname>Mattioni</surname><given-names>Stefania</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8279-6118</contrib-id><email>stefania.mattioni@uclouvain.be</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152649"><name><surname>Rezk</surname><given-names>Mohamed</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1866-8645</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152650"><name><surname>Battal</surname><given-names>Ceren</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9844-7630</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-96889"><name><surname>Bottini</surname><given-names>Roberto</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7941-7762</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152651"><name><surname>Cuculiza Mendoza</surname><given-names>Karen E</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-152652"><name><surname>Oosterhof</surname><given-names>Nikolaas N</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-96236"><name><surname>Collignon</surname><given-names>Olivier</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1882-3550</contrib-id><email>olivier.collignon@uclouvain.be</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Institute of research in Psychology (IPSY) &amp; Institute of Neuroscience (IoNS) - University of Louvain (UCLouvain)</institution><addr-line><named-content content-type="city">Louvain-la-Neuve</named-content></addr-line><country>Belgium</country></aff><aff id="aff2"><label>2</label><institution>Centre for Mind/Brain Sciences, University of Trento</institution><addr-line><named-content content-type="city">Trento</named-content></addr-line><country>Italy</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>28</day><month>02</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e50732</elocation-id><history><date date-type="received" iso-8601-date="2019-07-31"><day>31</day><month>07</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-02-14"><day>14</day><month>02</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Mattioni et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Mattioni et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-50732-v2.pdf"/><abstract><p>Is vision necessary for the development of the categorical organization of the Ventral Occipito-Temporal Cortex (VOTC)? We used fMRI to characterize VOTC responses to eight categories presented acoustically in sighted and early blind individuals, and visually in a separate sighted group. We observed that VOTC reliably encodes sound categories in sighted and blind people using a representational structure and connectivity partially similar to the one found in vision. Sound categories were, however, more reliably encoded in the blind than the sighted group, using a representational format closer to the one found in vision. Crucially, VOTC in blind represents the categorical membership of sounds rather than their acoustic features. Our results suggest that sounds trigger categorical responses in the VOTC of congenitally blind and sighted people that partially match the topography and functional profile of the visual response, despite qualitative nuances in the categorical organization of VOTC between modalities and groups.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>The world is full of rich and dynamic visual information. To avoid information overload, the human brain groups inputs into categories such as faces, houses, or tools. A part of the brain called the ventral occipito-temporal cortex (VOTC) helps categorize visual information. Specific parts of the VOTC prefer different types of visual input; for example, one part may tend to respond more to faces, whilst another may prefer houses. However, it is not clear how the VOTC characterizes information.</p><p>One idea is that similarities between certain types of visual information may drive how information is organized in the VOTC. For example, looking at faces requires using central vision, while looking at houses requires using peripheral vision. Furthermore, all faces have a roundish shape while houses tend to have a more rectangular shape. Another possibility, however, is that the categorization of different inputs cannot be explained just by vision, and is also be driven by higher-level aspects of each category. For instance, how humans use or interact with something may also influence how an input is categorized. If categories are established depending (at least partially) on these higher-level aspects, rather than purely through visual likeness, it is likely that the VOTC would respond similarly to both sounds and images representing these categories.</p><p>Now, Mattioni et al. have tested how individuals with and without sight respond to eight different categories of information to find out whether or not categorization is driven purely by visual likeness. Each category was presented to participants using sounds while measuring their brain activity. In addition, a group of participants who could see were also presented with the categories visually. Mattioni et al. then compared what happened in the VOTC of the three groups – sighted people presented with sounds, blind people presented with sounds, and sighted people presented with images – in response to each category.</p><p>The experiment revealed that the VOTC organizes both auditory and visual information in a similar way. However, there were more similarities between the way blind people categorized auditory information and how sighted people categorized visual information than between how sighted people categorized each type of input. Mattioni et al. also found that the region of the VOTC that responds to inanimate objects massively overlapped across the three groups, whereas the part of the VOTC that responds to living things was more variable.</p><p>These findings suggest that the way that the VOTC organizes information is, at least partly, independent from vision. The experiments also provide some information about how the brain reorganizes in people who are born blind. Further studies may reveal how differences in the VOTC of people with and without sight affect regions typically associated with auditory categorization, and potentially explain how the brain reorganizes in people who become blind later in life.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>blindness</kwd><kwd>category perception</kwd><kwd>crossmodal plasticity</kwd><kwd>fMRI-multivariate analyses</kwd><kwd>auditory</kwd><kwd>visual</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>Starting Grant MADVIS: 337573</award-id><principal-award-recipient><name><surname>Collignon</surname><given-names>Olivier</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Excellence of Science</institution></institution-wrap></funding-source><award-id>30991544</award-id><principal-award-recipient><name><surname>Collignon</surname><given-names>Olivier</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002661</institution-id><institution>Fonds De La Recherche Scientifique - FNRS</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Collignon</surname><given-names>Olivier</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The categorical organization of the ventral occipito-temporal cortex, typically thought to be a visual region, is actually partially independent of visual input and even visual experience.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The study of sensory deprived individuals represents a unique model system to test how sensory experience interacts with intrinsic biological constraints to shape the functional organization of the brain. One of the most striking demonstrations of experience-dependent plasticity comes from studies of blind individuals showing that the occipital cortex (traditionally considered as visual) massively extends its response repertoire to non-visual inputs (<xref ref-type="bibr" rid="bib82">Neville and Bavelier, 2002</xref>; <xref ref-type="bibr" rid="bib106">Sadato et al., 1998</xref>).</p><p>But what are the mechanisms guiding this process of brain reorganization? It was suggested that the occipital cortex of people born blind is repurposed toward new functions that are distant from the typical tuning of these regions for vision (<xref ref-type="bibr" rid="bib11">Bedny, 2017</xref>). In fact, the functional organization of occipital regions has been thought to develop based on innate protomaps implementing a computational bias for low-level visual features including ﻿retinal eccentricity bias (<xref ref-type="bibr" rid="bib75">Malach et al., 2002</xref>), orientation content (<xref ref-type="bibr" rid="bib104">Rice et al., 2014</xref>), spatial frequency content (<xref ref-type="bibr" rid="bib100">Rajimehr et al., 2011</xref>) and ﻿the average curvilinearity/rectilinearity of stimuli (<xref ref-type="bibr" rid="bib81">Nasr et al., 2014</xref>). This proto-organization would serve as low-level visual biases scaffolding experience-dependent domain specialization (<xref ref-type="bibr" rid="bib5">Arcaro and Livingstone, 2017</xref>; <xref ref-type="bibr" rid="bib44">Gomez et al., 2019</xref>). Consequently, in absence of visual experience, the functional organization of the occipital cortex could not develop according to this visual proto-organization and those regions may therefore switch their functional tuning toward distant computations (<xref ref-type="bibr" rid="bib11">Bedny, 2017</xref>).</p><p>In striking contrast with this view, several studies suggested that the occipital cortex of congenitally blind people maintains a division of computational labor somewhat similar to the one characterizing the sighted brain (<xref ref-type="bibr" rid="bib3">Amedi et al., 2010</xref>; <xref ref-type="bibr" rid="bib36">Dormal and Collignon, 2011</xref>; <xref ref-type="bibr" rid="bib103">Ricciardi et al., 2007</xref>). Perhaps, the most striking demonstration that the occipital cortex of blind people develops a similar coding structure and topography as the one typically observed in sighted people comes from studies exploring the response properties of the ventral occipito-temporal cortex (VOTC). In sighted individuals, lesion and neuroimaging studies have demonstrated that VOTC shows a medial to lateral segregation in response to living and non-living visual stimuli, respectively, and that some specific regions respond preferentially to visual objects of specific categories like the fusiform face area (FFA; <xref ref-type="bibr" rid="bib58">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib119">Tong et al., 2000</xref>), the extrastriate body area (EBA; <xref ref-type="bibr" rid="bib37">Downing et al., 2001</xref>) or the parahippocampal place area (PPA; <xref ref-type="bibr" rid="bib40">Epstein and Kanwisher, 1998</xref>). Interestingly, In early blind people, the functional preference for words (<xref ref-type="bibr" rid="bib102">Reich et al., 2011</xref>) or letters (<xref ref-type="bibr" rid="bib114">Striem-Amit et al., 2012</xref>), motion (<xref ref-type="bibr" rid="bib34">Dormal et al., 2016</xref>; <xref ref-type="bibr" rid="bib97">Poirier et al., 2004</xref>), places (<xref ref-type="bibr" rid="bib51">He et al., 2013</xref>; <xref ref-type="bibr" rid="bib127">Wolbers et al., 2011</xref>), bodies (<xref ref-type="bibr" rid="bib64">Kitada et al., 2014</xref>; <xref ref-type="bibr" rid="bib117">Striem-Amit and Amedi, 2014</xref>), tools (<xref ref-type="bibr" rid="bib91">Peelen et al., 2013</xref>) and shapes (<xref ref-type="bibr" rid="bib2">Amedi et al., 2007</xref>) partially overlaps with similar categorical responses in sighted people when processing visual inputs.</p><p>Distributed multivariate pattern analyses (<xref ref-type="bibr" rid="bib48">Haxby et al., 2001</xref>) have also supported the idea that the large-scale categorical layout in VOTC shares similarities between sighted and blind people (<xref ref-type="bibr" rid="bib45">Handjaras et al., 2016</xref>; <xref ref-type="bibr" rid="bib121">van den Hurk et al., 2017</xref>; <xref ref-type="bibr" rid="bib92">Peelen et al., 2014</xref>; <xref ref-type="bibr" rid="bib124">Wang et al., 2015</xref>). For example, it was shown that the tactile exploration of different manufactured objects (shoes and bottles) elicits distributed activity in VOTC of blind people similar to the one observed in sighted people in vision (<xref ref-type="bibr" rid="bib95">Pietrini et al., 2004</xref>). A recent study demonstrated that the response patterns elicited by sounds of four different categories in the VOTC of blind people could successfully predict the categorical response to images of the same categories in the VOTC of sighted controls, suggesting overlapping distributed categorical responses in sighted for vision and in blind for sounds (<xref ref-type="bibr" rid="bib121">van den Hurk et al., 2017</xref>). All together, these studies suggest that there is more to the development of the categorical response of VOTC than meets the eye (<xref ref-type="bibr" rid="bib28">Collignon et al., 2012</xref>).</p><p>However, these researches leave several important questions unanswered. If a spatial overlap exists between the sighted processing visual inputs and the blind processing non-visual material, whether VOTC represents similar informational content in both groups remains unknown. It is possible, for instance, that the overlap in categorical responses between groups comes from the fact that VOTC represents visual attributes in the sighted (<xref ref-type="bibr" rid="bib5">Arcaro and Livingstone, 2017</xref>; <xref ref-type="bibr" rid="bib44">Gomez et al., 2019</xref>) and acoustic attributes in the blind due to crossmodal plasticity (<xref ref-type="bibr" rid="bib9">Bavelier and Neville, 2002</xref>). Indeed, several studies involving congenitally blind have shown that their occipital cortex may represent acoustic features – for instance frequencies (<xref ref-type="bibr" rid="bib53">Huber et al., 2019</xref>; <xref ref-type="bibr" rid="bib126">Watkins et al., 2013</xref>) – which form the basis of the development of categorical selectivity in the auditory cortex (<xref ref-type="bibr" rid="bib80">Moerel et al., 2012</xref>). Such preferential responses for visual or acoustic features in the sighted and blind, respectively, may lead to overlapping patterns of activity for similar categories while implementing separate computations on the sensory inputs. Alternatively, it is possible that the VOTC of both groups code for higher-order categorical membership of stimuli presented in vision in sighted and audition in the blind, at least partial independently from low-level features of the stimuli.</p><p>Moreover, the degree of similarity between the categorical representation in sighted and in blind might differ across different categories: not all the regions in VOTC seem to be affected to the same extent by the crossmodal plasticity reorganization (<xref ref-type="bibr" rid="bib15">Bi et al., 2016</xref>; <xref ref-type="bibr" rid="bib35">Dormal et al., 2018</xref>; <xref ref-type="bibr" rid="bib124">Wang et al., 2015</xref>). This ‘domain–by–modality interaction’ suggests that intrinsic characteristics of objects belonging to different categories might drive this difference. However, a qualitative exploration of the structure of the categorical representation in the VOTC of blind and sighted is still missing.</p><p>Another unresolved but important question is whether sighted people also show categorical responses in VOTC to acoustic information similar to the one they show in vision. For instance, the two multivariate studies using sensory (not word) stimulation (tactile, <xref ref-type="bibr" rid="bib95">Pietrini et al., 2004</xref>; auditory, <xref ref-type="bibr" rid="bib121">van den Hurk et al., 2017</xref>) of various categories in sighted and blind either did not find the existence of category-related patterns of response in the ventral temporal cortex of sighted people (<xref ref-type="bibr" rid="bib95">Pietrini et al., 2004</xref>) or did not report overlapping distributed response between categories presented acoustically or visually in sighted people (<xref ref-type="bibr" rid="bib121">van den Hurk et al., 2017</xref>). Therefore, it remains controversial whether similar categorical responses in VOTC for visual and non-visual sensory stimuli only emerge in the absence of bottom-up visual inputs during development or whether it is an organizational property also found in the VOTC of sighted people.</p><p>Finally, it has been suggested that VOTC regions might display similar functional profile for sound and sight in sighted and blind because different portions of this region integrate specific large-scale brain networks sharing similar functional coding. However, empirical evidence supporting this mechanistic account remains scarce.</p><p>With these unsolved questions in mind, we relied on a series of complementary multivariate analyses in order to carry out a comprehensive mapping of the representational geometry underlying low-level (acoustic or visual features mapping) and categorical responses to images and sounds in the VOTC of sighted and early blind people.</p><p>All together our results demonstrate that early visual deprivation triggers an extension of the intrinsic, partially non-visual, categorical organization of VOTC, potentially supported by a connectivity bias of portions of VOTC with specific large-scale functional brain networks. However, the categorical representation of the auditory stimuli in VOTC of both blind and sighted individuals exhibits different qualitative nuances compared to the categorical organization generated by visual stimuli in sighted people.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Topographical selectivity map</title><p><xref ref-type="fig" rid="fig1">Figure 1B</xref> represents the topographical selectivity maps, which show the voxel-wise preferred stimulus condition based on a winner take-all approach (for the four main categories: animals, humans, small objects and places). In the visual modality, we found the well-known functional selectivity map for visual categories (<xref ref-type="bibr" rid="bib56">Julian et al., 2012</xref>; <xref ref-type="bibr" rid="bib59">Kanwisher, 2010</xref>). The auditory selectivity maps of the blind subjects partially matched the visual map obtained in sighted controls during vision (r = 0.19, p<sub>FDR</sub> &lt;0.001). The blind map and the visual control map are strongly correlated.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design and topographical selectivity maps.</title><p>(<bold>A</bold>) Categories of stimuli and design of the visual (VIS) and auditory (AUD) fMRI experiments. (<bold>B</bold>) Averaged untresholded topographical selectivity maps for the sighted-visual (top), the blind-auditory (center) and the sighted-auditory (bottom) participants. These maps visualize the functional topography of VOTC to the main four categories in each group. These group maps are created for visualization purpose only since statistics are run from single subject maps (see methods). To obtain those group maps, we first averaged the β-values among participants of the same group in each voxel inside the VOTC mask for each of our 4 main conditions (animals, humans, manipulable objects and places) separately and we then assign to each voxel the condition producing the highest β-value. We decided to represent maps including the 4 main categories (instead of 8) to simplify visualization of the main effects (the correlation values are almost identical with 8 categories and those maps can be found in supplemental material).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Topographical selectivity map for 8 categories.</title><p>The averaged untresholded topographical selectivity maps for the sighted-visual (top), the blind-auditory (center) and the sighted-auditory (bottom) participants. These maps visualize the functional topography of VOTC to the eight sub-categories. The average correlation between each group comparison is reported.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig1-figsupp1-v2.tif"/></fig></fig-group><p>In addition, a similar selectivity map was also observed in the sighted controls using sounds. The correlation was significant both with visual map in sighted (r = 0.14, p<sub>FDR</sub> &lt;0.001), and with the auditory map in blinds (r = 0.06, p<sub>FDR</sub> = 0.001). The correlation between EBa and SCa was significantly lower than both the correlation between SCv and EBa (p<sub>FDR</sub>. = 0.0003) and the correlation between SCv and SCa (p<sub>FDR</sub> = 0.0004). Instead, the magnitude of correlation between EBa and SCv was not significantly different from the correlation between SCa and SCv (p<sub>FDR</sub> = 0.233).</p><p>In <xref ref-type="fig" rid="fig1">Figure 1B</xref> we report the results on the four main categories for the simplicity of visualization, however in the supplemental material we show that the results including eight categories are almost identical (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>In order to look at the consistency of the topographical representation of the categories across subjects within the same group we computed the Jaccard similarity between the topographical selectivity map of each subject and the mean topographical map of his own group. The one sample T-test revealed a significant Jaccard similarity in each category and in each group (in all cases p&lt;0.001, after FDR correction for the 12 comparisons: three groups * four categories), highlighting a significant consistency of the topographical maps for each category between subjects belonging to the same group. We performed a repeated measures ANOVA to look at the differences between categories and groups. We obtained a significant main effect of Category (F<sub>(3,138)</sub>=18.369; p=&lt;0.001) and a significant interaction Group*Category (F<sub>(6,138)</sub>=4.9; p=&lt;0.001), instead the main effect of Group was not significant (F<sub>(2,46)</sub>=1.83; p.17). We then run post-hoc. In SCv we did not find any difference between categories, meaning that the consistency (i.e. the Jaccard similarity) of the topographical maps for the visual stimuli in sighted was similar for each category. In the SCa group, the only two significant differences emerged between the category ‘big objects and places’ and the two animate categories: humans (p=0.01) and animals (p&lt;0.008). In both cases the consistency of the topographical maps in the big object and places was significantly higher compared to the consistency in the humans and animals’ categories. Finally, in the blind group only the humans and the manipulable objects categories did not show a significant difference. The animal category was, indeed, significantly lower than the humans (p=0.01), the manipulable objects (p=0.008) and the big objects and places (p=0.002). Both, the human and the manipulable objects categories were significantly lower compared to the big objects and places category (p=0.004 and p=0.007). Finally, when we look at the difference between the three groups for each category, the main differences emerged from the animals and the big objects and places categories. The animals’ category showed a significantly lower Jaccard similarity within the EBa group compared to both SCa (p=0.008) and SCv (p=0.001) groups while the similarity of big objects and places category was significantly higher in EBa compared to SCv (p=0.038).</p><p>In addition, we wanted to explore the similarity and differences among the topographical representations of our categories when they were presented visually compared to when they were presented acoustically, both in sighted and in blind. To do that, we computed the Jaccard similarity index for each category, between the topographical map of each blind and sighted subject in the auditory experiment and the averaged topographical selectivity map of the sighted in the visual experiment (see <xref ref-type="fig" rid="fig2">Figure 2C</xref> for the results). The one-sample T-tests revealed a significant similarity between EBa and SCv and between SCa and SCv in each category (pFDR &lt;0.001 in all cases). The repeated measures ANOVA highlighted a significant main effect of Category (F<sub>(2.2,69.8)</sub> = 31.17, p&lt;0.001). In both groups’ comparisons the Jaccard similarity was higher between the big objects and places category compared to the other three categories (animal: p&lt;0.001; human: p&lt;0.001; manipulable: p&lt;0.001).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Voxels’ count and Jaccard analyses.</title><p>(<bold>A</bold>) Number of selective voxels for each category in each group, within VOTC. Each circle represents one subject, the colored horizontal lines represent the group average and the vertical black bars are the standard error of the mean across subjects. (<bold>B</bold>) The Jaccard similarities values within each group, for each of the four categories. (<bold>C</bold>) The Jaccard similarity indices between the EBa and the SCv groups (left side) and the Jaccard similarity indices between the SCa and the SCv groups (right side).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Voxels’ count and Jaccard analyses for eight categories.</title><p>Results on the eight sub-categories from the selective voxels’ count for each category (first line); from the Jaccard similarity analysis within each group (middle line); from the Jaccard similarity with SCv (bottom line). Each circle represents one subject, the colored horizontal line represents the group mean and the black vertical bars represent the standard error of the mean across subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig2-figsupp1-v2.tif"/></fig></fig-group><p>No difference emerged between groups, suggesting comparable level of similarity of the auditory topographical maps (both in blind and in sighted) with the visual topographical map in sighted participants.</p><p>Finally, since the degree of overlap highlighted by the Jaccard similarity values might be driven by the number of voxels selective for each category, we looked at the number of voxels showing the preference for each category in each group. The one sample T-test revealed a significant number of selective voxels in each category and in each group (in all cases p&lt;0.001, after FDR correction for the 12 comparisons: 3 groups * 4 categories). We performed a repeated measure ANOVA to look at the differences between categories and groups. In SCv we found that a smaller number of voxels shows selectivity for the human category compared to the others (human vs animal: t(15)=-3.27; pFDR = 0.03; human vs manipulable: t(15)=2.60; pFDR = 0.08; human vs big: t(15)=-4.16; pFDR = 0.01). In EBa, instead, there is a lower number of voxels preferring animals compare to non-living categories (animals vs manipulable: t(15)=-2.47; pFDR = 0.09; animals vs big: t(15)=-3.22; pFDR = 0.03). Finally, in SCa the number of voxels selective for the big and place category was significantly higher than the number of voxels selective for the manipulable category (t(16)=3.27; pFDR = 0.03). Importantly, when we look at the difference between the 3 groups for each category, the main difference emerged from the animal category. In this category, the ANOVA revealed a main effect of group (F(2,46)=3.91; p=0.03). The post hoc comparisons revealed that this difference was mainly driven by the reduced number of voxels selective for the animal category in EBa compared to SCv (p=0.02).</p><p>For the results of the same analysis on the 8 different categories see also <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></sec><sec id="s2-2"><title>Binary MVP classification</title><p><xref ref-type="fig" rid="fig3">Figure 3B</xref> represents the results from the average binary classification analyses for each group and every ROI (FDR corrected). In SCv and in EBa the averaged decoding accuracy was significantly higher than chance level in both EVC (SCv: DA = 69%; t<sub>(15)</sub>=6.69, p<sub>FDR</sub><italic> &lt;</italic>0.00001; EBa: DA = 55%; t<sub>(15)</sub>=4.48, p<sub>FDR</sub> = <italic>0.0006</italic>) and VOTC (SCv: DA = 71%; t<sub>(15)</sub>=7.37, p<sub>FDR</sub><italic> &lt;</italic>0.00001; EBa: DA = 57%; t<sub>(15)</sub>=8.00, p<sub>FDR</sub> &lt;<italic>0.0001</italic>). In the SCa the averaged decoding accuracy was significantly higher than the chance level in VOTC (DA = 54%; t<sub>(16)</sub>=4.32, p<sub>FDR</sub> = <italic>0.0006</italic>) but not in EVC (DA = 51%; t<sub>(16)</sub>=1.70, p<sub>FDR</sub> = <italic>0.11</italic>). Moreover, independent sample t-tests revealed higher decoding accuracy values in EBa when compared to SCa in both EVC (t<sub>(31)</sub>=2.52, p<sub>FDR</sub> = <italic>0.017</italic>) and VOTC (t<sub>(31)</sub>=2.08, p<sub>FDR</sub> = <italic>0.046</italic>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Regions of interest and classification results.</title><p>(<bold>A</bold>) Representation of the 2 ROIs in one representative subject’s brain; (<bold>B</bold>) Binary decoding averaged results in early visual cortex (EVC) and ventral occipito-temporal cortex (VOTC) for visual stimuli in sighted (green), auditory stimuli in blind (orange) and auditory stimuli in sighted (blue). ***p&lt;0.001, **p&lt;0.05.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig3-v2.tif"/></fig><p>Results from each binary classification analysis (n = 28) for each group are represented in <xref ref-type="fig" rid="fig4">Figure 4</xref> panel A1 for EVC and in panel B1 for VOTC. The p-values for each t-test is reported in the SI-table 3.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>EVC and VOTC functional profiles.</title><p>(<bold>A1</bold> and <bold>B1</bold>) Binary decoding bar plots. For each group (SCv: top; EBa: center; SCa: bottom) the decoding accuracy from the 28 binary decoding analyses are represented. Each column represents the decoding accuracy value coming from the classification analysis between 2 categories. The 2 dots under each column represent the 2 categories. (<bold>A2</bold> and <bold>B2</bold>) The 28 decoding accuracy values are represented in the form of a dissimilarity matrix. Each column and each row of the matrix represent one category. In each square there is the accuracy value coming from the classification analysis of 2 categories. Blue color means low decoding accuracy values and yellow color means high decoding accuracy values. (<bold>A3</bold> and <bold>B3</bold>) Binary decoding multidimensional scaling (MDS). The categories have been arranged such that their pairwise distances approximately reflect response pattern similarities (dissimilarity measure: accuracy values). Categories placed close together were based on low decoding accuracy values (similar response patterns). Categories arranged far apart generated high decoding accuracy values (different response patterns). The arrangement is unsupervised: it does not presuppose any categorical structure (<xref ref-type="bibr" rid="bib69">Kriegeskorte et al., 2008b</xref>). (<bold>A4</bold> and <bold>B4</bold>) Binary decoding dendrogram. We performed hierarchical cluster analysis (based on the accuracy values) to assess if EVC (A4) and VOTC (B4) response patterns form clusters corresponding to natural categories in the 3 groups (SCv: top; EBa: center; SCa: bottom).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig4-v2.tif"/></fig></sec><sec id="s2-3"><title>RSA: Correlation between the neural dissimilarity matrices of the 3 groups</title><p>We used the accuracy values from the binary classifications to build neural dissimilarity matrices for each subject in EVC (<xref ref-type="fig" rid="fig4">Figure 4</xref> - Panel A2) and in VOTC (<xref ref-type="fig" rid="fig4">Figure 4</xref> - Panel B2). Then, in every ROI we computed the correlations between DSMs for each groups’ pair (i.e. SCv-EBa; SCv-SCa; EBa-SCa).</p><p>In EVC, the permutation test revealed a significant positive correlation only between the SCv and the EBa DSMs (mean r = 0.19; <italic>p<sub>FDR</sub> = 0.0002</italic>), whereas negative correlation emerged from the correlation between the SCv and SCa DSMs (mean r = –.18; <italic>p<sub>FDR</sub> &lt;</italic>0.001) and the correlation between the SCa and the EBa DSMs (mean r = –0.03; <italic>p<sub>FDR</sub> = 0.18</italic>). Moreover, the correlation between SCv and EBa was significantly higher compared to both the correlation between SCv and SCa (mean corr. diff = 0.38; <italic>p<sub>FDR</sub> = 0.008</italic>) and the correlation between SCa and EBa (mean corr. diff = 0.22; <italic>p<sub>FDR</sub> = 0.008</italic>).</p><p>In VOTC, we observed a significant correlation for all the groups’ pairs: SCv and EBa (r = 0.34; <italic>p<sub>FDR</sub> = 0.0002</italic>), SCv and SCa (r = 0.1; <italic>p<sub>FDR</sub> = 0.002</italic>), SCa and EBa (r = 0.1; <italic>p<sub>FDR</sub> = 0.002</italic>). Moreover, the correlation between SCv and EBa was significantly higher compared to both the correlation between SCv and SCa (corr. diff = 0.25; <italic>p<sub>FDR</sub> = 0.008</italic>) and the correlation between SCa and EBa (corr. diff = 0.25; <italic>p<sub>FDR</sub> = 0.008</italic>).</p></sec><sec id="s2-4"><title>Hierarchical clustering analysis on the brain categorical representation</title><p>We implemented this analysis to go beyond the magnitude of correlation values and to qualitatively explore the representational structure of our ROIs in the 3 groups. Using this analysis, we can see which categories are clustered together in the brain representations according to a specified n number of clusters (see <xref ref-type="fig" rid="fig5">Figure 5</xref> for detailed results). In VOTC the most striking results are related to the way the animal category is represented in the EBa group compared to the SCv. In fact, when the hierarchical clustering was stopped at 2 clusters in SCv, we observed a clear living vs non-living distinction. In EBa, instead, the division was between humans and non-humans (including all the non-living categories plus the animals). When the clusters are 3, we see in SCv a separation into (1) non-living; (2) human; (3) animals. In EBa, instead, the animals keep in being clustered with non-living, in a way that the 3 clusters are: (1) Non-living and animals; (2) Human vocalization voices; (3) Human non-vocalization voices. In the case of the 4 clusters, in both SCv and EBa the additional 4th cluster is represented by the manipulable-tools category, while in the EBa the animals remain with the rest of the non-living categories. In the VOTC of SCa group, the structure of the categorical representation is less straightforward, despite the significant correlation with the DSMs of SCv. For example, we cannot clearly discern the distinction into living/non-living, or into humans and animals. However, there are some specific categories such as manipulable-graspable objects, human vocalizations and environmental sounds that show a segregated representation from the others. When we observe the clustering in EVC, we see that there is not a clear categorical clustering in this ROI in none of the groups, with the exception of the SCv in which the human stimuli tend to cluster together.</p><p>Finally, the clustering analysis on the behavioral data (see <xref ref-type="fig" rid="fig5">Figure 5</xref>) revealed a very similar way of clustering the categories in the 2 groups of sighted that in the 4 clusters step show exactly the same structure: (1) Animate categories including animals and humans; (2) Manipulable Objects; (3) Big mechanical objects; (4) Big Environmental category. In the EBa, instead, we find a different clustering structure with the animal and the human categories being separated.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Hierarchical clustering of brain data: VOTC and EVC.</title><p>Hierarchical clustering on the dissimilarity matrices extracted from EVC (left) and VOTC (right) in the three groups. The clustering was repeated three times for each DSM, stopping it at 2, 3 and 4 clusters, respectively. This allows to compare the similarities and the differences of the clusters at the different steps across the groups. In the figure each cluster is represented in a different color. The first line represents 2 clusters (green and red); the second line represents 3 clusters (green, red and pink); finally, the third line represents 4 clusters (green, red, pink and light blue).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Hierarchical clustering of the behavioral data.</title><p>Hierarchical clustering on the dissimilarity matrices built from the behavioral ratings. The clustering was repeated three times for each DSM, stopping it at 2, 3 and 4 clusters, respectively. This allows to compare the similarities and the differences of the clusters at the different steps across the groups. In the figure each cluster is represented in a different color. The first line represents 2 clusters (green and red); the second line represents 3 clusters (green, red and pink); finally the third line represents 4 clusters (green, red, pink and light blue).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig5-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-5"><title>RSA: correlation with representational low-level/behavioral models</title><p>In a behavioral session following the fMRI session, we asked our participants to rate each possible pair of stimuli in the experiment they took part in (either visual or acoustic) and we built three dissimilarity matrices based on their judgments. A visual exploration of the ratings using the dissimilarity matrix visualization revealed a clustering of the stimuli into a main living/non-living distinction, with some sub-clustering such as humans, animals and objects (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The three DSMs were highly correlated (SCa/EBa: r = 0.85, p&lt;<italic>0.001</italic>; SCa/SCv: r = 0.95, p&lt;<italic>0.001</italic>; EBa/SCv: r = 0.89, p&lt;<italic>0.001</italic>), revealing a similar way to group the stimuli across the three groups following mostly a categorical strategy to classify the stimuli. Based on this observation, we used the behavioral matrices as a categorical/high-level model to contrast with the low-level models built on the physical properties of the stimuli (HmaxC1 and pitch models, <xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Representational similarity analysis (RSA) between brain and representational low-level/behavioral models.</title><p>(<bold>A</bold>) In each ROI we computed the brain dissimilarity matrix (DSM) in every subject based on binary decoding of our 8 different categories. In the visual experiment (left) we computed the partial correlation between each subject’s brain DSM and the behavioral DSM from the same group (SCv-Behav.) regressing out the shared correlation with the HmaxC1 model, and vice versa. In the auditory experiment (right) we computed the partial correlation between each subject’s brain DSM (in both Early Blind and Sighted Controls) and the behavioral DSM from the own group (either EBa-Behav. or SCa-Behav.) regressing out the shared correlation with the pitch model, and vice versa. (<bold>B</bold>) Results from the Spearman’s correlation between representational low-level/behavioral models and brain DSMs from both EVC and VOTC. On the left are the results from the visual experiment. Dark green: partial correlation between SCv brain DSM and behavioral model; Light green: Partial correlation between SCv brain DSM and HmaxC1 model. On the right are the results from the auditory experiment in both early blind (EBa) and sighted controls (SCa). Orange: partial correlation between EBa brain DSM and behavioral model; Yellow: Partial correlation between EBa brain DSM and pitch model. Dark blue: partial correlation between SCa brain DSM and behavioral model; Light blue: partial correlation between SCa brain DSM and pitch model. For each ROI and group, the gray background bar represents the reliability of the correlational patterns, which provides an approximate upper bound of the observable correlations between representational low-level/behavioral models and neural data (<xref ref-type="bibr" rid="bib19">Bracci and Op de Beeck, 2016</xref>; <xref ref-type="bibr" rid="bib83">Nili et al., 2014</xref>). Error bars indicate SEM. ***p&lt;0.001, **p&lt;.005, *p&lt;0.05. P values are FDR corrected.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Auditory model selection.</title><p>To select the auditory model we tested different low-level acoustic models on the sighted participants that took part in the auditory experiment. The following models were tested: pitch; harmonicity to noise ratio (HNR); spectral centroid (S.Cent.); the voice frequency band (between 500–2500 Hz) and the sound energy per unit of time (envelope). We extracted these values from our 24 auditory stimuli and we built 5 dissimilarity matrices (DSM) each one based on one acoustic feature. Then, we extracted the brain dissimilarity matrix (1-Pearson’s correlation between each pattern of activity) from the auditory cortex of each sighted subject that took part in the auditory experiment. Here is represented the second-order correlation between each acoustic model and the brain DSMs, ranked from the highest to the lowest (left to right). Since pitch resulted the most represented model in the auditory cortex of sighted participants, we selected this acoustic model to run further analyses in the experiment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig6-figsupp1-v2.tif"/></fig></fig-group><p>In order to better understand the representational content of VOTC and EVC, we computed second-order partial correlations between each ROI’s DSM and our representational models (i.e. behavioral and low-level DSMs) for each participant. <xref ref-type="fig" rid="fig6">Figure 6B</xref> represents the results for the correlations between the brain DSMs in the 3 groups and the representational low-level/behavioral models DSMs (i.e. behavioral, pitch and Hmax-C1 DSMs).</p><p>The permutation test revealed that in SCv the EVC’s DSM was significantly correlated with the Hmax-C1 model (mean r = 0.13, <italic>p<sub>(one-tailed)FDR</sub> = 0.002</italic>) but not with the behavioral model (mean r = 0.03; <italic>p<sub>(one-tailed)FDR</sub> = 0.11</italic>). Even though the correlation was numerically higher with the Hmax-C1 model than with the behavioral model, a paired samples t-test did not reveal a significant difference between the two (t<sub>(15)=</sub>1.24, p<italic><sub>(one-tailed)FDR</sub></italic> = 0.23). The permutation test, showed that VOTC’s DSM, instead, was significantly positively correlated with the behavioral model (mean r = 0.34; <italic>p<sub>FDR</sub> &lt;</italic>0.001) but negative correlated with the Hmax-C1 model (r = –.07; <italic>p<sub>(one-tailed)FDR</sub> = 0.991.</italic>). A paired samples t-test revealed that the difference between the correlation with the two models was significant (t<sub>(15)=</sub>6.71, p<sub>FDR</sub> &lt;0.001).</p><p>In the EBa and SCa groups, EVC’s DSMs were not significantly correlated with neither the behavioral (EBa: mean r = 0.004; <italic>p<sub>(one-tailed)FDR</sub> = </italic>0.47; SCa: mean r = –0.11, p<italic><sub>(one-tailed)FDR</sub></italic> = 0.98) nor the pitch model (EBa: mean r = –0.08; <italic>p<sub>(one-tailed)FDR</sub> = </italic>0.94; SCa: mean r = –0.09, p<italic><sub>(one-tailed)FDR</sub></italic> = 0.98). In contrast, the VOTC’s DSMs were significantly correlated with the behavioral model in EBa (mean r = 0.12, <italic>p<sub>(one-tailed)FDR</sub> = 0.02</italic>) but not in SCa (mean r = 0.06; <italic>p<sub>(one-tailed)FDR</sub> = 0.17</italic>). Finally the VOTC’s DSMs were not significantly correlated with the pitch model neither in EBa(mean r = –0.03; p<italic><sub>(one-tailed)FDR</sub></italic> = 0.49), nor in SCa(mean r = 0.03; p<italic><sub>(one-tailed)FDR</sub></italic> = 0.39); In addition, a 2 Groups (EBa/SCa) X 2 Models (behavioral/pitch) ANOVA in VOTC revealed a significant main effect of Model (F<sub>(1,31)</sub>=11.37, p=0.002) and a significant interaction Group X Model (F<sub>(1,31)</sub>=4.03, p=0.05), whereas the main effect of Group (F<sub>(1,31)</sub>=2.38<sup>−4</sup>, p=0.98), was non-significant. A Bonferroni post-hoc test on the main effect of Model confirmed that the correlation was significantly higher for the behavioral model compared to the pitch model (t = 3.18, p=<italic>0.003</italic>). However, the Bonferroni post-hoc test on the interaction Group*Model revealed that the difference between behavioral and pitch models was significant only in EBa (t = 3.8, p=<italic>0.004</italic>).</p><p>For completeness of results, we report here also the correlation results before regressing out the partial correlation of the behavioral/low-level models from each other. In ECV, the mean correlation with the behavioral model was: r = 0.2 in SCv, r = 0.02 in EBa and r=–0.07 in SCa. In ECV, the mean correlation with the low level/model was: r = 0.21 in SCv (HmaxC1), r=–0.09 in EBa (pitch) and r = –0.06 in SCa (pitch). In VOTC, the mean correlation with the behavioral model was: r = 0.42 in SCv, r = 0.12 in EBa and r = 0.04 in SCa. In VOTC, the mean correlation with the low level/model was: r = 0.15 in SCv (HmaxC1), r = –0.08 in EBa (pitch) and r = 0.005 in SCa (pitch).</p></sec><sec id="s2-6"><title>RSA: Inter-subjects correlation</title><p>We run this analysis to understand how variable was the brain representation in VOTC across subjects belonging either to the same group or to different groups. Since we have 3 groups, this analysis resulted in 6 different correlation values: 3 values for the 3 within group correlation conditions (SCv; EBa; SCa) and 3 values for the 3 between groups correlation conditions (i.e. SCv-EBa; SCv-SCa; EBa-SCa). Results are represented in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>VOTC Inter-subject correlation within and between groups.</title><p>Upper panel represents the correlation matrix between the VOTC brain DSM of each subject with all the other subjects (from the same group and from different groups). The mean correlation of each within- and between-groups combination is reported in the bottom panel (bar graphs). The straight line ending with a square represents the average of the correlation between subjects from the same group (i.e. within groups conditions: SCv, EBa, SCa), the dotted line ending with the circle represents the average of the correlation between subjects from different groups (i.e. between groups conditions: SCv-EBa/SCv-SCa/EBa SCa). The mean correlations are ranked from the higher to the lower inter-subject correlation values.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig7-v2.tif"/></fig><p>The permutation test revealed that the correlation between subjects’ DSMs in the within group condition was significant in SCv (r = 0.42; <italic>p<sub>FDR</sub> &lt;</italic>0.001) and EBa (r = 0.10; <italic>p<sub>FDR</sub> &lt;</italic>0.001), whereas it was not significant in SCa (r = –.03; <italic>p<sub>FDR</sub> = </italic>0.98). Moreover, the correlation between subjects’ DSMs was significant in all the three between groups conditions (SCv-EBa: r = 0.17, <italic>p<sub>FDR</sub> &lt;</italic>0.001; SCv-SCa: r = 0.04, <italic>p<sub>FDR</sub> = 0.002</italic>; EBa-SCa: r = 0.02; <italic>p<sub>FDR</sub> = </italic>0.04). When we ranked the correlations values (<xref ref-type="fig" rid="fig7">Figure 7</xref>) we observed that the highest inter-subject correlation is the within SCv group condition, which was significantly higher compared to all the other five conditions. It was followed by inter-subject correlation between SCv and EBa group and the within EBa group correlation. Interestingly, both the between groups SCv-EBa and the within group EBa correlations were significantly higher compared to the last 3 inter-subjects correlation’s values (between SCv-SCa; between EBa-SCa; within SCa).</p></sec><sec id="s2-7"><title>Representational connectivity analysis</title><p><xref ref-type="fig" rid="fig8">Figure 8</xref> represents the results from the representational connectivity analysis in VOTC. The permutation analysis highlighted that the representational connectivity profile of VOTC with the rest of the brain is significantly correlated between all pairs of groups (SCv-EBa: mean r = 0.18, p<sub>FDR</sub> = 0.001; SCv-SCa: mean r = 0.14, p<sub>FDR</sub> &lt;0.001; EBa-SCa: mean r = 0.16, p<sub>FDR</sub> &lt;0.001), and with no difference between groups’ pairs.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Representational connectivity.</title><p>(<bold>A</bold>) Representation of the z-normalized correlation values between the dissimilarity matrix of the three VOTC seeds (left: Fusiform gyrus, center: Parahippocampal gyrus, Right: Infero-Temporal cortex) and the dissimilarity matrix of 27 parcels covering the rest of the cortex in the three groups (top: SCv, central: EBa, bottom: SCa). Blue color represents low correlation with the ROI seed; yellow color represents high correlation with the ROI seed. (<bold>B</bold>) The normalized correlation values are represented in format of one matrix for each group. This connectivity profile is correlated between groups. SCv: sighted control-vision; EBa: early blind-audition; SCa: sighted control-audition.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-fig8-v2.tif"/></fig><p>We performed the same analysis also in EVC. In this case the permutation analysis revealed a significant correlation only between the representational connectivity profile of the two groups of sighted: SCv and SCa (mean r = 0.17, p<sub>FDR</sub> &lt;0.001), whereas the correlation between the EBa was not significant neither with SCv (mean r = 0.06, p<sub>FDR</sub> = 0.12) nor with SCa (mean r = 0.06, p<sub>FDR</sub> = 0.11). Moreover, the correlation between SCv and SCa was significantly higher than both, the correlation between SCv and EBa (mean diff = 0.11, p<sub>FDR</sub> = 0.01) and the correlation between SCa and EBa (mean diff = 0.11, p<sub>FDR</sub> = 0.01).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In our study, we demonstrate that VOTC reliably encodes the categorical membership of sounds from eight different categories in sighted and blind people, using a topography (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), representational format (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and a representational connectivity profile (<xref ref-type="fig" rid="fig8">Figure 8</xref>) partially similar to the one observed in response to images of similar categories in vision.</p><p>Previous studies using linguistic stimuli had already suggested that VOTC may actually represent categorical information in a more abstracted fashion than previously thought (<xref ref-type="bibr" rid="bib45">Handjaras et al., 2016</xref>; <xref ref-type="bibr" rid="bib17">Borghesani et al., 2016</xref>; <xref ref-type="bibr" rid="bib116">Striem-Amit et al., 2018b</xref>; <xref ref-type="bibr" rid="bib93">Peelen and Downing, 2017</xref>). However, even if the use of words is very useful in the investigation of pre-existing representation of concepts (<xref ref-type="bibr" rid="bib77">Martin et al., 2017</xref>), it prevents the investigation of a bottom-up perceptual processing. By contrast, in our study we used sensory-related non-linguistic stimuli (i.e. sounds) in order to investigate both the sensory (acoustic) and categorical nature of the representation implemented in VOTC. To the limit of our knowledge, only one recent study investigated the macroscopic functional organization of VOTC during categorical processing of auditory and visual stimuli in sighted and in blind individuals (<xref ref-type="bibr" rid="bib121">van den Hurk et al., 2017</xref>). They found that it is possible to predict the global large-scale distributed pattern of activity generated by different categories presented visually in sighted using the pattern of activity generated by the same categories presented acoustically in early blind. Relying on a different analytical stream, focusing on representational matrices extracted from pairwise decoding of our eight categories, our study confirms and extends those findings by showing that VOTC reliably encodes sound categories in blind people using a representational structure relatively similar to the one found in vision.</p><p>Our study goes beyond previous results in at least six significant ways. First, our results demonstrate that VOTC shows categorical responses to sounds in the sighted and the blind in a representational format partially similar to the one elicited by images of the same categories in sighted people (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Observation of a similar categorical representational structure in VOTC for sounds and images in sighted people is crucial to support the idea that the intrinsic categorical organization of VOTC might be partially independent from vision even in sighted and that such intrinsic multisensory functional scaffolding may constrain the way crossmodal plasticity expresses in early blind people. Second, we observed that blind people show higher decoding accuracies and higher inter-subject consistency in the representation of auditory categories, and that the representational structure of visual categories in sighted was significantly closer to the structure of the auditory categories in blind than in sighted group (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="fig" rid="fig7">Figure 7</xref>). This points to the idea that in absence of feed-forward visual input, VOTC increases its intrinsic representation of non-visual information. Third, VOTC shows similar large-scale representational connectivity profiles when processing images in sighted and sounds in sighted and blind people (see <xref ref-type="fig" rid="fig8">Figure 8</xref>). This result provides strong support to the general hypothesis that the functional tuning of a region is determined by large-scale connectivity patterns with regions involved in similar coding strategies (<xref ref-type="bibr" rid="bib12">Behrens and Sporns, 2012</xref>; <xref ref-type="bibr" rid="bib74">Mahon and Caramazza, 2011</xref>; <xref ref-type="bibr" rid="bib90">Passingham et al., 2002</xref>). Fourth, our design allowed us to investigate which dimension of our stimuli, either categorical membership or acoustic properties, may determine the response properties of VOTC to sounds. By harnessing the opportunities provided by representational similarity analysis, we demonstrate that categorical membership is the main factor that predicts the representational structure of sounds in VOTC in blind people (see <xref ref-type="fig" rid="fig6">Figure 6</xref>), rather than lower-level acoustical attributes of sounds that are at least partially at the basis of category selectivity in the temporal cortex (<xref ref-type="bibr" rid="bib80">Moerel et al., 2012</xref>). These results elucidate for the first time the computational characteristics that determine the categorical response for sounds in VOTC. Fifth, we provided a qualitative exploration of the structure of the categorical representation in the VOTC. The between-groups Jaccard similarity analysis revealed a domain–by–modality interaction (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>), with the big objects and places category showing an higher degree of similarity between the auditory and visual representations compared to the other categories. In addition, both, the hierarchical clustering and the within-group Jaccard similarity analysis highlighted a domain-by-sensory experience interaction (see <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig2">Figure 2B</xref>), with the animal category represented differently in blind compared to sighted subjects (<xref ref-type="bibr" rid="bib15">Bi et al., 2016</xref>; <xref ref-type="bibr" rid="bib124">Wang et al., 2015</xref>). Finally, our study discloses that categorical membership is encoded in the EVC of blind people only, using a representational format that does not relate neither to the acoustic nor to the categorical structure of our stimuli, suggesting different mechanisms of reorganization in this posterior occipital region.</p><p>﻿ Different visual categories elicit distinct distributed responses in VOTC using a remarkable topographic consistency across individuals (<xref ref-type="bibr" rid="bib56">Julian et al., 2012</xref>; <xref ref-type="bibr" rid="bib59">Kanwisher, 2010</xref>). It was suggested that regular visual properties specific to each category like retinotopic eccentricity biases (<xref ref-type="bibr" rid="bib44">Gomez et al., 2019</xref>; <xref ref-type="bibr" rid="bib75">Malach et al., 2002</xref>), curvature (<xref ref-type="bibr" rid="bib81">Nasr et al., 2014</xref>) or spatial frequencies (<xref ref-type="bibr" rid="bib100">Rajimehr et al., 2011</xref>) could drive the development of categorical response in VOTC for visual information (<xref ref-type="bibr" rid="bib4">Andrews et al., 2010</xref>; <xref ref-type="bibr" rid="bib7">Baldassi et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Bracci et al., 2018</xref>; <xref ref-type="bibr" rid="bib104">Rice et al., 2014</xref>; see <xref ref-type="bibr" rid="bib86">Op de Beeck et al., 2019</xref> for a recent review on the emergence of category selectivity in VOTC). For instance, the parahippocampal place area (PPA) and the fusiform face area (FFA) receive dominant inputs from downstream regions of the visual system with differential selectivity for high vs low spatial frequencies and peripheral vs. foveal inputs, causing them to respond differentially to place and face stimuli (<xref ref-type="bibr" rid="bib72">Levy et al., 2001</xref>). These biases for specific visual attributes could be present at birth and represent a proto-organization driving the development of the categorical responses of VOTC based on experience (<xref ref-type="bibr" rid="bib5">Arcaro and Livingstone, 2017</xref>; <xref ref-type="bibr" rid="bib44">Gomez et al., 2019</xref>). For instance, a proto-eccentricity map is evident early in development (<xref ref-type="bibr" rid="bib5">Arcaro and Livingstone, 2017</xref>) and monkeys trained early in life to discriminate different categories varying in their curvilinearity/rectilinearity develop distinct and consistent functional clusters for these categories (<xref ref-type="bibr" rid="bib112">Srihasam et al., 2014</xref>). Further, adults who had intensive visual experience with Pokémon early in life demonstrate distinct distributed cortical responses to this trained visual category with a systematic location supposed to be based on retinal eccentricity (<xref ref-type="bibr" rid="bib44">Gomez et al., 2019</xref>).</p><p>﻿Although our results by no means disprove the observations that inherent visual biases can influence the development of the functional topography of high-level vision (<xref ref-type="bibr" rid="bib44">Gomez et al., 2019</xref>; <xref ref-type="bibr" rid="bib47">Hasson et al., 2002</xref>; <xref ref-type="bibr" rid="bib81">Nasr et al., 2014</xref>), our data however suggest that category membership independently of visual attributes is also a key developmental factor that determines the consistent functional topography of the VOTC. Our study demonstrates that VOTC responds to sounds using a similar distributed functional profile to the one found in response to vision, even in case of people that have never had visual experience.</p><p>By orthogonalizing category membership and visual features of visual stimuli, previous studies reported a residual categorical effect in VOTC, highlighting how some of the variance in the neural data of VOTC might be explained by high-level categorical properties of the stimuli even when the contribution of the basic low-level features has been controlled for (<xref ref-type="bibr" rid="bib19">Bracci and Op de Beeck, 2016</xref>; <xref ref-type="bibr" rid="bib57">Kaiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib98">Proklova et al., 2016</xref>). Category-selectivity has also been observed in VOTC during semantic tasks when word stimuli were used, suggesting an involvement of the occipito-temporal cortex in the retrieval of category-specific conceptual information (<xref ref-type="bibr" rid="bib45">Handjaras et al., 2016</xref>; <xref ref-type="bibr" rid="bib17">Borghesani et al., 2016</xref>; <xref ref-type="bibr" rid="bib93">Peelen and Downing, 2017</xref>). Moreover, previous research has shown that learning to associate semantic features (e.g., ‘floats’) and spatial contextual associations (e.g., ‘found in gardens’) with novel objects influences VOTC representations, such that objects with contextual connections exhibited higher pattern similarity after learning in association with a reduction in pattern information about the object's visual features (<xref ref-type="bibr" rid="bib26">Clarke et al., 2016</xref>).</p><p>Even if we cannot fully exclude that the processing of auditory information in the VOTC of sighted people could be the by-product of the visual imagery triggered by the non-visual stimulation (<xref ref-type="bibr" rid="bib23">Cichy et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Kosslyn et al., 1995</xref>; <xref ref-type="bibr" rid="bib101">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="bib111">Slotnick et al., 2005</xref>; <xref ref-type="bibr" rid="bib113">Stokes et al., 2009</xref>), we find it unlikely. First, we purposely included two separate groups of sighted people, each one performing the experiment in one modality only, in order to minimize the influence of having heard or seen the stimuli in the other modality in the context of the experiment. Also, we used a fast event-related design that restricted the time window to build a visual image of the actual sound since the next sound was presented quickly after (<xref ref-type="bibr" rid="bib73">Logie, 1989</xref>). Moreover, we would expect that visual imagery would also triggers information to be processed in posterior occipital regions (<xref ref-type="bibr" rid="bib66">Kosslyn et al., 1999</xref>). Instead, we found that EVC does not discriminate the different sounds in the sighted group (<xref ref-type="fig" rid="fig3">Figure 3B</xref> and <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Finally, to further test the visual imagery hypothesis, we correlated the brain representational space of EVC in SCa with low-level visual model (i.e. HmaxC1). A significant positive correlation between the two would be in support of the presence of visual imagery mechanism when sighted people hear sounds of categories. We found, instead, a non-significant negative correlation, making the visual imagery hypothesis further unlikely to explain our results.</p><p>Comparing blind and sighted individuals arguably provides the strongest evidence for the hypothesis that category-selective regions traditionally considered to be ‘high-level visual regions’ can develop independently of visual experience. Interestingly, we found that the decoding accuracy for the auditory categories in VOTC is significantly higher in the early blind compared to the sighted control group (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). In addition, the correlation between the topographic distribution of categorical response observed in VOTC was stronger in blind versus sighted people (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Moreover, the representational structure of visual categories in sighted was significantly closer to the structure of the auditory categories in blind than in sighted (<xref ref-type="fig" rid="fig4">Figure 4A2</xref>). Finally, the representation of the auditory stimuli in VOTC is more similar between blind than between sighted subjects (<xref ref-type="fig" rid="fig7">Figure 7</xref>), showing an increased inter-subject stability of the representation in case of early visual deprivation. All together, these results not only demonstrate that a categorical organization similar to the one found in vision could emerge in VOTC in absence of visual experience, but also that such categorical response to sounds is actually enhanced and more stable in congenitally blind people.</p><p>Several studies have shown that in absence of vision, the occipital cortex enhances its response to non-visual information processing (<xref ref-type="bibr" rid="bib28">Collignon et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Collignon et al., 2011</xref>; <xref ref-type="bibr" rid="bib106">Sadato et al., 1998</xref>). However, people debate on the mechanistic principles guiding the expression of this crossmodal plasticity. For instance, it was suggested that early visual deprivation changes the computational nature of the occipital cortex which would reorganize itself for higher-level functions, distant from the ones typically implemented for visual stimuli in the same region (<xref ref-type="bibr" rid="bib11">Bedny, 2017</xref>). In contrast with this view, our results demonstrate that the expression of crossmodal plasticity, at least in VOTC (see differences in EVC below), is constrained by the inherent categorical structure endowed in this region. First, we highlighted remarkably similar functional profile of VOTC for visual and auditory stimuli in sighted and in early blind individuals (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In addition, we showed that VOTC is encoding a similar categorical dimension of the stimuli across different inputs of presentation and different visual experiences (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). In support of such idea, we recently demonstrated that the involvement of right dorsal occipital region for arithmetic processing in blind people actually relates to the intrinsic ‘spatial’ nature of these regions, a process involved in specific arithmetic computation (e.g. subtraction but not multiplication) (<xref ref-type="bibr" rid="bib29">Crollen et al., 2019</xref>). Similarly, the involvement of VOTC during ‘language’ as observed in previous studies (<xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>; <xref ref-type="bibr" rid="bib21">Burton et al., 2006</xref>; <xref ref-type="bibr" rid="bib62">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib70">Lane et al., 2015</xref>; <xref ref-type="bibr" rid="bib105">Röder et al., 2002</xref>) may relate to the fact that some level of representation involved in language (e.g. semantic) can be intrinsically encoded in VOTC as supported by the current results (<xref ref-type="bibr" rid="bib54">Huth et al., 2016</xref>). In fact, we suggest that VOTC regions have innate predispositions relevant to important categorical distinctions that cause category-selective patches to emerge regardless of sensory experience. Why would the ‘visual’ system embed representation of categories independently of their perceptual features? One argument might be that items from a particular broad category (e.g. inanimate) are so diverse that they may not share systematic perceptual features and therefore a higher-level of representation, partially abstracted from vision, might prove important. Indeed, we gather evidence in support of an extension of the intrinsic categorical organization of VOTC that is already partially independent from vision in sighted. This finding represents an important step forward in understanding how experience and intrinsic constraints interact in shaping the functional properties of VOTC. An intriguing possibility raised by our results is that the crossmodal plasticity observed in early blind individuals may actually serve to maintain the functional homeostasis of occipital regions.</p><p>﻿ What would be the mechanism driving the preservation of the categorical organization of VOTC in case of congenital blindness? It is thought that the specific topographic location of a selective brain functions is constrained by an innate profile of functional and structural connections with extrinsic brain regions (<xref ref-type="bibr" rid="bib90">Passingham et al., 2002</xref>).﻿ Since the main fiber tracts are already present in full-term human neonates (<xref ref-type="bibr" rid="bib38">Dubois et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Dubois et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Kennedy et al., 1999</xref>; <xref ref-type="bibr" rid="bib67">Kostović and Judaš, 2010</xref>; <xref ref-type="bibr" rid="bib76">Marín-Padilla, 2011</xref>; <xref ref-type="bibr" rid="bib118">Takahashi et al., 2011</xref>), such initial connectome may at least partly drive the functional development of a specific area. Supporting this hypothesis, the visual word form area (VWFA) in VOTC (<xref ref-type="bibr" rid="bib79">McCandliss et al., 2003</xref>) shows robust and specific anatomical connectivity to EVC and to frontotemporal language networks and this connectivity fingerprint can predict the location of VWFA even before a child learn to read (<xref ref-type="bibr" rid="bib108">Saygin et al., 2016</xref>). Similarly, anatomical connectivity profile can predict the location of the fusiform face area (FFA) (<xref ref-type="bibr" rid="bib107">Saygin et al., 2012</xref>). In addition to intra-occipital connections, FFA has a direct structural connection with the temporal voice area (TVA) in the superior temporal sulcus (<xref ref-type="bibr" rid="bib14">Benetti et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Blank et al., 2011</xref>) thought to support similar computations applied on faces and voices as well as their integration (<xref ref-type="bibr" rid="bib122">von Kriegstein et al., 2005</xref>). Interestingly, recent studies suggested that the maintenance of those selective structural connections between TVA and FFA explains the preferential recruitment of TVA for face processing in congenitally deaf people (<xref ref-type="bibr" rid="bib14">Benetti et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Benetti et al., 2017</xref>). This TVA-FFA connectivity may explain why voices preferentially map slightly more lateral to the mid-fusiform sulcus (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Similarly, sounds of big objects or natural scenes preferentially recruit more mesial VOTC regions (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), overlapping with the parahippocampal place area, potentially due to the preserved pattern of structural connectivity of those regions in blind people (<xref ref-type="bibr" rid="bib125">Wang et al., 2017</xref>). The existence of these innate large-scale brain connections that are specific for each region supporting separate categorical domain may provide the structural scaffolding on which crossmodal inputs capitalize to reach VOTC in both sighted and blind people, potentially through feed-back connections. Indeed, it has been shown that the main white matter tracks including those involving occipital regions are not significantly different between blind and sighted individuals (<xref ref-type="bibr" rid="bib110">Shimony et al., 2006</xref>). In EB, the absence of competitive visual inputs typically coming from feed-forward inputs from EVC may actually trigger an enhanced weighting of those feed-back inter-modal connection leading to an extension of selective categorical response to sounds in VOTC, as observed in the current study. Our results provide crucial support for this ‘biased connectivity’ hypothesis (<xref ref-type="bibr" rid="bib46">Hannagan et al., 2015</xref>; <xref ref-type="bibr" rid="bib74">Mahon and Caramazza, 2011</xref>) showing that VOTC subregions are part of a large-scale functional network representing categorical information in a format that is at least partially independent from the modality of the stimuli presentation and from the visual experience.</p><p>Even though the categorical representation of VOTC appears, to a certain degree, immune to input modality and visual experience, there are also several differences emerging from the categorical representation of sight and sounds in the sighted and blind. Previous studies already suggested that intrinsic characteristics of objects belonging to different categories might drive different representations in the VOTC of the blind (<xref ref-type="bibr" rid="bib15">Bi et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">Büchel, 2003</xref>; <xref ref-type="bibr" rid="bib124">Wang et al., 2015</xref>). In line with this idea, the between-groups Jaccard similarity analysis (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>) revealed a domain–by–modality interaction, with the big objects and places categories showing the highest degree of similarity between the vision and audition (both in blind and in sighted). In contrast, the lowest topographical consistency between groups was found for the animal category. We found that in the early blind group the number of voxels selective for animals is reduced compared to the other categories (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>), suggesting that the animal category is under represented in the VOTC of early blind. Our hierarchical clustering analyses (see <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) also highlight a reduced animate/inanimate division in the EBa group, with the animal and the humans categories not clustering together and the animals being represented more like tools or big objects in the EBa. Interestingly, this is the case in both the categorical representation of VOTC (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and the behavioral evaluation of our stimuli made by blind individuals (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). An explanation for this effect could be the different way blind and sighted individuals might have in perceiving and interacting with animals. In fact, if we exclude pets (only 1 out of the six animals we included in this study), sighted individuals normally perceive the animacy of animals (such as bird, donkey, horse etc.) mostly throughout vision (either in real life or in pictures/movies). Blind people, instead, do normally learn the peculiar shape of each animal touching static miniature models of them. Moreover, when blind people hear the sounds of these animals without seeing them, they might combine these sounds with the rest of the environmental sounds, and this is indeed what we see in the behavioral ratings, in which only blind subjects cluster together animals and big environmental sounds. These results therefore reveal that the modality of presentation and/or the visual experience do affect the qualitative structure of the categorical representation in VOTC, and this effect is stronger for some categories (i.e. animals) compared to others (i.e. inanimate).</p><p>A different profile emerged from EVC. First, sound categories could be decoded in the EVC of EB (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) but not of the SC. In addition, the representational structure of EVC for sounds correlated to the one found in vision only in EB (<xref ref-type="fig" rid="fig4">Figure 4A2</xref>). However, neither the categorical membership nor the acoustic attributes of sounds correlated with the representational structure found in the EVC of EB (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). A possible explanation for this result is that the posterior part of the occipital cortex in EB is the region that distances itself the most from the native computation it typically implements (<xref ref-type="bibr" rid="bib15">Bi et al., 2016</xref>; <xref ref-type="bibr" rid="bib124">Wang et al., 2015</xref>). In support of this possibility, the representational connectivity profile of EVC in EBa did not show any similarity with the one of sighted (neither SCv nor SCa). Because this area has a native computation that does not easily transfer to another sensory modality (i.e. low-level vision), it may therefore rewire itself for distant functions (<xref ref-type="bibr" rid="bib11">Bedny, 2017</xref>). Some studies, for instance reported an involvement of EVC in high-level linguistic or memory tasks (<xref ref-type="bibr" rid="bib120">Van Ackeren et al., 2018</xref>; <xref ref-type="bibr" rid="bib1">Amedi et al., 2003</xref>; <xref ref-type="bibr" rid="bib10">Bedny et al., 2011</xref>). However, as demonstrated here, the categorical membership of sounds, which may be a proxy for semantic representation, does not explain the representational structure of EVC in our study. It would be interesting to investigate whether models based on linguistic properties such as word frequency or distributional statistic in language corpus (<xref ref-type="bibr" rid="bib8">Baroni et al., 2009</xref>) would, at least partially, explain the enhanced information that we found in EVC of EB. However, our design does not allow us to implement this analysis because the language-statistic DSM based on our stimuli space highly correlate with categorical models. Future studies should investigate this point using a set of stimuli in which the categorical and the linguistic dimensions should be orthogonalized. A further limitation of our study is the limited number of brain regions that we investigated. Since the experimental design and analyses we implemented were a priori focused on VOTC (target region) and EVC (as a control region), it is possible that other brain areas might show either similar or different representation across modalities and groups. In particular, since the brain is a highly interconnected organ (<xref ref-type="bibr" rid="bib30">de Pasquale et al., 2018</xref>), it is unlikely that early visual deprivation would affect exclusively a portion of the occipital cortex leaving the rest of the functional network unaffected. It would be of particular interest to investigate whether the reorganization of the visual cortex occurs together with changes in brain regions coding for the remaining senses, such as temporal regions typically coding for auditory stimuli (<xref ref-type="bibr" rid="bib78">Mattioni et al., 2018</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Thirty-four participants completed the auditory version of the fMRI study: 17 early blinds (EBa; 10F) and 17 sighted controls (SCa; 6F). An additional group of 16 sighted participants (SCv; 8F) performed the visual version of the fMRI experiment. All the blind participants lost sight at birth or before 4 years of age and all of them reported not having visual memories and never used vision functionally (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). The three groups were age (range 20–67 years, mean ± SD: 33.29 ± 10.24 for EBa subjects, respectively 23–63, 34.12 ± 8.69 for SCa subjects, and 23–51, 30.88 ± 7.24 for SCv subjects) and gender (χ<sup>2</sup> (2,50)=1.92, p=0.38) matched. One blind subject performed only 2 out of the 5 runs in the fMRI due to claustrophobia; because of that we excluded his data. All subjects were blindfolded during the auditory task and were instructed to keep the eyes closed during the entire duration of the experiment. Participants received monetary compensation for their participation. The ethical committee of the University of Trento approved this study (protocol 2014–007) and participants gave their informed consent before participation.</p></sec><sec id="s4-2"><title>Stimuli</title><p>We decided to use sounds and images, instead of words, because we wanted to access and model the bottom-up cascade of sensory processing starting from the low-level sensory processing up to the more conceptual level. This methodological decision was crucial in order to assess what level of sound representation is implemented in VOTC of blind and sighted individuals.</p><p>A preliminary experiment was carried out in order to select the auditory stimuli. Ten participants who did not participate in the main experiment were presented with 4 different versions of 80 acoustic stimuli from 8 different categories (human vocalization, human non-vocalization, birds, mammals, tools, graspable objects, environmental scenes, big mechanical objects). We asked the participants to name the sound and then to rate, from 1 to 7, how representative the sound was of its category. We selected only the stimuli that were recognized with at least 80% accuracy, and among those, we choose for each category the 3 most representative sounds for a total of 24 acoustical stimuli in the final set (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). All sounds were collected from the database <italic>Freesound</italic> (<ext-link ext-link-type="uri" xlink:href="https://freesound.org">https://freesound.org</ext-link>), except for the human vocalizations that were recorded in the lab. The sounds were edited and analysed using the softwares <italic>Audacity</italic> (<ext-link ext-link-type="uri" xlink:href="http://www.audacityteam.org">http://www.audacityteam.org</ext-link>) and Praat (<ext-link ext-link-type="uri" xlink:href="http://www.fon.hum.uva.nl/praat/">http://www.fon.hum.uva.nl/praat/</ext-link>). Each mono-sound (44,100 Hz sampling rate) was 2 s long (100msec fade in/out) and amplitude-normalized using root mean square (RMS) method.</p><p>The final acoustic stimulus set included 24 sounds from 8 different categories (human vocalization, human non-vocalization, birds, mammals, tools, graspable objects, environmental scenes, big mechanical objects) that could be reduced to 4 superordinate categories (human, animals, manipulable objects, big objects/places) (see <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><p>We created a visual version of the stimuli set. The images for the visual experiment were colored pictures collected from Internet and edited using <italic>GIMP</italic> (<ext-link ext-link-type="uri" xlink:href="https://www.gimp.org">https://www.gimp.org</ext-link>). Images were placed on a gray (129 RGB) 400 × 400 pixels background.</p></sec><sec id="s4-3"><title>Procedure</title><p>The experimental session was divided into two parts: first the subjects underwent the fMRI experiment and then they performed a behavioral rating judgment task on the same stimuli used in the fMRI experiment.</p></sec><sec id="s4-4"><title>Similarity rating</title><p>The behavioral experiment aimed to create individual behavioral dissimilarity matrices to understand how the participants perceived the similarity of our stimulus space. Due to practical constraints, only a subset of our participants underwent the behavioral experiment (15 EBa, 11 SCa, and 9 SCv). We created each possible pair from the 24 stimuli set leading to a total of 276 pairs of stimuli. In the auditory experiment, participants heard each sound of a pair sequentially and were asked to judge from 1 to 7 how similar the two stimuli producing these sounds were. In the visual experiment, we presented each pair of stimuli on a screen to the participants and we asked them to judge from 1 to 7 how similar the two stimuli were. Since their rating was strongly based on the categorical features of the stimuli, we used the data from the behavioral experiment to build the categorical models for the representational similarity analysis (see the section ‘<italic>Representational similarity analysis: correlation with representational low-level/behavioral models’).</italic></p></sec><sec id="s4-5"><title>fMRI experiment</title><p>Each participant took part in only one experiment, either in the auditory or in the visual version. We decided to include two separate groups of sighted people, one for each modality, for two crucial reasons. First, we wanted to limit as much as possible the possibility of triggering mental imagery from one modality to the other. Second, since cross-group comparisons of representational dissimilarity analyses represent a core component of our analyses stream, we wanted to ensure a cross-group variance comparable between the blind versus the sighted and the sighted in audition versus the sighted in vision.</p><p>The procedure for the two experiments was highly similar (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><p>Before entering the scanner, all the stimuli (either auditory or visual) were presented to each participant to ensure perfect recognition. In the fMRI experiment each trial consisted of the same stimulus repeated twice. Rarely (8% of the occurrences), a trial was made up of two different consecutive stimuli (catch trials). Only in this case participants were asked to press a key with the right index finger if the second stimulus belonged to the living category and with their right middle finger if the second stimulus belonged to the non-living category. This procedure ensured that the participants attended and processed the stimuli. In the auditory experiment, each pair of stimuli lasted 4 s (2 s per stimulus) and the inter-stimulus interval between one pair and the next was 2 s long for a total of 6 s for each trial (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the visual experiment, each pair of stimuli lasted 2 s (1 s per stimulus) and the inter-stimulus interval between one pair and the next was 2 s long for a total of 4 s for each trial (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><p>The use of a ‘‘quick’’ event-related fMRI paradigm balances the need for separable hemodynamic responses and the need for presenting many stimuli in the limited time-span of the fMRI experiment. Within both the auditory and the visual fMRI sessions, participants underwent five runs. Each run contained 3 repetitions of each of the 24 stimuli, eight catch trials and two 20s-long rest periods (one in the middle and another at the end of the run). The total duration of each run was 8 min and 40 s for the auditory experiment and 6 min for the visual experiment. For each run, the presentation of trials was pseudo-randomized: two stimuli from the same category were never presented in subsequent trials. The stimulus delivery was controlled using the Psychophysics toolbox implemented in Matlab R2012a (The MathWorks).</p></sec><sec id="s4-6"><title>fMRI data acquisition and preprocessing</title><p>We acquired our data on a 4T Bruker Biospin MedSpec equipped with an eight-channel birdcage head coil. Functional images were acquired with a T2*-weighted gradient-recalled echo-planar imaging (EPI) sequence (TR, 2000 ms; TE, 28 ms; flip angle, 73°; resolution, 3 × 3 mm<sup>3</sup>; 30 transverses slices in interleaved ascending order; 3 mm slice thickness; field of view (FoV) 192 × 192 mm<sup>2</sup>). The four initial scans were discarded for steady-state magnetization. Before each EPI run, we performed an additional scan to measure the point-spread function (PSF) of the acquired sequence, including fat saturation, which served for distortion correction that is expected with high-field imaging.</p><p>A structural T1-weighted 3D magnetization prepared rapid gradient echo sequence was also acquired for each subject (MP-RAGE; voxel size 1 × 1×1 mm<sup>3</sup>; GRAPPA acquisition with an acceleration factor of 2; TR 2700 ms; TE 4,18 ms; TI (inversion time) 1020 ms; FoV 256 mm; 176 slices).</p><p>To correct for distortions in geometry and intensity in the EPI images, we applied distortion correction on the basis of the PSF data acquired before the EPI scans (<xref ref-type="bibr" rid="bib130">Zeng and Constable, 2002</xref>). Raw functional images were pre-processed and analyzed with SPM8 (Welcome Trust Centre for Neuroimaging London, UK (<ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/software/spm8/">https://www.fil.ion.ucl.ac.uk/spm/software/spm8/</ext-link>) implemented in MATLAB R2013b (MathWorks). Pre-processing included slice-timing correction using the middle slice as reference, the application of temporally high-pass filtered at 128 Hz and motion correction.</p></sec><sec id="s4-7"><title>Regions of interest</title><p>Since we were interested in the brain representation of different categories we decided to focus on the ventro-occipito temporal cortex as a whole. This region is well known to contain several distinct macroscopic brain regions known to prefer a specific category of visual objects including faces, places, body parts, small artificial objects, etc. (<xref ref-type="bibr" rid="bib59">Kanwisher, 2010</xref>). We decided to focus our analyses on a full mask of VOTC, and not in specific sub-parcels because we were interested in looking at the categorical representation across categories and not at the preference of a specific category compared to the others. Our study therefore builds upon the paradigm shift of viewing VOTC as a distributed categorical system rather than a sum of isolated functionally specific areas, which reframes how we should expect to understand those areas (<xref ref-type="bibr" rid="bib48">Haxby et al., 2001</xref>). In fact, our main aim was to investigate how sensory input channel and visual experience impact on the general representation of different categories in the brain. Looking at one specific category-selective region at time would not allow us to address this specific question, since we would expect an imbalanced representation of the preferred category compared to the others. Indeed, to tackle our question, we need to observe the distributed representation of the categories over the entire ventral occipito-temporal cortex (<xref ref-type="bibr" rid="bib48">Haxby et al., 2001</xref>). This approach has already been validated by previous studies that investigated the categorical representation in the ventral-occipito temporal cortex, using a wide VOTC mask, such as <xref ref-type="bibr" rid="bib121">van den Hurk et al. (2017)</xref>; <xref ref-type="bibr" rid="bib69">Kriegeskorte et al. (2008b)</xref>; <xref ref-type="bibr" rid="bib124">Wang et al. (2015)</xref>. We also added the early visual cortex (EVC) as a control node. We decided to work in a structurally and individually defined mask of VOTC using the Desikan-Killiany atlas (<xref ref-type="bibr" rid="bib31">Desikan et al., 2006</xref>) implemented in FreeSurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu">http://surfer.nmr.mgh.harvard.edu</ext-link>). Six ROIs were selected in each hemisphere: Pericalcarine, Cuneus and Lingual areas were combined to define the early visual cortex (EVC) ROI; Fusiform, Parahippocampal and Infero-Temporal areas were combined to define the ventral occipito-temporal (VOTC) ROI. Then, we combined these areas in order to obtain one bilateral EVC ROI and one bilateral VOTC ROI (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><p>Our strategy to work on a limited number of relatively large brain parcels has the advantage to minimize unstable decoding results collected from small regions (<xref ref-type="bibr" rid="bib84">Norman et al., 2006</xref>) and reduce multiple comparison problems intrinsic to neuroimaging studies (<xref ref-type="bibr" rid="bib41">Etzel et al., 2013</xref>). All analyses, except for the topographical selectivity map (see below), were carried out in subject space for enhanced anatomico-functional precision and to avoid spatial normalization across subjects.</p></sec><sec id="s4-8"><title>General linear model</title><p>The pre-processed images for each participant were analyzed using a general linear model (GLM). For each of the five runs, we included 32 regressors: 24 regressors of interest (each stimulus), 1 regressor of no-interest for the target stimulus, six head-motion regressors of no-interest and one constant. From the GLM analysis, we obtained a β-image for each stimulus (i.e. 24 sounds) in each run, for a total of 120 (24 stimuli x five runs) beta maps.</p></sec><sec id="s4-9"><title>Topographical selectivity map</title><p>For this analysis, we needed all participants to be coregistered and normalized in a common volumetric space. To achieve maximal accuracy, we relied on the DARTEL (Diffeomorphic Anatomical Registration Through Exponentiated Lie Algebra; <xref ref-type="bibr" rid="bib6">Ashburner, 2007</xref>) toolbox. DARTEL normalization takes the gray and white matter templates from each subject to create an averaged template based on our own sample that will be used for the normalization. The creation of a study-specific template using DARTEL was performed to reduce deformation errors that are more likely to arise when registering single subject images to an unusually shaped template (<xref ref-type="bibr" rid="bib6">Ashburner, 2007</xref>). This is particularly relevant when comparing blind and sighted subjects given that blindness is associated with significant changes in the structure of the brain itself, particularly within the occipital cortex (<xref ref-type="bibr" rid="bib34">Dormal et al., 2016</xref>; <xref ref-type="bibr" rid="bib55">Jiang et al., 2009</xref>; <xref ref-type="bibr" rid="bib87">Pan et al., 2007</xref>; <xref ref-type="bibr" rid="bib88">Park et al., 2009</xref>).</p><p>To create the topographical selectivity map (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), we extracted in each participant the β-value for each of our four main conditions (animals, humans, manipulable objects and places) from each voxel inside the VOTC mask and we assigned to each voxel the condition producing the highest β-value (winner takes all). This analysis resulted in specific clusters of voxels that spatially distinguish themselves from their surround in terms of selectivity for a particular condition (<xref ref-type="bibr" rid="bib121">van den Hurk et al., 2017</xref>; <xref ref-type="bibr" rid="bib115">Striem-Amit et al., 2018a</xref>).</p><p>Finally, to compare how similar are the topographical selectivity maps in the three groups we followed, for each pair of groups (i.e. 1.SCv-EBa; 2.SCv-SCa; 3.EBa-SCa) these steps: (1) We computed the Spearman’s correlation between the topographical selectivity map of each subject from Group one with the averaged selectivity map of Group two and we compute the mean of these values. (2) We computed the Spearman’s correlation between the topographical selectivity map of each subject from Group two with the averaged selectivity map of Group one and we computed the mean of these values. (3) We averaged the two mean values obtained from step 1 and step 2, in order to have one mean value for each group comparison (see the section ‘<italic>Statistical analyses</italic>’ for details about the assessment of statistical differences).</p><p>We ran this analysis using the four (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) and the eight categories (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) and both analyses lead to almost identical results. We decided to represent the data of the four main categories for simpler visualization of the main effect (topographical overlap across modalities and groups).</p><p>In order to go beyond the magnitude of the correlation between the topographical selectivity maps and to explore the quality of the (dis)similarity between the topographical maps of our subjects and groups we computed the Jaccard index between them. The Jaccard similarity coefficient is a statistic used for measuring the similarity and diversity of sample sets (<xref ref-type="bibr" rid="bib32">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="bib128">Xu et al., 2018</xref>). The Jaccard coefficient is defined as the size of the intersection divided by the size of the union of the sample sets. This value is 0 when the two sets are disjoint, 1 when they are equal, and between 0 and 1 otherwise.</p><p>First, we looked at the consistency of the topographical representation of the categories across subjects within the same group. For this within group analysis we computed the Jaccard similarity between the topographical selectivity map of each subject and the mean topographical map of his own group. This analysis produces 4 Jaccard similarity indices (one for each of the main category: (1) animals, (2) humans, (3) manipulable objects and (4) big objects and places) for each group (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Low values for one category within a group mean that the topographical representation of that category varies a lot across subjects of that group. Statistical significance of the Jaccard similarity index within groups was assessed using parametric statistics: One sample t-tests to assess the difference against zero and a repeated measure ANOVA (4 categories * 3 groups) to compare the different categories and groups.</p><p>In addition, we computed the Jaccard similarity index for each category, between the topographical map of each blind and sighted subject in the auditory experiment (EBa and SCa) with the averaged topographical selectivity map from the sighted in the visual experiment (SCv; see <xref ref-type="fig" rid="fig2">Figure 2C</xref> for the results). In more practical terms, this means that we have 4 Jaccard similarity indices (one for each of the main category: (1) animal, (2) human, (3) manipulable objects and (4) big objects and places) for each of the two groups’ pairs: EBa-SCv and SCa-SCv. These values can help in explore more in detail the similarity and differences among the topographical representations of our categories when they are presented visually compared to when they are presented acoustically, both in sighted and in blind. Statistical significance of the Jaccard similarity index between groups was assessed using parametric statistics: one sample T-tests to test the difference against zero, a repeated measure ANOVA (4 categories * 2 groups) to compare the different categories and groups. The Greenhouse-Geisser sphericity correction was applied.</p><p>However, the Jaccard similarity values could be partially driven by the number of voxels that show selectivity for each category. For instance, the absence of overlap between the voxels that prefer animals in EBa and SCv could be explained by the fact that different voxels prefer animals in the two groups or by the fact that one or both groups have a limited number of voxels that show a preference for this category. To disentangle these two possibilities we counted in each group the number of voxels within VOTC that prefers each category (see <xref ref-type="fig" rid="fig2">Figure 2A</xref> for the results). We ended up with four values (one number for each category) for each group. Statistical significance of the number of voxels within groups was assessed using parametric statistics: One sample t-tests to assess the difference against zero, a repeated measure ANOVA (four categories * three groups) to compare the different categories and groups.</p></sec><sec id="s4-10"><title>MVP-classifications: Binary decoding</title><p>We performed a binary MVP-classification (using SVM - support vector machine classifier) to look at the ability of each ROI to distinguish between two categories at time. With eight categories we can have 28 possible pairs, resulting in 28 binary MVP-classification tests in each ROI. Statistical significance of the binary classification was assessed using t-test against the chance level. We, then, averaged the 28 accuracy values of each subject in order to have one mean accuracy value for subject. Statistical significance of the averaged binary classification was assessed using parametric statistics: t-test against zero and ANOVA.</p></sec><sec id="s4-11"><title>Representational similarity analysis (RSA): Correlation between neural dissimilarity matrices of the three groups</title><p>We further investigated the functional profile of the ROIs using RSA. RSA was performed using the CoSMoMVPA toolbox, implemented in Matlab (r2013b; Matworks). The basic concept of RSA is the dissimilarity matrix (DSM). A DSM is a square matrix where the number of columns and rows corresponds to the number of the conditions (8 × 8 in this experiment) and it is symmetrical about a diagonal of zeros. Each cell contains the dissimilarity index between the two stimuli. We used the binary MVP-classification as dissimilarity index to build neural DSMs (<xref ref-type="bibr" rid="bib22">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Cichy and Pantazis, 2017</xref>; <xref ref-type="bibr" rid="bib24">Cichy et al., 2013</xref>; <xref ref-type="bibr" rid="bib33">Dobs et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Haxby et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Haxby et al., 2011</xref>; <xref ref-type="bibr" rid="bib85">O'Toole et al., 2005</xref>; <xref ref-type="bibr" rid="bib94">Pereira et al., 2009</xref>; <xref ref-type="bibr" rid="bib99">Proklova et al., 2019</xref>) for each group, in order to compare the functional profile of the ROIs between the three groups. In this way, we ended up with a DSM for each group for every ROI.</p><p>We preferred to use binary MVP-classification as dissimilarity index to build neural DSMs rather than other types of dissimilarity measures (e.g. Pearson correlation, Euclidean distance, Spearman correlation) since two experimental conditions that do not drive a response and therefore have uncorrelated patterns (noise only, r ≈ 0) appear very dissimilar (1 – r ≈ 1). When using a decoding approach instead, due to the intrinsic cross-validation steps, we would find that the two conditions that don’t drive responses are indistinguishable, despite their substantial correlation distance (<xref ref-type="bibr" rid="bib123">Walther et al., 2016</xref>) since the noise is independent between the training and testing partitions, therefore cross-validated estimates of the distance do not grow with increasing noise. This was crucial in our study since we are looking at brain activity elicited by sounds in brain regions that are primarily visual (EVC and VOTC), therefore the level of noise is expected to be high, at least in sighted people.</p><p>Finally, to compare how similar are the DSMs in the three groups, for each pair of groups (i.e. 1.SCv-EBa; 2.SCv-SCa; 3.EBa-SCa) (1) we computed the Spearman’s correlation between the upper triangular DSM (excluding the diagonal) of each subject from Group one with the averaged upper triangular DSM (excluding the diagonal) of Group two and we compute the mean of these values. (2) We computed the Spearman’s correlation between the upper triangular DSM (excluding the diagonal) of each subject from Group two with the averaged upper triangular DSM (excluding the diagonal) of Group one and we computed the mean of these values. (3) We averaged the two mean values obtained from step 1 and step 2, in order to have one mean value for each groups’ comparison.</p><p>Considering the unidirectional hypothesis for this test (a positive correlation between neural similarity and models similarity) and the difficult interpretation of a negative correlation, one-tailed statistical tests were used. For all other tests (e.g., differences between groups), for which both directions might be hypothesized, two-tailed tests were applied (<xref ref-type="bibr" rid="bib92">Peelen et al., 2014</xref>; <xref ref-type="bibr" rid="bib42">Evans et al., 2019</xref>).</p><p>See the section ‘<italic>Statistical analyses</italic>’ for details about the assessment of statistical differences.</p></sec><sec id="s4-12"><title>Hierarchical clustering analysis on the brain categorical representations</title><p>In order to go beyond the correlation values and to explore more qualitatively the representational structure of VOTC and EVC in the three groups, we implemented a hierarchical clustering approach (<xref ref-type="bibr" rid="bib63">King et al., 2019</xref>). First, we created a hierarchical cluster tree for each brain DSM using the 'linkage’ function in Matlab, then we defined clusters from this hierarchical cluster tree with the ‘cluster’ function in Matlab. Hierarchical clustering starts by treating each observation as a separate cluster. Then, it identifies the two clusters that are closest together, and merges these two most similar clusters. This continues until all the clusters are merged together or until the clustering is ‘stopped’ to a n number of clusters. We repeated the clustering three times for each DSM, stopping it at 2, 3 and 4 clusters, respectively (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). In this way, we could compare the similarities and the differences of the clusters at the different scales across the groups. We applied the same clustering analysis also on the behavioral data (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p></sec><sec id="s4-13"><title>Representational similarity analysis (RSA): correlation with representational low-level/behavioral models</title><p>We then intended to investigate which features of the visual and auditory stimuli were represented in the different ROIs of sighted and blind subjects. RSA allows the comparisons between the brain DSMs extracted from specific ROIs with representational DSMs, based on physical properties of the stimuli or based on behavioral rating of the perceived categorical similarity of our stimuli.</p></sec><sec id="s4-14"><title>Low-level DSM in the auditory experiment: pitch DSM</title><p>Pitch corresponds to the perceived frequency content of a stimulus. We selected this specific low-level auditory feature for two reasons. First, previous studies showed that this physical property of the sounds is distinctly represented in the auditory cortex and may create some low-level bias of auditory category selective responses in the temporal cortex (<xref ref-type="bibr" rid="bib43">Giordano et al., 2013</xref>; <xref ref-type="bibr" rid="bib71">Leaver and Rauschecker, 2010</xref>; <xref ref-type="bibr" rid="bib80">Moerel et al., 2012</xref>). Second, we confirmed with our own SCa group that, among alternative auditory RDMs based on separate acoustic features (e.g. Harmonicity on noise ratio, Spectral centroid), the pitch model correlated most with brain RDM extracted from the temporal cortex. This provided strong support that this model was maximally efficient in capturing the encoding of sounds based on acoustic features in auditory cortical regions (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><p>We computed a pitch value for each of the 24 auditory stimuli, using the Praat software and an autocorrelation method. This method extracts the strongest periodic component of several time windows across the stimulus and averages them to have one mean pitch value for that stimulus. The ‘pitch floor’ determines the size of the time windows over which these values are calculated in Praat. Based on a previous study, we chose a default pitch floor of 60 Hz (<xref ref-type="bibr" rid="bib71">Leaver and Rauschecker, 2010</xref>). We then averaged the pitch values across stimuli belonging to the same category. Once we obtained one pitch value for each category, we built the DSM computing the absolute value of the pitch difference for each possible pairwise (see <xref ref-type="fig" rid="fig6">Figure 6A</xref>). The pitch DSM was not positively correlated with the behavioral DSM of neither SCa (r=–0.36, p=0.06) nor EBa (r = –0.29, p=<italic>0.13</italic>).</p></sec><sec id="s4-15"><title>Low-level DSM in the visual experiment: Hmax- C1 model</title><p>The Hmax model (<xref ref-type="bibr" rid="bib109">Serre et al., 2007</xref>) reflects the hierarchical organization of the visual cortex (<xref ref-type="bibr" rid="bib52">Hubel and Wiesel, 1962</xref>) in a series of layers from V1 to infero-temporal (IT) cortex. To build our low-level visual model we used the output from the V1- complex cells layer. The inputs for the model are the gray-value luminance images presented in the sighted group doing the visual experiment. Each image is first analysed by an array of simple cells (S1) units at 4 different orientations and 16 scales. At the next C1 layer, the image is subsampled through a local Max pooling operation over a neighbourhood of S1 units in both space and scale, but with the same preferred orientation (<xref ref-type="bibr" rid="bib109">Serre et al., 2007</xref>). C1 layer stage corresponds to V1 cortical complex cells, which shows some tolerance to shift and size (<xref ref-type="bibr" rid="bib109">Serre et al., 2007</xref>). The outputs of all complex cells were concatenated into a vector as the V1 representational pattern of each image (<xref ref-type="bibr" rid="bib61">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib68">Kriegeskorte et al., 2008a</xref>). We averaged the vectors of images from the same category in order to have one vector for each category. We, finally, built the (8 × 8) DSM computing 1- Pearson’s correlation of each pair of vectors (see <xref ref-type="fig" rid="fig6">Figure 6A</xref>). The Hmax-C1 DSM was significantly correlated with the SCv behavioral DSM (r = 0.56, p=<italic>0.002</italic>).</p></sec><sec id="s4-16"><title>Behavioral-categorical DSMs</title><p>We used the pairwise similarity judgments from the behavioral experiment to build the semantic DSMs. We computed one matrix for each subject that took part in the behavioral experiment and we averaged all the matrices of the participants from the same group to finally obtain three mean behavioral-categorical DSMs, one for each group (i.e. EBa, SCa, SCv; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). The three behavioral matrices were highly correlated between them (SCv-EBa: r = 0.89, p&lt;0.001; SCv-SCa: r = 0.94, p&lt;0.001; EBa-SCa: r = 0.85, p&lt;0.001), and the similarity judgment was clearly performed on a categorical-membership basis (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p><p>The last step consisted in comparing neural and external DSMs models using a second order correlation. Because we wanted to investigate each representational model independently from the other, we relied on Spearman's rank partial correlation: in the auditory experiment, we removed the influence of the pitch similarity when we were computing the correlation with the behavioral matrix, and vice versa; in the visual experiment, we removed the influence of the Hmax-C1 model similarity, when we were computing the correlation with the behavioral matrix, and vice versa. In this way, we could measure the partial correlation for each external model for each participant separately. Importantly, we did not correlate the full symmetrical DSMs but only the upper triangular DSM excluding the diagonal.</p><p>See the section ‘<italic>Statistical analyses</italic>’ for details about the assessment of statistical differences.</p></sec><sec id="s4-17"><title>RSA: Inter-subjects correlation</title><p>To examine the commonalities of the neural representational space across subjects in VOTC, we extracted the neural DSM of every subject individually and then correlated it with the neural DSM of every other subject. Since we have 49 participants in total, this analysis resulted in a 49 × 49 matrix (<xref ref-type="fig" rid="fig7">Figure 7</xref>) in which each line and column represents the correlation of one subject’s DSM with all other subjects’ DSM. The three main squares in the diagonal (<xref ref-type="fig" rid="fig7">Figure 7</xref>) represent the within group correlation of the 3 groups. We averaged the value within each main square (only the upper half excluding the diagonal) on the diagonal to obtain a mean value of within group correlation for each group. The three main off diagonal squares (<xref ref-type="fig" rid="fig7">Figure 7</xref>) represent the between groups correlation of the three possible groups’ pairs (i.e. 1.SCv/EBa; 2.SCv-SCa; 3.EBa-SCa). We averaged the value within each main off-diagonal square in order to obtain a mean value of between groups correlation for each groups’ pair.</p><p>See the section ‘<italic>Statistical analyses</italic>’ for details about the assessment of statistical differences.</p></sec><sec id="s4-18"><title>Representational connectivity analysis</title><p>Representational connectivity analysis were implemented to identify the representational relationship among the ROIs composing VOTC and the rest of the brain (<xref ref-type="bibr" rid="bib68">Kriegeskorte et al., 2008a</xref>; <xref ref-type="bibr" rid="bib96">Pillet et al., 2018</xref>). This approach can be considered a type of connectivity where similar RDMs of two ROIs indicate shared representational structure and therefore is supposed to be a proxy for information exchange (<xref ref-type="bibr" rid="bib69">Kriegeskorte et al., 2008b</xref>). Representational connectivity between two ROIs does not imply a direct structural connection but can provide connectivity information from a functional perspective, assessing to what extent two regions represent information similarly (<xref ref-type="bibr" rid="bib129">Xue et al., 2013</xref>).</p><p>To perform this analysis, we included 30 bilateral parcels (covering almost the entire cortex) extracted from the segmentation of individual anatomical scan following the Desikan-Killiany atlas implemented in FreeSurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu">http://surfer.nmr.mgh.harvard.edu</ext-link>). We only excluded three parcels (Entorhinal cortex, Temporal Pole and Frontal Pole) because their size was too small and signal too noisy (these regions are notably highly susceptible to signal drop in EPI acquisition) to allowed the extraction of reliable dissimilarity matrices in most of the participants. We merged together the left and right corresponding parcels in order to have a total of 30 bilateral ROIs for each subject. From each ROI we extracted the dissimilarity matrix based on binary decoding accuracies, as described in the section ‘<italic>Representational similarity analysis (RSA): Correlation between neural dissimilarity matrices of the three groups</italic>’. Finally, we computed the Spearman’s correlation between the three seed ROIs (i.e. fusiform gyrus, parahippocampal gyrus and infero-temporal cortex) and all the other 27 ROIs. We ended up with a connectivity profile of 3 (number of seeds) by 27 (ROIs representing the rest of the brain) for each subject. We considered this 3*27 matrix as one representational connectivity profile of the seed region in each subject.</p><p>Finally, to compare how similar are the RSA connectivity profiles in the three groups, for each pair of groups (i.e. 1.SCv-EBa; 2.SCv-SCa; 3.EBa-SCa): (1) We computed the Spearman’s correlation between the representational connectivity profile of each subject from Group one with the averaged representational connectivity profile from Group two and we compute the mean of these values. (2) We computed the Spearman’s correlation between the representational connectivity profile of each subject from Group two with the averaged representational connectivity profile of Group one and we computed the mean of these values. (3) We averaged the two mean values obtained from step 1 and step 2, in order to have one mean value for each group comparison.</p><p>We computed the same analysis also in the EVC, as a control ROI. In this case the three nodes ROIs were the pericalcarine cortex, the cuneus and the lingual gyrus.</p><p>See the section ‘<italic>Statistical analyses</italic>’ for details about the assessment of statistical differences.</p></sec><sec id="s4-19"><title>Statistical analyses</title><p>To assess statistical differences, we applied parametric tests (T-Test and ANOVA) in the analyses that met the main assumptions required by parametric statistics: normal distribution of the data and independency of the observations. Moreover, in case of statistical comparisons between different groups we ran the Levene’s test to check for the assumption of equality of variances between the groups, in case the test was significant (suggesting different levels of variance) we applied the Welch Homogeneity correction. Parametric tests were used in the Jaccard similarity analyses (both within and between groups), in the analysis on the selective voxel’s count and in the averaged binary decoding analysis. In all these analyses the correlation data were z-transformed before subjecting them to parametric statistics.</p><p>However, the correlation of the topographical selectivity maps, the correlation of the brain dissimilarity matrices, the correlation of the RSA connectivity profiles and the inter-subject DSMs correlation did not meet the assumption of independency of the data. In fact, in these analyses we contrast group’s comparisons, so data from the same subjects are always included in two comparisons (e.g. data from EBa subjects are included both in the SCv-EBa and in EBa-SCa comparisons). For this reason, the use of permutation was a preferable approach in the case of these analyses. In each of this analysis we have one vector of values for each subject in each ROI (i.e. The vector containing the categorical selectivity label of each voxel; The brain dissimilarity matrix in the format of pairwise distance vector; The vectorized RSA connectivity profile). In each analysis we want to correlate these values between each possible pair of groups, which are three in total: SCv-EBa; SCv-SCa; EBa-SCa. To compute the average correlation value between each pair of groups we followed these steps: (1) We computed the Spearman’s correlation between the vector of each subject from Group one with the mean vector of Group two and we computed the mean of these values (e.g. we correlated the vector from each EBa subject with the mean vector from the SCv group). (2) We computed the Spearman’s correlation between the vector of each subject from Group two with the mean vector of Group one and we computed the mean of these values (e.g. we correlated the vector from each SCv sub. with the mean vector from the EBa group). (3) We averaged the two mean values obtained from step 1 and step 2, in order to have one mean value for each group comparison. Since our data points are not completely independent, we cannot use parametric statistics (<xref ref-type="bibr" rid="bib89">Parsons et al., 2018</xref>). Therefore, to test statistical differences we used a permutation test (10.000 iterations): (4) We randomly permuted the conditions of the vector of each subject from Group 1 and of the mean vector of Group 2 and we computed the correlation (as in Step 1). (5) We randomly permuted the conditions of the vector of each subject from Group 2 and of the mean vector of Group 1 and we computed the correlation (as in Step 2). (6) We averaged the 2 mean values obtained from step 4 and step 5. (7) We repeated these steps 10.000 times to obtain a distribution of correlations simulating the null hypothesis that the two vectors are unrelated (<xref ref-type="bibr" rid="bib69">Kriegeskorte et al., 2008b</xref>). If the actual correlation falls within the top α ×100% of the simulated null distribution of correlations, the null hypothesis of unrelated vectors can be rejected with a false-positives rate of α.</p><p>Only in the case of the correlation of topographical maps, we constrained the permutation performed in the step five in order to take into consideration the inherent smoothness/spatial dependencies in the univariate fMRI data. In each subject, we individuated each cluster of voxels showing selectivity for the same category and we kept these clusters fixed in the permutation, assigning randomly a condition to each of these predefined clusters. In this way, the spatial structure of the topographical maps was kept identical to the original one, making very unlikely that a significant result could be explained by the voxels’ spatial dependencies. We may however note that this null-distribution is likely overly conservative since it assumes that size and position of clusters could be created only from task-independent spatial dependencies (either intrinsic to the acquisition or due to smoothing). We had to exclude one EBa subject from the analysis because he had less than seven clusters in his topographical map, which is not enough to have 10000 combinations needed for the permutation given our four categories tested (possible combinations = n_categories<sup>n_clusters</sup>; 4<sup>7</sup> = 16384).</p><p>To test the difference between the group pairs’ correlations (e.g. to test if the correlation between SCv and EBa was different from the correlation of SCv and SCa), we used a permutation test (10.000 iterations): (8) We computed the difference between the correlation of Pair one and Pair 2: mean correlation Pair1 – mean correlation Pair2. (9) We kept fixed the labels of the group common to the two pairs and we shuffled the labels of the subjects from the other two groups (e.g. if we are comparing SCv-EBa versus SCv-SCa, we keep the SCv group fixed and we shuffle the labels of EBa and SCa). (10) After shuffling the groups’ labels we computed again the point 1-2-3 and 8. (11) We repeated this step 10.000 times to obtain a distribution of differences simulating the null hypothesis that there is no difference between the two pairs’ correlations. If the actual difference falls within the top α ×100% of the simulated null distribution of difference, the null hypothesis of absence of difference can be rejected with a false-positives rate of α.</p><p>Finally, also the RSA with representational low-levels and behavioral models did not meet the assumption of independency of the data. In fact, for dissimilarity matrices the independence of the samples cannot be assumed, because each similarity is dependent on two response patterns, each of which also codetermines the similarities of all its other pairings in the RDM (<xref ref-type="bibr" rid="bib69">Kriegeskorte et al., 2008b</xref>). For each group, the statistical difference from zero was determined using permutation test (10000 iterations), building a null distribution for these correlation values by computing them after randomly shuffling the labels of the matrices. Similarly, the statistical difference between groups was assessed using permutation test (10000 iterations) building a null distribution for these correlation values by computing them after randomly shuffling the group labels. Moreover, considering the unidirectional hypothesis for this test (a positive correlation between neural similarity and models similarity) and the difficult interpretation of a negative correlation, one-tailed statistical tests were used. For all other tests (e.g., differences between groups), for which both directions might be hypothesized, two-tailed tests were used (<xref ref-type="bibr" rid="bib92">Peelen et al., 2014</xref>; <xref ref-type="bibr" rid="bib42">Evans et al., 2019</xref>).</p><p>In each analysis, all the p-values are reported after false discovery rate (FDR) correction implemented using the matlab function ‘mafdr’.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to express our gratitude to Marco Barilari, Stefania Benetti, Giorgia Bertonati, Francesca Barbero who have helped with the data acquisition, to Yangwen Xu, Matthew Bennet and Remi Gau for giving comments on a preliminary version of the paper, to Jorge Jovicich for helping to set-up the fMRI acquisition parameters and to Pietro Chiesa for continuing support with stimuli-delivery systems. We are also extremely thankful to our blind participants and to the Unioni Ciechi of Trento, Mantova, Genova, Savona, Cuneo, Torino, Trieste and Milano and the blind Institute of Milano for helping with the recruitment. The project was funded by the ERC starting grant MADVIS (Project: 337573) and the Belgian Excellence of Science program (Project: 30991544) awarded to Olivier Collignon. Olivier Collignon is research associate at the Fond National de la Recherche Scientifique de Belgique (FRS-FNRS).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con2"><p>Methodology</p></fn><fn fn-type="con" id="con3"><p>Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization</p></fn><fn fn-type="con" id="con5"><p>Software, Methodology</p></fn><fn fn-type="con" id="con6"><p>Data curation, Software, Methodology</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Data curation, Software, Supervision, Funding acquisition, Validation, Visualization, Methodology, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The ethical committee of the University of Trento approved this study (protocol 2014-007) and participants gave their informed consent before participation.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Categories and stimuli description.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-50732-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Characteristics of early blind participants.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-50732-supp2-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-50732-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Processed data have been made available on OSF at the link <ext-link ext-link-type="uri" xlink:href="https://osf.io/erdxz/">https://osf.io/erdxz/</ext-link>. To preserve participant anonymity and due to restrictions on data sharing in our ethical approval, fully anonymised raw data can only be shared upon request to the corresponding author.</p><p>The following dataset was generated:</p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Mattioni</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Categorical representation from sound and sight in the ventral occipito-temporal cortex of sighted and blind - open data</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="archive" xlink:href="https://osf.io/erdxz/">erdxz</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Raz</surname> <given-names>N</given-names></name><name><surname>Pianka</surname> <given-names>P</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Early 'visual' cortex activation correlates with superior verbal memory performance in the blind</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>758</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1038/nn1072</pub-id><pub-id pub-id-type="pmid">12808458</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Stern</surname> <given-names>WM</given-names></name><name><surname>Camprodon</surname> <given-names>JA</given-names></name><name><surname>Bermpohl</surname> <given-names>F</given-names></name><name><surname>Merabet</surname> <given-names>L</given-names></name><name><surname>Rotman</surname> <given-names>S</given-names></name><name><surname>Hemond</surname> <given-names>C</given-names></name><name><surname>Meijer</surname> <given-names>P</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Shape conveyed by visual-to-auditory sensory substitution activates the lateral occipital complex</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>687</fpage><lpage>689</lpage><pub-id pub-id-type="doi">10.1038/nn1912</pub-id><pub-id pub-id-type="pmid">17515898</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Raz</surname> <given-names>N</given-names></name><name><surname>Azulay</surname> <given-names>H</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical activity during tactile exploration of objects in blind and sighted humans</article-title><source>Restorative Neurology and Neuroscience</source><volume>28</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.3233/RNN-2010-0503</pub-id><pub-id pub-id-type="pmid">20404404</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews</surname> <given-names>TJ</given-names></name><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Pell</surname> <given-names>P</given-names></name><name><surname>Hartley</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Selectivity for low-level features of objects in the human ventral stream</article-title><source>NeuroImage</source><volume>49</volume><fpage>703</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.08.046</pub-id><pub-id pub-id-type="pmid">19716424</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A hierarchical, retinotopic proto-organization of the primate visual system at birth</article-title><source>eLife</source><volume>6</volume><elocation-id>e26196</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26196</pub-id><pub-id pub-id-type="pmid">28671063</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A fast diffeomorphic image registration algorithm</article-title><source>NeuroImage</source><volume>38</volume><fpage>95</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.07.007</pub-id><pub-id pub-id-type="pmid">17761438</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassi</surname> <given-names>C</given-names></name><name><surname>Alemi-Neissi</surname> <given-names>A</given-names></name><name><surname>Pagan</surname> <given-names>M</given-names></name><name><surname>Dicarlo</surname> <given-names>JJ</given-names></name><name><surname>Zecchina</surname> <given-names>R</given-names></name><name><surname>Zoccolan</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shape similarity, better than semantic membership, accounts for the structure of visual object representations in a population of monkey inferotemporal neurons</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003167</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003167</pub-id><pub-id pub-id-type="pmid">23950700</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baroni</surname> <given-names>M</given-names></name><name><surname>Bernardini</surname> <given-names>S</given-names></name><name><surname>Ferraresi</surname> <given-names>A</given-names></name><name><surname>Zanchetta</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The WaCky wide web: a collection of very large linguistically processed web-crawled corpora</article-title><source>Language Resources and Evaluation</source><volume>43</volume><fpage>209</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1007/s10579-009-9081-4</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bavelier</surname> <given-names>D</given-names></name><name><surname>Neville</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Cross-modal plasticity: where and how?</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>443</fpage><lpage>452</lpage><pub-id pub-id-type="doi">10.1038/nrn848</pub-id><pub-id pub-id-type="pmid">12042879</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname> <given-names>M</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><name><surname>Dodell-Feder</surname> <given-names>D</given-names></name><name><surname>Fedorenko</surname> <given-names>E</given-names></name><name><surname>Saxe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Language processing in the occipital cortex of congenitally blind adults</article-title><source>PNAS</source><volume>108</volume><fpage>4429</fpage><lpage>4434</lpage><pub-id pub-id-type="doi">10.1073/pnas.1014818108</pub-id><pub-id pub-id-type="pmid">21368161</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence from blindness for a cognitively pluripotent cortex</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>637</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.06.003</pub-id><pub-id pub-id-type="pmid">28821345</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Sporns</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Human connectomics</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>144</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.08.005</pub-id><pub-id pub-id-type="pmid">21908183</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benetti</surname> <given-names>S</given-names></name><name><surname>van Ackeren</surname> <given-names>MJ</given-names></name><name><surname>Rabini</surname> <given-names>G</given-names></name><name><surname>Zonca</surname> <given-names>J</given-names></name><name><surname>Foa</surname> <given-names>V</given-names></name><name><surname>Baruffaldi</surname> <given-names>F</given-names></name><name><surname>Rezk</surname> <given-names>M</given-names></name><name><surname>Pavani</surname> <given-names>F</given-names></name><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Functional selectivity for face processing in the temporal voice area of early deaf individuals</article-title><source>PNAS</source><volume>114</volume><fpage>E6437</fpage><lpage>E6446</lpage><pub-id pub-id-type="doi">10.1073/pnas.1618287114</pub-id><pub-id pub-id-type="pmid">28652333</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benetti</surname> <given-names>S</given-names></name><name><surname>Novello</surname> <given-names>L</given-names></name><name><surname>Maffei</surname> <given-names>C</given-names></name><name><surname>Rabini</surname> <given-names>G</given-names></name><name><surname>Jovicich</surname> <given-names>J</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>White matter connectivity between occipital and temporal regions involved in face and voice processing in hearing and early deaf individuals</article-title><source>NeuroImage</source><volume>179</volume><fpage>263</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.06.044</pub-id><pub-id pub-id-type="pmid">29908936</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname> <given-names>Y</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Object domain and modality in the ventral visual pathway</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>282</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.02.002</pub-id><pub-id pub-id-type="pmid">26944219</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blank</surname> <given-names>H</given-names></name><name><surname>Anwander</surname> <given-names>A</given-names></name><name><surname>von Kriegstein</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Direct structural connections between voice- and face-recognition Areas</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>12906</fpage><lpage>12915</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2091-11.2011</pub-id><pub-id pub-id-type="pmid">21900569</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghesani</surname> <given-names>V</given-names></name><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Buiatti</surname> <given-names>M</given-names></name><name><surname>Amadon</surname> <given-names>A</given-names></name><name><surname>Eger</surname> <given-names>E</given-names></name><name><surname>Piazza</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Word meaning in the ventral visual path: a perceptual to conceptual gradient of semantic coding</article-title><source>NeuroImage</source><volume>143</volume><fpage>128</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.068</pub-id><pub-id pub-id-type="pmid">27592809</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Kalfas</surname> <given-names>I</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The ventral visual pathway represents animal appearance rather than animacy, unlike human behavior and deep neural networks</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>552</elocation-id><pub-id pub-id-type="doi">10.1167/18.10.552</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>432</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2314-15.2016</pub-id><pub-id pub-id-type="pmid">26758835</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical hierarchy turned on its head</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>657</fpage><lpage>658</lpage><pub-id pub-id-type="doi">10.1038/nn0703-657</pub-id><pub-id pub-id-type="pmid">12830152</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname> <given-names>H</given-names></name><name><surname>McLaren</surname> <given-names>DG</given-names></name><name><surname>Sinclair</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reading embossed capital letters: an fMRI study in blind and sighted individuals</article-title><source>Human Brain Mapping</source><volume>27</volume><fpage>325</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1002/hbm.20188</pub-id><pub-id pub-id-type="pmid">16142777</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>T</given-names></name><name><surname>Tovar</surname> <given-names>DA</given-names></name><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational dynamics of object vision: the first 1000 ms</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Heinzle</surname> <given-names>J</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagery and perception share cortical representations of content and location</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>372</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr106</pub-id><pub-id pub-id-type="pmid">21666128</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name><name><surname>Heinzle</surname> <given-names>J</given-names></name><name><surname>Elliott</surname> <given-names>LT</given-names></name><name><surname>Ramirez</surname> <given-names>F</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probing principles of large-scale object representation: category preference and location encoding</article-title><source>Human Brain Mapping</source><volume>34</volume><fpage>1636</fpage><lpage>1651</lpage><pub-id pub-id-type="doi">10.1002/hbm.22020</pub-id><pub-id pub-id-type="pmid">22371355</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multivariate pattern analysis of MEG and EEG: a comparison of representational structure in time and space</article-title><source>NeuroImage</source><volume>158</volume><fpage>441</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.023</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Pell</surname> <given-names>PJ</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Learning warps object representations in the ventral temporal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>1010</fpage><lpage>1023</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00951</pub-id><pub-id pub-id-type="pmid">26967942</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Vandewalle</surname> <given-names>G</given-names></name><name><surname>Voss</surname> <given-names>P</given-names></name><name><surname>Albouy</surname> <given-names>G</given-names></name><name><surname>Charbonneau</surname> <given-names>G</given-names></name><name><surname>Lassonde</surname> <given-names>M</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specialization for auditory-spatial processing in the occipital cortex of congenitally blind humans</article-title><source>PNAS</source><volume>108</volume><fpage>4435</fpage><lpage>4440</lpage><pub-id pub-id-type="doi">10.1073/pnas.1013928108</pub-id><pub-id pub-id-type="pmid">21368198</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Dormal</surname> <given-names>G</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Building the Brain in the Dark: Functional and Speciﬁc Crossmodal Reorganization in the Occipital Cortex of Blind Individuals</chapter-title><source>Plasticity in Sensory Systems</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781139136907.007</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crollen</surname> <given-names>V</given-names></name><name><surname>Lazzouni</surname> <given-names>L</given-names></name><name><surname>Rezk</surname> <given-names>M</given-names></name><name><surname>Bellemare</surname> <given-names>A</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name><name><surname>Noël</surname> <given-names>M-P</given-names></name><name><surname>Seron</surname> <given-names>X</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recruitment of the occipital cortex by arithmetic processing follows computational Bias in the congenitally blind</article-title><source>NeuroImage</source><volume>186</volume><fpage>549</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.11.034</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Pasquale</surname> <given-names>F</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Betti</surname> <given-names>V</given-names></name><name><surname>Della Penna</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical cores in network dynamics</article-title><source>NeuroImage</source><volume>180</volume><fpage>370</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.063</pub-id><pub-id pub-id-type="pmid">28974453</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname> <given-names>RS</given-names></name><name><surname>Ségonne</surname> <given-names>F</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Quinn</surname> <given-names>BT</given-names></name><name><surname>Dickerson</surname> <given-names>BC</given-names></name><name><surname>Blacker</surname> <given-names>D</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Maguire</surname> <given-names>RP</given-names></name><name><surname>Hyman</surname> <given-names>BT</given-names></name><name><surname>Albert</surname> <given-names>MS</given-names></name><name><surname>Killiany</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devereux</surname> <given-names>BJ</given-names></name><name><surname>Clarke</surname> <given-names>A</given-names></name><name><surname>Marouchos</surname> <given-names>A</given-names></name><name><surname>Tyler</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational similarity analysis reveals commonalities and differences in the semantic processing of words and objects</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>18906</fpage><lpage>18916</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3809-13.2013</pub-id><pub-id pub-id-type="pmid">24285896</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobs</surname> <given-names>K</given-names></name><name><surname>Isik</surname> <given-names>L</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How face perception unfolds over time</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1258</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09239-1</pub-id><pub-id pub-id-type="pmid">30890707</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dormal</surname> <given-names>G</given-names></name><name><surname>Rezk</surname> <given-names>M</given-names></name><name><surname>Yakobov</surname> <given-names>E</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Auditory motion in the sighted and blind: early visual deprivation triggers a large-scale imbalance between auditory and &quot;visual&quot; brain regions</article-title><source>NeuroImage</source><volume>134</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.027</pub-id><pub-id pub-id-type="pmid">27107468</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dormal</surname> <given-names>G</given-names></name><name><surname>Pelland</surname> <given-names>M</given-names></name><name><surname>Rezk</surname> <given-names>M</given-names></name><name><surname>Yakobov</surname> <given-names>E</given-names></name><name><surname>Lepore</surname> <given-names>F</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Functional preference for object sounds and voices in the brain of early blind and sighted individuals</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>86</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01186</pub-id><pub-id pub-id-type="pmid">28891782</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dormal</surname> <given-names>G</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional selectivity in sensory-deprived cortices</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>2627</fpage><lpage>2630</lpage><pub-id pub-id-type="doi">10.1152/jn.00109.2011</pub-id><pub-id pub-id-type="pmid">21430281</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Jiang</surname> <given-names>Y</given-names></name><name><surname>Shuman</surname> <given-names>M</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A cortical area selective for visual processing of the human body</article-title><source>Science</source><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1126/science.1063414</pub-id><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubois</surname> <given-names>J</given-names></name><name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></name><name><surname>Kulikova</surname> <given-names>S</given-names></name><name><surname>Poupon</surname> <given-names>C</given-names></name><name><surname>Hüppi</surname> <given-names>PS</given-names></name><name><surname>Hertz-Pannier</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The early development of brain white matter: a review of imaging studies in fetuses, newborns and infants</article-title><source>Neuroscience</source><volume>276</volume><fpage>48</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2013.12.044</pub-id><pub-id pub-id-type="pmid">24378955</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubois</surname> <given-names>J</given-names></name><name><surname>Adibpour</surname> <given-names>P</given-names></name><name><surname>Poupon</surname> <given-names>C</given-names></name><name><surname>Hertz-Pannier</surname> <given-names>L</given-names></name><name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>MRI and M/EEG studies of the white matter development in human fetuses and Infants: Review and Opinion</article-title><source>Brain Plasticity</source><volume>2</volume><fpage>49</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.3233/BPL-160031</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>R</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><volume>392</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etzel</surname> <given-names>JA</given-names></name><name><surname>Zacks</surname> <given-names>JM</given-names></name><name><surname>Braver</surname> <given-names>TS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Searchlight analysis: promise, pitfalls, <italic>and potential</italic></article-title><source>NeuroImage</source><volume>78</volume><fpage>261</fpage><lpage>269</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.041</pub-id><pub-id pub-id-type="pmid">23558106</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Price</surname> <given-names>CJ</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name><name><surname>Gutierrez-Sigut</surname> <given-names>E</given-names></name><name><surname>MacSweeney</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sign and speech share partially overlapping conceptual representations</article-title><source>Current Biology</source><volume>29</volume><fpage>3739</fpage><lpage>3747</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.08.075</pub-id><pub-id pub-id-type="pmid">31668623</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname> <given-names>BL</given-names></name><name><surname>McAdams</surname> <given-names>S</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Belin</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Abstract encoding of auditory objects in cortical activity patterns</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>2025</fpage><lpage>2037</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs162</pub-id><pub-id pub-id-type="pmid">22802575</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez</surname> <given-names>J</given-names></name><name><surname>Barnett</surname> <given-names>M</given-names></name><name><surname>Grill-Spector</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Extensive childhood experience with pokémon suggests eccentricity drives organization of visual cortex</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>611</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0592-8</pub-id><pub-id pub-id-type="pmid">31061489</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Handjaras</surname> <given-names>G</given-names></name><name><surname>Ricciardi</surname> <given-names>E</given-names></name><name><surname>Leo</surname> <given-names>A</given-names></name><name><surname>Lenci</surname> <given-names>A</given-names></name><name><surname>Cecchetti</surname> <given-names>L</given-names></name><name><surname>Cosottini</surname> <given-names>M</given-names></name><name><surname>Marotta</surname> <given-names>G</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How concepts are encoded in the human brain: a modality independent, category-based cortical organization of semantic knowledge</article-title><source>NeuroImage</source><volume>135</volume><fpage>232</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.063</pub-id><pub-id pub-id-type="pmid">27132545</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannagan</surname> <given-names>T</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Dehaene-Lambertz</surname> <given-names>G</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Origins of the specialization for letters and numbers in ventral occipitotemporal cortex</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>374</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.05.006</pub-id><pub-id pub-id-type="pmid">26072689</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Levy</surname> <given-names>I</given-names></name><name><surname>Behrmann</surname> <given-names>M</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Eccentricity Bias as an organizing principle for human high-order object Areas</article-title><source>Neuron</source><volume>34</volume><fpage>479</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00662-1</pub-id><pub-id pub-id-type="pmid">11988177</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Furey</surname> <given-names>ML</given-names></name><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Schouten</surname> <given-names>JL</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Conroy</surname> <given-names>BR</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Hanke</surname> <given-names>M</given-names></name><name><surname>Ramadge</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A common, high-dimensional model of the representational space in human ventral temporal cortex</article-title><source>Neuron</source><volume>72</volume><fpage>404</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.026</pub-id><pub-id pub-id-type="pmid">22017997</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding neural representational spaces using multivariate pattern analysis</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>435</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170325</pub-id><pub-id pub-id-type="pmid">25002277</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname> <given-names>C</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Han</surname> <given-names>Z</given-names></name><name><surname>Lin</surname> <given-names>N</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Selectivity for large nonmanipulable objects in scene-selective visual cortex does not require visual experience</article-title><source>NeuroImage</source><volume>79</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.051</pub-id><pub-id pub-id-type="pmid">23624496</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname> <given-names>DH</given-names></name><name><surname>Wiesel</surname> <given-names>TN</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title><source>The Journal of Physiology</source><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><pub-id pub-id-type="pmid">14449617</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname> <given-names>E</given-names></name><name><surname>Jiang</surname> <given-names>F</given-names></name><name><surname>Fine</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Responses in area hMT+ reflect tuning for both auditory frequency and motion after blindness early in life</article-title><source>PNAS</source><volume>116</volume><fpage>10081</fpage><lpage>10086</lpage><pub-id pub-id-type="doi">10.1073/pnas.1815376116</pub-id><pub-id pub-id-type="pmid">31036666</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>de Heer</surname> <given-names>WA</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Theunissen</surname> <given-names>FE</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title><source>Nature</source><volume>532</volume><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1038/nature17637</pub-id><pub-id pub-id-type="pmid">27121839</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>J</given-names></name><name><surname>Zhu</surname> <given-names>W</given-names></name><name><surname>Shi</surname> <given-names>F</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Qin</surname> <given-names>W</given-names></name><name><surname>Li</surname> <given-names>K</given-names></name><name><surname>Yu</surname> <given-names>C</given-names></name><name><surname>Jiang</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Thick visual cortex in the early blind</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>2205</fpage><lpage>2211</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5451-08.2009</pub-id><pub-id pub-id-type="pmid">19228973</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Julian</surname> <given-names>JB</given-names></name><name><surname>Fedorenko</surname> <given-names>E</given-names></name><name><surname>Webster</surname> <given-names>J</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An algorithmic method for functionally defining regions of interest in the ventral visual pathway</article-title><source>NeuroImage</source><volume>60</volume><fpage>2357</fpage><lpage>2364</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.055</pub-id><pub-id pub-id-type="pmid">22398396</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Azzalini</surname> <given-names>DC</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shape-independent object category responses revealed by MEG and fMRI decoding</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2246</fpage><lpage>2250</lpage><pub-id pub-id-type="doi">10.1152/jn.01074.2015</pub-id><pub-id pub-id-type="pmid">26740535</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname> <given-names>N</given-names></name><name><surname>McDermott</surname> <given-names>J</given-names></name><name><surname>Chun</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional specificity in the human brain: a window into the functional architecture of the mind</article-title><source>PNAS</source><volume>107</volume><fpage>11163</fpage><lpage>11170</lpage><pub-id pub-id-type="doi">10.1073/pnas.1005062107</pub-id><pub-id pub-id-type="pmid">20484679</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy</surname> <given-names>DN</given-names></name><name><surname>O'Craven</surname> <given-names>KM</given-names></name><name><surname>Ticho</surname> <given-names>BS</given-names></name><name><surname>Goldstein</surname> <given-names>AM</given-names></name><name><surname>Makris</surname> <given-names>N</given-names></name><name><surname>Henson</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Structural and functional brain asymmetries in human situs inversus totalis</article-title><source>Neurology</source><volume>53</volume><elocation-id>1260</elocation-id><pub-id pub-id-type="doi">10.1212/WNL.53.6.1260</pub-id><pub-id pub-id-type="pmid">10522882</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>JS</given-names></name><name><surname>Kanjlia</surname> <given-names>S</given-names></name><name><surname>Merabet</surname> <given-names>LB</given-names></name><name><surname>Bedny</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Development of the visual word form area requires visual experience: evidence from blind braille readers</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>11495</fpage><lpage>11504</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0997-17.2017</pub-id><pub-id pub-id-type="pmid">29061700</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>ML</given-names></name><name><surname>Groen</surname> <given-names>IIA</given-names></name><name><surname>Steel</surname> <given-names>A</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Similarity judgments and cortical visual responses reflect different properties of object and scene categories in naturalistic images</article-title><source>NeuroImage</source><volume>197</volume><fpage>368</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.04.079</pub-id><pub-id pub-id-type="pmid">31054350</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kitada</surname> <given-names>R</given-names></name><name><surname>Yoshihara</surname> <given-names>K</given-names></name><name><surname>Sasaki</surname> <given-names>AT</given-names></name><name><surname>Hashiguchi</surname> <given-names>M</given-names></name><name><surname>Kochiyama</surname> <given-names>T</given-names></name><name><surname>Sadato</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The brain network underlying the recognition of hand gestures in the blind: the supramodal role of the extrastriate body area</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>10096</fpage><lpage>10108</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0500-14.2014</pub-id><pub-id pub-id-type="pmid">25057211</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname> <given-names>SM</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name><name><surname>Kim</surname> <given-names>IJ</given-names></name><name><surname>Alpert</surname> <given-names>NM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Topographical representations of mental images in primary visual cortex</article-title><source>Nature</source><volume>378</volume><fpage>496</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1038/378496a0</pub-id><pub-id pub-id-type="pmid">7477406</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname> <given-names>SM</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><name><surname>Felician</surname> <given-names>O</given-names></name><name><surname>Camposano</surname> <given-names>S</given-names></name><name><surname>Keenan</surname> <given-names>JP</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name><name><surname>Ganis</surname> <given-names>G</given-names></name><name><surname>Sukel</surname> <given-names>KE</given-names></name><name><surname>Alpert</surname> <given-names>NM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The role of area 17 in visual imagery: convergent evidence from PET and rTMS</article-title><source>Science</source><volume>284</volume><fpage>167</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1126/science.284.5411.167</pub-id><pub-id pub-id-type="pmid">10102821</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kostović</surname> <given-names>I</given-names></name><name><surname>Judaš</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The development of the subplate and thalamocortical connections in the human foetal brain</article-title><source>Acta Paediatrica</source><volume>99</volume><fpage>1119</fpage><lpage>1127</lpage><pub-id pub-id-type="doi">10.1111/j.1651-2227.2010.01811.x</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lane</surname> <given-names>C</given-names></name><name><surname>Kanjlia</surname> <given-names>S</given-names></name><name><surname>Omaki</surname> <given-names>A</given-names></name><name><surname>Bedny</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>&quot;Visual&quot; Cortex of Congenitally Blind Adults Responds to Syntactic Movement</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>12859</fpage><lpage>12868</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1256-15.2015</pub-id><pub-id pub-id-type="pmid">26377472</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaver</surname> <given-names>AM</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical representation of natural complex sounds: effects of acoustic features and auditory object category</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>7604</fpage><lpage>7612</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0296-10.2010</pub-id><pub-id pub-id-type="pmid">20519535</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname> <given-names>I</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Avidan</surname> <given-names>G</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Center-periphery organization of human object Areas</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>533</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1038/87490</pub-id><pub-id pub-id-type="pmid">11319563</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logie</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Characteristics of visual short-term memory</article-title><source>European Journal of Cognitive Psychology</source><volume>1</volume><fpage>275</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1080/09541448908403088</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname> <given-names>BZ</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What drives the organization of object knowledge in the brain?</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>97</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.01.004</pub-id><pub-id pub-id-type="pmid">21317022</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Levy</surname> <given-names>I</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The topography of high-order human object Areas</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>176</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(02)01870-3</pub-id><pub-id pub-id-type="pmid">11912041</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marín-Padilla</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>The Human Brain: Prenatal Development and Structure</source><publisher-name>Springer Nature</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-14724-1</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>C</given-names></name><name><surname>Douglas</surname> <given-names>D</given-names></name><name><surname>Newsome</surname> <given-names>R</given-names></name><name><surname>Man</surname> <given-names>L</given-names></name><name><surname>Barense</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integrative and distintive coding of perceptual and concptual features in the ventral stream</article-title><source>eLife</source><volume>29</volume><elocation-id>e31873</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31873</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattioni</surname> <given-names>S</given-names></name><name><surname>Rezk</surname> <given-names>M</given-names></name><name><surname>Battal</surname> <given-names>C</given-names></name><name><surname>Vadlamudi</surname> <given-names>J</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The balanced act of crossmodal and intramodal plasticity: enhanced representation of auditory categories in the occipital cortex of early blind people links to reduced temporal coding</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>554</elocation-id><pub-id pub-id-type="doi">10.1167/18.10.554</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCandliss</surname> <given-names>BD</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The visual word form area: expertise for reading in the fusiform gyrus</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>293</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(03)00134-7</pub-id><pub-id pub-id-type="pmid">12860187</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerel</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Processing of natural sounds in human auditory cortex: tonotopy, spectral tuning, and relation to voice sensitivity</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>14205</fpage><lpage>14216</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1388-12.2012</pub-id><pub-id pub-id-type="pmid">23055490</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname> <given-names>S</given-names></name><name><surname>Echavarria</surname> <given-names>CE</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Thinking outside the box: rectilinear shapes selectively activate scene-selective cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>6721</fpage><lpage>6735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4802-13.2014</pub-id><pub-id pub-id-type="pmid">24828628</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neville</surname> <given-names>H</given-names></name><name><surname>Bavelier</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Human brain plasticity: evidence from sensory deprivation and altered language experience</article-title><source>Progress in Brain Research</source><volume>138</volume><fpage>177</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(02)38078-6</pub-id><pub-id pub-id-type="pmid">12432770</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Wingfield</surname> <given-names>C</given-names></name><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Su</surname> <given-names>L</given-names></name><name><surname>Marslen-Wilson</surname> <given-names>W</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id><pub-id pub-id-type="pmid">24743308</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Polyn</surname> <given-names>SM</given-names></name><name><surname>Detre</surname> <given-names>GJ</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>424</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id><pub-id pub-id-type="pmid">16899397</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname> <given-names>AJ</given-names></name><name><surname>Jiang</surname> <given-names>F</given-names></name><name><surname>Abdi</surname> <given-names>H</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Partially distributed representations of objects and faces in ventral temporal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>580</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1162/0898929053467550</pub-id><pub-id pub-id-type="pmid">15829079</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname> <given-names>HP</given-names></name><name><surname>Pillet</surname> <given-names>I</given-names></name><name><surname>Ritchie</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Factors determining where Category-Selective Areas emerge in visual cortex</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>784</fpage><lpage>797</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.06.006</pub-id><pub-id pub-id-type="pmid">31327671</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname> <given-names>WJ</given-names></name><name><surname>Wu</surname> <given-names>G</given-names></name><name><surname>Li</surname> <given-names>CX</given-names></name><name><surname>Lin</surname> <given-names>F</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name><name><surname>Lei</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Progressive atrophy in the optic pathway and visual cortex of early blind chinese adults: a voxel-based morphometry magnetic resonance imaging study</article-title><source>NeuroImage</source><volume>37</volume><fpage>212</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.05.014</pub-id><pub-id pub-id-type="pmid">17560797</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>HJ</given-names></name><name><surname>Lee</surname> <given-names>JD</given-names></name><name><surname>Kim</surname> <given-names>EY</given-names></name><name><surname>Park</surname> <given-names>B</given-names></name><name><surname>Oh</surname> <given-names>MK</given-names></name><name><surname>Lee</surname> <given-names>S</given-names></name><name><surname>Kim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Morphological alterations in the congenital blind based on the analysis of cortical thickness and surface area</article-title><source>NeuroImage</source><volume>47</volume><fpage>98</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.03.076</pub-id><pub-id pub-id-type="pmid">19361567</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parsons</surname> <given-names>NR</given-names></name><name><surname>Teare</surname> <given-names>MD</given-names></name><name><surname>Sitch</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unit of analysis issues in laboratory-based research</article-title><source>eLife</source><volume>7</volume><elocation-id>e32486</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32486</pub-id><pub-id pub-id-type="pmid">29319501</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Passingham</surname> <given-names>RE</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Kötter</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The anatomical basis of functional localization in the cortex</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>606</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1038/nrn893</pub-id><pub-id pub-id-type="pmid">12154362</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Lu</surname> <given-names>X</given-names></name><name><surname>He</surname> <given-names>C</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Tool selectivity in left occipitotemporal cortex develops without vision</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1225</fpage><lpage>1234</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00411</pub-id><pub-id pub-id-type="pmid">23647514</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>He</surname> <given-names>C</given-names></name><name><surname>Han</surname> <given-names>Z</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Nonvisual and visual object shape representations in occipitotemporal cortex: evidence from congenitally blind and sighted adults</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>163</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1114-13.2014</pub-id><pub-id pub-id-type="pmid">24381278</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Category selectivity in human visual cortex: beyond visual object recognition</article-title><source>Neuropsychologia</source><volume>105</volume><fpage>177</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.03.033</pub-id><pub-id pub-id-type="pmid">28377161</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>F</given-names></name><name><surname>Mitchell</surname> <given-names>T</given-names></name><name><surname>Botvinick</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title><source>NeuroImage</source><volume>45</volume><fpage>S199</fpage><lpage>S209</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.11.007</pub-id><pub-id pub-id-type="pmid">19070668</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pietrini</surname> <given-names>P</given-names></name><name><surname>Furey</surname> <given-names>ML</given-names></name><name><surname>Ricciardi</surname> <given-names>E</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Wu</surname> <given-names>WH</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Guazzelli</surname> <given-names>M</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Beyond sensory images: object-based representation in the human ventral pathway</article-title><source>PNAS</source><volume>101</volume><fpage>5658</fpage><lpage>5663</lpage><pub-id pub-id-type="doi">10.1073/pnas.0400707101</pub-id><pub-id pub-id-type="pmid">15064396</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pillet</surname> <given-names>I</given-names></name><name><surname>Beeck</surname> <given-names>H</given-names></name><name><surname>De</surname> <given-names>O</given-names></name><name><surname>Masson</surname> <given-names>HL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Comparing the functional structure of neural networks from representational similarity analysis with those from functional connectivity and univariate analyses * correspondence</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/487199</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirier</surname> <given-names>C</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name><name><surname>Scheiber</surname> <given-names>C</given-names></name><name><surname>De Volder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Auditory motion processing in early blind subjects</article-title><source>Cognitive Processing</source><volume>5</volume><fpage>254</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1007/s10339-004-0031-1</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disentangling representations of object shape and object category in human visual cortex: the Animate-Inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00924</pub-id><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname> <given-names>D</given-names></name><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects</article-title><source>NeuroImage</source><volume>193</volume><fpage>167</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.028</pub-id><pub-id pub-id-type="pmid">30885785</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajimehr</surname> <given-names>R</given-names></name><name><surname>Devaney</surname> <given-names>KJ</given-names></name><name><surname>Bilenko</surname> <given-names>NY</given-names></name><name><surname>Young</surname> <given-names>JC</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The &quot;parahippocampal place area&quot; responds preferentially to high spatial frequencies in humans and monkeys</article-title><source>PLOS Biology</source><volume>9</volume><elocation-id>e1000608</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000608</pub-id><pub-id pub-id-type="pmid">21483719</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname> <given-names>L</given-names></name><name><surname>Tsuchiya</surname> <given-names>N</given-names></name><name><surname>Serre</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Reading the mind's eye: decoding category information during mental imagery</article-title><source>NeuroImage</source><volume>50</volume><fpage>818</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.084</pub-id><pub-id pub-id-type="pmid">20004247</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reich</surname> <given-names>L</given-names></name><name><surname>Szwed</surname> <given-names>M</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A ventral visual stream reading center independent of visual experience</article-title><source>Current Biology</source><volume>21</volume><fpage>363</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.01.040</pub-id><pub-id pub-id-type="pmid">21333539</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricciardi</surname> <given-names>E</given-names></name><name><surname>Vanello</surname> <given-names>N</given-names></name><name><surname>Sani</surname> <given-names>L</given-names></name><name><surname>Gentili</surname> <given-names>C</given-names></name><name><surname>Scilingo</surname> <given-names>EP</given-names></name><name><surname>Landini</surname> <given-names>L</given-names></name><name><surname>Guazzelli</surname> <given-names>M</given-names></name><name><surname>Bicchi</surname> <given-names>A</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The effect of visual experience on the development of functional architecture in hMT+</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2933</fpage><lpage>2939</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm018</pub-id><pub-id pub-id-type="pmid">17372275</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rice</surname> <given-names>GE</given-names></name><name><surname>Watson</surname> <given-names>DM</given-names></name><name><surname>Hartley</surname> <given-names>T</given-names></name><name><surname>Andrews</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Low-level image properties of visual objects predict patterns of neural response across category-selective regions of the ventral visual pathway</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>8837</fpage><lpage>8844</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5265-13.2014</pub-id><pub-id pub-id-type="pmid">24966383</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Röder</surname> <given-names>B</given-names></name><name><surname>Stock</surname> <given-names>O</given-names></name><name><surname>Bien</surname> <given-names>S</given-names></name><name><surname>Neville</surname> <given-names>H</given-names></name><name><surname>Rösler</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Speech processing activates visual cortex in congenitally blind humans</article-title><source>European Journal of Neuroscience</source><volume>16</volume><fpage>930</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2002.02147.x</pub-id><pub-id pub-id-type="pmid">12372029</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadato</surname> <given-names>N</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><name><surname>Grafman</surname> <given-names>J</given-names></name><name><surname>Deiber</surname> <given-names>MP</given-names></name><name><surname>Ibañez</surname> <given-names>V</given-names></name><name><surname>Hallett</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neural networks for braille reading by the blind</article-title><source>Brain</source><volume>121</volume><fpage>1213</fpage><lpage>1229</lpage><pub-id pub-id-type="doi">10.1093/brain/121.7.1213</pub-id><pub-id pub-id-type="pmid">9679774</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saygin</surname> <given-names>ZM</given-names></name><name><surname>Osher</surname> <given-names>DE</given-names></name><name><surname>Koldewyn</surname> <given-names>K</given-names></name><name><surname>Reynolds</surname> <given-names>G</given-names></name><name><surname>Gabrieli</surname> <given-names>JDE</given-names></name><name><surname>Saxe</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Anatomical connectivity patterns predict face selectivity in the fusiform gyrus</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>321</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1038/nn.3001</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saygin</surname> <given-names>ZM</given-names></name><name><surname>Osher</surname> <given-names>DE</given-names></name><name><surname>Norton</surname> <given-names>ES</given-names></name><name><surname>Youssoufian</surname> <given-names>DA</given-names></name><name><surname>Beach</surname> <given-names>SD</given-names></name><name><surname>Feather</surname> <given-names>J</given-names></name><name><surname>Gaab</surname> <given-names>N</given-names></name><name><surname>Gabrieli</surname> <given-names>JD</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Connectivity precedes function in the development of the visual word form area</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1250</fpage><lpage>1255</lpage><pub-id pub-id-type="doi">10.1038/nn.4354</pub-id><pub-id pub-id-type="pmid">27500407</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Wolf</surname> <given-names>L</given-names></name><name><surname>Bileschi</surname> <given-names>S</given-names></name><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Robust object recognition with cortex-like mechanisms</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>29</volume><fpage>411</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.56</pub-id><pub-id pub-id-type="pmid">17224612</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shimony</surname> <given-names>JS</given-names></name><name><surname>Burton</surname> <given-names>H</given-names></name><name><surname>Epstein</surname> <given-names>AA</given-names></name><name><surname>McLaren</surname> <given-names>DG</given-names></name><name><surname>Sun</surname> <given-names>SW</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Diffusion tensor imaging reveals white matter reorganization in early blind humans</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>1653</fpage><lpage>1661</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj102</pub-id><pub-id pub-id-type="pmid">16400157</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slotnick</surname> <given-names>SD</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name><name><surname>Kosslyn</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Visual mental imagery induces retinotopically organized activation of early visual Areas</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1570</fpage><lpage>1583</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi035</pub-id><pub-id pub-id-type="pmid">15689519</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srihasam</surname> <given-names>K</given-names></name><name><surname>Vincent</surname> <given-names>JL</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Novel domain formation reveals proto-architecture in inferotemporal cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1776</fpage><lpage>1783</lpage><pub-id pub-id-type="doi">10.1038/nn.3855</pub-id><pub-id pub-id-type="pmid">25362472</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>M</given-names></name><name><surname>Thompson</surname> <given-names>R</given-names></name><name><surname>Cusack</surname> <given-names>R</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Top-down activation of shape-specific population codes in visual cortex during mental imagery</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>1565</fpage><lpage>1572</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4657-08.2009</pub-id><pub-id pub-id-type="pmid">19193903</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname> <given-names>E</given-names></name><name><surname>Cohen</surname> <given-names>L</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reading with sounds: sensory substitution selectively activates the visual word form area in the blind</article-title><source>Neuron</source><volume>76</volume><fpage>640</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.026</pub-id><pub-id pub-id-type="pmid">23141074</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname> <given-names>E</given-names></name><name><surname>Vannuscorps</surname> <given-names>G</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Plasticity based on compensatory effector use in the association but not primary sensorimotor cortex of people born without hands</article-title><source>PNAS</source><volume>115</volume><fpage>7801</fpage><lpage>7806</lpage><pub-id pub-id-type="doi">10.1073/pnas.1803926115</pub-id><pub-id pub-id-type="pmid">29997174</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname> <given-names>E</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Neural representation of visual concepts in people born blind</article-title><source>Nature Communications</source><volume>9</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41467-018-07574-3</pub-id><pub-id pub-id-type="pmid">30531889</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname> <given-names>E</given-names></name><name><surname>Amedi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual cortex extrastriate body-selective area activation in congenitally blind people &quot;seeing&quot; by using sounds</article-title><source>Current Biology</source><volume>24</volume><fpage>687</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.02.010</pub-id><pub-id pub-id-type="pmid">24613309</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname> <given-names>N</given-names></name><name><surname>Sakurai</surname> <given-names>T</given-names></name><name><surname>Davis</surname> <given-names>KL</given-names></name><name><surname>Buxbaum</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Linking oligodendrocyte and myelin dysfunction to neurocircuitry abnormalities in schizophrenia</article-title><source>Progress in Neurobiology</source><volume>93</volume><fpage>13</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2010.09.004</pub-id><pub-id pub-id-type="pmid">20950668</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname> <given-names>F</given-names></name><name><surname>Nakayama</surname> <given-names>K</given-names></name><name><surname>Moscovitch</surname> <given-names>M</given-names></name><name><surname>Weinrib</surname> <given-names>O</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Response properties of the human fusiform face area</article-title><source>Cognitive Neuropsychology</source><volume>17</volume><fpage>257</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1080/026432900380607</pub-id><pub-id pub-id-type="pmid">20945183</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Ackeren</surname> <given-names>MJ</given-names></name><name><surname>Barbero</surname> <given-names>FM</given-names></name><name><surname>Mattioni</surname> <given-names>S</given-names></name><name><surname>Bottini</surname> <given-names>R</given-names></name><name><surname>Collignon</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neuronal populations in the occipital cortex of the blind synchronize to the temporal dynamics of speech</article-title><source>eLife</source><volume>7</volume><elocation-id>e31640</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31640</pub-id><pub-id pub-id-type="pmid">29338838</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Hurk</surname> <given-names>J</given-names></name><name><surname>Van Baelen</surname> <given-names>M</given-names></name><name><surname>Op de Beeck</surname> <given-names>HP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Development of visual category selectivity in ventral visual cortex does not require visual experience</article-title><source>PNAS</source><volume>114</volume><fpage>E4501</fpage><lpage>E4510</lpage><pub-id pub-id-type="doi">10.1073/pnas.1612862114</pub-id><pub-id pub-id-type="pmid">28507127</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Kriegstein</surname> <given-names>K</given-names></name><name><surname>Kleinschmidt</surname> <given-names>A</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name><name><surname>Giraud</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Interaction of face and voice Areas during speaker recognition</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>367</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1162/0898929053279577</pub-id><pub-id pub-id-type="pmid">15813998</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Ejaz</surname> <given-names>N</given-names></name><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id><pub-id pub-id-type="pmid">26707889</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Han</surname> <given-names>Z</given-names></name><name><surname>He</surname> <given-names>C</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>How visual is the visual cortex? comparing connectional and functional fingerprints between congenitally blind and sighted individuals</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>12545</fpage><lpage>12559</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3914-14.2015</pub-id><pub-id pub-id-type="pmid">26354920</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>He</surname> <given-names>C</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Zhong</surname> <given-names>S</given-names></name><name><surname>Gong</surname> <given-names>G</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Domain selectivity in the parahippocampal gyrus is predicted by the same structural connectivity patterns in blind and sighted individuals</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>4705</fpage><lpage>4716</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3622-16.2017</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname> <given-names>KE</given-names></name><name><surname>Shakespeare</surname> <given-names>TJ</given-names></name><name><surname>O'Donoghue</surname> <given-names>MC</given-names></name><name><surname>Alexander</surname> <given-names>I</given-names></name><name><surname>Ragge</surname> <given-names>N</given-names></name><name><surname>Cowey</surname> <given-names>A</given-names></name><name><surname>Bridge</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Early auditory processing in area V5/MT+ of the congenitally blind brain</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>18242</fpage><lpage>18246</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2546-13.2013</pub-id><pub-id pub-id-type="pmid">24227733</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolbers</surname> <given-names>T</given-names></name><name><surname>Klatzky</surname> <given-names>RL</given-names></name><name><surname>Loomis</surname> <given-names>JM</given-names></name><name><surname>Wutte</surname> <given-names>MG</given-names></name><name><surname>Giudice</surname> <given-names>NA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Modality-independent coding of spatial layout in the human brain</article-title><source>Current Biology</source><volume>21</volume><fpage>984</fpage><lpage>989</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.04.038</pub-id><pub-id pub-id-type="pmid">21620708</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>Y</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Men</surname> <given-names>W</given-names></name><name><surname>Gao</surname> <given-names>JH</given-names></name><name><surname>Bi</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Doctor, Teacher, and Stethoscope: Neural Representation of Different Types of Semantic Relations</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>3303</fpage><lpage>3317</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2562-17.2018</pub-id><pub-id pub-id-type="pmid">29476016</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xue</surname> <given-names>S</given-names></name><name><surname>Weng</surname> <given-names>X</given-names></name><name><surname>He</surname> <given-names>S</given-names></name><name><surname>Li</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Similarity representation of pattern-information fMRI</article-title><source>Chinese Science Bulletin</source><volume>58</volume><fpage>1236</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1007/s11434-013-5743-0</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname> <given-names>H</given-names></name><name><surname>Constable</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Image distortion correction in EPI: comparison of field mapping with point spread function mapping</article-title><source>Magnetic Resonance in Medicine</source><volume>48</volume><fpage>137</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1002/mrm.10200</pub-id><pub-id pub-id-type="pmid">12111941</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.50732.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Reviewer</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The study adds important support, using brain decoding, to the theory that the visual cortex in early blindness processes similar category boundaries as in vision. This is an extremely well designed study and extends beyond previous research in the field by adopting a substantially richer stimulus space, and by carefully considering alternative contributions to the stimulus domains. Together with detailed and thoughtful analysis, the study provides a comprehensive account of the similarities and differences of categorical representation in the blind ventral occipito-temporal cortex.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Similar categorical representation from sound and sight in the occipito-temporal cortex of sighted and blind&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Tamar R Makin as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary</p><p>This study is aimed at assessing categorical representational similarities between sighted and blind individuals in the early and ventral occipitotemporal visual cortex. The authors used sounds of objects, animals, humans and scenes to construct a representational structure in the blinds visual cortex and compared that to the representational structure evoked by images and sounds to the same categories in sighted individuals. They showed that the blind response pattern and response pattern connectivity for such sounds is broadly similar to that found in sighted using images in the VOTC, and that this representational structure didn't merely reflect auditory pitch similarity or low-level visual processing. Additionally, they showed that the inter-subject consistency is higher in the blind for sounds than in the sighted for the same sounds, but lower than for sight. Overall the study adds important support, using RSA, to the theory that the visual cortex in early blindness processes similar category boundaries as in vision. This is an extremely well designed study and extends beyond previous research in the field by adopting a richer stimulus space and more detailed analyses that provide a more comprehensive account of the similarities and differences between the different groups and stimulus domains.</p><p>All three reviewers were particularly enthusiastic about the experimental design and were excited towards the opportunity of developing a more comprehensive understanding of the similarities and differences between blind and sighted individuals. In that respect, we felt that although interesting, the results highlighting the similarities across groups shouldn't obstruct the authors from exploring potential differences between groups. In addition, multiple questions were raised relating to the specific statistical tests and ROI definition, which required further justification or better clarity of the test specifics. In some cases, the manuscript would benefit from more mainstream/standardised analyses, as there were concerns that the current approaches were at times inflating the effects/their significance. The consensus across reviewers was that we are interested in a more nuanced understanding of your findings, even if this might potentially weaken some of your key conclusions (i.e. similar categorical representation from sound and sight in the occipito-temporal cortex of sighted and blind).</p><p>Major comments</p><p>1) The reviewers felt that there are some missed opportunities here to explore beyond the simple DSM correlations. One reviewer suggested more formal model comparisons, another suggested interrogating the representation structure with more detail. I'm appending the specific comments below:</p><p>Reviewer 1: In Abstract and elsewhere (e.g. “the representational structure of visual and auditory categories was almost double in the bind when compared to the sighted group&quot;) – The authors are confusing the R with the effect size (R square) which is actually almost four times greater. In general – they might like to consider more formal comparison across models (PCM or equivalent), while accounting for noise levels (if the inter-subject variance is greater than the correlation is likely to be lower).</p><p>Reviewer 3: The authors suggest that &quot;VOTC reliably encodes sounds categories in blind people using a representational structure strikingly similar to the one found in vision&quot;. This conclusion is based on the magnitude of correlations between DSMs (e.g. Figure 3). However, I think it is important to go beyond the significant correlations and look carefully at the representational structure (e.g. see King et al., 2019). For example, while the dendrogram for SCv shows an initial split between the animate and inanimate items, for EBa the initial split is for the human conditions (HV, HN) from all others, and the animal conditions (AM, AM) and more closely grouped with BM and MG. Thus, the high correlation between SCv and EBa actually belies some differences in the representational structure that I think are worth commenting on. Similar comparison can be made for EVC and for SCa. So, I recommend the authors consider the representational structure in more detail and probably should draw back slightly from the claim that the representational structure in EBa and SCv are &quot;strikingly similar&quot;.</p><p>2) One of the reviewers was particularly concerned about the usage of group means to characterise differences between groups, without adequately representing inter-subject variance within each group. In some analyses (e.g. the topographical selectivity map) the mean is taken as the sole group statistics (as a fixed effect), making the analysis highly susceptible to outliers and precluding generalisation to other samples. In other analyses (e.g. DSMs) a permutation test is included to determine group differences; however the specifics of this permutation tests are not well explained and require more thought – does this permutation test adequately captures the variance of the categorical representation between sighted and blind individuals? Or are the authors potentially confusing their units of analysis (see Parsons et al., <italic>eLife</italic>, 2018)? Ideally, variance should be assessed by measuring the effect in each individual participant and comparing the effect across groups (indeed this approach is used in some of the tests in the current study). In the case of DSM, each participant (e.g. from the EBa group) should be correlated to the SCv group, generating a distribution of the effect of interest. To an extent, a similar analysis is presented in Figure 5, providing much lower effect sizes than highlighted throughout the paper. So, we are looking for a more considered quantification of the main effects. A lot of confusion/misinterpretation could be avoided if the authors used standard statistics that takes into consideration the inter-subject variance, which is the key unit of your analysis (e.g. t-test and its variants). If the authors feel that their own approach is preferable, then they should increase the clarity on their analysis and its validity, with a clear statement of what null is.</p><p>3) A few of the reviewers were commenting on inconsistent statistical criteria, with the significance test flipping between one-tail to two-tails. For example, correlations between DSM were tested for significance using a one-tailed permutation test. This resulted in identifying a very strong negative correlation between the SCv and SCa in the early visual cortex (r = -.50), as non-significant, where in fact this effect is stronger than the correlation presented between SCv and EBa DSMs – r=.41, p=0.03). Similarly, the SCv DSM in the early visual cortex may be significantly negatively correlated to the behavioral similarity ratings (r = -0.09; the in sighted EVC DSM is reported to be significantly correlated with both the behavioural DSMs at r=.09, p=0.01). This could be an interesting finding, e.g. if cross-modal inhibition is categorically selective (allowing for decoding of sound categories in EVC in another study; (Vetter et al., 2014)). This finding could also add to the mechanistic explanation of the plasticity in the blind. But at present it is uninterpretable. We therefore request that one-tail testing would be avoided, unless there's strong statistical justification to usage it (e.g. in case of a replication of previous findings). In that case, the reasons for the one-tail hypothesis should be reported more transparently and interpreted more cautiously. But if the authors are confirming a new hypothesis, which we believe is the case for most of the tests reported, we'd encourage them to stick to two-tailed hypothesis testing. The authors are welcomed to consider correction for multiple comparisons, but again, we are looking for consistency throughout the manuscript and justification of the criteria used (or abandoned).</p><p>4) The focus on VOTC, and the specific definition of the ROIs requires further consideration. In particular the reviewers mentioned the lack of discrimination within it, limited coverage of the ventral stream, and lack of consideration of other areas which might be involved in categorical representation, e.g. high order auditory areas or other visual areas in the lateral occipital cortex (see also comment 6 below). Further exploratory analysis, focused by previous research on similar topics, would strengthen the interoperability of the results. A searchlight approach might be particularly helpful, though we leave it to the authors to decide on the specific methodology.</p><p>5) The topographic selectivity analysis raised multiple comments from the reviewers. Beyond the issue raised in comment 2 above, it was agreed that this analysis potentially reveals important differences between the groups that are not adequately captured in the current presentation and analysis. For example, Reviewer 2 mentions: &quot;the blind show nearly no preference for animal sounds in accordance with claims made by (Bi et al., 2016); in the sighted the human non-vocalizations are the preferred category for face selective areas in the visual cortex but in the auditory version (in both blind and sighted) there is mostly vocalization selectivity; The blind show little or no preference for the large-environmental sounds whereas the sighted show a differentiation between the two sound types&quot;. Reviewer 3 mentions: &quot;In SCv, the medial ventral temporal cortex is primarily BIG-ENV, but in EBa, that same region is primarily BIG-MEC. Similarly, there appears to be more cortex showing VOC. In SCa than in either of the other two groups&quot;. We appreciate that some of the analysis might have been used as a replication of the Van den Hurk study, but we also expect them to stand alone, in terms of quality of analysis and interpretation. In particular, these topographical preference differences could be interesting and meaningful with relation to understanding blind visual cortex function, but it is hard to judge their strength or significance as the topographical selectivity maps are not thresholded, by activation strength or selectivity significance. In addition, we would like some further clarification of how the correlations have been constructed, if indeed a use-takes-all approach is used to label each voxel.</p><p>6) In the representational connectivity analysis, the authors may be overestimating the connectivity between regions due to contribution of intrinsic fluctuations between areas. To more accurately estimate the representational connectivity, it would be better if the authors used separate trials for the seed and target regions. See Henriksson et al., 2015, Neuroimage for a demonstration and discussion of this issue. Indeed, considering the clear differences found elsewhere between groups, the high between-group correlations are striking. It could have been informative to examine a control node (e.g. low-level visual cortex; low level auditory cortex) just to gain a better sense for what these correlations reveal. Also, in this section it is not clear why the authors use a separate ROI for each of the 3 VOTC sub-regions, rather than the combination of all three, as they have for the main analysis? As a minor point, the reviewer wasn't clear how the authors switched from 3 nodes for each group to only one comparison for each group pair.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.50732.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Major comments</p><p>1) The reviewers felt that there are some missed opportunities here to explore beyond the simple DSM correlations. One reviewer suggested more formal model comparisons, another suggested interrogating the representation structure with more detail. I'm appending the specific comments below:</p><p>Reviewer 1: In Abstract and elsewhere (e.g. 'the representational structure of visual and auditory categories was almost double in the bind when compared to the sighted group&quot;) – The authors are confusing the R with the effect size (R square) which is actually almost four times greater.</p></disp-quote><p>We thank the reviewers for highlighting this confusing statement. We agree that such claims were unclear and potentially leading to misunderstanding the outcome of correlation values and effect size. In order to avoid such confusion, we removed the sentence from the Abstract and we rephrased the sentence in the Discussion. The sentence now reads:</p><p>“The representational structure of visual categories in sighted was significantly closer to the structure of the auditory categories in blind than in sighted”.</p><disp-quote content-type="editor-comment"><p>In general – they might like to consider more formal comparison across models (PCM or equivalent), while accounting for noise levels (if the inter-subject variance is greater than the correlation is likely to be lower).</p></disp-quote><p>We thank the reviewers for this suggestion. We fully agree that the intersubject variance within a brain region and the consequent noise level within each group are important points to take into consideration when looking at the correlation with our models. The Pattern Component Modeling (PCM) is indeed a useful analysis to perform a formal comparison between models notably normalizing the results according to the noise ceiling (Diedrichsen et al., 2018).</p><p>However, we believe that the use of normalization according to the noise ceiling might not be the best approach in our study. A high noise ceiling is indicative of high inter-subject reliability in the representational structure of a region for our stimuli space. Therefore, if we normalize our data according to the noise ceiling, we would lose important information about the inter-subject variability itself and this could potentially bring to some erroneous conclusion. One illustrative example of a possible misinterpretation of the data after such normalization comes from the comparison of occipital and temporal brain regions in the sighted in the auditory experiment (SCa). As expected, in <xref ref-type="fig" rid="respfig1">Author response image 1</xref> we see much higher decoding of auditory categories in the temporal cortex (~75% decoding accuracy using mean binary decoding of our 8 categories) when compared to VOTC (~55% decoding accuracy using mean binary decoding of our 8 categories).</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>MVP-classification results in the SCa group.</title><p>The 28 binary classification accuracies averaged are represented for the VOTC (in pink) and the temporal ROI (in green). Each circle represents one subject, the coloured horizontal line represents the group mean and the black vertical bars represent the standard error of the mean across subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-resp-fig1-v2.tif"/></fig><p>We can now go a step further and look at the RSA analysis in the TEMP and VOTC parcels. In <xref ref-type="fig" rid="respfig2">Author response image 2</xref>, left side, you see a dot-plot graph representing the correlation of the behavioral model with the DSMs extracted from both VOTC and TEMP parcels in SCa. We see, again as expected, that the categorical model correlates more with the temporal DSM than with the VOTC DSM. In addition, the inter-subject correlation -therefore the noise ceiling- (represented by the grey line) is higher in the temporal region than in VOTC. This is an expected result, suggesting that the inter-subject variance in VOTC is higher (i.e. the noise ceiling is lower) compared to the one in the temporal region for the obvious reason that temporal regions likely have a more coherent response code across subjects given their prominent role in coding auditory information (including low-level features of the sounds that can be segregated from our different categories). Such effect has been extensively described by the work of Hasson et al., (Hasson et al., 2004, 2008), in which the authors reported a striking level of voxelby-voxel synchronization between individuals, especially in the sensory cortices involved in the processing of the stimuli at play (e.g. primary and secondary visual and auditory cortices during the presentation of visual and auditory video clips, respectively).</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Dot-plot graphs representing the correlation of the behavioral model with the DSMs extracted from both VOTC (pink) and a TEMP (green) parcels, in sighted for auditory stimuli (SCa).</title><p>Each dot represents one subject, the colored (pink and green) lines represent the group mean values. The vertical black bars represent the standard error of the mean across subjects. Left panel: Spearman’s correlation with the behavioural model is represented here. The grey lines represent the noise ceiling (e.g. inter-subject correlation in each ROI). High inter-subject correlation means low variance. Right panel: The Spearman’s correlation values are represented after normalization for the noise ceiling. Since the noise ceiling was obviously higher than the group mean correlation in both ROI, negative values were expected after normalization.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-resp-fig2-v2.tif"/></fig><p>This information is, by itself, important. However, if we would normalize the results of the model comparisons according to these noise ceilings we would lose this information and we would even reverse the results of the correlation with the behavioural models (see <xref ref-type="fig" rid="respfig2">Author response image 2</xref>, right side) and we would observe a higher correlation in VOTC than in the temporal lobe, which we believe would misleadingly suggest higher auditory categorical information in VOTC than in the temporal cortex if one did not consider the noise ceiling being much higher in the temporal cortex. As a side note, the negative values after the normalization were expected here, since in both ROIs the noise ceiling was higher compared to the group average correlation values.</p><p>We therefore think that the noise not only provide interesting information about the brain representations in the different groups but also, in complement to the correlation between the brain and model DSM, allows to evaluate fully and transparently the fine-grained structure of how a region codes the stimuli space. This is a point that we actually address more directly with the new inter-subjects analysis that we now ran to understand how variable is the brain representation in VOTC across subjects belonging either to the same group or to different groups (see new Figure 5 in the main manuscript for further details).</p><p>If we normalize the correlation between the representational similarity of VOTC and the behavioural model according to the noise ceiling of each group, we would indeed find a comparable amount of correlation in VOTC both for EBa and SCa (<xref ref-type="fig" rid="respfig3">Author response image 3</xref>, right panel).</p><fig id="respfig3"><label>Author response image 3.</label><caption><title>Dot-plot graphs representing the correlation of the behavioral model with the DSMs extracted from VOTC in both SCa (pink) and a EBa (green) groups.</title><p>Each dot represents one subject, the colored (pink and green) lines represent the group mean values. The vertical black bars represent the standard error of the mean across subjects. Left panel: Spearman’s correlation with the behavioural model. The grey lines represent the noise ceiling (e.g. inter-subject correlation in each group). High inter-subject correlation means low variance. Right panel: The Spearman’s correlation values are represented after normalization for the noise ceiling. Since the noise ceiling was higher than the group mean correlation in both group, negative values were expected after normalization.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-resp-fig3-v2.tif"/></fig><p>However, if we look at the data without the normalization (<xref ref-type="fig" rid="respfig3">Author response image 3</xref>, left panel), we clearly see that there is a difference between the two groups, with the representation in SCa being much noisier and variable across subjects (therefore having a lower noise ceiling). We are convinced that, in case of normalization for noise ceiling, such absence of difference (following the reasoning we made above when comparing temporal and occipital regions) would be misleading.</p><p>For these reasons we think that the use of RSA, with the representation of both the correlation results and the noise ceiling (provided in each of our representation of RSA analyses) for each group and ROI is preferable for fully and transparently appreciating our results, also avoiding misleading the reader. Indeed, RSA provides a straightforward way of measuring the correspondence between the predicted (our models) and observed (the brain response) distances, in order to select the model that better explains the data in each group, given a certain level of noise.</p><p>In addition, there are also some methodological points that make RSA the best choice for our investigation (as highlighted by Diedrichsen and Kriegeskorte, 2017 in their work comparing PCM and RSA analyses). First of all, in case of a condition-rich design and for simple models such as it is the case in our study, RSA is much more computationally efficient compared than PCM. More importantly, the assumption of a linear relationship between predicted and measured representational dissimilarities (which is made by PCM), is not always a desirable choice for fMRI measurements since it might be violated in many cases (Driediechsen and Kriegeskorte, 2017). We now use the Spearman’s correlation to compare our models and the brain representations since this rank-correlation-based RSA provides a robust method without relying on a linear relationship between the predicted and measured dissimilarities (Driediechsen and Kriegeskorte, 2017).</p><disp-quote content-type="editor-comment"><p>Reviewer 3: The authors suggest that &quot;VOTC reliably encodes sounds categories in blind people using a representational structure strikingly similar to the one found in vision&quot;. This conclusion is based on the magnitude of correlations between DSMs (e.g. Figure 3). However, I think it is important to go beyond the significant correlations and look carefully at the representational structure (e.g. see King et al., 2019). For example, while the dendrogram for SCv shows an initial split between the animate and inanimate items, for EBa the initial split is for the human conditions (HV, HN) from all others, and the animal conditions (AM, AM) and more closely grouped with BM and MG. Thus, the high correlation between SCv and EBa actually belies some differences in the representational structure that I think are worth commenting on. Similar comparison can be made for EVC and for SCa. So, I recommend the authors consider the representational structure in more detail and probably should draw back slightly from the claim that the representational structure in EBa and SCv are &quot;strikingly similar&quot;.</p></disp-quote><p>We fully take this point and we followed the recommendation of the reviewers to go beyond the correlation values and to look more in details at the representational structure. This further exploration of our data is in line with the suggestion of toning down our claim about the similarity of the VOTC representational structure in EBa and SCv. In the Abstract, and elsewhere in the paper, we systematically substituted the wording “strikingly similar” with “partially similar”. More generally, we moved our attention from the similarity between the representational structure of our three groups towards highlighting some interesting between-groups differences.</p><p>To do so, we now use a hierarchical clustering approach similar to the one of King et al., 2019, as suggested by the reviewer. We describe the analysis in the new section of the Materials and Methods entitled Hierarchical clustering analysis on the brain categorical representations:</p><p>“In order to go beyond the correlation values and to explore more qualitatively the representational structure of VOTC and EVC in the 3 groups, we implemented a hierarchical clustering approach (King et al., 2019). […] We applied the same clustering analysis also on the behavioural data (see Figure 4—figure supplement 2)”.</p><p>Here are the results, reported in the paper under the section “Hierarchical clustering analysis on the brain categorical representation”:</p><p>“We implemented this analysis to go beyond the magnitude of correlation values and to qualitatively explore the representational structure of our ROIs in the 3 groups. […] In the EBa, instead, we find a different clustering structure with the animal and the human categories being separate“.</p><p>As now reported in the Discussion section, the results from the hierarchical clustering analysis, together with the Jaccard similarity analysis, highlight a domain-by-sensory experience interaction, with the animal category represented differently in blind compared to the sighted subjects.</p><p>As we now report in the Discussion:</p><p>“Previous studies already suggested that intrinsic characteristics of objects belonging to different categories might drive different representation in the VOTC of the blind (Wang et al., 2015; Bi et al., 2016).”</p><p>Compared to previous literature, our results highlight a further distinction within the animate category in early blind people. In this group, the animal category does not cluster together with the human stimuli both at behavioural and at brain level but tend to be assimilated to the inanimate categories. An explanation for this effect could be the different way blind and sighted individuals might have in perceiving and interacting with animals. In fact, if we exclude pets (only 1 out of the 6 animals we included in this study), sighted individuals normally perceive the animacy of animals (such as birds, donkey, horses etc.) mostly throughout vision (either in real life or in documentaries or videos). Blind people, instead, do normally learn the peculiar shape of each animal touching miniature, static, models of them. Moreover, when blind people hear the sounds of these animals without seeing them, they might combine these sounds with the rest of the environmental sounds, and this is indeed what we see in the behavioral ratings, in which only blind subjects cluster together animals and big environmental sounds.</p><disp-quote content-type="editor-comment"><p>2) One of the reviewers was particularly concerned about the usage of group means to characterise differences between groups, without adequately representing inter-subject variance within each group. In some analyses (e.g. the topographical selectivity map) the mean is taken as the sole group statistics (as a fixed effect), making the analysis highly susceptible to outliers and precluding generalisation to other samples. In other analyses (e.g. DSMs) a permutation test is included to determine group differences; however the specifics of this permutation tests are not well explained and require more thought – does this permutation test adequately captures the variance of the categorical representation between sighted and blind individuals? Or are the authors potentially confusing their units of analysis (see Parsons et al., eLife, 2018)? Ideally, variance should be assessed by measuring the effect in each individual participant and comparing the effect across groups (indeed this approach is used in some of the tests in the current study). In the case of DSM, each participant (e.g. from the EBa group) should be correlated to the SCv group, generating a distribution of the effect of interest. To an extent, a similar analysis is presented in Figure 5, providing much lower effect sizes than highlighted throughout the paper. So, we are looking for a more considered quantification of the main effects. A lot of confusion/misinterpretation could be avoided if the authors used standard statistics that takes into consideration the inter-subject variance, which is the key unit of your analysis (e.g. t-test and its variants). If the authors feel that their own approach is preferable, then they should increase the clarity on their analysis and its validity, with a clear statement of what null is.</p></disp-quote><p>We thank the reviewer for bringing this to our attention. We agree that in some of our statistical analyses we were not adequately considering the intersubject variance within each group. Following the recommendations made by the reviewers, we now implemented a different way of computing statistics. In general, the main point is that we do not use anymore, in any of the analyses, the group mean as a fixed effect, but we consider the variance measuring the effect in each individual participant.</p><p>To assess statistical differences, we now apply parametric tests (T-Test and ANOVA) in the analyses that met all the assumptions required by parametric statistics: normal distribution of the data, homogeneity of variances and independency of the observations. This was the case of the Jaccard similarity analyses (both within and between groups), of the voxels’ count analysis and of the averaged binary decoding analysis. However, in the rest of the cases we considered non-parametric statistic more appropriate since not meeting the criteria for parametric testing. Our choice of relying on non-parametric statistic (i.e. permutation analysis) is mainly driven by the most recent guidelines about the more appropriate way to run statistics on multivariate fMRI data (e.g. Stelzer et al., 2013). In this work the authors outline several theoretical points, supported by simulated data, according to which MVPA data might not respect some of the assumptions imposed by the t-test:</p><p>1) Normal distribution of the samples. One fundamental assumption of the t-test is that the samples need to be distributed normally, especially when the sample size is small. On the contrary, the unknown distribution of decoding accuracies is generally skewed and long-tailed and, in practice, depends massively on the classifier used and the input data itself.</p><p>2) Continuous distribution of the samples. A further assumption is that the underlying distribution from which samples are drawn should be continuous. This is not the case in accuracy values from decoding. In fact, the indicator function, which maps the number of correctly predicted labels to an accuracy value between 0 and 1, can only take certain values: for k cross-validation steps and a test set of size t, only k×t+1 different values between 0 and 1 can be taken.</p><p>3) Low variance of the samples is important for the t-test. Quite the opposite, the single subject accuracies are in general highly variable. In fact, the number of observations available for classification is very limited. This limitation of samples represents one of the main causes of the high variance of the accuracy values.</p><p>Similar guidelines concern also the correlation values between dissimilarity matrices (at the base of RSA analyses). In their seminal paper about how to implement RSA, Kriegeskorte and collaborators (2008) highlight that for dissimilarity matrices the independence of the samples cannot be assumed, because each similarity is dependent on two response patterns, each of which also codetermines the similarities of all its other pairings in the RDM. Therefore, they suggested to test the relatedness of dissimilarity matrices by using permutation tests (e.g. randomly permuting the conditions, to reorder rows and columns of one of the two dissimilarity matrices to be compared according to this permutation, and to compute the correlation).</p><p>That being said, we agree with the reviewers that in certain cases the use of standard statistics might avoid some confusion/misinterpretation and might be more straightforward. Therefore, we decided to move to parametric statistics for the analyses that allowed it (i.e. the analyses that met the main three assumptions required to apply the parametric statistics: normality of the distribution/ homogeneity of variances/ independency of data).</p><p>However, some of our analysis (i.e. the correlation of the topographical selectivity maps, the correlation of the brain dissimilarity matrices, the correlation of the RSA connectivity profiles and the inter-subject variance analysis) did not meet the assumption of independency of the data. This issue is related to the identification of the unit of analysis also mentioned by the reviewer in the current comment. We thank the reviewer for pointing out the interesting paper from Parsons et al., 2018, in which the problem of independency of the data is formally described. We now consider the guidance of this paper in building our statistical procedure.</p><p>Only in the experimental setting where each experimental unit provides a single outcome or observation, the experimental unit is the same as the unit of analysis (i.e. the one analyzed). In this case the independency of the data is maintained, and the use of parametric statistic is allowed. However, this is not always the case. For example, in our analyses previously mentioned (i.e. the correlation of the topographical selectivity maps, the correlation of the brain dissimilarity matrices, the correlation of the RSA connectivity profiles and the inter-subject variance analysis), our units of analysis are the correlation values between experimental units. In fact, in these analyses we contrast group’s comparisons, so data from the same subjects are always included in two comparisons (e.g. data from EBa subjects are included both in the SCv-EBa and in EBa-SCa comparisons). Therefore, the independency of the data is not respected here. In this case, using classical parametric statistics we would treat equally all observations in the analysis, ignoring the dependency in the data and this would lead to inflation of the false positive rate and incorrect (often highly inflated) estimates of statistical power, resulting in invalid statistical inference (Parsons et al., 2018).</p><p>For this reason, we believe that the use of permutation is a preferable approach in the case of these analyses. This approach is more flexible than the parametric statistics and allows to overcome the problem of independency. For example, in testing the difference between the comparisons we could build our null distribution keeping fixed the labels of the group common to the 2 pairs and shuffling the labels of the subjects from the other two groups (see next paragraphs for a more detailed description of all the statistical procedure used in the permutation analysis).</p><p>Also in the case of RSA with representational models we stick to permutation statistics, since also in this case there is a problem of independency of the data at the level of the DSMs’ building, as Kriegeskorte and collaborators highlighted in their seminal paper on RSA (2008): For dissimilarity matrices the independence of the samples cannot be assumed, because each similarity is dependent on two response patterns, each of which also codetermines the similarities of all its other pairings in the RDM. Therefore, the authors suggest testing the relatedness of dissimilarity matrices by using permutation tests (e.g. randomly permuting the conditions, to reorder rows and columns of one of the two dissimilarity matrices to be compared according to this permutation, and to compute the correlation).</p><p>In contrast to the previous version of our manuscript, we now changed the way of computing permutation, considering the variance of our values and we also explained more clearly our null hypothesis. In order to avoid confusion about the way we run statistics, we now explain all the information related to our statistical tests in a new section at the end of Materials and methods titled “Statistical analyses”:</p><p>“To assess statistical differences, we applied parametric tests (T-Test and ANOVA) in the analyses that met the main assumptions required by parametric statistics: normal distribution of the data and independency of the observations. […]</p><p>In each analysis, all the p-values are reported after false discovery rate (FDR) correction implemented using the matlab function ‘mafdr’”.</p><p>The main consequence of this change in the statistical methods is a decrease of the effect sizes in all the three analyses (correlation of the topographical selectivity maps; correlation of the brain dissimilarity matrices and correlation of the RSA connectivity profiles), as anticipated by the reviewer. However, even though the magnitude of the effect is reduced, this did not affect the significance of the results.</p><p>For example, if we look at the correlation between the brain DSMs, both in EVC and VOTC, we see that even though the average correlation between the DSMs in the new version of the results is lower compare to the previous results, however the same correlations that were significant in the previous results stay significant also in the new results (see <xref ref-type="fig" rid="respfig4">Author response image 4</xref>).</p><fig id="respfig4"><label>Author response image 4.</label><caption><title>New version of the results from the correlation of brain DSMs in EVC (left) and in VOTC (right).</title><p>Despite a reduction of size effect in the new results, there is not any major change in the statistical results.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-resp-fig4-v2.tif"/></fig><p>Similarly, the correlation values between the topographical maps remain significant (see Figure 1B).</p><p>Finally, a similar effect appears also in the correlation results between the RSA connectivity profiles (see Figure 8B). Also, in this case we find correlation values that are decreased in magnitude, but no difference in the significant tests:</p><disp-quote content-type="editor-comment"><p>3) A few of the reviewers were commenting on inconsistent statistical criteria, with the significance test flipping between one-tail to two-tails. For example, correlations between DSM were tested for significance using a one-tailed permutation test. This resulted in identifying a very strong negative correlation between the SCv and SCa in the early visual cortex (r = -.50), as non-significant, where in fact this effect is stronger than the correlation presented between SCv and EBa DSMs – r=.41, p=0.03). Similarly, the SCv DSM in the early visual cortex may be significantly negatively correlated to the behavioral similarity ratings (r=-0.09; the in sighted EVC DSM is reported to be significantly correlated with both the behavioural DSMs at r=.09, p=0.01). This could be an interesting finding, e.g. if cross-modal inhibition is categorically selective (allowing for decoding of sound categories in EVC in another study; (Vetter et al., 2014)). This finding could also add to the mechanistic explanation of the plasticity in the blind. But at present it is uninterpretable. We therefore request that one-tail testing would be avoided, unless there's strong statistical justification to usage it (e.g. in case of a replication of previous findings). In that case, the reasons for the one-tail hypothesis should be reported more transparently and interpreted more cautiously. But if the authors are confirming a new hypothesis, which we believe is the case for most of the tests reported, we'd encourage them to stick to two-tailed hypothesis testing. The authors are welcomed to consider correction for multiple comparisons, but again, we are looking for consistency throughout the manuscript and justification of the criteria used (or abandoned).</p></disp-quote><p>We thank the reviewer for raising this important point which give us the opportunity to clarify further our statistical strategy. We use one-tailed permutation test only in two analyses: the correlation of neural DSMs between groups and the correlation between neural DSMs and representational models (i.e. behavioural/low-level models); otherwise we systematically used 2-sided hypothesis testing. Our decision to use one-tail hypothesis testing in those specific condition is based on the lack of interpretability of negative correlation in RSA.</p><p>First, we would like to point out that the interesting possibility of a categorically selective cross-modal inhibition, suggested by the reviewer, would actually still produce a positive correlation with our behavioral/categorical model. In the use of RSA, in fact, we look at the structure of the representation of our categories in a specific brain region (i.e. how the representation of each category is similar or different from the representation of the other categories in a given ROI), however the structure of the representation does not tell anything about the level of activity of this region. For instance, we could see that in a specific brain ROI the representation of tools looks different from the representation of animals. From this information we cannot infer if the categorical representation embedded in the multivoxel pattern was the product of a deactivation or an activation, we can only infer that the brain activity patterns for tools are different from the brain activity patterns for animals. Therefore, a categorically selective cross-modal inhibition and a categorically selective cross-modal activation would create a similar structure of the representation, in both cases positively correlated with a categorical model.</p><p>A negative correlation with a representational model, such as the one we find in EVC of SCa with the behavioural model, highlight the fact that categories that look similar in our model have a different brain pattern activity while categories that look different in our model share a similar pattern of activity. These results are very difficult to interpret. One possibility is that there is another model, that we did not test, which is anti-correlated with our behavioural model and that can explain the representational structure of this ROI. This is one of the main limitations of RSA: there are in theory infinite models that we could test (Kriegeskorte et al., 2008). In practice, we need to select our model a priori for obvious statistical reasons (e.g. fitting a model a posteriori after observing the structure of our data).</p><p>Based on these limitations, studies relying on RSA typically do not interpret negative correlations’ results and tend to build unidirectional hypothesis testing (for an example with a design similar to the one of our study, including blind and sighted participants and using RSA to test different representational models, see Peelen et al. 2014; for recent examples of one-tailed test of RSA correlation see also (Fischer-Baum et al., 2017; Handjaras et al., 2017; Leshinskaya et al., 2017; Zhao et al., 2017; Wang et al., 2018; Evans et al., 2019).</p><p>In other words, since we cannot easily interpret negative correlations, it makes more sense to build a unidirectional hypothesis. We now added a short explanation in the paper to justify our choice of using a one-tailed instead than a two-tailed test:</p><p>”Considering the unidirectional hypothesis for this test (a positive correlation between neural similarity and models similarity) and the difficult interpretation of a negative correlation, one-tailed statistical tests were used. For all other tests (e.g., differences between groups), for which both directions might be hypothesized, two-tailed tests were (Peelen et al., 2014; Evans et al., 2019)”.</p><p>That being said, and in order to reassure the reviewers that using one-tailed instead of two-tailed test has no significant impact on the (positive) correlation results, we show the permutation results reporting the p values for both the one-tailed and the two-tailed test after FDR correction for the 12 multiple comparisons (see <xref ref-type="fig" rid="respfig5">Author response image 5</xref>). As you can see, the only correlation values that are significant with the two-tailed and not with the one-tailed are the negative ones, that in any case (for the reasons previously reported) we would not have interpreted.</p><fig id="respfig5"><label>Author response image 5.</label><caption><title>Comparison of the p-values (one-tailed vs two-tailed) resulting from the correlation of the brain DSMs (EVC on the left and VOTC on the right) with the representational behavioural and low-level models in the 3 groups (first line: SCv, middle line: EBa, lower line: SCa).</title><p>The null distribution is represented in dark blue. The red line represents the actual correlation value. For each permutation test p-values are reported both for two-tailed and one-tailed tests. P value reported in red are significant according to the selected threshold of 0.05. p values are reported after FDR correction for 12 multiple comparisons.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-resp-fig5-v2.tif"/></fig><p>Finally, the reviewer might have noticed that there is one difference compared to the results of this analysis in the previous version of the paper: the correlation with the behavioural model and the neural DSM from VOTC in SCa is not anymore significant. This is, however, unrelated to the use of one- or two-tailed test but instead related to the use of Spearman instead of Pearson correlation in our RSA analyses.</p><disp-quote content-type="editor-comment"><p>4) The focus on VOTC, and the specific definition of the ROIs requires further consideration. In particular the reviewers mentioned the lack of discrimination within it, limited coverage of the ventral stream, and lack of consideration of other areas which might be involved in categorical representation, e.g. high order auditory areas or other visual areas in the lateral occipital cortex (see also comment 6 below). Further exploratory analysis, focused by previous research on similar topics, would strengthen the interoperability of the results. A searchlight approach might be particularly helpful, though we leave it to the authors to decide on the specific methodology.</p></disp-quote><p>We thank the reviewer for raising this important point that leads us to clarify further our hypothesis-driven analytical strategy. There are three aspects that need to be clarified: 1) Why we decided to take the entire VOTC (and not separate subregions within it as region of interest; 2) How did we define our VOTC mask and why we chose that way; 3) Why we did not include other regions in our analyses.</p><p>First, since we were interested in the brain representation of different categories, we decided to focus on the ventro-occipito temporal cortex as a whole. This region is well known to contain several distinct macroscopic brain regions known to prefer a specific category of visual objects including faces, places, body parts, small artificial objects, etc. (Kanwisher, 2010). We decided to focus our analyses on a full mask of VOTC, and not in specific sub-parcels (such as FFA, PPA, etc.) because we were interested in looking at the categorical representation across categories and not at within a specific category. Our study therefore builds upon the paradigm shift of viewing VOTC as a distributed categorical system rather than a sum of isolated functionally specific areas, which reframes how we should expect to understand those areas (Haxby et al., 2001). In fact, our main aim was to investigate how input of presentation and visual experience impact on the general representation of different categories in the brain. Looking at one specific category selective region at a time would not allow us to address this specific question. This approach has already been validated by previous studies that investigated the categorical representation in the ventral-occipito temporal cortex using a wide VOTC mask (Kriegeskorte et al., 2008; Grill-Spector and Weiner, 2014; Wang et al., 2015; Xu et al., 2016; Hurk et al., 2017; Peelen and Downing, 2017; Ritchie et al., 2017).</p><p>That being said, our winner take all topographic analyses (see Figure 1B) in combination with calculation of Jaccard index (assessing topographic overlap) provide a clear measure of how each category maps onto a similar brain region within the VOTC mask across modalities and groups. This analyze suggest that there is a partial spatial overlap onto where separate categories map, but also highlight some differences (e.g. for animals).</p><p>Secondly, why did we decide to use a structural definition on VOTC mask and not a functional localizer? The main reason is that it is quite challenging to functionally localize VOTC in blind subjects functionally. Indeed, all the classical localizers are based on visual stimulation. Therefore, we thought that it would be misleading to functionally localize the mask only in the sighted and then apply it both to sighted and blind participants. In relation to the previous point, this is even more true if we had to localize specific categorically specific regions since we first need to localize those regions (e.g. FFA, PPA, LO) based on functional localizers since their locations show important inter-individual variability (Kanwisher, 2010; Julian et al., 2012). This is not feasible in blind people where clear functional localizers for those regions are not trivial to define. For this reason, we decided to rely on a structural definition of the mask. We decided to work in the subject space limiting the transformation and normalization of the brain’s images. This is particularly relevant when comparing blind and sighted subjects given that blindness is associated with significant changes in the structure of the brain itself, particularly within the occipital cortex (Pan et al., 2007; Jiang et al., 2009; Park et al., 2009; Dormal et al., 2016). That is why we used the anatomical scan to segment the brain in separate regions according to the Desikan-Killiany atlas (Desikan et al., 2006) implemented in FreeSurfer (<underline>http://surfer.nmr.mgh.harvard.edu</underline>). And finally, among the parcels produced by this atlas, we selected the regions laying within the ventral-occipito-temporal cortex and known to be involved in visual categorical representation of different categories.</p><p>Finally, we did not include other regions because our a priori hypothesis was based on VOTC. The exploration of additional parcels would be more based on an exploratory approach and would increase the multiple comparisons problem of our study. Because of this reason we decided to focus our analysis selectively on VOTC, adding the EVC as a control node. What drives the functional organization of VOTC (visual experience, retinotopy, curvature etc…) is a burgeoning topic with recent influential paper focusing precisely on such question (Hurk et la., 2017, Grill-Spector et al., 2014; Bracci et al., 2017;Bi et al., 2016; Peelen et al., 2017; Wang et al., 2015). We decided to insert our study in that topic by testing the role visual experience plays in driving the functional organization of VOTC by testing sighted and blind individuals with categorical sounds.</p><p>We agree with the reviewers that it would be interesting to investigate the categorical representation in other parts of the brain, such as the lateral occipital complex or the temporal cortex. We are indeed investigating at the moment the categorical representation in the temporal cortex of sighted and blind subjects (using a part of the actual dataset), with also an additional extension including blind subjects with late onset of blindness. However, we believe that this topic is beyond the scope of the present study that is already theoretically and technically challenging. Adding other regions would necessarily need changing the theoretical focus of the study and likely result in a paper presenting a patchwork results that are difficult to integrate in a streamlined global theoretical framework.</p><p>That being said, we agree with the reviewers that we needed to better explain our choice about the definition of our ROIs. We added some clarification in the section “Regions of interest”:</p><p>“Since we were interested in the brain representation of different categories we decided to focus on the ventro-occipito temporal cortex as a whole. […] Then, we combined these areas in order to obtain one bilateral EVC ROI and one bilateral VOTC ROI (Figure 3A). […]”</p><p>Finally, we thank the reviewer for highlighting the important point related to the searchlight approach. The searchlight approach might be, indeed, a useful and powerful analysis in the case of a more exploratory study. However, we again believe that this analysis is beyond the scope of the current paper, and we think that it might not help in adding value to our study which has a more hypothesis driven orientation (such as Wang et al., 2015; Hurk et al., 2017; Bi et al., 2016). On the opposite, the control for the (copious) multiple comparisons could even hinder part of the effects that emerge using the hypothesis-driven ROI approach. Moreover, as highlighted in the previous part of this comment, we decided to work in the subject space since there are plenty of evidences that early blindness is associated with significant changes in the structure of the brain itself, particularly within the occipital cortex (Dormal et al., 2016; Jiang et al., 2009; Pan et al., 2007; Park et al., 2009). Therefore, even though we could implement the searchlight approach in each individual subject, a normalization step would be required to move to a common space in order to infer the location at the group level. Working in individually defined anatomical masks allowed as to circumvent this problem.</p><p>In addition, even though we agree that the searchlight approach might be helpful as a side analysis to explore our data more extensively, we respectfully remind the reviewer that we build our neural dissimilarity matrices using a crossvalidation approach for each possible pair of conditions (i.e. for each ROI and for each group we run 28 binary decoding tests); we believe that it would be extremely challenging (especially for a side-analysis), at the computational level, to repeat the same analysis for each voxel.</p><p>As a final note, the multidimensional nature of fMRI data can always trigger new hypothesis-testing based on sensibilities of reviewers (which region is selected, which method to test the hypothesis etc.). We however believe that this can be problematic in generating post-hoc hypotheses and enlarging the theoretical and statistical space of a research project, potentially producing scientific malpractice (Zimring, Nature, 2016). More generally speaking, the request for additional experiments or analyses in the review phase has been recently eloquently debated by Hidde Ploegh in a Nature News called “End the Wasteful Tyranny of Reviewer Experiments” or by Derek Lowe in Science Translational Medicine: “Just A Few More Month’s Work, That’s All I’m Asking Here”. Quoting Ploegh here: “Submit a biomedical-research paper to Nature or other high-profile journals, and a common recommendation often comes back from reviewers: perform additional experiments. Although such extra work can provide important support for the results being presented, all too frequently it represents instead an entirely new phase of the project or does not extend the reach of what is reported”.</p><p>Again, our study was strongly hypothesis-driven by being focused on testing whether different modalities of input and sensory experiences affect the categorical representation in VOTC; following a focused analytical strategy as implemented by previous major literature in this field (Kriegeskorte et al., 2008; Grill-Spector et al., 2014; Bracci et al., 2017;Bi et al., 2016; Peelen et al., 2017; Wang et al., 2015).</p><disp-quote content-type="editor-comment"><p>5) The topographic selectivity analysis raised multiple comments from the reviewers. Beyond the issue raised in comment 2 above, it was agreed that this analysis potentially reveals important differences between the groups that are not adequately captured in the current presentation and analysis. For example, Reviewer 2 mentions: &quot;the blind show nearly no preference for animal sounds in accordance with claims made by (Bi et al., 2016); in the sighted the human non-vocalizations are the preferred category for face selective areas in the visual cortex but in the auditory version (in both blind and sighted) there is mostly vocalization selectivity; The blind show little or no preference for the large-environmental sounds whereas the sighted show a differentiation between the two sound types&quot;. Reviewer 3 mentions: &quot;In SCv, the medial ventral temporal cortex is primarily BIG-ENV, but in EBa, that same region is primarily BIG-MEC. Similarly, there appears to be more cortex showing VOC. In SCa than in either of the other two groups&quot;. We appreciate that some of the analysis might have been used as a replication of the Van den Hurk study, but we also expect them to stand alone, in terms of quality of analysis and interpretation. In particular, these topographical preference differences could be interesting and meaningful with relation to understanding blind visual cortex function, but it is hard to judge their strength or significance as the topographical selectivity maps are not thresholded, by activation strength or selectivity significance.</p></disp-quote><p>We thank the reviewers for raising these interesting comments about the topographic selectivity analysis. We agree that this analysis might reveal important differences between the groups and that we should try to adequately capture and highlight these interesting differences in our work.</p><p>We now compute the Jaccard index to quantify the similarity between the topographical maps preferentially elicited by each category in our 3 groups. The Jaccard similarity coefficient is a statistic used for measuring the topographic similarity and diversity of sample sets. The Jaccard coefficient is defined as the size of the intersection divided by the size of the union of the sample sets (see <xref ref-type="fig" rid="respfig6">Author response image 6</xref>):</p><fig id="respfig6"><label>Author response image 6.</label><caption><title>Visual representation of the intersection (left) and union (right) of two set of samples.</title><p>The Jaccard coefficient is based on these two measures.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-resp-fig6-v2.tif"/></fig><p>This value is 0 when the two sets are disjoint, 1 when they are equal, and between 0 and 1 otherwise. We used the Jaccard similarity index to look at two important aspects of our data: (1) the similarity of the topographical maps (for each category) between subjects from the same group (within group similarity). This analysis provides information about the consistency of the topographical representation of the categories across subjects within the same group (See Figure 1B-left). (2) The similarity between the topographical maps from both the blind and the sighted in the auditory experiment and the topographical map of the sighted in the visual experiment (between groups similarity). This analysis provides information about the similarity between the visual and auditory (both in sighted and in blind) topographical representations (see figure 1C).</p><p>For the within group similarity we computed the Jaccard similarity between the topographical selectivity map of each subject and the mean topographical map of his own group. This analysis produces 4 Jaccard similarity indices (one for each of the main category: (1) animal, (2) human, (3) manipulable objects and (4) big objects and places) for each group. Low values for one category mean that the topographical representation of that category varies a lot across subjects of the same group (e.g. the animal category in EBa). The results of the Jaccard similarity within each group are represented in the Figure 2B.</p><p>The results from the Jaccard similarity within groups are now reported in our article as follow:</p><p>“In order to look at the consistency of the topographical representation of the categories across subjects within the same group we computed the Jaccard similarity between the topographical selectivity map of each subject and the mean topographical map of his own group. […] The animals’ category showed a significantly lower Jaccard similarity within the EBa group compared to both SCa (p<sub>FDR</sub> = 0.002) and SCv (p<sub>FDR</sub> &lt;0.001) groups while the similarity of big objects and places category was significantly higher in EBa compared to SCv (p<sub>FDR</sub> = 0.038).”</p><p>For the between group analysis we computed the Jaccard similarity index for each category, between the topographical map of each blind and sighted subject in the auditory experiment and the averaged topographical selectivity map of the sighted in the visual experiment (see Figure 1C for the results). In more practical terms, this means that we have 4 Jaccard similarity indices (one for each of the main category: (1) animal, (2) human, (3) manipulable objects and (4) big objects and places) for each of the two group pairs: SCv-EBa and SCv-SCa.</p><p>The Figure 2C represents the results from this analysis.</p><p>We now present these new results from the Jaccard similarity between groups:</p><p>“In addition, we wanted to explore the similarity and differences among the topographical representations of our categories when they were presented visually compared to when they were presented acoustically, both in sighted and in blind. […] No difference emerged between groups, suggesting comparable level of similarity of the auditory topographical maps (both in blind and in sighted) with the visual topographical map in sighted participants.“</p><p>In addition to the Jaccard similarity analyses, we thought that a further relevant analysis would be to look at the number of voxels showing the preference for each category, in each group. In fact, the degree of overlap might be driven by the number of voxels selective for each category (e.g. it might be that there is no overlap for the animal category between SCv and EBa because almost no voxel prefer animal in blind).</p><p>The Figure 2B represents the number of selective voxels for each category and in every group.</p><p>We now present the results from the analysis on number of voxels selective for each category:</p><p>“Finally, since the degree of overlap highlighted by the Jaccard similarity values might be driven by the number of voxels selective for each category, we looked at the number of voxels showing the preference for each category in each group. […] The post hoc comparisons revealed that this difference was mainly driven by the reduced number of voxels selective for the animal category in EBa compared to SCv (p=0.02).”</p><p>We now include a new figure (Figure 2) in the new version of our manuscript in which we represent the results from these three analyses (i.e. number of selective voxels, Jaccard similarity within groups and Jaccard similarity with SCv).</p><p>We also added a section in the Discussion of our manuscript to integrate these results in the general framework of our paper and to relate them with previous studies. This section now reads as follows:</p><p>“Even though the categorical representation of VOTC appears, to a certain degree, immune to input modality and visual experience, there are also several differences emerging from the categorical representation of sight and sounds in the sighted and blind. […] Interestingly, in case of early visual deprivation this difference is also in the behavioural evaluation of the similarity of our stimuli.”</p><p>To go even more in details, in the supplemental material we included a representation of the same analyses applied on the 8 categories (see Figure 2—figure supplement 1).</p><p>The most striking observation from this analysis is that a large number of voxels show a preference for the big mechanical category (almost the double than the number of voxels preferring the other categories) in the VOTC of EBa group. In line with this result, the topographical map for the big mechanical objects is the most stable across EBa subjects, while the map for big environmental is the one less stable (i.e. lowest Jaccard similarity within EBa group). Moreover, also in this analysis, we see that in EBa a limited number of voxels is selective for both birds and mammals and the topographical maps of these two animals’ categories show a lower consistency across blind subjects.</p><p>Finally, it is true that our topographical selectivity maps are not thresholded by activation strength or selectivity significance. Our experiment was, definitely, designed to suit a multivariate fMRI analysis approach (e.g. fast event related design, short inter-stimulus interval, etc.) therefore our statistical power for univariate analyses might not be optimal. Moreover, it is important to keep in mind that we are looking at the activity of the occipital cortex for sounds, including in sighted individual, therefore we are not expecting high β-values in our univariate GLM. However, we believe that our topographical selectivity analysis is anyway valuable to make inference about some aspects of the categorical representation of the different categories across the different groups (for a similar approach see also Hurk et al., 2017; Striem-Amit et al., 2018). In other words, even though we cannot (and we do not) make any inference about the topographical category selectivity itself since our map are not thresholded, we however can use those maps for a second-order statistics (i.e. the correlation between the winner-take-all maps). It is important to highlight that if those maps were noisy, we would not observe any significant correlation. On the opposite, our results (supported by stringent statistics) show that the topographical map generated in VOTC by the auditory stimuli (both in blind and in sighted) is not stochastic; quite the opposite, it partially reflects the topographical map generated by the visual stimuli in sighted (see Figure 1B).</p><disp-quote content-type="editor-comment"><p>In addition, we would like some further clarification of how the correlations have been constructed, if indeed a use-takes-all approach is used to label each voxel.</p></disp-quote><p>To create the topographical selectivity map, we assign a label to each voxel within the VOTC mask. Because of the issue raised at point #2 concerning the usage of group mean to test statistical differences, we now compute a topographical selectivity map in each subject, in order to adequately represent inter-subject variance within each group. We improved the description of the analysis and the statistical tests in the “Materials and methods”, in the section related to the “<italic>Topographical selectivity map”:</italic> </p><p>“To create the topographical selectivity map (Figure 1B) we extracted in each participant the b-value for each of our 4 main conditions (animals, humans, manipulable objects and places) from each voxel inside the VOTC mask and we assigned to each voxel the condition producing the highest β-value (winner takes all). This analysis resulted in specific clusters of voxels that spatially distinguish themselves from their surround in terms of selectivity for a particular condition (Hurk et al., 2017, Striem-Amit et al., 2018).”</p><disp-quote content-type="editor-comment"><p>6) In the representational connectivity analysis, the authors may be overestimating the connectivity between regions due to contribution of intrinsic fluctuations between areas. To more accurately estimate the representational connectivity, it would be better if the authors used separate trials for the seed and target regions. See Henriksson et al., 2015, Neuroimage for a demonstration and discussion of this issue. Indeed, considering the clear differences found elsewhere between groups, the high between-group correlations are striking. It could have been informative to examine a control node (e.g. low-level visual cortex; low level auditory cortex) just to gain a better sense for what these correlations reveal.</p></disp-quote><p>We thank the reviewer for highlighting this point and for pointing out the relevant paper from Henriksson et al., 2015. In this paper, the authors reported that intrinsic cortical dynamics might strongly affect the representational geometry of a brain region, as reflected in response-pattern dissimilarities, and might exaggerate the similarity (quantified using a correlation measure) of representations between brain regions (Henriksson et al., 2015). In the paper the authors show that visual areas closer in cortex tended to exhibit greater similarity of representations. To bypass this problem, they suggest using independent data (e.g. data acquired in different runs) to compute the DSM in the seed ROI and the DSMs from the other brain ROIs that we want to correlate with the seed. This would, indeed, be a clever way to remove the intrinsic fluctuation bias.</p><p>However, we think that we are not under the influence of similar problems due to the way we built our dissimilarity matrices. As we are building our neural dissimilarity matrices using a cross-validation approach for each possible pair of conditions (and not 1 – the correlation value), we drastically reduce the possibility of this intrinsic fluctuation bias. As we explain in the Materials and methods section: “we preferred to use binary MVP-classification as dissimilarity index to build neural DSMs rather than other types of dissimilarity measures […] because when using a decoding approach, due to the intrinsic cross-validation steps, we would find that the two conditions that don’t drive responses are indistinguishable, despite their substantial correlation (Walther et al., 2016) since the noise is independent between the training and testing partitions, therefore cross-validated estimates of the distance do not grow with increasing noise. This was crucial in our study since we are looking at brain activity elicited by sounds in brain regions that are primarily visual (EVC and VOTC) where the SNR is expected to be low, at least in sighted people”.</p><p>In other words, the use of cross-validation across imaging runs ensures that the estimated distances between neural patterns are not systematically biased by run-specific noise (Whalther et al., 2016; Evans et al., 2019). Therefore, we believe that in our case the method suggested by Henriksson et al. is not necessary.</p><p>Moreover, the results we obtained from the representational connectivity analysis in VOTC already suggest that this intrinsic fluctuation bias cannot explain our results. If we look at the representational connectivity profiles represented in Figure 6, we clearly see that the proximity of the ROIs in the cortex is not systematically linked with a higher representational similarity (in contrast with what Henriksson et al. showed in their data). One example is the DSM form the fusiform node in that shows a higher similarity with the DSM from the inferior parietal cortex compared to the cuneus and even the lingual gyrus, which are nevertheless much closer in cortex.</p><p>Following the suggestion of the reviewers, we performed the same analysis also in the EVC as a control node, in order to gain a better sense for what these correlations reveal. In this ROI, we found a significant correlation only between the representational connectivity profiles of the two groups of sighted (SCv and SCa) and not between neither the EBa and the SCv nor between the EBa and the SCa. This highlight that the conclusion drawn from VOTC are specific and not the byproduct of a methodological property of how we compute our representational connectivity analysis. As we highlight in the Discussion:</p><p>“In support of this possibility, the representational connectivity profile of EVC in EBa did not show any similarity with the one of sighted (neither SCv nor SCa), suggesting a different way of crossmodal plasticity expression in this brain region”.</p><p>These results (together with other evidences coming from the decoding analysis and the RSA analysis) support the hypothesis that the posterior part of the occipital cortex in EB is the region that distance itself the most from the native computation it typically implements (Bi, Wang, and Caramazza, 2016; Büchel, 2003; Wang et al., 2015).</p><p>Importantly for the sake of this comment, from a methodological point of view, this is in support of the idea that the intrinsic cortical dynamics alone cannot explain our results, since the correlation between the representational connectivity profiles is sometimes absent.</p><disp-quote content-type="editor-comment"><p>Also, in this section it is not clear why the authors use a separate ROI for each of the 3 VOTC sub-regions, rather than the combination of all three, as they have for the main analysis? As a minor point, the reviewer wasn't clear how the authors switched from 3 nodes for each group to only one comparison for each group pair.</p></disp-quote><p>Our decision of using a separate ROI for each of the 3 VOTC sub-regions is mostly driven by previous literature. Previous studies have shown that, in sighted people, specific region within VOTC are functionally and/or structurally connected with specific extrinsic brain regions. For instance, the visual word form area (VWFA) in VOTC shows robust and specific anatomical connectivity to EVC and to frontotemporal language networks (Saygin et al., 2016). Similarly, the fusiform face area (FFA) shows a specific connectivity profile with other occipital regions (Saygin et al., 2012), and also a direct structural connection with the temporal voice area (TVA) in the superior temporal sulcus (Blank et al., 2011; Benetti et al., 2018) thought to support similar computations applied on faces and voices as well as their integration (Von Kriegstein, Kleinschmidt, Sterzer, and Giraud, 2005). In order to explore these large-scale brain connections in our 3 groups, we thought that it would be better to keep the specificity of the representational connectivity profile of each sub-region of VOTC. In this way we did not cancel out the differences across the representational connectivity profile of each VOTC sub-region and we ended-up with a higher variance in the connectivity profile of each subject, which is methodologically good for the following correlation analysis between the connectivity profiles. It is important to understand that we did not compare the connectivity profile of the single sub-regions, but in line with the rest of the paper, we concatenated the connectivity profile obtained from the three ROIs: our main aim was to keep more granularity in our connectivity profile.</p><p>However, to reassure the reviewers that keeping the entire VOTC as seed ROI would have a major impact on the results, we report the results from this analysis (<xref ref-type="fig" rid="respfig7">Author response image 7</xref>, bottom panel), together with the same analysis using the 3 seeds ROIs (<xref ref-type="fig" rid="respfig7">Author response image 7</xref>, top panel). As you can see the correlation values do not change critically in the two analyses.</p><fig id="respfig7"><label>Author response image 7.</label><caption><title>Comparisons of the results from the RSA connectivity analysis using the 3 sub-regions of VOTC (fusiform, parahippocampal, infero-temporal) as seeds ROIs or VOTC as a single ROI.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-50732-resp-fig7-v2.tif"/></fig><p>Moreover, since the reviewers highlighted that our explanation on how we switched from 3 nodes for each group to only one comparison for each group pair was not clear enough, we now clarified this section in the new version of our manuscript:</p><p>”Finally, we computed the Spearman’s correlation between the 3 seed ROIs (i.e. fusiform gyrus, parahippocampal gyrus and infero-temporal cortex) and all the other 27 ROIs. We ended up with a connectivity profile of 3 (number of seeds) by 27 (ROIs representing the rest of the brain) for each subject. We considered this 3*27 matrix as one representational connectivity profile of the seed region (e.g. VOTC) in each subject.”</p></body></sub-article></article>