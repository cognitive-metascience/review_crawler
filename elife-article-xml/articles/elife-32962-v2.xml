<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">32962</article-id><article-id pub-id-type="doi">10.7554/eLife.32962</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Distinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-79480"><name><surname>Groen</surname><given-names>Iris IA</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5536-6128</contrib-id><email>iris.groen@nyu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-100049"><name><surname>Greene</surname><given-names>Michelle R</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-59530"><name><surname>Baldassano</surname><given-names>Christopher</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3540-5019</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-32296"><name><surname>Fei-Fei</surname><given-names>Li</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-59535"><name><surname>Beck</surname><given-names>Diane M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9802-5828</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-28129"><name><surname>Baker</surname><given-names>Chris I</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6861-8964</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Laboratory of Brain and Cognition</institution><institution>National Institutes of Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Psychology</institution><institution>New York University</institution><addr-line><named-content content-type="city">New York City</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Neuroscience Program</institution><institution>Bates College</institution><addr-line><named-content content-type="city">Maine</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Princeton Neuroscience Institute</institution><institution>Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Stanford Vision Lab</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Department of Psychology</institution><institution>University of Illinois</institution><addr-line><named-content content-type="city">Urbana-Champaign</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution content-type="dept">Beckman Institute</institution><institution>University of Illinois</institution><addr-line><named-content content-type="city">Urbana-Champaign</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-12395"><name><surname>Tsao</surname><given-names>Doris Y</given-names></name><role>Reviewing Editor</role><aff id="aff8"><institution>California Institute of Technology</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>03</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e32962</elocation-id><history><date date-type="received" iso-8601-date="2017-10-19"><day>19</day><month>10</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-03-02"><day>02</day><month>03</month><year>2018</year></date></history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-32962-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.32962.001</object-id><p>Inherent correlations between visual and semantic features in real-world scenes make it difficult to determine how different scene properties contribute to neural representations. Here, we assessed the contributions of multiple properties to scene representation by partitioning the variance explained in human behavioral and brain measurements by three feature models whose inter-correlations were minimized <italic>a priori</italic> through stimulus preselection. Behavioral assessments of scene similarity reflected unique contributions from a functional feature model indicating potential actions in scenes as well as high-level visual features from a deep neural network (DNN). In contrast, similarity of cortical responses in scene-selective areas was uniquely explained by mid- and high-level DNN features only, while an object label model did not contribute uniquely to either domain. The striking dissociation between functional and DNN features in their contribution to behavioral and brain representations of scenes indicates that scene-selective cortex represents only a subset of behaviorally relevant scene information.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>scene perception</kwd><kwd>variance partitioning</kwd><kwd>behavioral categorization</kwd><kwd>deep neural network</kwd><kwd>computational model</kwd><kwd>fMRI</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>ZIAMH002909</award-id><principal-award-recipient><name><surname>Groen</surname><given-names>Iris IA</given-names></name><name><surname>Baker</surname><given-names>Chris I</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>Rubicon Fellowship</award-id><principal-award-recipient><name><surname>Groen</surname><given-names>Iris IA</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>Multidisciplinary Research Initiative Grant N000141410671</award-id><principal-award-recipient><name><surname>Fei-Fei</surname><given-names>Li</given-names></name><name><surname>Beck</surname><given-names>Diane M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Deep network features exhibit a robust correlation with brain activity in scene-selective cortex, but are not sufficient to explain human scene categorization behavior, which is strongly shaped by information about the function (possibility for action) of the scene.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Although researchers of visual perception often use simplified, highly controlled images in order to isolate the underlying neural processes, real-life visual perception requires the continuous processing of complex visual environments to support a variety of behavioral goals, including recognition, navigation and action planning (<xref ref-type="bibr" rid="bib43">Malcolm et al., 2016</xref>). In the human brain, the perception of complex scenes is characterized by the activation of three scene-selective regions, the Parahippocampal Place Area (PPA; <xref ref-type="bibr" rid="bib1">Aguirre et al., 1998</xref>; <xref ref-type="bibr" rid="bib16">Epstein and Kanwisher, 1998</xref>), Occipital Place Area (OPA; <xref ref-type="bibr" rid="bib29">Hasson et al., 2002</xref>; <xref ref-type="bibr" rid="bib14">Dilks et al., 2013</xref>), and Medial Place Area (MPA; <xref ref-type="bibr" rid="bib57">Silson et al., 2016</xref>), also referred to as the Retrosplenial Complex (<xref ref-type="bibr" rid="bib3">Bar and Aminoff, 2003</xref>). A growing functional magnetic resonance imaging (fMRI) literature focuses on how these regions facilitate scene understanding by investigating what information drives neural responses in these regions when human observers view scene stimuli. Currently, a large set of candidate low- and high-level characteristics have been identified, including but not limited to: a scene’s constituent objects and their co-occurrences; spatial layout; surface textures; contrast and spatial frequency, as well as scene semantics, contextual associations, and navigational affordances (see <xref ref-type="bibr" rid="bib19">Epstein, 2014</xref>; <xref ref-type="bibr" rid="bib43">Malcolm et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Groen et al., 2017</xref> for recent reviews).</p><p>This list of candidate characteristics highlights two major challenges in uncovering neural representations of complex real-world scenes (<xref ref-type="bibr" rid="bib43">Malcolm et al., 2016</xref>). First, the presence of multiple candidate models merits careful comparison of the contribution of each type of information to scene representation. However, given the large number of possible models and the limited number that can realistically be tested in a single study, how do we select which models to focus on? Second, there are many inherent correlations between different scene properties. For example, forests are characterized by the presence of spatial boundaries and numerous vertical edges, whereas beaches are typically open with a prominent horizon, resulting in correlations between semantic category, layout and spatial frequency (<xref ref-type="bibr" rid="bib49">Oliva and Torralba, 2001</xref>; <xref ref-type="bibr" rid="bib60">Torralba and Oliva, 2003</xref>). This makes it problematic to explain neural representations of scenes based on just one of these properties (<xref ref-type="bibr" rid="bib66">Walther et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">Kravitz et al., 2011</xref>; <xref ref-type="bibr" rid="bib51">Park et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Rajimehr et al., 2011</xref>) without taking into account their covariation. Indeed, an explicit test of spatial frequency, subjective distance and semantic properties found that due to inherent feature correlations, all three properties explained the same variance in fMRI responses to real-world scenes (<xref ref-type="bibr" rid="bib40">Lescroart et al., 2015</xref>).</p><p>In the current fMRI study, we addressed the first challenge by choosing models based on a prior study that investigated scene categorization behavior (<xref ref-type="bibr" rid="bib22">Greene et al., 2016</xref>). This study assessed the relative contributions of different factors that have traditionally been considered important for scene understanding, including a scene’s component objects (e.g., <xref ref-type="bibr" rid="bib6">Biederman, 1987</xref>) and its global layout (e.g, <xref ref-type="bibr" rid="bib49">Oliva and Torralba, 2001</xref>), but also included novel visual feature models based on state-of-the-art computer vision algorithms (e.g., <xref ref-type="bibr" rid="bib56">Sermanet et al., 2013</xref>) as well as models that reflect conceptual scene properties, such as superordinate categories, or the types of actions afforded by scene. Using an online same-different categorization paradigm on hundreds of scene categories from the SUN database (<xref ref-type="bibr" rid="bib69">Xiao et al., 2014</xref>), a large-scale scene category distance matrix was obtained (reflecting a total of 5 million trials), which was subsequently compared to predicted category distances for the various candidate models. The three models that contributed most to human scene categorization were (1) a model based on human-assigned labels of actions that can be carried out in the scene (‘functional model’), (2) a deep convolutional neural network (‘DNN model’) that was trained to map visual features natural images to a set of a 1000 image classes from the ImageNet database (<xref ref-type="bibr" rid="bib13">Deng et al., 2009</xref>), and (3) human-assigned labels for all the objects in the scene (‘object model’). Given the superior performance of these top three models in explaining scene categorization, we deemed these models most relevant to test in terms of their contribution to brain representations. Specifically, we determined the relative contribution of these three models to scene representation by comparing them against multi-voxel patterns in fMRI data collected while participants viewed a reduced set of scene stimuli from <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref>.</p><p>To address the second challenge, we implemented a stimulus selection procedure that reduced inherent correlations between the three models of interest <italic>a priori</italic>. Specifically, we compared predicted category distances for repeated samples of stimuli from the SUN database, and selected a final set of stimuli for fMRI for which the predictions were minimally correlated across the function, DNN and object model. To assess whether scene categorization behavior for this reduced stimulus set was consistent with the previous behavioral findings, participants additionally performed a behavioral multi-arrangement task outside the scanner. To isolate the unique contribution of each model to fMRI and behavioral scene similarity, we applied a variance partitioning analysis, accounting for any residual overlap in representational structure between models.</p><p>To anticipate, our data reveal a striking dissociation between the feature model that best describes behavioral scene similarity and the model that best explains similarity of fMRI responses in scene-selective cortex. While we confirmed that behavioral scene categorization was best explained a combination of the function model and DNN features, there was no unique representation of scene functions in scene-selective brain regions, which instead were best described by DNN features only. Follow-up analyses indicated that scene functions correlated with responses in regions outside of scene-selective cortex, some of which have been previously associated with action observation. However, a direct comparison between behavioral scene similarity and fMRI responses indicated that behavioral scene categorization correlated most strongly with scene-selective regions, with no discernible contribution of other regions. This dissociation between the features that contribute uniquely to behavioral versus fMRI scene similarity suggests that scene-selective cortex and DNN feature models represent only a subset of the information relevant for scene categorization.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Disentangling function, DNN and object features in scenes</title><p>The goal of the study was to determine the contributions of function, DNN and object feature models to neural representations in scene-selective cortex. To do this, we created a stimulus set by iteratively sampling from the large set of scenes previously characterized in terms of these three types of information by <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref>. The DNN feature model was derived using a high-level layer of an AlexNet (<xref ref-type="bibr" rid="bib38">Krizhevsky et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Sermanet et al., 2013</xref>) that was pre-trained using ImageNet class labels (<xref ref-type="bibr" rid="bib13">Deng et al., 2009</xref>), while the object and function feature models were derived based on object and action labels assigned by human observers through Amazon Mechanical Turk (see Materials and methods for details). On each iteration, pairwise distances between a subset of pseudo-randomly sampled categories were determined for each of these feature models, resulting in three representational dissimilarity matrices (RDMs) reflecting the predicted category distances for either the function, DNN or object model (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) for that sample. Constraining the set to include equal numbers of indoor, urban, and natural landscape environments, our strategy was inspired by the odds algorithm of <xref ref-type="bibr" rid="bib9">Bruss (2000)</xref>, in that we rejected the first 10,000 solutions, selecting the next solution that had lower inter-feature correlations than had been observed thus far. Thus, a final selection of 30 scene categories was selected in which the three RDMs were minimally correlated (Pearson’s <italic>r</italic>: 0.23–0.26; <xref ref-type="fig" rid="fig1">Figure 1B–C</xref>; see Materials and methods).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.32962.002</object-id><label>Figure 1.</label><caption><title>Models and predicted stimulus dissimilarity.</title><p>(<bold>A</bold>) Stimuli were characterized in three different ways: functions (derived using human-generated action labels), objects (derived using human-generated object labels) and DNN features (derived using layer 7 of a 1000-class trained convolutional neural network). (<bold>B</bold>) RDMs showing predicted representational dissimilarity in terms of functions, objects and DNN features for the 30 scene categories sampled from <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref>. Scenes were sampled to achieve minimal between-RDM correlations. The category order in the RDMs is determined based on a k-means clustering on the functional RDM; clustering was performed by requesting eight clusters, which explained 80% of the variance in that RDM. RDMs were rank-ordered for visualization purposes only. (<bold>C</bold>) Multi-dimensional scaling plots of the model RDMs, color-coded based on the functional clusters depicted in B). Functional model clusters reflected functions such as ‘sports’, and ‘transportation’; note however that these semantic labels were derived post-hoc after clustering, and did not affect stimulus selection. Critically, representational dissimilarity based on the two other models (objects and DNN features) predicted different cluster patterns. All stimuli and model RDMs, along with the behavioral and fMRI measurements, are provided in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>.</p><p><supplementary-material id="fig1sdata1"><object-id pub-id-type="doi">10.7554/eLife.32962.003</object-id><label>Figure 1—source data 1.</label><caption/><media mime-subtype="zip" mimetype="application" xlink:href="elife-32962-fig1-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32962-fig1-v2"/></fig><p>Twenty participants viewed the selected scenes while being scanned on a high-field 7T Siemens MRI scanner using a protocol sensitive to blood oxygenation level dependent (BOLD) contrast (see Materials and methods). Stimuli were presented for 500 ms each while participants performed an orthogonal task on the fixation cross. To assess how each feature model contributed to scene categorization behavior for our much reduced stimulus set (30 instead of the 311 categories of <xref ref-type="bibr" rid="bib22">Greene et al., 2016</xref>), participants additionally performed a behavioral multi-arrangement task (<xref ref-type="bibr" rid="bib37">Kriegeskorte and Mur, 2012</xref>) on the same stimuli, administered on a separate day after scanning. In this task, participants were presented with all stimuli in the set arranged around a large white circle on a computer screen, and were instructed to drag-and-drop these scenes within the white circle according to their perceived similarity (see Materials and methods and <xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.32962.004</object-id><label>Figure 2.</label><caption><title>Behavioral multi-arrangement paradigm and results.</title><p>(<bold>A</bold>) Participants organized the scenes inside a large white circle according to their perceived similarity as determined by their own judgment, without receiving instructions as to what information to use to determine scene similarity. (<bold>B</bold>) RDM displaying the average dissimilarity between categories in the behavioral arrangements, ordered the same way as <xref ref-type="fig" rid="fig1">Figure 1B</xref> (rank-ordered for visualization only). (<bold>C</bold>) Average (bar) and individual participant (gray dots) correlations between the behavioral RDM and the model RDMs for objects (red), DNN features (yellow) and functions (blue). Stars (*) indicate p&lt;0.05 for model-specific one-sided signed-rank tests against zero, while horizontal bars indicate p&lt;0.05 for two-sided pairwise signed-rank tests between models; <italic>p</italic>-values were FDR-corrected across both types of comparisons. The light-blue shaded rectangular region reflects the upper and lower bound of the noise ceiling, indicating RDM similarity between individual participants and the group average (see Materials and methods). Error bars reflect SEM across participants. (<bold>D</bold>) Count of participants whose behavioral RDM correlated highest with either objects, DNN features or functions. (<bold>E</bold>) Partial correlations for each model RDM. Statistical significance was determined the same way as in C). (<bold>F</bold>) Euler diagram depicting the results of a variance partitioning analysis on the behavioral RDM for objects (red circle), DNN features (yellow circle) and functions (blue circle). Unique (non-overlapping diagram portions) and shared (overlapping diagram portions) variances are expressed as percentages of the total variance explained by all models combined.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32962-fig2-v2"/></fig></sec><sec id="s2-2"><title>Function and DNN model both contribute uniquely to scene categorization behavior</title><p>To determine what information contributed to behavioral similarity judgments in the multi-arrangement task, we created RDMs based on each participant’s final arrangement by measuring the pairwise distances between all 30 categories in the set (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), and then computed correlations of these RDMs with the three model RDMs that quantified the similarity of the scenes in terms of either functions, objects, or DNN features, respectively (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>Replicating <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref>, this analysis indicated that all three feature models were significantly correlated with scene categorization behavior, with the function model having the highest correlation on average (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; objects: mean <italic>r</italic> = 0.16; DNN features: mean <italic>r</italic> = 0.26; functions: mean <italic>r</italic> = 0.29, Wilcoxon one-sided signed-rank test, all W(20) &gt; 210, all <italic>z</italic> &gt; 3.9, all p&lt;0.0001). The correlation with functions was higher than with objects (Wilcoxon two-sided signed-rank test, W(20) = 199, <italic>z</italic> = 3.5, p=0.0004), but not than with DNN features (W(20) = 134, <italic>z</italic> = 1.1, p=0.28), which also correlated higher than objects (W(20) = 194, <italic>z</italic> = 3.3, p=0.0009). However, comparison at the level of individual participants indicated that functions outperformed both the DNN and object models for the majority of participants (highest correlation with functions: n = 12; with DNN features: n = 7; with objects: n = 1; <xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>While these correlations indicate that scene dissimilarity based on the function model best matched the stimulus arrangements that participants made, they do not reveal to what extent functional, DNN or object features <italic>independently</italic> contribute to the behavior. To assess this, we performed two additional analyses. First, we computed partial correlations between models and behavior whereby the correlation of each model with the behavior was determined whilst taking into account the contributions of the other two models. The results indicated that each model independently contributed to the behavioral data: significant partial correlations were obtained for the object (W(20) = 173, <italic>z</italic> = 2.5, p=0.006), DNN features (W(20) = 209, <italic>z</italic> = 3.9, p&lt;0.0001) and function models (W(20) = 209, <italic>z</italic> = 3.9, p&lt;0.0001), with the function model having the largest partial correlation (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Direct comparisons yielded a similar pattern as the independent correlations, with weaker contributions of objects relative to both functional (W(20) = 201, <italic>z</italic> = 3.6, p&lt;0.0003) and DNN features (W(20) = 195, <italic>z</italic> = 3.4, p=0.0008), whose partial correlations did not differ from one another (W(20) = 135, <italic>z</italic> = 1.12, p=0.26).</p><p>Second, we conducted a variance partitioning analysis, in which the function, DNN and object feature models were entered either separately or in combination as predictors in a set of multiple regression analyses aimed at explaining the multi-arrangement categorization behavior. By comparing the explained variance based on regression on individual models versus models in combination, we computed portions of unique variance contributed by each model as well as portions of shared variance across models (see Materials and methods for details).</p><p>A full model in which all three models were included explained 50.3% of the variance in the average multi-arrangement behavior (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). Highlighting the importance of functional features for scene categorization, the largest portion of this variance could be uniquely attributed to the function model (unique <italic>r<sup>2</sup></italic> = 37.6%), more than the unique variance explained by the DNN features (unique <italic>r<sup>2</sup></italic> = 29.0%) or the objects (unique <italic>r<sup>2</sup></italic> = 1.4%). This result is consistent with the findings of <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref>, who found unique contributions of 45.2% by the function model, 7.1% by the DNN model and 0.3% by objects, respectively. (When performing the variance partitioning on the behavioral categorization measured in <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref> but limited to the reduced set of 30 scene categories used here, we obtained a highly similar distribution of unique variances as for the current behavioral data, namely 42.8% for functions, 28.0% for DNN features, and 0.003% for objects, respectively. This suggests that the higher contribution of the DNN relative to <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref> is a result of the reduced stimulus set used here, rather than a qualitative difference in experimental results between studies.) One interesting difference with this previous study is that the degree of shared variance between all three models is notably smaller (8.4% versus 27.4%). This is presumably a result of our stimulus selection procedure that was explicitly aimed at minimizing correlations between models. Importantly, a reproducibility test indicated that the scene similarity reflected in the multi-arrangement behavior was highly generalizable, resulting in an RDM correlation of <italic>r</italic> = 0.73 (95% confidence interval = [0.73–0.88], p=0.0001) across two different sets of scene exemplars that were evenly distributed across participants (see Materials and methods).</p><p>In sum, these results confirm a unique, independent contribution of the function model to scene categorization behavior, here assessed using a multi-arrangement sorting task (as opposed to a same/different categorization task). We also found a unique but smaller contribution of the DNN feature model, while the unique contribution of the object model was negligible. Next, we examined to what extent this information is represented in brain responses to the same set of scenes as measured with fMRI.</p></sec><sec id="s2-3"><title>DNN model uniquely predicts responses in scene-selective cortex</title><p>To determine the information that is represented in scene-selective brain regions PPA, OPA and MPA, we created RDMs based on the pairwise comparisons of multi-voxel activity patterns for each scene category in these cortical regions (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), which we subsequently correlated with the RDMs based on the object, function and DNN feature models. Similar to the behavioral findings, all three feature models correlated with the fMRI response patterns in PPA (objects: W(20) = 181, <italic>z</italic> = 2.8, p=0.002; DNN: W(20) = 206, <italic>z</italic> = 3.8, p&lt;0.0001; functions: W(20) = 154, <italic>z</italic> = 1.8, p=0.035, see <xref ref-type="fig" rid="fig3">Figure 3B</xref>). However, PPA correlated more strongly with the DNN feature model than the object (W(20) = 195, <italic>z</italic> = 2.5, p=0.012) and function (W(20) = 198, <italic>z</italic> = 3.5, p&lt;0.0005) models, which did not differ from one another (W(20) = 145, <italic>z</italic> = 1.5, p=0.14). In OPA, only the DNN model correlated with the fMRI response patterns (W(20) = 165, <italic>z</italic> = 2,2, p=0.013), and this correlation was again stronger than for the object model (W(20) = 172, <italic>z</italic> = 2.5, p=0.012), but not the function model (W(20) = 134, <italic>z</italic> = 1.1, p=0.28). In MPA, no correlations were significant (all W(14) &lt; 76, all <italic>z</italic> &lt; 1.4, all p&gt;0.07).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.32962.005</object-id><label>Figure 3.</label><caption><title>RDMs and model comparisons for fMRI Experiment 1 (n = 20).</title><p>(<bold>A</bold>) RDMs displaying average dissimilarity between categories in multi-voxel patterns in PPA, OPA and MPA, ordered as in <xref ref-type="fig" rid="fig1">Figure 1B</xref> (rank-ordered for visualization only). (<bold>B</bold>) Average (bar) and individual participant (gray dots) correlations between the ROIs in A) and the model RDMs for objects (red), DNN features (yellow) and functions (blue) (FDR-corrected). See legend of <xref ref-type="fig" rid="fig2">Figure 2B</xref> for explanation of the statistical indicators and noise ceiling. (<bold>C</bold>) Partial correlations for each model RDM. Statistics are the same as in B). (<bold>D</bold>) Euler diagram depicting results of variance partitioning the average dissimilarity in each ROI between models, expressed as percentages of unique and shared variance of the variance explained by all three models together.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32962-fig3-v2"/></fig><p>When the three models were considered in combination, only the DNN model yielded a significant partial correlation (PPA: W(20) = 203, <italic>z</italic> = 3.6, p&lt;0.0001, OPA: W(20) = 171, <italic>z</italic> = 2.5, p=0.007, <xref ref-type="fig" rid="fig3">Figure 3C</xref>), further showing that DNN features best capture responses in scene-selective cortex. No significant partial correlation was found for the object model (PPA: W(20) = 148, <italic>z</italic> = 1.6, p=0.056; OPA: W(20) = 74, <italic>z</italic> = 1.2, p=0.88) or the function model (PPA: W(20) = 98, <italic>z</italic> = 0.3, p=0.61, OPA: W(20) = 127, <italic>z</italic> = 0.8, p=0.21), or for any model in MPA (all W(14) &lt; 63, all <italic>z</italic> &lt; 0.66, all p&gt;0.50). Variance partitioning of the fMRI RDMs (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) indicated that the DNN model also contributed the largest portion of unique variance: in PPA and OPA, DNN features contributed 71.1% and 68.9%, respectively, of the variance explained by all models combined, more than the unique variance explained by the object (PPA: 5.3%; OPA, 2.3%) and function (PPA: 0.3%; OPA: 2.6%) models. In MPA, a larger share of unique variance was found for the function model (41.5%) than for the DNN (38.7%) and object model (3.2%); however, overall explained variance in MPA was much lower than in the other ROIs. A reproducibility test indicated that RDMs generalized across participants and stimulus sets for PPA (<italic>r</italic> = 0.26 [0.03–0.54], p=0.009) and OPA (<italic>r</italic> = 0.23 [0.04–0.51], p=0.0148), but not in MPA (<italic>r</italic> = 0.06 [−0.16–0.26], p=0.29), suggesting that the multi-voxel patterns measured in MPA were less stable (this is also reflected in the low noise ceiling in MPA in <xref ref-type="fig" rid="fig3">Figure 3B and C</xref>).</p><p>Taken together, the fMRI results indicate that of the three models considered, deep network features (derived using a pre-trained convolutional network) best explained the coding of scene information in PPA and OPA, more so than object or functional information derived from semantic labels that were explicitly generated by human observers. For MPA, results were inconclusive, as none of the models adequately captured the response patterns measured in this region, which also did not generalize across stimulus sets and participants. This result reveals a discrepancy between measurements of brain responses versus behavioral scene similarity, which indicated a large contribution of functions to scene representation independent of the DNN features. To better understand if and how scene-selective cortex represents behaviorally relevant information, we next compared measurements of behavioral scene similarity to the fMRI responses directly.</p></sec><sec id="s2-4"><title>Scene-selective cortex correlation with behavior reflects DNN model</title><p>To assess the extent to which fMRI response patterns in scene-selective cortex predicted behavioral scene similarity, we correlated each of the scene-selective ROIs with three measures of behavioral categorization: (1) the large-scale online categorization behavior measured in <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref>, (2) the average multi-arrangement behavior of the participants in the current study, and (3) each individual participant’s own multi-arrangement behavior. This analysis revealed a significant correlation with behavior in all scene-selective ROIs (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). In PPA, all three measures of behavioral categorization correlated with fMRI responses (signed-rank test, online categorization behavior: W(20) = 168, <italic>z</italic> = 2.3, p=0.010; average multi-arrangement behavior: W(20) = 195, <italic>z</italic> = 3.3, p=0.0004; own arrangement behavior: W(20) = 159, <italic>z</italic> = 2.0, p=0.023). In OPA, significant correlations were found for both of the average behavioral measures (online categorization behavior: W(20) = 181, <italic>z</italic> = 2.8, p=0.002; average multi-arrangement behavior: W(20) = 158, <italic>z</italic> = 1.96, p=0.025), but not for the participant’s own multi-arrangement behavior (W(20) = 106, <italic>z</italic> = 0.02, p=0.49), possibly due to higher noise in the individual data. Interestingly, however, MPA showed the opposite pattern: participant’s own behavior was significantly related to the observed fMRI responses (W(14) = 89, <italic>z</italic> = 2.26, p=0.011), but the average behavioral measures were not (online behavior: W(14) = 47, <italic>z</italic> = 0.4, p=0.65; average behavior: W(14) = 74, <italic>z</italic> = 1.3, p=0.09). Combined with the reproducibility test results (see above), this suggests that the MPA responses are more idiosyncratic to individual participants or stimulus sets.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.32962.006</object-id><label>Figure 4.</label><caption><title>Correlations and variance partitioning of behavioral measurements of scene categorization and similarity of fMRI responses.</title><p>(<bold>A</bold>) Correlations of three measures of behavioral categorization (see Results section for details) with fMRI response patterns in PPA, OPA and MPA. See legend of <xref ref-type="fig" rid="fig2">Figure 2B</xref> for explanation of the statistical indicators and noise ceiling. (<bold>B</bold>) Euler diagram depicting the results of variance partitioning the fMRI responses in PPA, OPA and MPA for DNN features (yellow), functions (blue) and average sorting behavior (green), indicating that the majority of the variance in the fMRI signal that is explained by categorization behavior is shared with the DNN features.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32962-fig4-v2"/></fig><p>While these results support an important role for scene-selective regions in representing information that informs scene categorization behavior, they also raise an intriguing question: what aspect of categorization behavior is reflected in these neural response patterns? To address this, we performed another variance partitioning analysis, now including the average multi-arrangement behavior as a predictor of the fMRI response patterns, in combination with the two models that correlated most strongly with this behavior, that is the DNN and function models. The purpose of this analysis was to determine how much variance in neural responses each of the models <italic>shared</italic> with the behavior, and whether there was any behavioral variance in scene cortex that was not explained by our models. If the behaviorally relevant information in the fMRI responses is primarily of a functional nature, we would expect portions of the variance explained by behavior to be shared with the function model. Alternatively, if this variance reflects mainly DNN features (which also contributed uniquely to the behavioral categorization; <xref ref-type="fig" rid="fig2">Figure 2F</xref>), we would expect it to be shared primarily with the DNN model.</p><p>Consistent with this second hypothesis, the variance partitioning results indicated that in OPA and PPA, most of the behavioral variance in the fMRI response patterns was shared with the DNN model (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In PPA, the behavioral RDMs on average shared 25.7% variance with the DNN model, while a negligible portion was shared with the function model (less than 1%); indeed, nearly all variance shared between the function model and the behavior was also shared with the DNN model (10.1%). In OPA, a similar trend was observed, with behavior sharing 38.9% of the fMRI variance with the DNN model. In OPA, the DNN model also eclipsed nearly all variance that behavior shared with the function model (9.7% shared by behavior, functions and DNN features), leaving only 1.6% of variance shared exclusively by functions and behavior. In contrast, in MPA, behavioral variance was shared with either the DNN model or the function model to a similar degree (14.7% and 17.7%, respectively), with an additional 27.1% shared with both; note, however, again MPA’s low explained variance overall.</p><p>In sum, while fMRI response patterns in PPA and OPA reflect information that contributes to scene similarity judgments, this information aligns best with the DNN feature model; it does not reflect the unique contribution of functions to scene categorization behavior. While in MPA, the behaviorally relevant representations may partly reflect other information, the overall explained variance in MPA was again quite low, limiting interpretation of this result.</p></sec><sec id="s2-5"><title>Relative model contributions to fMRI responses do not change with task manipulation</title><p>An important difference between the behavioral and the fMRI experiment was that participants had access to the entire stimulus set when performing the behavioral multi-arrangement task, which they could perform at their own pace, while they performed a task unrelated to scene categorization in the fMRI scanner. Therefore, we reasoned that a possible explanation of the discrepancy between the fMRI and behavioral findings could be a limited engagement of participants with the briefly presented scenes while in the scanner, resulting in only superficial encoding of the images in terms of visual features that are well captured by the DNN model, rather than functional or object features that might be more conceptual in nature.</p><p>To test this possible explanation, we ran Experiment 2 and collected another set of fMRI data using the exact same visual stimulation, but with a different task instruction (n = 8; four of these participants also participated in Experiment 1, allowing for direct comparison of tasks within individuals). Specifically, instead of performing an unrelated fixation task, participants covertly named the presented scene. Covert naming has been shown to facilitate stimulus processing within category-selective regions and to enhance semantic processing (<xref ref-type="bibr" rid="bib64">van Turennout et al., 2000</xref>; <xref ref-type="bibr" rid="bib63">van Turennout et al., 2003</xref>). Before entering the scanner, participants were familiarized with all the individual scenes in the set, whereby they explicitly generated a name for each individual scene (see Materials and methods). Together, these manipulations were intended to ensure that participants attended to the scenes and processed their content to a fuller extent than in Experiment 1.</p><p>Despite this task difference, Experiment 2 yielded similar results as Experiment 1 (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Reflecting participant’s enhanced engagement with the scenes when performing the covert naming task, overall model correlations were considerably higher than in Experiment 1, and now yielded significant correlations with the function model in both OPA and MPA (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The direct test of reproducibility also yielded significant, and somewhat increased, correlations for PPA (<italic>r</italic> = 0.35 [0.26–0.55], p=0.0001) and OPA (<italic>r</italic> = 0.27 [0.18–0.60], p=0.039), but not in MPA (<italic>r</italic> = 0.10 [−0.07–0.28], p=0.17). Importantly, in all three ROIs, the DNN model correlations were again significantly stronger than the function and object model correlations, which again contributed very little unique variance (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Direct comparison of RDM correlations across the two Experiments indicated that in PPA and OPA, the naming task resulted in increased correlations for the DNN model only (two-sided Wilcoxon ranksum test, PPA: p=0.0048; OPA p=0.0056), without any difference in correlations for the other models (all p&gt;0.52). In MPA, none of the model correlations differed across tasks (all p&gt;0.21). Increased correlation with the DNN model was present within the participants that participated in both experiments (n = 4; see Materials and methods): in PPA and OPA, 4/4 and 3/4 participants showed an increased correlation, respectively, whereas no consistent patterns was observed for the other models and MPA (<xref ref-type="fig" rid="fig5">Figure 5D</xref>).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.32962.007</object-id><label>Figure 5.</label><caption><title>RDMs and model comparisons for Experiment 2 (n = 8, covert naming task).</title><p>(<bold>A</bold>) Average dissimilarity between categories in multi-voxel patterns measured in PPA, OPA and MPA (rank-ordered as in <xref ref-type="fig" rid="fig1">Figure 1B</xref>). (<bold>B</bold>) Correlations between the ROIs in A) and the model RDMs for objects (red), DNN features (yellow) and functions (blue) (FDR-corrected). See legend of <xref ref-type="fig" rid="fig2">Figure 2B</xref> for explanation of the statistical indicators and noise ceiling. Note how in PPA, the DNN model correlation approaches the noise ceiling, suggesting that this model adequately captures the information reflected in this ROI. (<bold>C</bold>) Euler diagram depicting the results of variance partitioning the average dissimilarity in each ROI. (<bold>D</bold>) Average (bars) and individual (dots/lines) within-participant (n = 4) comparison of fMRI-model correlations across the different task manipulations in Experiment 1 and 2 (participants were presented with a different set of scenes in each task, see Materials and methods). Note how covert naming mainly enhances the correlation with DNN features.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32962-fig5-v2"/></fig><p>In sum, the results of Experiment 2 indicate that the strong contribution of DNN features to fMRI responses in scene-selective cortex is not likely the result of limited engagement of participants with the scenes when viewed in the scanner. If anything, enhanced attention to the scenes under an explicit naming instruction resulted in even stronger representation of the DNN features, without a clear increase in contributions of the function or object models.</p></sec><sec id="s2-6"><title>Contributions of the function model outside scene-selective cortex</title><p>Our results so far indicate a dissociation between brain and behavioral assessments of the representational similarity of scenes. In the behavioral domain, visual features in a deep convolutional network uniquely contributed to behavioral scene categorization, but the function model also exhibited a large unique contribution, regardless of whether this behavior was assessed using a same-different categorization or a multi-arrangement task. In contrast, fMRI responses in scene-selective cortex were primarily driven by DNN features, without convincing evidence of an independent contribution of functions. Given this lack of correlation with the function model in the scene-selective cortex, we explored whether this information could be reflected elsewhere in the brain by performing whole-brain searchlight analyses. Specifically, we extracted the multi-voxel patterns from spherical ROIs throughout each participant’s entire brain volume and performed partial correlation analyses including all three models (DNN features, objects, functions) to extract corresponding correlation maps for each model. The resulting whole-brain searchlight maps were then fed into a to surface-based group analysis (see Materials and methods) to identify clusters of positive correlations indicating significant model contributions to brain representation throughout all measured regions of cortex.</p><p>The results of these searchlight analyses were entirely consistent with the ROI analyses: for the DNN feature model, significant searchlight clusters were found in PPA and OPA (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), but not MPA, whereas no significant clusters were found for the function model in any of the scene-selective ROIs. (The object model yielded no positive clusters). However, two clusters were identified for the function model outside of scene-selective cortex (<xref ref-type="fig" rid="fig6">Figure 6B</xref>): 1) a bilateral cluster on the ventral surface, lateral to PPA, overlapping with the fusiform and temporal lateral gyri, and 2) a unilateral cluster on the left lateral surface, located adjacent to, but more ventral than, OPA, overlapping the posterior middle and inferior temporal gyrus.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.32962.008</object-id><label>Figure 6.</label><caption><title>Medial (left) and lateral (right) views of group-level searchlights for (A) the DNN and (B) function model, overlaid on surface reconstructions of both hemispheres of one participant.</title><p>Each map was created by submitting the partial correlation maps for each model and hemisphere to one-sample tests against a mean of zero, cluster-corrected for multiple comparisons using Threshold-Free Cluster Enhancement (thresholded on z = 1.64, corresponding to one-sided p&lt;0.05). Unthresholded versions of the average partial correlation maps are inset above. Group-level ROIs PPA, OPA and MPA are highlighted in solid white lines. Consistent with the ROI analyses, the DNN feature model contributed uniquely to representation in PPA and OPA. The function model uniquely correlated with a bilateral ventral region, as well as a left-lateralized region overlapping with the middle temporal and occipital gyri.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32962-fig6-v2"/></fig><p>The observed dissociation between behavioral categorization and scene-selective cortex suggests that the functional features of scenes that we found to be important for scene categorization behavior are potentially represented outside of scene-selective cortex. If so, we would expect the searchlight clusters that correlated with the function model to show a correspondence with the behavioral scene categorization. To test this, we directly correlated the multi-arrangement behavior with multi-voxel pattern responses throughout the brain. Consistent with the results reported in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we found a significant searchlight correlation between the behavioral measurements and response patterns in PPA and OPA (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Surprisingly, however, behavioral categorization did not correlate with any regions outside these ROIs, including the clusters that correlated with the function model.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.32962.009</object-id><label>Figure 7.</label><caption><title>Multi-arrangement behavior searchlights and post-hoc analysis of functional clusters. </title><p>(<bold>A</bold>) Searchlight result for behavioral scene categorization. Maps reflect correlation (Pearson’s <italic>r</italic>) of the group-average behavior in the multi-arrangement task from the participants of Experiment 1. Scene-selective ROIs are outlined in white solid lines; the searchlight clusters showing a significant contribution of the functional model are outlined in dashed white lines for reference. See <xref ref-type="fig" rid="fig6">Figure 6</xref> for further explanation of the searchlight display. (<bold>B</bold>) RDM and MDS plots based on the MVPA patterns in the function model searchlight clusters. RDM rows are ordered as in <xref ref-type="fig" rid="fig1">Figure 1B</xref> and category color coding in the MDS plots is as in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. (<bold>C</bold>) Illustrative exemplars of the four categories that were most dissimilar from other categories within the searchlight-derived clusters depicted in B.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32962-fig7-v2"/></fig><p>In order to better understand how representational dissimilarity in those clusters related to the functional feature model, we extracted the average RDM from each searchlight cluster and inspected which scene categories were grouped together in these ROIs. Visual inspection of the RDM and MDS plots of the RDMs (<xref ref-type="fig" rid="fig7">Figure 7B</xref>) indicates that in both the bilateral ventral and left-lateralized searchlight clusters, there is some grouping by category according to the function feature model (indicated by grouping by color in the MDS plot). However, it is also clear that the representational space in these ROIs does not <italic>exactly</italic> map onto the functional feature model in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. Specifically, a few categories clearly ‘stand out’ with respect to the other categories, as indicated by a large average distance relative to the other categories in the stimulus set. Most of the scene categories that were strongly separated all contained scene exemplars depicting humans that performed actions (see <xref ref-type="fig" rid="fig7">Figure 7C</xref>), although it is worth noting that scene exemplars in the fourth most distinct category, ‘volcano’, did not contain any humans but may be characterized by implied motion. These post-hoc observations suggest that (parts of) the searchlight correlation with the functional feature model may be due to the presence of human-, body- and/or motion selective voxels in these searchlight clusters.</p><p>In sum, the searchlight analyses indicate that the strongest contributions of the DNN model were found in scene-selective cortex. While some aspects of the function model may be reflected in regions outside of scene-selective cortex, these regions did not appear to contribute to the scene categorization behavior, and may reflect selectivity for only a subset of scene categories that clustered together in the function model.</p></sec><sec id="s2-7"><title>Scene-selective cortex correlates with features from both mid- and high-level DNN layers</title><p>Our results highlight a significant contribution of DNN features to representations in scene-selective cortex. DNNs consist of multiple layers that capture a series of transformations from pixels in the input image to a class label, implementing a non-linear mapping of local convolutional filters responses (layers 1–5) onto a set of fully-connected layers that consist of classification nodes (layers 6–8) culminating in a vector of output ‘activations’ for labels assigned in the DNN training phase. Visualization and quantification methods of the learned feature selectivity (e.g., <xref ref-type="bibr" rid="bib71">Zhou et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib4">Bau et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Wen et al., 2017</xref>) suggest that while earlier layers contain local filters that resemble V1-like receptive fields, higher layers develop selectivity for entire objects or object parts, perhaps resembling category-selective regions in visual cortex. Our deep network feature model was derived using a single high-level layer, fully-connected layer 7 (‘fc7’). Moreover, this model was derived using the response patterns of a DNN that was pretrained on ImageNet (<xref ref-type="bibr" rid="bib13">Deng et al., 2009</xref>), an image database largely consisting of object labels. Given the strong performance of the DNN feature model in explaining the fMRI responses in scene-selective cortex, it is important to determine whether this result was exclusive to higher DNN layers, and whether the task used for DNN training influences how well the features represented in individual layers explain responses in scene-selective cortex. To do so, we conducted a series of exploratory analyses to assess the contribution of other DNN layers to fMRI responses, whereby we compared DNNs that were trained using either object or scene labels.</p><p>To allow for a clean test of the influence of DNN training on features representations in each layer, we derived two new sets of RDMs by passing our stimuli through (1) a novel 1000-object label ImageNet-trained network implemented in Caffe (<xref ref-type="bibr" rid="bib32">Jia et al., 2014</xref>) (‘ReferenceNet’) and (2) a 250-scene label Places-trained network (‘Places’) (<xref ref-type="bibr" rid="bib71">Zhou et al., 2014</xref>), (see Materials and methods). Direct comparisons of the layer-by-layer RDMs of these two DNNs (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) indicated that both models extracted similar features, evidenced by strong between-model correlations overall (all layers <italic>r</italic> &gt; 0.6). However, the similarity between models decreased with higher layers, suggesting that features in higher DNN layers become tailored to the task they are trained on. Moreover, this suggests that higher layers of the scene-trained DNN could potentially capture different features than the object-trained DNN. To investigate this, we next computed correlations between the features in each DNN layer and the three original feature models (<xref ref-type="fig" rid="fig8">Figure 8B</xref>).</p><fig-group><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.32962.010</object-id><label>Figure 8.</label><caption><title>DNN layer and DNN training comparisons in terms of correlation with fMRI responses in scene-selective cortex.</title><p>Panels show convolutional and fully-connected (FC) layer-by-layer RDM correlations between (<bold>A</bold>) an object-trained (ReferenceNet) and a scene-trained (Places) DNN; (<bold>B</bold>) both DNNs and the <italic>a priori</italic> selected feature models; (<bold>C</bold>) the object-trained DNN and scene-selective ROIs; (<bold>D</bold>) the scene-trained DNN and scene-selective ROIs (all comparisons FDR-corrected within ROI; See legend of <xref ref-type="fig" rid="fig2">Figure 2B</xref> for explanation of the statistical indicators and noise ceiling). While the decreasing correlation between DNNs indicates stronger task-specificity of higher DNN layers, the original fc7 DNN feature model correlated most strongly with high-level layers of both DNNs. The object-trained and the scene-trained DNN correlated similarly with PPA and OPA, with both showing remarkable good performance for mid-level layers. The RDMs for each individual DNN layer are provided in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>. Searchlight maps for each layer of the object- and scene-trained DNN are provided in <xref ref-type="video" rid="fig8video1">Figure 8—video 1</xref> and <xref ref-type="video" rid="fig8video2">Figure 8—video 2</xref>, respectively.</p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-32962-fig8-v2"/></fig><media id="fig8video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-32962-fig8-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.32962.011</object-id><label>Figure 8—video 1.</label><caption><title>Layer-by-layer searchlight results for the object-trained DNN (ReferenceNet).</title><p>The first half of the movie shows group-average correlation maps for layer 1–8, cluster-corrected for multiple comparisons using Threshold-Free Cluster Enhancement (thresholded on z = 1.64, corresponding to one-sided p&lt;0.05), overlaid on medial and lateral views of inflated surface reconstructions of the left (LH) and right (RH) hemisphere of one participant. The second half of the movie shows the same data but without thresholding. Group-level ROIs PPA, OPA and MPA are highlighted in solid white lines.</p></caption></media><media id="fig8video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-32962-fig8-video2.mp4"><object-id pub-id-type="doi">10.7554/eLife.32962.012</object-id><label>Figure 8—video 2.</label><caption><title>Layer-by-layer searchlight results for the scene-trained DNN (Places).</title><p>See legend of <xref ref-type="video" rid="fig8video1">Figure 8—video 1</xref> for details.</p></caption></media></fig-group><p>As expected, the original fc7 DNN model (which was derived using DNN responses to the large set of images in the <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref> database, and thus not corresponding directly to the reduced set of stimuli used in the current study) correlated most strongly with the new DNN layer representations, showing steadily increasing correlations with higher layers of both object-trained and the scene-trained DNN. By design, the object and functional feature models should correlate minimally with layer 7 of the object-trained ReferenceNet DNN. However, the function model correlated somewhat better with higher layers of the scene-trained DNN, highlighting a potential overlap of the function model with the scene-trained DNN features, again suggesting that the higher layers of the scene-trained DNN potentially capture additional information that is not represented in the object-trained DNN. Therefore, we next tested whether the scene-trained DNN correlated more strongly with fMRI responses in scene-selective cortex.</p><p>Layer-by-layer correlations of the object-trained (<xref ref-type="fig" rid="fig8">Figure 8C</xref>) and the scene-trained DNN (<xref ref-type="fig" rid="fig8">Figure 8D</xref>) with fMRI responses in PPA, OPA and MPA however did not indicate a strong difference in DNN performance as a result of training. In PPA, both the object-trained and place-trained DNN showed increased correlation with higher DNN layers, consistent with previous work showing a hierarchical mapping of DNN layers to low vs. high-level visual cortex (<xref ref-type="bibr" rid="bib26">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib12">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib68">Wen et al., 2017</xref>). Note however that the slope of this increase is quite modest; while higher layers overall correlate better than layers 1 and 2, in both DNNs the correlation with layer three is not significantly different from the correlation of layers 7 and 8. In OPA, we observed no evidence for increased performance with higher layers for the object-trained DNN; none of the pairwise tests survived multiple comparisons correction. In fact, for the scene-trained DNN, the OPA correlation significantly <italic>decreased</italic> rather than increased with higher layers, showing a peak correlation with layer 3. No significant correlations were found for any model layer with MPA. These observations were confirmed by searchlight analyses in which whole-brain correlation maps were derived for each layer of the object- and scene-trained DNN: see <xref ref-type="video" rid="fig8video1">Figure 8—video 1</xref> and <xref ref-type="video" rid="fig8video2">Figure 8—video 2</xref> for layer-by-layer searchlight results for the ReferenceNet and the Places DNN, respectively.</p><p>These results indicate that despite a divergence in representation in high-level layers for differently-trained DNNs, their performance in predicting brain responses in scene-selective cortex is quite similar. In PPA, higher layers perform significantly better than (very) low-level layers, but mid-level layers already provide a relatively good correspondence with PPA activity. This result was even more pronounced for OPA where mid-level layers yielded the maximal correlations for both DNNs regardless of training. Therefore, these results suggest that fMRI responses in scene-selective ROIs may reflect a contribution of visual DNN features of intermediate complexity rather than, or in addition to, the fc7 layer that was selected <italic>a priori</italic>.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We assessed the contribution of three scene feature models previously implicated to be important for behavioral scene understanding to neural representations of scenes in the human brain. First, we confirmed earlier reports that functions strongly contribute to scene categorization by replicating the results of <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref>, now using a multi-arrangement task. Second, we found that brain responses to visual scenes in scene-selective regions were best explained by a DNN feature model, with no discernible unique contribution of functions. Thus, although parts of variance in the multi-arrangement behavior were captured by the DNN feature model - and this part of the behavior was reflected in the scene-selective cortex - there are clearly aspects of scene categorization behavior that were not reflected in the activity of these regions. Collectively, these results thus reveal a striking dissociation between the information that is most important for behavioral scene categorization and the information that best describes representational dissimilarity of fMRI responses in regions of cortex that are thought to support scene recognition. Below, we discuss two potential explanations for this dissociation.</p><p>First, one possibility is that functions are represented outside scene-selective cortex. Our searchlight analysis indeed revealed clusters of correlations with the function model in bilateral ventral and left lateral occipito-temporal cortex. Visual inspection of these maps suggests that these clusters potentially overlap with known face- and body-selective regions such as the Fusiform Face (FFA; <xref ref-type="bibr" rid="bib33">Kanwisher et al., 1997</xref>) and Fusiform Body (FBA; <xref ref-type="bibr" rid="bib52">Peelen and Downing, 2007</xref>) areas on ventral surface, as well as the Extrastriate Body Area (EBA; <xref ref-type="bibr" rid="bib15">Downing et al., 2001</xref>) on the lateral surface. This lateral cluster could possibly include motion-selective (<xref ref-type="bibr" rid="bib70">Zeki et al., 1991</xref>; <xref ref-type="bibr" rid="bib59">Tootell et al., 1995</xref>) and tool-selective (<xref ref-type="bibr" rid="bib45">Martin et al., 1996</xref>) regions as well. Our results further indicated that these searchlight clusters contained distinct representations of scenes that contained <italic>acting</italic> bodies, and may therefore partially overlap with regions important for action observation (e.g., <xref ref-type="bibr" rid="bib27">Hafri et al., 2017</xref>). Lateral occipital-temporal cortex in particular is thought to support action observation by containing ‘representations which capture perceptual, semantic and motor knowledge of how actions change the state of the world’ (<xref ref-type="bibr" rid="bib41">Lingnau and Downing, 2015</xref>). While our searchlight results suggest a possible contribution of these non-scene-selective regions to scene understanding, more research is needed to address how the functional feature model as defined here relates to the action observation network, and to what extent the correlations with functional features can be explained by mere coding of the presence of bodies and motion versus more abstract action-associated features. Importantly, the lack of a correlation between these regions and the multi-arrangement behavior suggests that these regions do not fully capture the representational space that is reflected in the function model.</p><p>The second possible explanation for the dissociation between brain and behavioral data is that the task performed during fMRI did not engage the same mental processes that participants employed during the two behavioral tasks we investigated. Specifically, both the multi-arrangement task used here and the online same-different behavioral paradigm used in (<xref ref-type="bibr" rid="bib22">Greene et al., 2016</xref>) required participants to directly compare simultaneously presented scenes, while we employed a ‘standard’ fixation task in the scanner to prevent biasing our participants towards one of our feature models. Therefore, one possibility is that scene functions only become relevant for scene categorization when participants are engaged in a <italic>contrastive</italic> task, that is explicitly comparing two scene exemplars side-by-side (as in <xref ref-type="bibr" rid="bib22">Greene et al., 2016</xref>) or within the context of the entire stimulus set being present on the screen (as in our multi-arrangement paradigm). Thus, the fMRI results might change with an explicit contrastive task in which multiple stimuli are presented at the same time, or with a task that explicitly requires participants to consider functional aspects of the scenes. Although we investigated one possible influence of task in the scanner by using a covert naming task in Experiment 2, resulting in deeper and more conceptual processing, it did not result in a clear increase in the correlation with the function model in scene-selective cortex. The evidence for task effects on fMRI responses in category-selective cortex is somewhat mixed: Task differences have been reported to affect multi-voxel pattern activity in both object-selective (<xref ref-type="bibr" rid="bib28">Harel et al., 2014</xref>) and scene-selective cortex (<xref ref-type="bibr" rid="bib42">Lowe et al., 2016</xref>), but other studies suggest that task has a minimal influence on representation in ventral stream regions, instead being reflected in fronto-parietal networks (<xref ref-type="bibr" rid="bib20">Erez and Duncan, 2015</xref>; <xref ref-type="bibr" rid="bib8">Bracci et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Bugatus et al., 2017</xref>). Overall, our findings suggest that not all the information that contributes to scene categorization is reflected in scene-selective cortex activity ‘by default’, and that explicit task requirements may be necessary in order for this information to emerge in the neural activation patterns in these regions of cortex.</p><p>Importantly, the two explanations outlined above are not mutually exclusive. For example, it is possible that a task instruction to explicitly label the scenes with potential actions will activate components of both the action observation network (outside scene-selective cortex) as well as task-dependent processes within scene-selective cortex. Furthermore, given reports of potentially separate scene-selective networks for memory versus perception (<xref ref-type="bibr" rid="bib2">Baldassano et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Silson et al., 2016</xref>), it is likely that differences in mnemonic demands between tasks may have an important influence on scene-selective cortex activity. Indeed, memory-based navigation or place recognition tasks (<xref ref-type="bibr" rid="bib18">Epstein et al., 2007</xref>; <xref ref-type="bibr" rid="bib44">Marchette et al., 2014</xref>) have been shown to more strongly engage the medial parietal cortex and MPA. In contrast, our observed correlation with DNN features seems to support a primary role for PPA and OPA in bottom-up visual scene analysis, and fits well with the growing literature showing correspondences between extrastriate cortex activity and DNN features (<xref ref-type="bibr" rid="bib11">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib26">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib12">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Horikawa and Kamitani, 2017</xref>; <xref ref-type="bibr" rid="bib68">Wen et al., 2017</xref>). Our analyses further showed that DNN correlations with scene-selective cortex were not exclusive to higher DNN layers, but already emerged at earlier layers, suggesting that the neural representation in PPA/OPA may be driven more by visual features than semantic information (<xref ref-type="bibr" rid="bib67">Watson et al., 2017</xref>).</p><p>One limitation of our study is that we did not exhaustively test all possible DNN models. While our design - in which we explicitly aimed to minimize inherent correlations between the feature models - required us to ‘fix’ the DNN features to be evaluated beforehand, many more variants of DNN models have been developed, consisting of different architectures such as VGG, GoogleNet and ResNet (<xref ref-type="bibr" rid="bib21">Garcia-Garcia et al., 2017</xref>), as well as different training regimes. Here, we explored the effect of DNN training by comparing the feature representations between an object- versus a place-trained DNN, but we did not observe strong differences in terms of their ability to explain fMRI responses in either scene-selective cortex or other parts of the brain (see whole-brain searchlights for the two DNNs in <xref ref-type="video" rid="fig8video1">Figure 8—video 1</xref> and <xref ref-type="video" rid="fig8video2">Figure 8—video 2</xref>). However, this does not exclude the possibility that other DNNs will map differently onto brain responses, and possibly also explain more of the behavioral measures of human scene categorization. For example, a DNN trained on the Atomic Visual Actions (AVA) dataset (<xref ref-type="bibr" rid="bib25">Gu et al., 2017</xref>), or the DNNs developed in context of event understanding (e.g., the Moments in Time Dataset; <xref ref-type="bibr" rid="bib47">Monfort et al., 2018</xref>) could potentially capture more of the variance explained by the function model in the scene categorization behavior. To facilitate the comparison of our results with alternative and future models, we have made the fMRI and the behavioral data reported in this paper publicly available in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>.</p><p>These considerations highlight an important avenue for future research in which multiple feature models (including DNNs that vary by training and architecture) and brain and behavioral measurements are carefully compared. However, our current results suggest that when participants perform scene categorization, either explicitly (<xref ref-type="bibr" rid="bib22">Greene et al., 2016</xref>) or within a multi-arrangement paradigm (<xref ref-type="bibr" rid="bib37">Kriegeskorte and Mur, 2012</xref>), they incorporate information that is not reflected in either the DNNs or in PPA and OPA. Our results thus highlight a significant gap between the information that is captured in both scene–selective cortex and a set of commonly used off-the-shelf DNNs, relative to the information that drives human understanding of visual environments. Visual environments are highly multidimensional, and scene understanding encompasses many behavioral goals, including not just visual object or scene recognition, but also navigation and action planning (<xref ref-type="bibr" rid="bib43">Malcolm et al., 2016</xref>). While visual or DNN features likely feed into multiple of these goals - for example, by signaling navigable paths in the environment (<xref ref-type="bibr" rid="bib7">Bonner and Epstein, 2017</xref>), or landmark suitability (<xref ref-type="bibr" rid="bib61">Troiani et al., 2014</xref>) - it is probably not appropriate to think about the neural representations relevant to all these different behavioral goals as being contained within one single brain region or a single neural network model. Ultimately, unraveling the neural coding of scene information will require careful manipulations of both multiple tasks and multiple scene feature models, as well as a potential expansion of our focus on a broader set of regions than those characterized by the presence of scene-selectivity.</p><sec id="s3-1"><title>Summary and conclusion</title><p>We successfully disentangled the type of information represented in scene-selective cortex: out of three behaviorally relevant feature models, only one provided a robust correlation with activity in scene-selective cortex. This model was derived from deep neural network features from a widely used computer vision algorithm for object and scene recognition. Intriguingly, however, the DNN model was not sufficient to explain scene categorization behavior, which was characterized by an additional strong contribution of functional information. This highlights a limitation of current DNNs in explaining scene understanding, as well as a potentially more distributed representation of scene information in the human brain beyond scene-selective cortex.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Twenty healthy participants (13 female, mean age 25.4 years, SD = 4.6) completed the first fMRI experiment and subsequent behavioral experiment. Four of these participants (three female, mean age 24.3 years, SD = 4.6) additionally participated in the second fMRI experiment, as well as four new participants (two female, mean age 25 years, SD = 1.6), yielding a total of eight participants in this experiment. Criteria for inclusion were that participants had to complete the entire experimental protocol (i.e., the fMRI scan and the behavioral experiment). Beyond the participants reported, three additional subjects were scanned but behavioral data was either not obtained or lost. Four additional participants did not complete the scan session due to discomfort or technical difficulties. All participants had normal or corrected-to-normal vision and gave written informed consent as part of the study protocol (93 M-0170, NCT00001360) prior to participation in the study. The study was approved by the Institutional Review Board of the National Institutes of Health and was conducted according to the Declaration of Helsinki.</p></sec><sec id="s4-2"><title>MRI acquisition</title><p>Participants were scanned on a research-dedicated Siemens 7T Magnetom scanner in the Clinical Research Center on the National Institutes of Health Campus (Bethesda, MD). Partial T2*-weighted functional image volumes were acquired using a gradient echo planar imaging (EPI) sequence with a 32-channel head coil (47 slices; 1.6 × 1.6×1.6 mm; 10% interslice gap; TR, 2 s; TE, 27 ms; matrix size, 126 × 126; FOV, 192 mm). Oblique slices were oriented approximately parallel to the base of the temporal lobe and were positioned such that they covered the occipital, temporal, and parietal cortices, and as much as possible of frontal cortex. After the functional imaging runs, standard MPRAGE (magnetization-prepared rapid-acquisition gradient echo) and corresponding GE-PD (gradient echo–proton density) images were acquired, and the MPRAGE images were then normalized by the GE-PD images for use as a high-resolution anatomical image for the following fMRI data analysis (<xref ref-type="bibr" rid="bib62">Van de Moortele et al., 2009</xref>).</p></sec><sec id="s4-3"><title>Stimuli and models</title><p>Experimental stimuli consisted of color photographs of real-world scenes (256 × 256 pixels) from 30 scene categories that were selected from a larger image database previously described in (<xref ref-type="bibr" rid="bib22">Greene et al., 2016</xref>). These scene categories were picked using an iterative sampling procedure that minimized the correlation between the categories across three different models of scene information: functions, object labels and DNN features, with the additional constraint that the final stimulus set should have equal portions of categories from indoor, outdoor man-made and outdoor natural scenes, which is the largest superordinate distinction present in the largest scene-database that is publicly available, the SUN database (<xref ref-type="bibr" rid="bib69">Xiao et al., 2014</xref>). As obtaining a guaranteed minimum was impractical, we adopted a variant of the odds algorithm (<xref ref-type="bibr" rid="bib9">Bruss, 2000</xref>) as our stopping rule. Specifically, we created 10,000 sets of 30 categories and measured the correlations between functional, object, and DNN RDMs (distance metric: Spearman’s <italic>rho</italic>), noting the minimal value from the set. We persisted in this procedure until we observed a set with lower inter-feature correlations than was observed in the initial 10,000. From each of the final selected scene categories, eight exemplars were randomly selected and divided across two separate stimulus sets of 4 exemplars per scene category. Stimulus sets were assigned randomly to individual participants (Experiment 1: stimulus set 1, n = 10; stimulus set 2, n = 10; Experiment 2, stimulus set 1, n = 5; stimulus set 2, n = 3). Participants from Experiment 2 that had also participated in Experiment 1 were presented with the other stimulus set than the one they saw in Experiment 1.</p></sec><sec id="s4-4"><title>fMRI procedure</title><p>Participants were scanned while viewing the stimuli on a back-projected screen through a rear-view mirror that was mounted on the head coil. Stimuli were presented at a resolution of 800 × 600 pixels such that they subtended ~10×10 degrees of visual angle. Individual scenes were presented in an event-related design for a duration of 500 ms, separated by a 6 s interval. Throughout the experimental run, a small fixation cross (&lt;0.5 degrees) was presented in the center of the screen.</p><p>In Experiment 1, participants performed a task on the central fixation cross that was unrelated to the scenes. Specifically, simultaneous with the presentation of each scene, either the vertical or horizontal arm of the fixation cross became slightly elongated and participants indicated which arm was longer by pressing one of two buttons indicated on a hand-held button box. Both arms changed equally often within a given run and arm changes were randomly assigned to individual scenes. In Experiment 2, the fixation cross had a constant size, and participants were instructed to covertly name the scene whilst simultaneously pressing one button on the button box. To assure that participants in Experiment 2 were able to generate a name for each scene, they were first familiarized with the stimuli. Specifically, prior to scanning, participants were presented with all scenes in the set in randomized order on a laptop in the console room. Using a self-paced procedure, each scene was presented in isolation on the screen accompanied by the question ‘How would you name this scene?’. The participants were asked to type one or two words to describe the scene; as they typed, their answer appeared under the question, and they were able to correct mistakes using backspace. After typing the self-generated name, participants hit enter and the next scene would appear until all 120 scenes had been seen by the participant. This procedure took about ~10 min.</p><p>In both Experiment 1 and 2, participants completed eight experimental runs of 6.4 min each (192 TRs per run); one participant from Experiment 1 only completed seven runs due to time constraints. Each run started and ended with a 12 s fixation period. Each run contained two exemplar presentations per scene category. Individual exemplars were balanced across runs such that all stimuli were presented after two consecutive runs, yielding four presentations per exemplar in total. Exemplars were randomized across participants such that each participant always saw the same two exemplars within an individual run; however the particular combination was determined anew for each individual participant and scene category. Stimulus order was randomized independently for each run. Stimuli were presented using PsychoPy v1.83.01 (<xref ref-type="bibr" rid="bib53">Peirce, 2007</xref>).</p></sec><sec id="s4-5"><title>Functional localizers</title><p>Participants additionally completed four independent functional block-design runs (6.9 min, 208 TRs) that were used to localize scene-selective regions of interest (ROIs). Per block, twenty gray-scale images (300 × 300 pixels) were presented from one of eight different categories: faces, man-made and natural objects, buildings, and four different scene types (man-made open, man-made closed, natural open, natural closed; <xref ref-type="bibr" rid="bib35">Kravitz et al., 2011</xref>) while participants performed a one-back repetition-detection task. Stimuli were presented on a gray background for 500 ms duration, separated by 300 ms gaps, for blocks of 16 s duration, separated by 8 s fixation periods. Categories were counterbalanced both within runs (such that each category occurred twice within a run in a mirror-balanced sequence) and across runs (such that each category was equidistantly spaced in time relative to each other category across all four runs). Two localizer runs were presented after the first four experimental runs and two after the eight experimental runs were completed but prior to the T1 acquisition. For four participants, only two localizer runs were collected due to time constraints.</p></sec><sec id="s4-6"><title>Behavioral experiment</title><p>On a separate day following the MRI data acquisition, participants performed a behavioral multi-arrangement experiment. In a behavioral testing room, participants were seated in front of a desktop computer with a Dell U3014 monitor (30 inches, 2560 x 1600 pixels) on which all 120 stimuli that the participant had previously seen in the scanner were displayed as thumbnails around a white circular arena. A mouse-click on an individual thumbnail displayed a larger version of that stimulus in the upper right corner. Participants were instructed to arrange the thumbnails within the white circle in such a way that the arrangement would reflect ‘how similar the scenes are, whatever that means to you’, by means of dragging and dropping the individual exemplar thumbnails. We purposely avoided providing specific instructions in order to not bias participants towards using either functions, objects or DNN features to determine scene similarity. Participants were instructed to perform the task at their own pace; if the task took longer than 1 hr, they were encouraged to finish the experiment (almost all participants took less time, averaging a total experiment duration of ~45 mins). Stimuli were presented using the MATLAB code provided in (<xref ref-type="bibr" rid="bib37">Kriegeskorte and Mur, 2012</xref>). To obtain insight in the sorting strategies used by participants, they were asked (after completing the experiment) to take a few minutes to describe how they organized the scenes, using a blank sheet of paper and a pen, using words, bullet-points or drawings.</p></sec><sec id="s4-7"><title>Behavioral data analysis</title><p>Behavioral representational dissimilarity matrices (RDMs) were constructed for each individual participant by computing the pairwise squared on-screen distances between the arranged thumbnails and averaging the obtained distances across the exemplars within each category. The relatedness of the models and the behavioral data was determined in the same manner as for the fMRI analysis, that is by computing both individual model correlations and unique and shared variance across models via hierarchical regression (see below).</p></sec><sec id="s4-8"><title>fMRI preprocessing</title><p>Data were analyzed using AFNI software (<ext-link ext-link-type="uri" xlink:href="https://afni.nimh.nih.gov">https://afni.nimh.nih.gov</ext-link>). Before statistical analysis, the functional scans were slice-time corrected and all the images for each participant were motion corrected to the first image of the first functional run after removal of the first and last six TRs from each run. After motion correction, the localizer runs were smoothed with a 5 mm full-width at half-maximum Gaussian kernel; the event-related data was not smoothed.</p></sec><sec id="s4-9"><title>fMRI statistical analysis: localizers</title><p>Bilateral ROIs were created for each individual participant based on the localizer runs by conducting a standard general linear model implemented in AFNI. A response model was built by convolving a standard gamma function with a 16 s square wave for each condition and compared against the activation time courses using Generalized Least Squares (GLSQ) regression. Motion parameters and four polynomials accounting for slow drifts were included as regressors of no interest. To derive the response magnitude per category, t-tests were performed between the category-specific beta estimates and baseline. Scene-selective ROIs were generated by thresholding the statistical parametric maps resulting from contrasting scenes &gt; faces at p&lt;0.0001 (uncorrected). Only contiguous clusters of voxels (&gt;25) exceeding this threshold were then inspected to define scene-selective ROIs consistent with previously published work (<xref ref-type="bibr" rid="bib17">Epstein, 2005</xref>). For participants in which clusters could not be disambiguated, the threshold was raised until individual clusters were clearly identifiable. While PPA and OPA were identified in all participants for both Experiment 1 and 2, MPA/RSC was detected in only 14 out 20 participants in Experiment 1, and all analyses for this ROI in Experiment 1 are thus based on this subset of participants.</p></sec><sec id="s4-10"><title>fMRI statistical analysis: event-related data</title><p>Each event-related run was deconvolved independently using the standard GLSQ regression model in AFNI. The regression model included a separate regressor for each of the 30 scene categories as well as motion parameters and four polynomials to account for slow drifts in the signal. The resulting beta-estimates were then used to compute representational dissimilarity matrices (RDMs; (<xref ref-type="bibr" rid="bib36">Kriegeskorte et al., 2008</xref>) based on the multi-voxel patterns extracted from individual ROIs. Specifically, we computed pairwise cross-validated Mahalanobis distances between each of the scene 30 categories following the approach outlined in (<xref ref-type="bibr" rid="bib65">Walther et al., 2016</xref>). First, multi-variate noise normalization was applied by normalizing the beta-estimates by the covariance matrix of the residual time-courses between voxels within the ROI. Covariance matrices were regularized using shrinkage toward the diagonal matrix (<xref ref-type="bibr" rid="bib39">Ledoit and Wolf, 2004</xref>). Unlike univariate noise normalization, which normalizes each voxel’s response by its own error term, multivariate noise normalization also takes into account the noise covariance between voxels, resulting in more reliable RDMs (<xref ref-type="bibr" rid="bib65">Walther et al., 2016</xref>). After noise normalization, squared Euclidean distances were computed between individual runs using a leave-one-run-out procedure, resulting in cross-validated Mahalanobis distance estimates. Note that unlike correlation distance measures, cross-validated distances provide unbiased estimates of pattern dissimilarity on a ratio scale (<xref ref-type="bibr" rid="bib65">Walther et al., 2016</xref>), thus providing a distance measure suitable for direct model comparisons.</p></sec><sec id="s4-11"><title>Model comparisons: individual models</title><p>To test the relatedness of the three models of scene dissimilarity with the measured fMRI dissimilarity, the off-diagonal elements of each model RDM were correlated (Pearson’s <italic>r</italic>) with the off-diagonal elements of the RDMs of each individual participant's fMRI ROIs. Following (<xref ref-type="bibr" rid="bib48">Nili et al., 2014</xref>), the significance of these correlations was determined using one-sided signed-rank tests against zero, while pairwise differences between models in terms of their correlation with fMRI dissimilarity were determined using two-sided signed-ranked tests. For each test, we report the sum of signed ranks for the number of observations W(<italic>n</italic>) and the corresponding p-value; for tests with n &gt; 10 we also report the z-ratio approximation. The results were corrected for multiple comparisons (across both individual model correlations and pairwise comparisons) using FDR correction (<xref ref-type="bibr" rid="bib5">Benjamini and Hochberg, 1995</xref>) for each individual ROI separately. Noise ceilings were computed following (<xref ref-type="bibr" rid="bib48">Nili et al., 2014</xref>): an upper bound was estimated by computing the correlation between each participant’s individual RDM and the group-average RDM, while a lower bound was estimated by correlating each participant’s RDM with the average RDM of the other participants (leave-one-out approach). The participant-averaged RDM was converted to rank order for visualization purposes only.</p></sec><sec id="s4-12"><title>Model comparisons: partial correlations and variance partitioning</title><p>To determine the contribution of each individual model when considered in conjunction with the other models, we performed two additional analyses: partial correlations, in which each model was correlated (Pearsons <italic>r</italic>) while partialling out the other two models, and variance partitioning based on multiple linear regression. For the latter, the off-diagonal elements of each ROI RDM were assigned as the dependent variable, while the off-diagonal elements of the three model RDMs were entered as independent variables (predictors). To obtain unique and shared variance across the three models, seven multiple regression analyses were run in total: one ‘full’ regression that included all three feature models as predictors; and six reduced models that included as predictors either combinations of two models in pairs (e.g., functions and objects), or including each model by itself. By comparing the explained variance (<italic>r<sup>2</sup></italic>) of a model used alone to the <italic>r<sup>2</sup></italic> of that model in conjunction with another model, we can infer the amount of variance that is independently explained by that model, that is partition the variance (see <xref ref-type="bibr" rid="bib23">Groen et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Ramakrishnan et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Lescroart et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Çukur et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">Greene et al., 2016</xref>; <xref ref-type="bibr" rid="bib30">Hebart et al., 2018</xref> for similar approaches).</p><p>Analogous to the individual model correlation analyses, partial correlations were calculated for each individual participant separately, and significance was determined using one-sided signed-rank tests across participants (FDR-corrected across all comparisons within a given ROI). To allow comparison with the results reported in (<xref ref-type="bibr" rid="bib22">Greene et al., 2016</xref>), variance partitioning was performed on the participant-average RDMs. Similar results were found, however, when variance was partitioned for individual participant’s RDMs and then averaged across participants. To visualize this information in an Euler diagram, we used the EulerAPE software (<xref ref-type="bibr" rid="bib46">Micallef and Rodgers, 2014</xref>).</p></sec><sec id="s4-13"><title>Variance partitioning of fMRI based on models and behavior</title><p>Using the same approach as in the previous section, a second set of regression analyses was performed to determine the degree of shared variance between the behavioral categorization on the one hand, and the functions and DNN features on the other, in terms of the fMRI response pattern dissimilarity. The Euler diagrams were derived using the group-average RDMs, taking the average result of the multi-arrangement task of these participants as the behavioral input into the analysis.</p></sec><sec id="s4-14"><title>Direct reproducibility test of representational structure in behavior and fMRI</title><p>To assess how well the obtained RDMs were reproducible within each measurement domain (behavior and fMRI), we compared the average RDMs obtained for the two separate stimulus sets. Since these two sets of stimuli were viewed by different participants (see above under ‘Stimuli and models’), this comparison provides a strong test of generalizability, across both scene exemplars and across participant pools. Set-average RDMs were compared by computing inter-RDM correlations (Pearson’s <italic>r</italic>) and 96% confidence intervals (CI) and statistically tested for reproducibility using a random permutation test based on 10.000 randomizations of the category labels.</p></sec><sec id="s4-15"><title>DNN comparisons</title><p>The original, <italic>a priori</italic> fc7 DNN feature model was determined based on the large set of exemplars (average of 65 exemplars per scene category) used in <xref ref-type="bibr" rid="bib22">Greene et al. (2016)</xref>. To investigate the influence of DNN layer and training images on the learned visual features and their correspondence with activity in scene-selective cortex, we derived two new sets of RDMs by passing our scene stimuli through two pre-trained, 8-layer AlexNet (<xref ref-type="bibr" rid="bib38">Krizhevsky et al., 2017</xref>) architecture networks: (1) a 1000-object label ImageNet-trained (<xref ref-type="bibr" rid="bib13">Deng et al., 2009</xref>) network implemented in Caffe (<xref ref-type="bibr" rid="bib32">Jia et al., 2014</xref>) (‘ReferenceNet’) and (2) a 250-scene label Places-trained network (‘Places’) (<xref ref-type="bibr" rid="bib71">Zhou et al., 2014</xref>). By extracting the node activations from each layer, we computed pairwise dissimilarity (1 – Pearson’s <italic>r</italic>) resulting in one RDM per layer and per model. These RDMs were then each correlated with the fMRI RDMs from each participant in PPA, OPA and MPA (Pearson’s <italic>r</italic>). These analyses were performed on the combined data of Experiment 1 and 2; RDMs for participants that participated in both Experiments (n = 4) were averaged prior to group-level analyses.</p></sec><sec id="s4-16"><title>Searchlight analyses</title><p>To test the relatedness of functions, objects and visual feature models with fMRI activity recorded outside scene-selective ROIs, we conducted whole-brain searchlight analyses. RDMs were computed in the same manner as for the ROI analysis, that is by computing cross-validated Mahalanobis distances based on multivariate noise-normalized multi-voxel patterns, but now within spherical ROIs of 3 voxel diameter (i.e. 123 voxels/searchlight). Analogous to the ROI analyses, we computed partial correlations of each feature model, correcting for the contributions of the remaining two models. These partial correlation coefficients were assigned to the center voxel of each searchlight, resulting in one whole-volume map per model. Partial correlation maps were computed for each participant separately in their native volume space. To allow comparison at the group level, individual participant maps were first aligned to their own high-resolution anatomical scan and then to surface reconstructions of the grey and white matter boundaries created from these high-resolution scans using the Freesurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>) 5.3 autorecon script using SUMA (Surface Mapping with AFNI) software (<ext-link ext-link-type="uri" xlink:href="https://afni.nimh.nih.gov/Suma">https://afni.nimh.nih.gov/Suma</ext-link>). The surface images for each participant were then smoothed with a Gaussian 10 mm FWHM filter in surface coordinate units using the SurfSmooth function with the HEAT_07 smoothing method.</p><p>Group-level significance was determined by submitting these surface maps to node-wise one-sample t-tests in conjunction with Threshold Free Cluster Enhancement (<xref ref-type="bibr" rid="bib58">Smith and Nichols, 2009</xref>) through Monte Carlo simulations using the algorithm implemented in the CoSMoMVPA toolbox (<xref ref-type="bibr" rid="bib50">Oosterhof et al., 2016</xref>), which performs group-level comparisons using sign-based permutation testing (n = 10,000) to correct for multiple comparisons. To increase power, the data of Experiment 1 and 2 were combined; coefficient maps for participants that participated in both Experiments (n = 4) were averaged prior to proceeding to group-level analyses.</p><p>For searchlight comparisons with scene categorization behavior and feature models based on different DNN layers, we computed regular correlations (Pearson’s <italic>r)</italic> rather than partial correlations. For the behavioral searchlight, we used the average multi-arrangement behavior from Experiment 1 (since the participants from Experiment 2 did not perform this task). For the DNN searchlights, we used the same layer-by-layer RDMs as for the ROI analyses, independently correlating those with the RDMs of each spherical ROI. Group-level significance was determined in the same manner as for the <italic>a priori</italic> selected feature models (see above).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Intramural Research Program (ZIAMH002909) of the National Institutes of Health – National Institute of Mental Health Clinical Study Protocol 93 M-0170, NCT00001360. IIAG was also supported by a Rubicon Fellowship from the Netherlands Organization for Scientific Research (NWO). LF and DMB were funded by the Office of Naval Research Multidisciplinary University Research Initiative Grant N000141410671.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Software, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Supervision, Funding acquisition, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants had normal or corrected-to-normal vision and gave written informed consent as part of the study protocol (93 M-0170, NCT00001360) prior to participation in the study. The study was approved by the Institutional Review Board of the National Institutes of Health and was conducted according to the Declaration of Helsinki.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.32962.013</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-32962-transrepform-v2.docx"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguirre</surname> <given-names>GK</given-names></name><name><surname>Zarahn</surname> <given-names>E</given-names></name><name><surname>D'Esposito</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>An area within human ventral cortex sensitive to &quot;building&quot; stimuli: evidence and implications</article-title><source>Neuron</source><volume>21</volume><fpage>373</fpage><lpage>383</lpage><pub-id pub-id-type="pmid">9728918</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname> <given-names>C</given-names></name><name><surname>Esteva</surname> <given-names>A</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Two distinct scene-processing networks connecting vision and memory</article-title><source>eNeuro</source><volume>3</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1523/ENEURO.0178-16.2016</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Aminoff</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cortical analysis of visual context</article-title><source>Neuron</source><volume>38</volume><fpage>347</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00167-3</pub-id><pub-id pub-id-type="pmid">12718867</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bau</surname> <given-names>D</given-names></name><name><surname>Zhou</surname> <given-names>B</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Network dissection: quantifying interpretability of deep visual representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.05796">https://arxiv.org/abs/1704.05796</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Recognition-by-components: a theory of human image understanding</article-title><source>Psychological Review</source><volume>94</volume><fpage>115</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id><pub-id pub-id-type="pmid">3575582</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonner</surname> <given-names>MF</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Coding of navigational affordances in the human visual system</article-title><source>PNAS</source><volume>114</volume><fpage>4793</fpage><lpage>4798</lpage><pub-id pub-id-type="doi">10.1073/pnas.1618228114</pub-id><pub-id pub-id-type="pmid">28416669</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Daniels</surname> <given-names>N</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task context overrules object- and category-related representational content in the human parietal cortex</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>310</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw419</pub-id><pub-id pub-id-type="pmid">28108492</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruss</surname> <given-names>FT</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Sum the odds to one and stop</article-title><source>The Annals of Probability</source><volume>28</volume><fpage>1384</fpage><lpage>1391</lpage><pub-id pub-id-type="doi">10.1214/aop/1019160340</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bugatus</surname> <given-names>L</given-names></name><name><surname>Weiner</surname> <given-names>KS</given-names></name><name><surname>Grill-Spector</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task alters category representations in prefrontal but not high-level visual cortex</article-title><source>NeuroImage</source><volume>155</volume><fpage>437</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.062</pub-id><pub-id pub-id-type="pmid">28389381</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname> <given-names>CF</given-names></name><name><surname>Hong</surname> <given-names>H</given-names></name><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>Pinto</surname> <given-names>N</given-names></name><name><surname>Ardila</surname> <given-names>D</given-names></name><name><surname>Solomon</surname> <given-names>EA</given-names></name><name><surname>Majaj</surname> <given-names>NJ</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003963</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id><pub-id pub-id-type="pmid">25521294</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Khosla</surname> <given-names>A</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><fpage>1</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname> <given-names>J</given-names></name><name><surname>Dong</surname> <given-names>W</given-names></name><name><surname>Socher</surname> <given-names>R</given-names></name><name><surname>Li L-J</surname> <given-names>LK</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>ImageNet: A large-scale hierarchical image database</article-title><conf-name>2009 IEEE Conf Comput Vis Pattern Recognit</conf-name><fpage>248</fpage><lpage>255</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dilks</surname> <given-names>DD</given-names></name><name><surname>Julian</surname> <given-names>JB</given-names></name><name><surname>Paunov</surname> <given-names>AM</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The occipital place area is causally and selectively involved in scene perception</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>1331</fpage><lpage>1336</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4081-12.2013</pub-id><pub-id pub-id-type="pmid">23345209</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Jiang</surname> <given-names>Y</given-names></name><name><surname>Shuman</surname> <given-names>M</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A cortical area selective for visual processing of the human body</article-title><source>Science</source><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1126/science.1063414</pub-id><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>R</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><volume>392</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The cortical basis of visual scene processing</article-title><source>Visual Cognition</source><volume>12</volume><fpage>954</fpage><lpage>978</lpage><pub-id pub-id-type="doi">10.1080/13506280444000607</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>RA</given-names></name><name><surname>Parker</surname> <given-names>WE</given-names></name><name><surname>Feiler</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Where am I now? Distinct roles for parahippocampal and retrosplenial cortices in place recognition</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>6141</fpage><lpage>6149</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0799-07.2007</pub-id><pub-id pub-id-type="pmid">17553986</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Neural systems for visual scene recognition</chapter-title><person-group person-group-type="editor"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Kveraga</surname> <given-names>K</given-names></name></person-group><source>Scene Vision</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>105</fpage><lpage>134</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erez</surname> <given-names>Y</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Discrimination of visual categories based on behavioral relevance in widespread regions of frontoparietal cortex</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>12383</fpage><lpage>12393</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1134-15.2015</pub-id><pub-id pub-id-type="pmid">26354907</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Garcia-Garcia</surname> <given-names>A</given-names></name><name><surname>Orts-Escolano</surname> <given-names>S</given-names></name><name><surname>Oprea</surname> <given-names>S</given-names></name><name><surname>Villena-Martinez</surname> <given-names>V</given-names></name><name><surname>Garcia-Rodriguez</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A review on deep learning techniques applied to semantic segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1704.06857">http://arxiv.org/abs/1704.06857</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greene</surname> <given-names>MR</given-names></name><name><surname>Baldassano</surname> <given-names>C</given-names></name><name><surname>Esteva</surname> <given-names>A</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual scenes are categorized by function</article-title><source>Journal of Experimental Psychology: General</source><volume>145</volume><fpage>82</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1037/xge0000129</pub-id><pub-id pub-id-type="pmid">26709590</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname> <given-names>II</given-names></name><name><surname>Ghebreab</surname> <given-names>S</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spatially pooled contrast responses predict neural and perceptual similarity of naturalistic image categories</article-title><source>PLoS Computational Biology</source><volume>8</volume><elocation-id>e1002726</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002726</pub-id><pub-id pub-id-type="pmid">23093921</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname> <given-names>IIA</given-names></name><name><surname>Silson</surname> <given-names>EH</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contributions of low- and high-level properties to neural processing of visual scenes in the human brain</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><fpage>20160102</fpage><lpage>20160111</lpage><pub-id pub-id-type="doi">10.1098/rstb.2016.0102</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gu</surname> <given-names>C</given-names></name><name><surname>Sun</surname> <given-names>C</given-names></name><name><surname>Ross</surname> <given-names>DA</given-names></name><name><surname>Vondrick</surname> <given-names>C</given-names></name><name><surname>Pantofaru</surname> <given-names>C</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Vijayanarasimhan</surname> <given-names>S</given-names></name><name><surname>Toderici</surname> <given-names>G</given-names></name><name><surname>Ricco</surname> <given-names>S</given-names></name><name><surname>Sukthankar</surname> <given-names>R</given-names></name><name><surname>Schmid</surname> <given-names>C</given-names></name><name><surname>Malik</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>AVA: a video dataset of spatio-temporally localized atomic visual actions</article-title><source>bioArchiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1705.08421">http://arxiv.org/abs/1705.08421</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname> <given-names>U</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafri</surname> <given-names>A</given-names></name><name><surname>Trueswell</surname> <given-names>JC</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural representations of observed actions generalize across static and dynamic visual input</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>3056</fpage><lpage>3071</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2496-16.2017</pub-id><pub-id pub-id-type="pmid">28209734</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harel</surname> <given-names>A</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Task context impacts visual object processing differentially across the cortex</article-title><source>PNAS</source><volume>111</volume><fpage>E962</fpage><lpage>E971</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312567111</pub-id><pub-id pub-id-type="pmid">24567402</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>Levy</surname> <given-names>I</given-names></name><name><surname>Behrmann</surname> <given-names>M</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Eccentricity bias as an organizing principle for human high-order object areas</article-title><source>Neuron</source><volume>34</volume><fpage>479</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00662-1</pub-id><pub-id pub-id-type="pmid">11988177</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Bankson</surname> <given-names>BB</given-names></name><name><surname>Harel</surname> <given-names>A</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The representational dynamics of task and object processing in humans</article-title><source>eLife</source><volume>7</volume><elocation-id>e32816</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id><pub-id pub-id-type="pmid">29384473</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horikawa</surname> <given-names>T</given-names></name><name><surname>Kamitani</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15037</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15037</pub-id><pub-id pub-id-type="pmid">28530228</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jia</surname> <given-names>Y</given-names></name><name><surname>Shelhamer</surname> <given-names>E</given-names></name><name><surname>Donahue</surname> <given-names>J</given-names></name><name><surname>Karayev</surname> <given-names>S</given-names></name><name><surname>Long</surname> <given-names>J</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name><name><surname>Guadarrama</surname> <given-names>S</given-names></name><name><surname>Darrell</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Caffe: convolutional architecture for fast feature embedding</article-title><conf-name>Proceedings of the 22Nd ACM International Conference on Multimedia</conf-name><fpage>675</fpage><lpage>678</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname> <given-names>N</given-names></name><name><surname>McDermott</surname> <given-names>J</given-names></name><name><surname>Chun</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Peng</surname> <given-names>CS</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Real-world scene representations in high-level visual cortex: it's the spaces more than the places</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>7322</fpage><lpage>7333</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4588-10.2011</pub-id><pub-id pub-id-type="pmid">21593316</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Inverse MDS: inferring dissimilarity structure from multiple item arrangements</article-title><source>Frontiers in Psychology</source><volume>3</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00245</pub-id><pub-id pub-id-type="pmid">22848204</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><source>Communications of the ACM</source><volume>60</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledoit</surname> <given-names>O</given-names></name><name><surname>Wolf</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Honey, i shrunk the sample covariance matrix</article-title><source>The Journal of Portfolio Management</source><volume>30</volume><fpage>110</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.3905/jpm.2004.110</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lescroart</surname> <given-names>MD</given-names></name><name><surname>Stansbury</surname> <given-names>DE</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fourier power, subjective distance, and object categories all provide plausible models of BOLD responses in scene-selective visual areas</article-title><source>Frontiers in Computational Neuroscience</source><volume>9</volume><elocation-id>135</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2015.00135</pub-id><pub-id pub-id-type="pmid">26594164</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lingnau</surname> <given-names>A</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The lateral occipitotemporal cortex in action</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>268</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.03.006</pub-id><pub-id pub-id-type="pmid">25843544</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname> <given-names>MX</given-names></name><name><surname>Gallivan</surname> <given-names>JP</given-names></name><name><surname>Ferber</surname> <given-names>S</given-names></name><name><surname>Cant</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Feature diagnosticity and task context shape activity in human scene-selective cortex</article-title><source>NeuroImage</source><volume>125</volume><fpage>681</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.10.089</pub-id><pub-id pub-id-type="pmid">26541082</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malcolm</surname> <given-names>GL</given-names></name><name><surname>Groen</surname> <given-names>IIA</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Making sense of real-world scenes</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>843</fpage><lpage>856</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.09.003</pub-id><pub-id pub-id-type="pmid">27769727</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marchette</surname> <given-names>SA</given-names></name><name><surname>Vass</surname> <given-names>LK</given-names></name><name><surname>Ryan</surname> <given-names>J</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Anchoring the neural compass: coding of local spatial reference frames in human medial parietal lobe</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1598</fpage><lpage>1606</lpage><pub-id pub-id-type="doi">10.1038/nn.3834</pub-id><pub-id pub-id-type="pmid">25282616</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>A</given-names></name><name><surname>Wiggs</surname> <given-names>CL</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neural correlates of category-specific knowledge</article-title><source>Nature</source><volume>379</volume><fpage>649</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1038/379649a0</pub-id><pub-id pub-id-type="pmid">8628399</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Micallef</surname> <given-names>L</given-names></name><name><surname>Rodgers</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>eulerAPE: drawing area-proportional 3-Venn diagrams using ellipses</article-title><source>PLoS One</source><volume>9</volume><fpage>e101717</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0101717</pub-id><pub-id pub-id-type="pmid">25032825</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Monfort</surname> <given-names>M</given-names></name><name><surname>Zhou</surname> <given-names>B</given-names></name><name><surname>Bargal</surname> <given-names>SA</given-names></name><name><surname>Andonian</surname> <given-names>A</given-names></name><name><surname>Yan</surname> <given-names>T</given-names></name><name><surname>Ramakrishnan</surname> <given-names>K</given-names></name><name><surname>Brown</surname> <given-names>L</given-names></name><name><surname>Fan</surname> <given-names>Q</given-names></name><name><surname>Gutfruend</surname> <given-names>D</given-names></name><name><surname>Vondrick</surname> <given-names>C</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Moments in time dataset: one million videos for event understanding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1801.03150">http://arxiv.org/abs/1801.03150</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Wingfield</surname> <given-names>C</given-names></name><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Su</surname> <given-names>L</given-names></name><name><surname>Marslen-Wilson</surname> <given-names>W</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id><pub-id pub-id-type="pmid">24743308</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modeling the shape of the scene: A holistic representation of the spatial envelope</article-title><source>International Journal of Computer Vision</source><volume>42</volume><fpage>145</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1023/A:1011139631724</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CoSMoMVPA: multi-modal multivariate pattern analysis of neuroimaging data in matlab/GNU octave</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id><pub-id pub-id-type="pmid">27499741</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>S</given-names></name><name><surname>Brady</surname> <given-names>TF</given-names></name><name><surname>Greene</surname> <given-names>MR</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Disentangling scene content from spatial boundary: complementary roles for the parahippocampal place area and lateral occipital complex in representing real-world scenes</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1333</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3885-10.2011</pub-id><pub-id pub-id-type="pmid">21273418</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of visual body perception</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>636</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1038/nrn2195</pub-id><pub-id pub-id-type="pmid">17643089</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>PsychoPy--Psychophysics software in Python</article-title><source>Journal of Neuroscience Methods</source><volume>162</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id><pub-id pub-id-type="pmid">17254636</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajimehr</surname> <given-names>R</given-names></name><name><surname>Devaney</surname> <given-names>KJ</given-names></name><name><surname>Bilenko</surname> <given-names>NY</given-names></name><name><surname>Young</surname> <given-names>JC</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The &quot;parahippocampal place area&quot; responds preferentially to high spatial frequencies in humans and monkeys</article-title><source>PLoS Biology</source><volume>9</volume><elocation-id>e1000608</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000608</pub-id><pub-id pub-id-type="pmid">21483719</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramakrishnan</surname> <given-names>K</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Groen</surname> <given-names>II</given-names></name><name><surname>Smeulders</surname> <given-names>AW</given-names></name><name><surname>Ghebreab</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual dictionaries as intermediate features in the human brain</article-title><source>Frontiers in computational neuroscience</source><volume>8</volume><elocation-id>168</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2014.00168</pub-id><pub-id pub-id-type="pmid">25642183</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sermanet</surname> <given-names>P</given-names></name><name><surname>Eigen</surname> <given-names>D</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Mathieu</surname> <given-names>M</given-names></name><name><surname>Fergus</surname> <given-names>R</given-names></name><name><surname>LeCun</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>OverFeat: integrated recognition, localization and detection using convolutional networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6229">https://arxiv.org/abs/1312.6229</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silson</surname> <given-names>EH</given-names></name><name><surname>Steel</surname> <given-names>AD</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Scene-selectivity and retinotopy in medial parietal cortex</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2016.00412</pub-id><pub-id pub-id-type="pmid">27588001</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Nichols</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname> <given-names>RB</given-names></name><name><surname>Reppas</surname> <given-names>JB</given-names></name><name><surname>Kwong</surname> <given-names>KK</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Born</surname> <given-names>RT</given-names></name><name><surname>Brady</surname> <given-names>TJ</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Belliveau</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Functional analysis of human MT and related visual cortical areas using magnetic resonance imaging</article-title><source>Journal of Neuroscience</source><volume>15</volume><fpage>3215</fpage><lpage>3230</lpage><pub-id pub-id-type="pmid">7722658</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Statistics of natural image categories</article-title><source>Network: Computation in Neural Systems</source><volume>14</volume><fpage>391</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_14_3_302</pub-id><pub-id pub-id-type="pmid">12938764</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troiani</surname> <given-names>V</given-names></name><name><surname>Stigliani</surname> <given-names>A</given-names></name><name><surname>Smith</surname> <given-names>ME</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multiple object properties drive scene-selective regions</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>883</fpage><lpage>897</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs364</pub-id><pub-id pub-id-type="pmid">23211209</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Auerbach</surname> <given-names>EJ</given-names></name><name><surname>Olman</surname> <given-names>C</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Uğurbil</surname> <given-names>K</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>T1 weighted brain images at 7 Tesla unbiased for Proton Density, T2* contrast and RF coil receive B1 sensitivity with simultaneous vessel visualization</article-title><source>NeuroImage</source><volume>46</volume><fpage>432</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.009</pub-id><pub-id pub-id-type="pmid">19233292</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Turennout</surname> <given-names>M</given-names></name><name><surname>Bielamowicz</surname> <given-names>L</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Modulation of neural activity during object naming: effects of time and practice</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>381</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1093/cercor/13.4.381</pub-id><pub-id pub-id-type="pmid">12631567</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Turennout</surname> <given-names>M</given-names></name><name><surname>Ellmore</surname> <given-names>T</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Long-lasting cortical plasticity in the object naming system</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>1329</fpage><lpage>1334</lpage><pub-id pub-id-type="doi">10.1038/81873</pub-id><pub-id pub-id-type="pmid">11100155</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Ejaz</surname> <given-names>N</given-names></name><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id><pub-id pub-id-type="pmid">26707889</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname> <given-names>DB</given-names></name><name><surname>Caddigan</surname> <given-names>E</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name><name><surname>Beck</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Natural scene categories revealed in distributed patterns of activity in the human brain</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>10573</fpage><lpage>10581</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0559-09.2009</pub-id><pub-id pub-id-type="pmid">19710310</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname> <given-names>DM</given-names></name><name><surname>Andrews</surname> <given-names>TJ</given-names></name><name><surname>Hartley</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A data driven approach to understanding the organization of high-level visual cortex</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>3596</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-03974-5</pub-id><pub-id pub-id-type="pmid">28620238</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname> <given-names>H</given-names></name><name><surname>Shi</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Lu</surname> <given-names>KH</given-names></name><name><surname>Cao</surname> <given-names>J</given-names></name><name><surname>Liu</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural encoding and decoding with deep learning for dynamic natural vision</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx268</pub-id><pub-id pub-id-type="pmid">29059288</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname> <given-names>J</given-names></name><name><surname>Ehinger</surname> <given-names>KA</given-names></name><name><surname>Hays</surname> <given-names>J</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>SUN database: exploring a large collection of scene categories</article-title><source>International Journal of Computer Vision</source><volume>119</volume><fpage>3</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1007/s11263-014-0748-y</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeki</surname> <given-names>S</given-names></name><name><surname>Watson</surname> <given-names>JD</given-names></name><name><surname>Lueck</surname> <given-names>CJ</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Kennard</surname> <given-names>C</given-names></name><name><surname>Frackowiak</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>A direct demonstration of functional specialization in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>11</volume><fpage>641</fpage><lpage>649</lpage><pub-id pub-id-type="pmid">2002358</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>B</given-names></name><name><surname>Lapedriza</surname> <given-names>A</given-names></name><name><surname>Xiao</surname> <given-names>J</given-names></name><name><surname>Torralba</surname> <given-names>A</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning deep features for scene recognition using places database</article-title><source>Advances in Neural Information Processing Systems</source><volume>27</volume><fpage>487</fpage><lpage>495</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Çukur</surname> <given-names>T</given-names></name><name><surname>Huth</surname> <given-names>AG</given-names></name><name><surname>Nishimoto</surname> <given-names>S</given-names></name><name><surname>Gallant</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional subdomains within scene-selective cortex: parahippocampal place area, retrosplenial complex, and occipital place area</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>10257</fpage><lpage>10273</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4033-14.2016</pub-id><pub-id pub-id-type="pmid">27707964</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.32962.015</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Tsao</surname><given-names>Doris Y</given-names></name><role>Reviewing Editor</role><aff id="aff9"><institution>California Institute of Technology</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Distinct contributions of functional and deep neural network features to scene representation in brain and behavior&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The manuscript compares how three models of scene properties explain human behavior (scene similarity judgments) and fMRI activation patterns in scene-selective regions. Behavior can be explained by function/action labels and by features of a deep neural network (DNN). Brain activation patterns, on the other hand, were mainly explained by the DNN features, with little contribution from function/action features (or from the third model, based on object features). The authors emphasize the apparent distinction between behavior and brain activation patterns in their use of functional features.</p><p>Overall, this is a very clearly written, straightforward paper. The paper is informative and useful in understanding both the general organization of visual processing brain regions, and the relation between DNNs and human brains. The approach of pre-selecting images to minimize intrinsic correlations between the three feature spaces is clever. There's novelty in how they describe unique contributions of each models: in addition to a conventional RSA correlation, the paper reports results of a variance partitioning analysis along with Euler diagram visualization which allows readers to easily see independent and shared contribution of each models. Experiment 2 logically follows questions from Experiment 1, such as whether the discrepancy between behavioral and fMRI results comes from task differences.</p><p>Essential revisions:</p><p>1) My main concern was that the authors tend to over-generalize and overstate their results. The results will remain interesting when presented at face value, and the description will be much more accurate.</p><p>In an apparent effort to hype up the results (or maybe to keep the conclusions compatible with the previous related paper by Greene, 2016?), the authors insist that functional features explain behavior while DNN features explain brain activity. But in fact, DNN features explain a very large part of behavior as well. It would seem more accurate to conclude that both functional and DNN features explain behavior. As functional features do not contribute to brain activation patterns in scene regions (but DNN features do), there is still an interesting dissociation.</p><p>2) The authors talk about &quot;behavior&quot; as if the results could be generalized to any scene categorization behavior. But in practice, the functional (or DNN) features only explain a very specific type of behavior, a scene multi-arrangement similarity sorting task. This limitation is addressed a bit in the Discussion, but I would feel much more comfortable if the authors could systematically qualify their statements about behavior (in the title, Abstract, Results).</p><p>3) Similarly, DNN features are referred to as if that was a unique, unambiguous &quot;thing&quot;, but there are many types of DNN features depending on network architecture, training data, and hierarchical layer. The authors address the latter two factors to some extent, by reporting correlations with all 8 layers of AlexNet, and with another version of AlexNet trained on places rather than objects. But these are not the only two possibilities. In particular, it seems plausible that a deep network trained on functional (or action) labels for each scene (like the AVA dataset or the YouTube-8M dataset) could behave very differently, and possibly explain more of the behavioral data. In addition, there are many more (and more advanced) image categorization architectures than AlexNet (VGG, Inception, ResNet, etc.). Talking about DNNs &quot;in general&quot; based on data from AlexNet alone is like talking about &quot;mammals&quot; based on mouse data alone. I am not suggesting that the study should include other DNNs, but rather that the descriptions and conclusions should more accurately reflect the actual experimental manipulations.</p><p>4) A mild concern is that some of the analysis and interpretations seem too post-hoc. In other words, a theoretical motivation for choosing the three models is not explained enough, other than that these are the models used in a previous study from the same group. These models are chosen with an assumption that feature spaces that were relevant for a behavioral scene study should be relevant for brain responses, which seems natural but creates a disadvantage that each models are not explained/introduced enough to understand why these models may be relevant for behavior and neural representation of scenes. These models deserve more background in the introduction. Are there any alternative models that might contribute which is not tested in their previous study? Does the scene categorization task (similarity sorting) used in behavioral task relates to what the visual system might naturally do while perceiving real world scenes?</p><p>5) Overall conclusion and a theoretical take home point seem relatively weak. It is clear that there is a discrepancy between behavioral and neural representations of scenes. What does this tell us about neural representations in scene cortex? For example, a discussion about what exactly different layers of DNN that contributes to neural response is/might be representing should help. It is vaguely mentioned that mid and high level DNN layers play important roles, with specific names of DNN layer, e.g., fc7. However, this doesn't clarify what features are represented in these layers and how it corresponds to different processes involved in scene perception/recognition. Richer introduction and discussion about DNN should help readers understand the theoretical contribution of this work on what and how features are represented in scene perception.</p><p>6) What does the current data tell us about existing behavioral findings by Greene et al., 2017? Should the contribution of functional model be interpreted differently and specifically for the behavioral scene categorization task or the types of images (e.g., images with humans/actions) used in the previous research? If not, that should be discussed too. In other words, there needs to be bi-directional discussion between what current discrepancy results inform us about behavioral and neural findings about scene representation.</p><p>Minor points [abridged]:</p><p>[…] 7.Doesn't the absence of DNN feature space similarity with brain activity (outside of the two small ventral and lateral occipitotemporal clusters) in the searchlight analysis contradict previous findings by Khaligh-Razavi &amp; Kriegeskorte or Guclu &amp; van Gerven? Shouldn't that be discussed explicitly?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.32962.016</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) My main concern was that the authors tend to over-generalize and overstate their results. The results will remain interesting when presented at face value, and the description will be much more accurate.</p><p>In an apparent effort to hype up the results (or maybe to keep the conclusions compatible with the previous related paper by Greene, 2016?), the authors insist that functional features explain behavior while DNN features explain brain activity. But in fact, DNN features explain a very large part of behavior as well. It would seem more accurate to conclude that both functional and DNN features explain behavior. As functional features do not contribute to brain activation patterns in scene regions (but DNN features do), there is still an interesting dissociation.</p></disp-quote><p>Our findings indeed indicate that DNN features also contribute unique variance to behavior, and a similar result was reported in Greene et al., (2016). In the current manuscript, we highlight that the unique variance explained by the functional features is (numerically) larger than that explained by the DNN features (see <xref ref-type="fig" rid="fig2">Figure 2E</xref>), and that the functional feature space is also more often the best-fitting model in individual subjects (see <xref ref-type="fig" rid="fig2">Figure 2D</xref>), because we wanted to emphasize that this ‘ranking’ of the feature spaces constitutes a direct replication of the earlier study which used a much larger stimulus set and a different experimental task and subject population. Nevertheless, we did not intend to sweep the contribution of the DNNs under the rug. In fact, we think this finding constitutes an important validation of the increased popularity of the application of DNNs in neuroscience experiments. Therefore, we have amended our statements throughout the manuscript to better reflect the conclusion that behavior is explained by both functional features and DNNs (see Abstract, Introduction, Results and Materials and methods).</p><disp-quote content-type="editor-comment"><p>2) The authors talk about &quot;behavior&quot; as if the results could be generalized to any scene categorization behavior. But in practice, the functional (or DNN) features only explain a very specific type of behavior, a scene multi-arrangement similarity sorting task. This limitation is addressed a bit in the Discussion, but I would feel much more comfortable if the authors could systematically qualify their statements about behavior (in the title, Abstract, Results).</p></disp-quote><p>We agree with the reviewer that using ‘behavior’ as a blanket term is too broad. We have therefore amended our title, Abstract and Results and Discussion sections to more precisely indicate the type of behavior we are referring to, now using the term ‘multi-arrangement’, ‘similarity assessment’ or ‘similarity judgments’ for the behavioral data we present here (see title page and Abstract, Introduction, Results and Materials and methods). However, we do think it is important to emphasize that the behavioral similarity assessments obtained in this paper with the multi-arrangement task are consistent with the results obtained with an explicit same-different categorization task in Greene et al., (2016), and that those assessments also correlate with responses in scene-selective regions (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>/B). This suggests that the multi-arrangement task does capture elements of perhaps more general scene categorization behavior.</p><disp-quote content-type="editor-comment"><p>3) Similarly, DNN features are referred to as if that was a unique, unambiguous &quot;thing&quot;, but there are many types of DNN features depending on network architecture, training data, and hierarchical layer. The authors address the latter two factors to some extent, by reporting correlations with all 8 layers of AlexNet, and with another version of AlexNet trained on places rather than objects. But these are not the only two possibilities. In particular, it seems plausible that a deep network trained on functional (or action) labels for each scene (like the AVA dataset or the YouTube-8M dataset) could behave very differently, and possibly explain more of the behavioral data. In addition, there are many more (and more advanced) image categorization architectures than AlexNet (VGG, Inception, ResNet, etc.). Talking about DNNs &quot;in general&quot; based on data from AlexNet alone is like talking about &quot;mammals&quot; based on mouse data alone. I am not suggesting that the study should include other DNNs, but rather that the descriptions and conclusions should more accurately reflect the actual experimental manipulations.</p></disp-quote><p>We agree that a DNN trained on a different type of scene label may result in a different outcome. Our motivation to use the ImageNet-trained AlexNet as one of our three feature models was based on it being trained on one the largest database available at the time, meaning that its features could be expected to reflect a representative sampling of the visual world. Moreover, this version was trained on an object categorization task, which previous work has suggested correlates with brain activity (e.g. Khaligh-Razavi and Kriegeskorte, 2014). Thus, our objective was not to find the ‘best’ DNN to match our behavior, but to test how a feature space previously studied in the literature compared to other feature models that were previously also shown to predict behavioral scene categorization. However, since then indeed many other networks have been published with a wide range of architectures and training data, which is why we decided to include at least one alternate DNN (Places: trained on scene categorization) in the paper, albeit as a post-hoc analysis. In addition, we now explicitly mention the possibility that future DNNs will be able to capture more of the behavioral and fMRI variance, and highlight that this is an important avenue for future research: see new Discussion paragraph (fifth). Furthermore, we have removed the statement from the Abstract that DNNs represent only a subset of behaviorally relevant scene information, since we cannot claim that we have exhaustively tested all DNNs. Importantly, this is one of the reasons we make the behavioral and fMRI data publicly available along with the manuscript, in order to facilitate future comparisons of these data against other candidate models.</p><disp-quote content-type="editor-comment"><p>4) A mild concern is that some of the analysis and interpretations seem too post-hoc. In other words, a theoretical motivation for choosing the three models is not explained enough, other than that these are the models used in a previous study from the same group. These models are chosen with an assumption that feature spaces that were relevant for a behavioral scene study should be relevant for brain responses, which seems natural but creates a disadvantage that each models are not explained/introduced enough to understand why these models may be relevant for behavior and neural representation of scenes. These models deserve more background in the introduction. Are there any alternative models that might contribute which is not tested in their previous study? Does the scene categorization task (similarity sorting) used in behavioral task relates to what the visual system might naturally do while perceiving real world scenes?</p></disp-quote><p>Two important points are raised here. First, the reviewer asks for clarification on the selection of models. It is important to stress that the three feature spaces used here were not the only models tested in the previous behavioral study, but that these were the best performing models out of a large range of low-to-high level scene feature spaces, including other assessments of function-like scene features (i.e., an attribute model). In Greene et al., (2016) all of these models were compared with the largest-scale assessment of scene similarity to date, namely online behavioral categorization of the entire SUN database, and unique variance was assessed for several of the top-performing models. While it remains possible that there are alternative models that contribute to scene representation (including unknown future models), we reasoned that this extensive assessment of behavioral relevance of features was an appropriate ground for pre-selecting models for a neuroimaging experiment (in which we are necessarily limited in the number of scene exemplars we can show) in an informed way. An important (if not the most important) goal of neuroscience is to uncover how neural representations subserve behavior, but this aspect has not necessarily always been as prominent in previous studies on the scene representation, where stimulus selection was often based on intuitions about what the relevant scene dimensions are, not explicit behavioral assessments. For studies that did include scene categorization behavior, the behavioral ‘reality’ of the examined feature spaces has often been assessed after rather than prior to stimulus selection (e.g. Walther et al., 2009; Kravitz et al., 2011). Following the reviewer’s request, we have elaborated the description of the previous study in the Introduction (third paragraph), now providing more context for the types of models that were tested before, and we also added the explicit statement that our goal was to test these top behavioral models.</p><p>The second point concerns the degree to which the similarity assessments in our behavioral paradigm can be said to be representative of ‘real’ scene perception behavior. This is an intriguing question. A literal interpretation of this task would imply that we are constantly engaged in categorizing or making similarity judgments of the environments we are in. This seems unlikely: performing a repeated categorization task on a second-to-second basis (“This is an office. This is an office”) does not seem like plausible behavior in real-world settings. In a similar vein, judgments of pictures on a screen can be said to be impoverished way of assessing any type of ‘real’ visual behavior, and scene understanding actually constitutes a whole set of behaviors that supersede rapid categorization of images (Malcolm et al., 2016). On the other hand, a classic view in cognitive science does argue that ‘similarity’ is a fundamental way of organizing the world (Tversky, 1977; Edelman, 1998), i.e. ‘to recognize is to categorize’. Therefore, by using a scene similarity task, we are aligning ourselves with this long-standing framework which suggests that perception occurs within <italic>some</italic> similarity reference frame. In our study, we ask: what are the dimensions of the natural ‘similarity space’ that the visual system operates in? Which reference frame best matches the data? We chose the free arrangement task precisely because it allows for testing multiple reference frames: participants were allowed to judge scene similarity along any dimension they see fit, rather than being forced to use image-based properties only, as might for example be the case in a rapid one-back matching task. Using a free arrangement task to assess scene similarity thus opens up the possibility of getting closer to what the visual system ‘naturally does’ when perceiving real-world scenes.</p><disp-quote content-type="editor-comment"><p>5) Overall conclusion and a theoretical take home point seem relatively weak. It is clear that there is a discrepancy between behavioral and neural representations of scenes. What does this tell us about neural representations in scene cortex? For example, a discussion about what exactly different layers of DNN that contributes to neural response is/might be representing should help. It is vaguely mentioned that mid and high level DNN layers play important roles, with specific names of DNN layer, e.g., fc7. However, this doesn't clarify what features are represented in these layers and how it corresponds to different processes involved in scene perception/recognition. Richer introduction and discussion about DNN should help readers understand the theoretical contribution of this work on what and how features are represented in scene perception.</p></disp-quote><p>The question what exactly is represented in different layers of DNN models is an important one. In fact, this is an active area of research in both computer vision and computational neuroscience, where various methods are currently being developed to visualize and quantify the learned feature selectivity (e.g., Zhou et al., 2014; Güçlü and van Gerven, 2015; Bau et al., 2017; more relevant papers are cited on http://netdissect.csail.mit.edu). So far, these methods suggest that while earlier layers contain local filters that resemble V1-like receptive fields, higher layers develop selectivity for entire objects or object parts, perhaps resembling category-selective regions in visual cortex. We now clarify the role of different layers in the Results section that accompanies the layer-by-layer analyses in <xref ref-type="fig" rid="fig8">Figure 8</xref> (see subsection “Scene-selective cortex correlates with features in both mid- and high-level DNN layers”). However – not unlike our understanding of intermediate visual areas in the brain – the exact representations of features in mid-level DNN layers remains somewhat elusive (but see Bonner and Epstein (2017) for an interesting exploration of this in the context of scene representation). This ‘black-box’ characteristic is one of the reasons why the utility of DNNs for understanding neural representation is debated (Kay, 2017; Scholte, 2018; Tripp, 2018; Turner et al., 2018). We now include more discussion of DNNs in the new Discussion paragraph (fifth paragraph) where we also discuss the possibility of testing other DNNs, in response to suggestions under point 3 above.</p><p>While we think the nature of DNN representations is a relevant and interesting question, the main focus of our paper is on another problem, which applies to any model of scene representation including DNNs, namely that model features are likely correlated with many other model features (or layers), which makes it difficult to assign a unique role to a specific model (or layer). The general assumption in the field is that because DNNs achieve classification performance at human-like levels, the representations in the DNN layers must be relevant for human behavior. While reports of superior correlations between DNN representations and neural measurement relative to other models of image representation (Yamins et al., 2013; Cadieu et al., 2014; Khaligh-Razavi and Kriegeskorte, 2014) indeed seem to support the relevance of DNNs for ‘biological’ perception, we here tested this hypothesis carefully in the context of other known behaviorally relevant feature spaces, asking to what extent the DNN features <italic>uniquely</italic> contribute not only to brain responses but also to scene categorization behavior. Our results show that a significant correlation between DNN features and the brain does not imply that DNNs are able to capture all elements of scene categorization behavior. Thus, while the theoretical implications of our results are indeed not easily summarized in a single take home point, we think our study highlights important limitations of focusing only on the brain-DNN relationship without simultaneously taking into account human categorization behavior. At the same time, we provide a methodological approach (variance partitioning) that might help better tease which DNN features contribute unique variance between layers, for example.</p><disp-quote content-type="editor-comment"><p>6) What does the current data tell us about existing behavioral findings by Greene et al., 2017? Should the contribution of functional model be interpreted differently and specifically for the behavioral scene categorization task or the types of images (e.g., images with humans/actions) used in the previous research? If not, that should be discussed too. In other words, there needs to be bi-directional discussion between what current discrepancy results inform us about behavioral and neural findings about scene representation.</p></disp-quote><p>We do not think the current findings should lead to a different interpretation of the findings in Greene et al., (2016). As highlighted in our response to points 1 and 2 above, we replicated the main finding from that study with a much-reduced stimulus set, an explicit minimization of correlation with other feature spaces, a different participant population (online workers vs. lab participants), and a different task paradigm (side-by-side categorization vs. free arrangement). Based on our exploratory result of a searchlight correlation of the function feature space with parts of cortex that exhibit potential overlap with body- or motion-selective regions, we raised the possibility that some of the functional feature space dimensions correlate with the presence of acting humans or implied motion in a subset of the scene categories. However, this is less likely to be an explanation of the results in Greene et al., (2016) because the set of images used there was much larger and was also explicitly compared to many other feature spaces. Moreover, the lack of a correlation with behavior in these regions (see below) suggests that the variance captured by the function model there may not be the same variance that drives the relationship of the function model with the behavior. As highlighted in the second and third paragraphs of the Discussion, one explanation for the discrepancy between the behavioral and fMRI findings is the experimental circumstances under which the functional features become ‘activated’. The lack of correlation between fMRI responses and the functional model suggests that brain representations of functions may be partly task-driven, happening only when participants are engaged in a contrastive task.</p><p>We agree that the question in the other direction, i.e. whether previous neural findings on scene-selective regions should be interpreted differently based on our results, is also relevant. Previous studies have reported correlations between fMRI responses in for example PPA and scene categorization behavior (e.g. Walther et al., 2009; Kravitz et al., 2011). Our results are not inconsistent with these reports: we also find that PPA responses correlate with behavior (see <xref ref-type="fig" rid="fig4">Figure 4</xref> and new <xref ref-type="fig" rid="fig7">Figure 7</xref>). In addition, we show that DNN features likely underlie this correlation, since these features correlate both with behavior and PPA (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>/E, <xref ref-type="fig" rid="fig3">Figure 3B</xref>/C, and <xref ref-type="fig" rid="fig8">Figure 8</xref>). Had we considered <italic>only</italic> this feature model, then we would have been able to present a ‘full circle’ account that confirms the importance of scene-selective cortex for scene perception. However, both the Greene et al., (2016) study and our current findings indicate that DNN features do not fully account for scene perception, because functional features are also (and perhaps even more) important for scene categorization behavior. Of course, it could have been the case that these features were also represented in PPA, or other scene-selective regions. However, our fMRI results clearly show that they were not, at least not under the experimental conditions we considered. Thus, apart from suggesting that neural representations relevant for scene categorization behavior may be partly task-driven (see above), these findings also raise questions about whether scene categorization should be thought of as relating exclusively to scene-selective cortex. We bring up this possibility in multiple places in the Discussion, but since our results are mainly exploratory in this regard, we think it is better not to speculate much further based on this particular study.</p><disp-quote content-type="editor-comment"><p>Minor points [abridged]:</p><p>[…] 7.Doesn't the absence of DNN feature space similarity with brain activity (outside of the two small ventral and lateral occipitotemporal clusters) in the searchlight analysis contradict previous findings by Khaligh-Razavi &amp; Kriegeskorte or Guclu &amp; van Gerven? Shouldn't that be discussed explicitly?</p></disp-quote><p>To clarify, for our a priori selected DNN feature space (based on layer fc7), we do see similarity with brain activity: the ROI analyses and the searchlight in Figure 6A demonstrate that features extracted from this layer correlate with scene-selective regions. These regions could possibly be part of the large IT ROI used in Khaligh-Razavi and Kriegeskorte (2014). Similarly, scene-selective regions may be part of the ‘high-level’ end of the gradient reported in Güçlü and van Gerven (2015), although they were not explicitly labeled in the figures of that study. Thus, our findings are not necessarily inconsistent with these results. However, it could be considered surprising that in our searchlights, significant cluster-corrected correlations with layer fc7 were restricted to the scene-selective regions, while other DNN-to-fMRI mapping studies report a more distributed correlation for higher layers (e.g. Cichy et al., 2016, their Figure 4). (Note however, that our unthresholded searchlight maps suggest a somewhat more extensive correlation, of which the cluster-correction only maintains the peaks). For completeness and comparison with previous reports, we now include thresholded and unthresholded searchlights for all layers of the object- and scene-trained DNNs in Figure 8—Figure Supplement 1 and 2 in a movie format; see the Results and Materials and methods sections. These analyses confirm the general impression of a low-to-high gradient in DNN correspondence: low DNN layers appear to map onto earlier visual regions while higher layers map onto higher visual regions. However - as we already observed in Figure 8C-D of our manuscript – scene-selective regions already show a significant correspondence from quite early on in the DNN hierarchy, and the gradient is not necessarily as steep as one might expect based on previous reports. It should also be noted that our a priori selected DNN feature space (but not the searchlight maps in Figure 8—Figure Supplements 1 and 2, which were based on “post-hoc” acquired DNN activations to the scene exemplars used only in this study), were explicitly decorrelated (i.e. selected to be minimally correlated) from the functional and object feature space. This may also explain the smaller extent of the DNN correlation in our study relative to previous work in which DNN features were not explicitly decorrelated from other models and may thus have been characterized by correlations with other feature spaces. Finally, many of the previous findings with DNNs were obtained with object rather than real-world scene images, which may be another factor that contributes to differences between studies. These considerations all merit future work in which stimulus sets, DNN training, DNN architectures and measurement domain (e.g. behavior vs. fMRI) are carefully compared.</p><p><bold>List of references:</bold></p><p>Bau D, Zhou B, Khosla A, Oliva A, Torralba A (2017) Network Dissection: Quantifying Interpretability of Deep Visual Representations. In: Computer Vision and Pattern Recognition (CVPR), pp 1–9.</p><p>Bonner MF, Epstein RA (2017) Computational mechanisms underlying cortical responses to the affordance properties of visual scenes. bioRxiv:doi: https://doi.org/10.1101/177329.</p><p>Cadieu CF, Hong H, Yamins DLK, Pinto N, Ardila D, Solomon EA, Majaj NJ, DiCarlo JJ (2014) Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition. PLoS Comput Biol 10.</p><p>Cichy RM, Khosla A, Pantazis D, Torralba A, Oliva A (2016) Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence. Sci Rep 6:1–35.</p><p>Edelman S (1998) Representation is representation of similarities. Behav Brain Sci 21:449–467.</p><p>Greene MR, Baldassano C, Esteva A, Beck DM (2016) Visual Scenes Are Categorized by Function. J Exp Psychol Gen 145:82–94.</p><p>Güçlü U, van Gerven MAJ (2015) Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream. J Neurosci 35:10005–10014.</p><p>Kay KN (2017) Principles for models of neural information processing. Neuroimage:1–9.</p><p>Khaligh-Razavi SM, Kriegeskorte N (2014) Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation. PLoS Comput Biol 10.</p><p>Kravitz DJ, Peng CS, Baker CI (2011) Real-world scene representations in high-level visual cortex: it’s the spaces more than the places. J Neurosci 31:7322–7333.</p><p>Malcolm GL, Groen IIA, Baker CI (2016) Making sense of real-world scenes. Trends Cogn Sci 20:843–856.</p><p>Scholte HS (2018) Fantastic DNimals and where to find them. Neuroimage:1–2.</p><p>Tripp B (2018) A deeper understanding of the brain. Neuroimage:1–3.</p><p>Turner BM, Miletić S, Forstmann BU (2018) Outlook on deep neural networks in computational cognitive neuroscience. Neuroimage:1–2.</p><p>Tversky A (1977) Features of similarity. Psychol Rev 84:327–352.</p><p>Walther DB, Caddigan E, Fei-Fei L, Beck DM (2009) Natural scene categories revealed in distributed patterns of activity in the human brain. J Neurosci 29:10573–10581.</p><p>Yamins DLK, Hong H, Cadieu C, Dicarlo JJ (2013) Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream. Adv Neural Inf Process Syst 26:3093–3101.</p><p>Zhou B, Lapedriza A, Xiao J, Torralba A, Oliva A (2014) Learning Deep Features for Scene Recognition using Places Database. Adv Neural Inf Process Syst 27:487–495.</p></body></sub-article></article>