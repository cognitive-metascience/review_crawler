<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">20944</article-id><article-id pub-id-type="doi">10.7554/eLife.20944</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Rules and mechanisms for efficient two-stage learning in neural circuits</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-58080"><name><surname>Teşileanu</surname><given-names>Tiberiu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3107-3088</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-15316"><name><surname>Ölveczky</surname><given-names>Bence</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2499-2705</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-15603"><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6497-3819</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Initiative for the Theoretical Sciences</institution>, <institution>CUNY Graduate Center</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution>David Rittenhouse Laboratories, University of Pennsylvania</institution>, <addr-line><named-content content-type="city">Philadelphia</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Organismic and Evolutionary Biology and Center for Brain Science</institution>, <institution>Harvard University</institution>, <addr-line><named-content content-type="city">Cambridge</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Theoretische Natuurkunde</institution>, <institution>Vrije Universiteit Brussel &amp; International Solvay Institutes</institution>, <addr-line><named-content content-type="city">Brussels</named-content></addr-line>, <country>Belgium</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing editor</role><aff id="aff5"><institution>Brown University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>vijay@physics.upenn.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>04</day><month>04</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e20944</elocation-id><history><date date-type="received"><day>25</day><month>08</month><year>2016</year></date><date date-type="accepted"><day>04</day><month>03</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Teşileanu et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Teşileanu et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-20944-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.20944.001</object-id><p>Trial-and-error learning requires evaluating variable actions and reinforcing successful variants. In songbirds, vocal exploration is induced by LMAN, the output of a basal ganglia-related circuit that also contributes a corrective bias to the vocal output. This bias is gradually consolidated in RA, a motor cortex analogue downstream of LMAN. We develop a new model of such two-stage learning. Using stochastic gradient descent, we derive how the activity in ‘tutor’ circuits (<italic>e.g.,</italic> LMAN) should match plasticity mechanisms in ‘student’ circuits (<italic>e.g.,</italic> RA) to achieve efficient learning. We further describe a reinforcement learning framework through which the tutor can build its teaching signal. We show that mismatches between the tutor signal and the plasticity mechanism can impair learning. Applied to birdsong, our results predict the temporal structure of the corrective bias from LMAN given a plasticity rule in RA. Our framework can be applied predictively to other paired brain areas showing two-stage learning.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.001">http://dx.doi.org/10.7554/eLife.20944.001</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>zebra finch</kwd><kwd>birdsong</kwd><kwd>learning theory</kwd><kwd>motor control</kwd><kwd>reinforcement learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Swartz Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Teşileanu</surname><given-names>Tiberiu</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Balasubramanian</surname><given-names>Vijay</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Teaching signals from &quot;tutor&quot; brain areas should be adapted to the plasticity mechanisms in &quot;student&quot; areas to achieve efficient learning in two-stage systems such as the vocal control circuit of the songbird.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Two-stage learning has been described in a variety of different contexts and neural circuits. During hippocampal memory consolidation, recent memories, that are dependent on the hippocampus, are transferred to the neocortex for long-term storage (<xref ref-type="bibr" rid="bib12">Frankland and Bontempi, 2005</xref>). Similarly, the rat motor cortex provides essential input to sub-cortical circuits during skill learning, but then becomes dispensable for executing certain skills (<xref ref-type="bibr" rid="bib22">Kawai et al., 2015</xref>). A paradigmatic example of two-stage learning occurs in songbirds learning their courtship songs (<xref ref-type="bibr" rid="bib2">Andalman and Fee, 2009</xref>; <xref ref-type="bibr" rid="bib37">Turner and Desmurget, 2010</xref>; <xref ref-type="bibr" rid="bib39">Warren et al., 2011</xref>). Zebra finches, commonly used in birdsong research, learn their song from their fathers as juveniles, and keep the same song for life (<xref ref-type="bibr" rid="bib20">Immelmann, 1969</xref>).</p><p>The birdsong circuit has been extensively studied; see <xref ref-type="fig" rid="fig1">Figure 1A</xref> for an outline. Area HVC is a timebase circuit, with projection neurons that fire sparse spike bursts in precise synchrony with the song (<xref ref-type="bibr" rid="bib15">Hahnloser et al., 2002</xref>; <xref ref-type="bibr" rid="bib26">Lynch et al., 2016</xref>; <xref ref-type="bibr" rid="bib30">Picardo et al., 2016</xref>). A population of neurons from HVC projects to the robust nucleus of the arcopallium (RA), a pre-motor area, which then projects to motor neurons controlling respiratory and syringeal muscles (<xref ref-type="bibr" rid="bib25">Leonardo and Fee, 2005</xref>; <xref ref-type="bibr" rid="bib32">Simpson and Vicario, 1990</xref>; <xref ref-type="bibr" rid="bib40">Yu and Margoliash, 1996</xref>). A second input to RA comes from the lateral magnocellular nucleus of the anterior nidopallium (LMAN). Unlike HVC and RA activity patterns, LMAN spiking is highly variable across different renditions of the song (<xref ref-type="bibr" rid="bib21">Kao et al., 2008</xref>; <xref ref-type="bibr" rid="bib41">Ölveczky et al., 2005</xref>). LMAN is the output of the anterior forebrain pathway, a circuit involving the song-specialized basal ganglia (<xref ref-type="bibr" rid="bib29">Perkel, 2004</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.002</object-id><label>Figure 1.</label><caption><title>Relation between the song system in zebra finches and our model.</title><p>(<bold>A</bold>) Diagram of the major brain regions involved in birdsong. (<bold>B</bold>) Conceptual model inspired by the birdsong system. The line from output to tutor is dashed because the reinforcement signal can reach the tutor either directly or, as in songbirds, indirectly. (<bold>C</bold>) Plasticity rule measured in bird RA (measurement done in slice). When an HVC burst leads an LMAN burst by about <inline-formula><mml:math id="inf1"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>100</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>, the HVC–RA synapse is strengthened, while coincident firing leads to suppression. Figure adapted from <xref ref-type="bibr" rid="bib27">Mehaffey and Doupe (2015)</xref>. (<bold>D</bold>) Plasticity rule in our model that mimics the <xref ref-type="bibr" rid="bib27">Mehaffey and Doupe (2015)</xref> rule.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.002">http://dx.doi.org/10.7554/eLife.20944.002</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-fig1-v1"/></fig></p><p>Because of the variability in its activity patterns, it was thought that LMAN’s role was simply to inject variability into the song (<xref ref-type="bibr" rid="bib41">Ölveczky et al., 2005</xref>). The resulting vocal experimentation would enable reinforcement-based learning. For this reason, prior models tended to treat LMAN as a pure Poisson noise generator, and assume that a reward signal is received directly in RA (<xref ref-type="bibr" rid="bib10">Fiete et al., 2007</xref>). More recent evidence, however, suggests that the reward signal reaches Area X, the song-specialized basal ganglia, rather than RA (<xref ref-type="bibr" rid="bib13">Gadagkar et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Hoffmann et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Kubikova et al., 2010</xref>). Taken together with the fact that LMAN firing patterns are not uniformly random, but rather contain a corrective bias guiding plasticity in RA (<xref ref-type="bibr" rid="bib2">Andalman and Fee, 2009</xref>; <xref ref-type="bibr" rid="bib39">Warren et al., 2011</xref>), this suggests that we should rethink our models of song acquisition.</p><p>Here we build a general model of two-stage learning where one neural circuit ‘tutors’ another. We develop a formalism for determining how the teaching signal should be adapted to a specific plasticity rule, to best instruct a student circuit to improve its performance at each learning step. We develop analytical results in a rate-based model, and show through simulations that the general findings carry over to realistic spiking neurons. Applied to the vocal control circuit of songbirds, our model reproduces the observed changes in the spiking statistics of RA neurons as juvenile birds learn their song. Our framework also predicts how the LMAN signal should be adapted to properties of RA synapses. This prediction can be tested in future experiments.</p><p>Our approach separates the mechanistic question of <italic>how</italic> learning is implemented from what the resulting learning rules are. We nevertheless demonstrate that a simple reinforcement learning algorithm suffices to implement the learning rule we propose. Our framework makes general predictions for how instructive signals are matched to plasticity rules whenever information is transferred between different brain regions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Model</title><p>We considered a model for information transfer that is composed of three sub-circuits: a conductor, a student, and a tutor (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The conductor provides input to the student in the form of temporally precise patterns. The goal of learning is for the student to convert this input to a predefined output pattern. The tutor provides a signal that guides plasticity at the conductor–student synapses. For simplicity, we assumed that the conductor always presents the input patterns in the same order, and without repetitions. This allowed us to use the time <inline-formula><mml:math id="inf2"><mml:mi>t</mml:mi></mml:math></inline-formula> to label input patterns, making it easier to analyze the on-line learning rules that we studied. This model of learning is based on the logic implemented by the vocal circuits of the songbird (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Relating this to the songbird, the conductor is HVC, the student is RA, and the tutor is LMAN. The song can be viewed as a mapping between clock-like HVC activity patterns and muscle-related RA outputs. The goal of learning is to find a mapping that reproduces the tutor song.</p><p>Birdsong provides interesting insights into the role of variability in tutor signals. If we focus solely on information transfer, the tutor output need not be variable; it can deterministically provide the best instructive signal to guide the student. This, however, would require the tutor to have a detailed model of the student. More realistically, the tutor might only have access to a scalar representation of how successful the student rendition of the desired output is, perhaps in the form of a reward signal. A tutor in this case has to solve the so-called ‘credit assignment problem’—it needs to identify which student neurons are responsible for the reward. A standard way to achieve this is to inject variability in the student output and reinforce the firing of neurons that precede reward (see for example (<xref ref-type="bibr" rid="bib10">Fiete et al., 2007</xref>) in the birdsong context). Thus, in our model, the tutor has a dual role of providing both an instructive signal and variability, as in birdsong.</p><p>We described the output of our model using a vector <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf4"><mml:mi>a</mml:mi></mml:math></inline-formula> indexed the various output channels (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). In the context of motor control <inline-formula><mml:math id="inf5"><mml:mi>a</mml:mi></mml:math></inline-formula> might index the muscle to be controlled, or, more abstractly, different features of the motor output, such as pitch and amplitude in the case of birdsong. The output <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was a function of the activity of the student neurons <inline-formula><mml:math id="inf7"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The student neurons were in turn driven by the activity of the conductor neurons <inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The student also received tutor signals to guide plasticity; in the songbird, the guiding signals for each RA neuron come from several LMAN neurons (<xref ref-type="bibr" rid="bib4">Canady et al., 1988</xref>; <xref ref-type="bibr" rid="bib14">Garst-Orozco et al., 2014</xref>; <xref ref-type="bibr" rid="bib18">Herrmann and Arnold, 1991</xref>). In our model, we summarized the net input from the tutor to the <inline-formula><mml:math id="inf9"><mml:mi>j</mml:mi></mml:math></inline-formula>th student neuron as a single function <inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.003</object-id><label>Figure 2.</label><caption><title>Schematic representation of our rate-based model.</title><p>(<bold>A</bold>) Conductor neurons fire precisely-timed bursts, similar to HVC neurons in songbirds. Conductor and tutor activities, <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, provide excitation to student neurons, which integrate these inputs and respond linearly, with activity <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Student neurons also receive a constant inhibitory input, <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>x</mml:mi><mml:mtext>inh</mml:mtext></mml:msub></mml:math></inline-formula>. The output neurons linearly combine the activities from groups of student neurons using weights <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The linearity assumptions were made for mathematical convenience but are not essential for our qualitative results (see Appendix). (<bold>B</bold>). The conductor–student synaptic weights <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are updated based on a plasticity rule that depends on two parameters, <inline-formula><mml:math id="inf17"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:mi>β</mml:mi></mml:math></inline-formula>, and two timescales, <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> and Materials and methods). The tutor signal enters this rule as a deviation from a constant threshold <inline-formula><mml:math id="inf21"><mml:mi>θ</mml:mi></mml:math></inline-formula>. The figure shows how synaptic weights change (<inline-formula><mml:math id="inf22"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>) for a student neuron that receives a tutor burst and a conductor burst separated by a short lag. Two different choices of plasticity parameters are illustrated in the case when the threshold <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>C</bold>) The amount of mismatch between the system’s output and the target output is quantified using a loss (error) function. The figure sketches the loss landscape obtained by varying the synaptic weights <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and calculating the loss function in each case (only two of the weight axes are shown). The blue dot shows the lowest value of the loss function, corresponding to the best match between the motor output and the target, while the orange dot shows the starting point. The dashed line shows how learning would proceed in a gradient descent approach, where the weights change in the direction of steepest descent in the loss landscape.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.003">http://dx.doi.org/10.7554/eLife.20944.003</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-fig2-v1"/></fig></p><p>We started with a rate-based implementation of the model (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) that was analytically tractable but averaged over tutor variability. We further took the neurons to be in a linear operating regime (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) away from the threshold and saturation present in real neurons. We then relaxed these conditions and tested our results in spiking networks with initial parameters selected to imitate measured firing patterns in juvenile birds prior to song learning. The student circuit in both the rate-based and spiking models included a global inhibitory signal that helped to suppress excess activity driven by ongoing conductor and tutor input. Such recurrent inhibition is present in area RA of the bird (<xref ref-type="bibr" rid="bib33">Spiro et al., 1999</xref>). In the spiking model we implemented the suppression as an activity-dependent inhibition, while for the analytic calculations we used a constant negative bias for the student neurons.</p></sec><sec id="s2-2"><title>Learning in a rate-based model</title><p>Learning in our model was enabled by plasticity at the conductor–student synapses that was modulated by signals from tutor neurons (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Many different forms of such hetero-synaptic plasticity have been observed. For example, in rate-based synaptic plasticity high tutor firing rates lead to synaptic potentiation and low tutor firing rates lead to depression (<xref ref-type="bibr" rid="bib6">Chistiakova and Volgushev, 2009</xref>; <xref ref-type="bibr" rid="bib5">Chistiakova et al., 2014</xref>). In timing-dependent rules, such as the one recently measured by <xref ref-type="bibr" rid="bib27">Mehaffey and Doupe (2015)</xref> in slices of zebra finch RA (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>), the relative arrival times of spike bursts from different input pathways set the sign of synaptic change. To model learning that lies between these rate and timing-based extremes, we introduced a class of plasticity rules governed by two parameters <inline-formula><mml:math id="inf25"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mi>β</mml:mi></mml:math></inline-formula> (see also Materials and methods and <xref ref-type="fig" rid="fig2">Figure 2B</xref>):<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mlabeledtr><mml:mtd><mml:mtext>(1)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mi>α</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mi>β</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the weight of the synapse from the <inline-formula><mml:math id="inf28"><mml:mi>i</mml:mi></mml:math></inline-formula>th conductor to the <inline-formula><mml:math id="inf29"><mml:mi>j</mml:mi></mml:math></inline-formula>th student neuron, <inline-formula><mml:math id="inf30"><mml:mi>η</mml:mi></mml:math></inline-formula> is a learning rate, <inline-formula><mml:math id="inf31"><mml:mi>θ</mml:mi></mml:math></inline-formula> is a threshold on the firing rate of tutor neurons, and <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are timescales associated with the plasticity. This is similar to an STDP rule, except that the dependence on postsynaptic activity was replaced by dependence on the input from the tutor. Thus plasticity acts heterosynaptically, with activation of the tutor–student synapse controlling the change in the conductor–student synaptic weight. The timescales <inline-formula><mml:math id="inf34"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, as well as the coefficients <inline-formula><mml:math id="inf36"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:mi>β</mml:mi></mml:math></inline-formula>, can be thought of as effective parameters describing the plasticity observed in student neurons. As such, they do not necessarily have a simple correspondence in terms of the biochemistry of the plasticity mechanism, and the framework we describe here is not specifically tied to such an interpretation.</p><p>If we set <inline-formula><mml:math id="inf38"><mml:mi>α</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf39"><mml:mi>β</mml:mi></mml:math></inline-formula> to zero in our rule, <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>, the sign of the synaptic change is determined solely by the firing rate of the tutor <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as compared to a threshold, reproducing the rate rules observed in experiments. When <inline-formula><mml:math id="inf41"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>/</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, if the conductor leads the tutor, potentiation occurs, while coincident signals lead to depression (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), which mimics the empirical findings from <xref ref-type="bibr" rid="bib27">Mehaffey and Doupe (2015)</xref>. For general <inline-formula><mml:math id="inf42"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf43"><mml:mi>β</mml:mi></mml:math></inline-formula>, the sign of plasticity is controlled by both the firing rate of the tutor relative to the baseline, and by the relative timing of tutor and conductor. The overall scale of the parameters <inline-formula><mml:math id="inf44"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:mi>β</mml:mi></mml:math></inline-formula> can be absorbed into the learning rate <inline-formula><mml:math id="inf46"><mml:mi>η</mml:mi></mml:math></inline-formula> and so we set <inline-formula><mml:math id="inf47"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> in all our simulations without loss of generality (see Materials and methods). Note that if <inline-formula><mml:math id="inf48"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:mi>β</mml:mi></mml:math></inline-formula> are both large, it can be that <inline-formula><mml:math id="inf50"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf51"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>/</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> also, as needed to realize the <xref ref-type="bibr" rid="bib27">Mehaffey and Doupe (2015)</xref> curve.</p><p>We can ask how the conductor–student weights <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) should change in order to best improve the output <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We first need a loss function <inline-formula><mml:math id="inf54"><mml:mi>L</mml:mi></mml:math></inline-formula> that quantifies the distance between the current output <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the target <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). We used a quadratic loss function, but other choices can also be incorporated into our framework (see Appendix). Learning should change the synaptic weights so that the loss function is minimized, leading to a good rendition of the targeted output. This can be achieved by changing the synaptic weights in the direction of steepest descent of the loss function (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p><p>We used the synaptic plasticity rule from <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> to calculate the overall change of the weights, <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, over the course of the motor program. This is a function of the time course of the tutor signal, <inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Not every choice for the tutor signal leads to motor output changes that best improve the match to the target. Imposing the condition that these changes follow the gradient descent procedure described above, we derived the tutor signal that was best matched to the student plasticity rule (detailed derivation in Materials and methods). The result is that the best tutor for driving gradient descent learning must keep track of the motor error<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>a</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>integrated over the recent past<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ζ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the weights describing the linear relationship between student activities and motor outputs (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and <inline-formula><mml:math id="inf60"><mml:mi>ζ</mml:mi></mml:math></inline-formula> is a learning rate. Moreover, for effective learning, the parameter <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> appearing in <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref>, which quantifies the timescale on which error information is integrated into the tutor signal, should be related to the synaptic plasticity parameters according to<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mtext>where</mml:mtext></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≡</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>is the optimal timescale for the error integration.</p><p>In short, motor learning with a heterosynaptic plasticity rule requires convolving the motor error with a kernel whose timescale is related to the structure of the plasticity rule, but is otherwise independent of the motor program. As explained in more detail in Materials and methods, this result is derived in an approximation that assumes that the tutor signal does not vary significantly over timescales of the order of the student timescales <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. Given <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>, this implies that we are assuming <inline-formula><mml:math id="inf64"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. This is a reasonable approximation because variations in the tutor signal that are much faster than the student timescales <inline-formula><mml:math id="inf65"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> have little effect on learning since the plasticity rule (1) blurs conductor inputs over these timescales.</p></sec><sec id="s2-3"><title>Matched <italic>vs.</italic> unmatched learning</title><p>Our rate-based model predicts that when the timescale on which error information is integrated into the tutor signal (<inline-formula><mml:math id="inf66"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula>) is matched to the student plasticity rule as described above, learning will proceed efficiently. A mismatched tutor should slow or disrupt convergence to the desired output. To test this, we numerically simulated the birdsong circuit using the linear model from <xref ref-type="fig" rid="fig2">Figure 2A</xref> with a motor output <inline-formula><mml:math id="inf67"><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> filtered to more realistically reflect muscle response times (see Materials and methods). We selected plasticity rules as described in <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> and <xref ref-type="fig" rid="fig2">Figure 2B</xref> and picked a target output pattern to learn. The target was chosen to resemble recordings of air-sac pressure from singing zebra finches in terms of smoothness and characteristic timescales (<xref ref-type="bibr" rid="bib38">Veit et al., 2011</xref>), but was otherwise arbitrary. In our simulations, the output typically involved two different channels, each with its own target, but for brevity, in figures we typically showed the output from only one of these.</p><p>For our analytical calculations, we made a series of assumptions and approximations meant to enhance tractability, such as linearity of the model and a focus on the regime <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. These constraints can be lifted in our simulations, and indeed below we test our numerical model in regimes that go beyond the approximations made in our derivation. In many cases, we found that the basic findings regarding tutor–student matching from our analytical model remain true even when some of the assumptions we used to derive it no longer hold.</p><p>We tested tutors that were matched or mismatched to the plasticity rule to see how effectively they instructed the student. <xref ref-type="fig" rid="fig3">Figure 3A</xref> and online <xref ref-type="other" rid="media1">Video 1</xref> show convergence with a matched tutor when the sign of plasticity is determined by the tutor’s firing rate. We see that the student output rapidly converged to the target. <xref ref-type="fig" rid="fig3">Figure 3B</xref> and online <xref ref-type="other" rid="media2">Video 2</xref> show convergence with a matched tutor when the sign of plasticity is largely determined by the relative timing of the tutor signal and the student output. We see again that the student converged steadily to the desired output, but at a somewhat slower rate than in <xref ref-type="fig" rid="fig3">Figure 3A</xref>.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.004</object-id><label>Figure 3.</label><caption><title>Learning with matched or mismatched tutors in rate-based simulations.</title><p>(<bold>A</bold>) Error trace showing how the average motor error evolved with the number of repetitions of the motor program for a rate-based (<inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) plasticity rule paired with a matching tutor. (See online <xref ref-type="other" rid="media1">Video 1</xref>). (<bold>B</bold>) The error trace and final motor output shown for a timing-based learning rule matched by a tutor with a long integration timescale. (See online <xref ref-type="other" rid="media2">Video 2</xref>.) In both <bold>A</bold> and <bold>B</bold> the inset shows the final motor output for one of the two output channels (thick orange line) compared to the target output for that channel (dotted black line). The output on the first rendition and at two other stages of learning indicated by orange arrows on the error trace are also shown as thin orange lines. (<bold>C</bold>) Effects of mismatch between student and tutor on reproduction accuracy. The heatmap shows the final reproduction error of the motor output after 1000 learning cycles in a rate-based simulation where a student with parameters <inline-formula><mml:math id="inf70"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf71"><mml:mi>β</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> was paired with a tutor with memory timescale <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula>. On the <inline-formula><mml:math id="inf75"><mml:mi>y</mml:mi></mml:math></inline-formula> axis, <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> were kept fixed at <inline-formula><mml:math id="inf78"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>40</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>, respectively, while <inline-formula><mml:math id="inf80"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf81"><mml:mi>β</mml:mi></mml:math></inline-formula> were varied (subject to the constraint <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>; see text). Different choices of <inline-formula><mml:math id="inf83"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf84"><mml:mi>β</mml:mi></mml:math></inline-formula> lead to different optimal timescales <inline-formula><mml:math id="inf85"><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> according to <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>. The diagonal elements correspond to matched tutor and student, <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. Note that the color scale is logarithmic. (<bold>D</bold>) Error evolution curves as a function of the mismatch between student and tutor. Each plot shows how the error in the motor program changed during 1000 learning cycles for the same conditions as those shown in the heatmap. The region shaded in light pink shows simulations where the mismatch between student and tutor led to a deteriorating instead of improving performance during learning.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.004">http://dx.doi.org/10.7554/eLife.20944.004</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-fig3-v1"/></fig><media content-type="glencoe play-in-place height-250 width-310" id="media1" mime-subtype="mp4" mimetype="video" xlink:href="elife-20944-media1.mp4"><object-id pub-id-type="doi">10.7554/eLife.20944.005</object-id><label>Video 1.</label><caption><title>Evolution of motor output during learning in a rate-based simulation using a rate-based (<inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) plasticity rule paired with a matching tutor.</title><p>This video relates to <xref ref-type="fig" rid="fig3">Figure 3A</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.005">http://dx.doi.org/10.7554/eLife.20944.005</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media2" mime-subtype="mp4" mimetype="video" xlink:href="elife-20944-media2.mp4"><object-id pub-id-type="doi">10.7554/eLife.20944.006</object-id><label>Video 2.</label><caption><title>Evolution of motor output during learning in a rate-based simulation using a timing-based (<inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>α</mml:mi><mml:mo>≈</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula>) plasticity rule paired with a matching tutor.</title><p>This video relates to <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.006">http://dx.doi.org/10.7554/eLife.20944.006</ext-link></p></caption></media></p><p>To test the effects of mismatch between tutor and student, we used tutors with timescales that did not match <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>. All student plasticity rules had the same effective time constants <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, but different parameters <inline-formula><mml:math id="inf91"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf92"><mml:mi>β</mml:mi></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), subject to the constraint <inline-formula><mml:math id="inf93"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> described in the previous section. Different tutors had different memory time scales <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). <xref ref-type="fig" rid="fig3">Figure 3C and D</xref> demonstrate that learning was more rapid for well-matched tutor-student pairs (the diagonal neighborhood, where <inline-formula><mml:math id="inf95"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>). When the tutor error integration timescale was shorter than the matched value in <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>, <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, learning was often completely disrupted (many pairs below the diagonal in <xref ref-type="fig" rid="fig3">Figure 3C and D</xref>). When the tutor error integration timescale was longer than the matched value in <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>, <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> learning was slowed down. <xref ref-type="fig" rid="fig3">Figure 3C</xref> also shows that a certain amount of mismatch between the tutor error integration timescale <inline-formula><mml:math id="inf98"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> and the matched timescale <inline-formula><mml:math id="inf99"><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> implied by the student plasticity rule is tolerated by the system. Interestingly, the diagonal band over which learning is effective in <xref ref-type="fig" rid="fig3">Figure 3C</xref> is roughly of constant width—note that the scale on both axes is logarithmic, so that this means that the tutor error integration timescale <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> has to be within a constant factor of the optimal timescale <inline-formula><mml:math id="inf101"><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> for good learning. We also see that the breakdown in learning is more abrupt when <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> than in the opposite regime.</p><p>An interesting feature of the results from <xref ref-type="fig" rid="fig3">Figure 3C and D</xref> is that the difference in performance between matched and mismatched pairs becomes less pronounced for timescales shorter than about <inline-formula><mml:math id="inf103"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>100</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>. This is due to the fact that the plasticity rule (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) implicitly smooths over timescales of the order of <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, which in our simulations were equal to <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf106"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>40</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, variations of the tutor signal on shorter timescales have little effect on learning. Using different values for the effective timescales <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> describing the plasticity rule can increase or decrease the range of parameters over which learning is robust against tutor–student mismatches (see Appendix).</p></sec><sec id="s2-4"><title>Robust learning with nonlinearities</title><p>In the model above, firing rates for the tutor were allowed to grow as large as necessary to implement the most efficient learning. However, the firing rates of realistic neurons typically saturate at some fixed bound. To test the effects of this nonlinearity in the tutor, we passed the ideal tutor activity (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) through a sigmoidal nonlinearity,<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>ρ</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mi>ζ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf108"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:math></inline-formula> is the range of firing rates. We typically chose <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>Hz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to constrain the rates to the range 0–160 Hz (<xref ref-type="bibr" rid="bib41">Ölveczky et al., 2005</xref>; <xref ref-type="bibr" rid="bib14">Garst-Orozco et al., 2014</xref>). Learning slowed down with this change (<xref ref-type="fig" rid="fig4">Figure 4A</xref> and online <xref ref-type="other" rid="media3">Video 3</xref>) as a result of the tutor firing rates saturating when the mismatch between the motor output and the target output was large. However, the accuracy of the final rendition was not affected by saturation in the tutor (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, inset). An interesting effect occurred when the firing rate constraint was imposed on a matched tutor with a long memory timescale. When this happened and the motor error was large, the tutor signal saturated and stopped growing in relation to the motor error before the end of the motor program. In the extreme case of very long integration timescales, learning became sequential: early features in the output were learned first, before later features were addressed, as in <xref ref-type="fig" rid="fig4">Figure 4B</xref> and online <xref ref-type="other" rid="media4">Video 4</xref>. This is reminiscent of the learning rule described in (<xref ref-type="bibr" rid="bib28">Memmesheimer et al., 2014</xref>).<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.007</object-id><label>Figure 4.</label><caption><title>Effects of adding a constraint on the tutor firing rate to the simulations.</title><p>(<bold>A</bold>) Learning was slowed down by the firing rate constraint, but the accuracy of the final rendition stayed the same (inset, shown here for one of two simulated output channels). Here <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>40</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. (See online <xref ref-type="other" rid="media3">Video 3</xref>.) (<bold>B</bold>) Sequential learning occurred when the firing rate constraint was imposed on a matched tutor with a long memory scale. The plots show the evolution of the motor output for one of the two channels that were used in the simulation. Here <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>23</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf115"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>1000</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. (See online <xref ref-type="other" rid="media4">Video 4</xref>.).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.007">http://dx.doi.org/10.7554/eLife.20944.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-fig4-v1"/></fig><media content-type="glencoe play-in-place height-250 width-310" id="media3" mime-subtype="mp4" mimetype="video" xlink:href="elife-20944-media3.mp4"><object-id pub-id-type="doi">10.7554/eLife.20944.008</object-id><label>Video 3.</label><caption><title>Effects of adding a constraint on tutor firing rates on the evolution of motor output during learning in a rate-based simulation.</title><p>The plasticity rule here was rate-based (<inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). This video relates to <xref ref-type="fig" rid="fig4">Figure 4A</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.008">http://dx.doi.org/10.7554/eLife.20944.008</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media4" mime-subtype="mp4" mimetype="video" xlink:href="elife-20944-media4.mp4"><object-id pub-id-type="doi">10.7554/eLife.20944.009</object-id><label>Video 4.</label><caption><title>Evolution of motor output showing sequential learning in a rate-based simulation when the firing rate constraint is imposed on a tutor with a long memory timescale.</title><p>This video relates to <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.009">http://dx.doi.org/10.7554/eLife.20944.009</ext-link></p></caption></media></p><p>Nonlinearities can similarly affect the activities of student neurons. Our model can be readily extended to describe efficient learning even in this case. The key result is that for efficient learning to occur, the synaptic plasticity rule should depend not just on the tutor and conductor, but also on the activity of the postsynaptic student neurons (details in Appendix). Such dependence on postsynaptic activity is commonly seen in experiments (<xref ref-type="bibr" rid="bib6">Chistiakova and Volgushev, 2009</xref>; <xref ref-type="bibr" rid="bib5">Chistiakova et al., 2014</xref>).</p><p>The relation between student neuron activations <inline-formula><mml:math id="inf117"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and motor outputs <inline-formula><mml:math id="inf118"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) is in general also nonlinear. Compared to the linear assumption that we used, the effect of a monotonic nonlinearity, <inline-formula><mml:math id="inf119"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> an increasing function, is similar to modifying the loss function <inline-formula><mml:math id="inf121"><mml:mi>L</mml:mi></mml:math></inline-formula>, and does not significantly change our results (see Appendix). We also checked that imposing a rectification constraint that conductor–student weights <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> must be positive does not modify our results either (see Appendix). This shows that our model continues to work with biologically realistic synapses that cannot change sign from excitatory to inhibitory during learning.</p></sec><sec id="s2-5"><title>Spiking neurons and birdsong</title><p>To apply our model to vocal learning in birds, we extended our analysis to networks of spiking neurons. Juvenile songbirds produce a ‘babble’ that converges through learning to an adult song strongly resembling the tutor song. This is reflected in the song-aligned spiking patterns in pre-motor area RA, which become more stereotyped and cluster in shorter, better-defined bursts as the bird matures (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We tested whether our model could reproduce key statistics of spiking in RA over the course of song learning. In this context, our theory of efficient learning, derived in a rate-based scenario, predicts a specific relation between the teaching signal embedded in LMAN firing patterns, and the plasticity rule implemented in RA. We tested whether these predictions continued to hold in the spiking context.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.010</object-id><label>Figure 5.</label><caption><title>Results from simulations in spiking neural networks.</title><p>(<bold>A</bold>) Spike patterns recorded from zebra finch RA during song production, for a juvenile (top) and an adult (bottom). Each color corresponds to a single neuron, and the song-aligned spikes for six renditions of the song are shown. Adapted from <xref ref-type="bibr" rid="bib42">Ölveczky et al. (2011)</xref>. (<bold>B</bold>) Spike patterns from model student neurons in our simulations, for the untrained (top) and trained (bottom) models. The training used <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf124"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf125"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and ran for 600 iterations of the song. Each model neuron corresponds to a different output channel of the simulation. In this case, the targets for each channel were chosen to roughly approximate the time course observed in the neural recordings. (<bold>C</bold>) Progression of reproduction error in the spiking simulation as a function of the number of repetitions for the same conditions as in panel B. The inset shows the accuracy of reproduction in the trained model for one of the output channels. (See online <xref ref-type="other" rid="media5">Video 5</xref>.) (<bold>D</bold>) Effects of mismatch between student and tutor on reproduction accuracy in the spiking model. The heatmap shows the final reproduction error of the motor output after 1000 learning cycles in a spiking simulation where a student with parameters <inline-formula><mml:math id="inf126"><mml:mi>α</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf127"><mml:mi>β</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf128"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> was paired with a tutor with memory timescale <inline-formula><mml:math id="inf130"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula>. On the <inline-formula><mml:math id="inf131"><mml:mi>y</mml:mi></mml:math></inline-formula> axis, <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> were kept fixed at <inline-formula><mml:math id="inf134"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf135"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>40</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>, respectively, while <inline-formula><mml:math id="inf136"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf137"><mml:mi>β</mml:mi></mml:math></inline-formula> were varied (subject to the constraint <inline-formula><mml:math id="inf138"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>; see section &quot;Learning in a rate-based model&quot;). Different choices of <inline-formula><mml:math id="inf139"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf140"><mml:mi>β</mml:mi></mml:math></inline-formula> lead to different optimal timescales <inline-formula><mml:math id="inf141"><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> according to <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>. The diagonal elements correspond to matched tutor and student, <inline-formula><mml:math id="inf142"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. Note that the color scale is logarithmic.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.010">http://dx.doi.org/10.7554/eLife.20944.010</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-fig5-v1"/></fig><media content-type="glencoe play-in-place height-250 width-310" id="media5" mime-subtype="mp4" mimetype="video" xlink:href="elife-20944-media5.mp4"><object-id pub-id-type="doi">10.7554/eLife.20944.011</object-id><label>Video 5.</label><caption><title>Evolution of motor output during learning in a spiking simulation.</title><p>The plasticity rule parameters were <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and the tutor had a matching timescale <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This video relates to <xref ref-type="fig" rid="fig5">Figure 5C</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.011">http://dx.doi.org/10.7554/eLife.20944.011</ext-link></p></caption></media></p><p>Following the experiments of <xref ref-type="bibr" rid="bib15">Hahnloser et al. (2002)</xref>, we modeled each neuron in HVC (the conductor) as firing one short, precisely timed burst of 5–6 spikes at a single moment in the motor program. Thus the population of HVC neurons produced a precise timebase for the song. LMAN (tutor) neurons are known to have highly variable firing patterns that facilitate experimentation, but also contain a corrective bias (<xref ref-type="bibr" rid="bib2">Andalman and Fee, 2009</xref>). Thus we modeled LMAN as producing inhomogeneous Poisson spike trains with a time-dependent firing rate given by <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref> in our model. Although biologically there are several LMAN neurons projecting to each RA neuron, we again simplified by ‘summing’ the LMAN inputs into a single, effective tutor neuron, similarly to the approach in (<xref ref-type="bibr" rid="bib10">Fiete et al., 2007</xref>). The LMAN-RA synapses were modeled in a current-based approach as a mixture of AMPA and NMDA receptors, following the songbird data (<xref ref-type="bibr" rid="bib14">Garst-Orozco et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Stark and Perkel, 1999</xref>). The initial weights for all synapses were tuned to produce RA firing patterns resembling juvenile birds (<xref ref-type="bibr" rid="bib42">Ölveczky et al., 2011</xref>), subject to constraints from direct measurements in slice recordings (<xref ref-type="bibr" rid="bib14">Garst-Orozco et al., 2014</xref>) (see Materials and methods for details, and <xref ref-type="fig" rid="fig5">Figure 5B</xref> for a comparison between neural recordings and spiking in our model). In contrast to the constant inhibitory bias that we used in our rate-based simulations, for the spiking simulations we chose an activity-dependent global inhibition for RA neurons. We also tested that a constant bias produced similar results (see Appendix).</p><p>Synaptic strength updates followed the same two-timescale dynamics that was used in the rate-based models (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The firing rates <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that appear in the plasticity equation were calculated in the spiking model by filtering the spike trains from conductor and tutor neurons with exponential kernels. The synaptic weights were constrained to be non-negative. (See Materials and methods for details.)</p><p>As long as the tutor error integration timescale was not too large, learning proceeded effectively when the tutor error integration timescale and the student plasticity rule were matched (see <xref ref-type="fig" rid="fig5">Figure 5C</xref> and online <xref ref-type="other" rid="media5">Video 5</xref>), with mismatches slowing down or abolishing learning, just as in our rate-based study (compare <xref ref-type="fig" rid="fig5">Figure 5D</xref> with <xref ref-type="fig" rid="fig3">Figure 3C</xref>). The rate of learning and the accuracy of the trained state were lower in the spiking model compared to the rate-based model. The lower accuracy arises because the tutor neurons fire stochastically, unlike the deterministic neurons used in the rate-based simulations. The stochastic nature of the tutor firing also led to a decrease in learning accuracy as the tutor error integration timescale <inline-formula><mml:math id="inf148"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> increased (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). This happens through two related effects: (1) the signal-to-noise ratio in the tutor guiding signal decreases as <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> increases once the tutor error integration timescale is longer than the duration <inline-formula><mml:math id="inf150"><mml:mi>T</mml:mi></mml:math></inline-formula> of the motor program (see Appendix); and (2) the fluctuations in the conductor–student weights lead to some weights getting clamped at 0 due to the positivity constraint, which leads to the motor program overshooting the target (see Appendix). The latter effect can be reduced by either allowing for negative weights, or changing the motor output to a push-pull architecture in which some student neurons enhance the output while others inhibit it. The signal-to-noise ratio effect can be attenuated by increasing the gain of the tutor signal, which inhibits early learning, but improves the quality of the guiding signal in the latter stages of the learning process. It is also worth emphasizing that these effects only become relevant once the tutor error integration timescale <inline-formula><mml:math id="inf151"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> becomes significantly longer than the duration of the motor program, <inline-formula><mml:math id="inf152"><mml:mi>T</mml:mi></mml:math></inline-formula>, which for a birdsong motif would be around 1 s.</p><p>Spiking in our model tends to be a little more regular than that in the recordings (compare <xref ref-type="fig" rid="fig5">Figure 5A</xref> and <xref ref-type="fig" rid="fig5">Figure 5B</xref>). This could be due to sources of noise that are present in the brain which we did not model. One detail that our model does not capture is the fact that many LMAN spikes occur in bursts, while in our simulation LMAN firing is Poisson. Bursts are more likely to produce spikes in downstream RA neurons particularly because of the NMDA dynamics, and thus a bursty LMAN will be more effective at injecting variability into RA (<xref ref-type="bibr" rid="bib23">Kojima et al., 2013</xref>). Small inaccuracies in aligning the recorded spikes to the song are also likely to contribute apparent variability between renditions in experiments. Indeed, some of the variability in <xref ref-type="fig" rid="fig5">Figure 5A</xref> looks like it could be due to time warping and global time shifts that were not fully corrected.</p></sec><sec id="s2-6"><title>Robust learning with credit assignment errors</title><p>The calculation of the tutor output in our rule involved estimating the motor error <inline-formula><mml:math id="inf153"><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>. This required knowledge of the assignment between student activities and motor output, which in our model was represented by the matrix <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). In our simulations, we typically chose an assignment in which each student neuron contributed to a single output channel, mimicking the empirical findings for neurons in bird RA. Mathematically, this implies that each column of <inline-formula><mml:math id="inf155"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> contained a single non-zero element. In <xref ref-type="fig" rid="fig6">Figure 6A</xref>, we show what happened in the rate-based model when the tutor incorrectly assigned a certain fraction of the neurons to the wrong output. Specifically, we considered two output channels, <inline-formula><mml:math id="inf156"><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf157"><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, with half of the student neurons contributing only to <inline-formula><mml:math id="inf158"><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and the other half contributing only to <inline-formula><mml:math id="inf159"><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. We then scrambled a fraction <inline-formula><mml:math id="inf160"><mml:mi>ρ</mml:mi></mml:math></inline-formula> of this assignment when calculating the motor error, so that the tutor effectively had an imperfect knowledge of the student–output relation. <xref ref-type="fig" rid="fig6">Figure 6A</xref> shows that learning is robust to this kind of mis-assignment even for fairly large values of the error fraction <inline-formula><mml:math id="inf161"><mml:mi>ρ</mml:mi></mml:math></inline-formula> up to about 40%, but quickly deteriorates as this fraction approaches 50%.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.012</object-id><label>Figure 6.</label><caption><title>Credit assignment and reinforcement learning.</title><p>(<bold>A</bold>) Effects of credit mis-assignment on learning in a rate-based simulation. Here, the system learned output sequences for two independent channels. The student–output weights <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were chosen so that the tutor wrongly assigned a fraction of student neurons to an output channel different from the one it actually mapped to. The graph shows how the accuracy of the motor output after 1000 learning steps depended on the fraction of mis-assigned credit. (<bold>B</bold>) Learning curve and trained motor output (inset) for one of the channels showing two-stage reinforcement-based learning for the memory-less tutor (<inline-formula><mml:math id="inf163"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). The accuracy of the trained model is as good as in the case where the tutor was assumed to have a perfect model of the student–output relation. However, the speed of learning is reduced. (See online <xref ref-type="other" rid="media6">Video 6</xref>.) (<bold>C</bold>) Learning curve and trained motor output (inset) for one of the output channels showing two-stage reinforcement-based learning when the tutor circuit needs to integrate information about the motor error on a certain timescale. Again, learning was slow, but the accuracy of the trained state was unchanged. (See online <xref ref-type="other" rid="media7">Video 7</xref>.) (<bold>D</bold>) Evolution of the average number of HVC inputs per RA neuron with learning in a reinforcement example. Synapses were considered pruned if they admitted a current smaller than 1 nA after a pre-synaptic spike in our simulations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.012">http://dx.doi.org/10.7554/eLife.20944.012</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-fig6-v1"/></fig><media content-type="glencoe play-in-place height-250 width-310" id="media6" mime-subtype="mp4" mimetype="video" xlink:href="elife-20944-media6.mp4"><object-id pub-id-type="doi">10.7554/eLife.20944.013</object-id><label>Video 6.</label><caption><title>Evolution of motor output during learning in a spiking simulation with a reinforcement-based tutor.</title><p>Here the tutor was memory-less (<inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). This video relates to <xref ref-type="fig" rid="fig6">Figure 6B</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.013">http://dx.doi.org/10.7554/eLife.20944.013</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media7" mime-subtype="mp4" mimetype="video" xlink:href="elife-20944-media7.mp4"><object-id pub-id-type="doi">10.7554/eLife.20944.014</object-id><label>Video 7.</label><caption><title>Evolution of motor output during learning in a spiking simulation with a reinforcement-based tutor.</title><p>Here the tutor needed to integrate information about the motor error on a timescale <inline-formula><mml:math id="inf165"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>440</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This video relates to <xref ref-type="fig" rid="fig6">Figure 6C</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.014">http://dx.doi.org/10.7554/eLife.20944.014</ext-link></p></caption></media></p><p>Due to environmental factors that affect development of different individuals in different ways, it is unlikely that the student–output mapping can be innate. As such, the tutor circuit must learn the mapping. Indeed, it is known that LMAN in the bird receives an indirect evaluation signal <italic>via</italic> Area X, which might be used to effect this learning (<xref ref-type="bibr" rid="bib2">Andalman and Fee, 2009</xref>; <xref ref-type="bibr" rid="bib13">Gadagkar et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Hoffmann et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Kubikova et al., 2010</xref>). One way in which this can be achieved is through a reinforcement paradigm. We thus considered a learning rule where the tutor circuit receives a reward signal that enables it to infer the student–output mapping. In general the output of the tutor circuit should depend on an integral of the motor error, as in <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref>, to best instruct the student. For simplicity, we start with the memory-less case, <inline-formula><mml:math id="inf166"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, in which only the instantaneous value of the motor error is reflected in the tutor signal; we then show how to generalize this for <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>As before, we took the tutor neurons to fire Poisson spikes with time-dependent rates <inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which were initialized arbitrarily. Because of stochastic fluctuations, the actual tutor activity on any given trial, <inline-formula><mml:math id="inf169"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, differs somewhat from the average, <inline-formula><mml:math id="inf170"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Denoting the difference by <inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the update rule for the tutor firing rates was given by<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf172"><mml:msub><mml:mi>η</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> is a learning rate, <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the instantaneous reward signal, and <inline-formula><mml:math id="inf174"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> is its average over recent renditions of the motor program. In our implementation, <inline-formula><mml:math id="inf175"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> is obtained by convolving <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with an exponential kernel (timescale = 1 s). The reward <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at the end of one rendition becomes the baseline at the start of the next rendition <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The baseline <inline-formula><mml:math id="inf179"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the tutor activity is calculated by averaging over recent renditions of the song with exponentially decaying weights (one <inline-formula><mml:math id="inf180"><mml:mi>e</mml:mi></mml:math></inline-formula>-fold of decay for every five renditions). Further implementation details are available in our code at <ext-link ext-link-type="uri" xlink:href="https://github.com/ttesileanu/twostagelearning">https://github.com/ttesileanu/twostagelearning</ext-link> (<xref ref-type="bibr" rid="bib35">Teşileanu, 2016</xref>) (with a copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/twostagelearning">https://github.com/elifesciences-publications/twostagelearning</ext-link>). </p><p>The intuition behind this rule is that, whenever a fluctuation in the tutor activity leads to better-than-average reward (<inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>), the tutor firing rate changes in the direction of the fluctuation for subsequent trials, ‘freezing in’ the improvement. Conversely, the firing rate moves away from the directions in which fluctuations tend to reduce the reward.</p><p>To test our learning rule, we ran simulations using this reinforcement strategy and found that learning again converges to an accurate rendition of the target output (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, inset and online <xref ref-type="other" rid="media6">Video 6</xref>). The number of repetitions needed for training is greatly increased compared to the case in which the credit assignment is assumed known by the tutor circuit (compare <xref ref-type="fig" rid="fig6">Figure 6B</xref> to <xref ref-type="fig" rid="fig5">Figure 5C</xref>). This is because the tutor needs to use many training rounds for experimentation before it can guide conductor–student plasticity. The rate of learning in our model is similar to the songbird (<italic>i.e.,</italic> order <inline-formula><mml:math id="inf182"><mml:mn>10 000</mml:mn></mml:math></inline-formula> repetitions for learning, given that a zebra finch typically sings about 1000 repetitions of its song each day, and takes about one month to fully develop adult song).</p><p>Because of the extra training time needed for the tutor to adapt its signal, the motor output in our reward-based simulations tends to initially overshoot the target (leading to the kink in the error at around 2000 repetitions in <xref ref-type="fig" rid="fig6">Figure 6B</xref>). Interestingly, the subsequent reduction in output that leads to convergence of the motor program, combined with the positivity constraint on the synaptic strengths, leads to many conductor–student connections being pruned (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). This mirrors experiments on songbirds, where the number of connections between HVC and RA first increases with learning and then decreases (<xref ref-type="bibr" rid="bib14">Garst-Orozco et al., 2014</xref>).</p><p>The reinforcement rule described above responds only to instantaneous values of the reward signal and tutor firing rate fluctuations. In general, effective learning requires that the tutor keep a memory trace of its activity over a timescale <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, as in <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>. To achieve this in the reinforcement paradigm, we can use a simple generalization of <xref ref-type="disp-formula" rid="equ6">Equation (6)</xref> where the update rule is filtered over the tutor memory timescale:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>η</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We tested that this rule leads to effective learning when paired with the corresponding student, <italic>i.e.,</italic> one for which <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref> is obeyed (<xref ref-type="fig" rid="fig6">Figure 6C</xref> and online <xref ref-type="other" rid="media7">Video 7</xref>).</p><p>The reinforcement rules proposed here are related to the learning rules from (<xref ref-type="bibr" rid="bib11">Fiete and Seung, 2006</xref>; <xref ref-type="bibr" rid="bib10">Fiete et al., 2007</xref>) and (<xref ref-type="bibr" rid="bib8">Farries and Fairhall, 2007</xref>). However, those models focused on learning in a single pass, instead of the two-stage architecture that we studied. In particular, in <xref ref-type="bibr" rid="bib10">Fiete et al. (2007)</xref>, area LMAN was assumed to generate pure Poisson noise and reinforcement learning took place at the HVC–RA synapses. In our model, which is in better agreement with recent evidence regarding the roles of RA and LMAN in birdsong (<xref ref-type="bibr" rid="bib2">Andalman and Fee, 2009</xref>), reinforcement learning first takes place in the anterior forebrain pathway (AFP), for which LMAN is the output. A reward-independent heterosynaptic plasticity rule then solidifies the information in RA.</p><p>In our simulations, tutor neurons fire Poisson spikes with specific time-dependent rates which change during learning. The timecourse of the firing rates in each repetition must then be stored somewhere in the brain. In fact, in the songbird, there are indirect projections from HVC to LMAN, going through the basal ganglia (Area X) and the dorso-lateral division of the medial thalamus (DLM) in the anterior forebrain pathway (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) (<xref ref-type="bibr" rid="bib29">Perkel, 2004</xref>). These synapses could store the required time-dependence of the tutor firing rates. In addition, the same synapses can provide the timebase input that would ensure synchrony between LMAN firing and RA output, as necessary for learning. Our reinforcement learning rule for the tutor area, <xref ref-type="disp-formula" rid="equ6">Equation (6)</xref>, can be viewed as an effective model for plasticity in the projections between HVC, Area X, DLM, and LMAN, as in <xref ref-type="bibr" rid="bib9">Fee and Goldberg (2011)</xref>. In this picture, the indirect HVC–LMAN connections behave somewhat like the ‘hedonistic synapses’ from <xref ref-type="bibr" rid="bib31">Seung (2003)</xref>, though we use a simpler synaptic model here. Implementing the integral from <xref ref-type="disp-formula" rid="equ7">Equation (7)</xref> would require further recurrent circuitry in LMAN which is beyond the scope of this paper, but would be interesting to investigate in future work.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We built a two-stage model of learning in which one area (the student) learns to generate a patterned motor output under guidance from a tutor area. This architecture is inspired by the song system of zebra finches, where area LMAN provides a corrective bias to the song that is then consolidated in the HVC–RA synapses. Using an approach rooted in the efficient coding literature, we showed analytically that, in a simple model, the tutor output that is most likely to lead to effective learning by the student involves an integral over the recent magnitude of the motor error. We found that efficiency requires that the timescale for this integral should be related to the synaptic plasticity rule used by the student. Using simulations, we tested our findings in more general settings. In particular, we demonstrated that tutor-student matching is important for learning in a spiking-neuron model constructed to reproduce spiking patterns similar to those measured in zebra finches. Learning in this model changes the spiking statistics of student neurons in realistic ways, for example, by producing more bursty, stereotyped firing events as learning progresses. Finally, we showed how the tutor can build its error-correcting signal by means of reinforcement learning.</p><p>If the birdsong system supports efficient learning, our model can predict the temporal structure of the firing patterns of RA-projecting LMAN neurons, given the plasticity rule implemented at the HVC–RA synapses. These predictions can be directly tested by recordings from LMAN neurons in singing birds, assuming that a good measure of motor error is available, and that we can estimate how the neurons contribute to this error. Moreover, recordings from a tutor circuit, such as LMAN, could be combined with a measure of motor error to infer the plasticity rule in a downstream student circuit, such as RA. This could be compared with direct measurements of the plasticity rule obtained in slice. Conversely, knowledge of the student plasticity rule could be used to predict the time-dependence of tutor firing rates. According to our model, the firing rate should reflect the integral of the motor error with the timescale predicted by the model. A different approach would be to artificially tutor RA by stimulating LMAN neurons electrically or optogenetically. We would predict that if the tutor signal is delivered appropriately (<italic>e.g.,</italic> in conjunction with a particular syllable [<xref ref-type="bibr" rid="bib36">Tumer and Brainard, 2007</xref>]), then the premotor bias produced by the stimulation should become incorporated into the motor pathway faster when the timescale of the artificial LMAN signal is properly matched to the RA synaptic plasticity rule.</p><p>Our model can be applied more generally to other systems in the brain exhibiting two-stage learning, such as motor learning in mammals. If the plasticity mechanisms in these systems are different from those in songbirds, our predictions for the structure of the guiding signal will vary correspondingly. This would allow a further test of our model of ‘efficient learning’ in the brain. It is worth pointing out that our model was derived assuming a certain hierarchy among the timescales that model the student plasticity and the tutor signal. A mismatch between the model predictions and observations could also imply a breakdown of these approximations, rather than failure of the hypothesis that the particular system under study evolved to support efficient learning. Of course our analysis could be extended by relaxing these assumptions, for example by keeping more terms in the Taylor expansion that we used in our derivation of the matched tutor signal.</p><p>Applied to birdsong, our model is best seen as a mechanism for learning song syllables. The ordering of syllables in song motifs seems to have a second level of control within HVC and perhaps beyond (<xref ref-type="bibr" rid="bib3">Basista et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Hamaguchi et al., 2016</xref>). Songs can also be distorted by warping their timebase through changes in HVC firing without alterations of the HVC–RA connectivity (<xref ref-type="bibr" rid="bib1">Ali et al., 2013</xref>). In view of these phenomena, it would be interesting to incorporate our model into a larger hierarchical framework in which the sequencing and temporal structure of the syllables are also learned. A model of transitions between syllables can be found in <xref ref-type="bibr" rid="bib7">Doya and Sejnowski (2000)</xref>, where the authors use a ‘weight perturbation’ optimization scheme in which each HVC–RA synaptic weight is perturbed individually. We did not follow this approach because there is no plausible mechanism for LMAN to provide separate guidance to each HVC–RA synapse; in particular, there are not enough LMAN neurons (<xref ref-type="bibr" rid="bib10">Fiete et al., 2007</xref>).</p><p>In this paper we assumed a two-stage architecture for learning, inspired by birdsong. An interesting question is whether and under what conditions such an architecture is more effective than a single-step model. Possibly, having two stages is better when a single tutor area is responsible for training several different dedicated controllers, as is likely the case in motor learning. It would then be beneficial to have an area that can learn arbitrary behaviors, perhaps at the cost of using more resources and having slower reaction times, along with the ability to transfer these behaviors into low-level circuitry that is only capable of producing stereotyped motor programs. The question then arises whether having more than two levels in this hierarchy could be useful, what the other levels might do, and whether such hierarchical learning systems are implemented in the brain.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Equations for rate-based model</title><p>The basic equations we used for describing our rate-based model (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) are the following:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>inh</mml:mtext></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In simulations, we further filtered the output using an exponential kernel,<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>with a timescale <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>out</mml:mtext></mml:msub></mml:math></inline-formula> that we typically set to 25 ms. The smoothing produces more realistic outputs by mimicking the relatively slow reaction time of real muscles, and stabilizes learning by filtering out high-frequency components of the motor output. The latter interfere with learning because of the delay between the effect of conductor activity on synaptic strengths <italic>vs.</italic> motor output. This delay is of the order <inline-formula><mml:math id="inf185"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mtext>out</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> (see the plasticity rule below).</p><p>The conductor activity in the rate-based model is modeled after songbird HVC (<xref ref-type="bibr" rid="bib15">Hahnloser et al., 2002</xref>): each neuron fires a single burst during the motor program. Each burst corresponds to a sharp increase of the firing rate <inline-formula><mml:math id="inf186"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from 0 to a constant value, and then a decrease <inline-formula><mml:math id="inf187"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>10</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula> later. The activities of the different neurons are spread out to tile the whole duration of the output program. Other choices for the conductor activity also work, provided no patterns are repeated (see Appendix).</p></sec><sec id="s4-2"><title>Mathematical description of plasticity rule</title><p>In our model the rate of change of the synaptic weights obeys a rule that depends on a filtered version of the conductor signal (see <xref ref-type="fig" rid="fig2">Figure 2B</xref>). This is expressed mathematically as<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>η</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf188"><mml:mi>η</mml:mi></mml:math></inline-formula> is a learning rate and <inline-formula><mml:math id="inf189"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, with the star representing convolution and <inline-formula><mml:math id="inf190"><mml:mi>K</mml:mi></mml:math></inline-formula> being a filtering kernel. We considered a linear combination of two exponential kernels with timescales <inline-formula><mml:math id="inf191"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf192"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>,<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf193"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> given by<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Different choices for the kernels give similar results (see Appendix). The overall scale of <inline-formula><mml:math id="inf194"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf195"><mml:mi>β</mml:mi></mml:math></inline-formula> can be absorbed into the learning rate <inline-formula><mml:math id="inf196"><mml:mi>η</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ10">Equation (10)</xref>. In our simulations, we fix <inline-formula><mml:math id="inf197"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and keep the learning rate constant as we change the plasticity rule (see <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>).</p><p>In the spiking simulations with and without reinforcement learning in the tutor circuit, the firing rates <inline-formula><mml:math id="inf198"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf199"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were estimated by filtering spike trains with exponential kernels whose timescales were in the range <inline-formula><mml:math id="inf200"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>5</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>–<inline-formula><mml:math id="inf201"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>40</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>. The reinforcement studies typically required longer timescales for stability, possibly because of delays between conductor activity and reward signals.</p></sec><sec id="s4-3"><title>Derivation of the matching tutor signal</title><p>To find the tutor signal that provides the most effective teaching for the student, we first calculate how much synaptic weights change according to our plasticity rule, <xref ref-type="disp-formula" rid="equ10">Equation (10)</xref>. Then we require that this change matches the gradient descent direction. We have<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mpadded width="+1.7pt"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>t</mml:mi></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Because of the linearity assumptions in our model, it is sufficient to focus on a case in which each conductor neuron, <inline-formula><mml:math id="inf202"><mml:mi>i</mml:mi></mml:math></inline-formula>, fires a single short burst, at a time <inline-formula><mml:math id="inf203"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. We write this as <inline-formula><mml:math id="inf204"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and so<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mpadded width="+1.7pt"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>t</mml:mi></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we used the definition of <inline-formula><mml:math id="inf205"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If the time constants <inline-formula><mml:math id="inf206"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf207"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are short compared to the timescale on which the tutor input <inline-formula><mml:math id="inf208"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> varies, only the values of <inline-formula><mml:math id="inf209"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> around time <inline-formula><mml:math id="inf210"><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> will contribute to the integral. If we further assume that <inline-formula><mml:math id="inf211"><mml:mrow><mml:mi>T</mml:mi><mml:mo>≫</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we can use a Taylor expansion of <inline-formula><mml:math id="inf212"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> around <inline-formula><mml:math id="inf213"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to perform the calculation:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≈</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>t</mml:mi><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Doing the integrals involving the exponential kernels <inline-formula><mml:math id="inf214"><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf215"><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, we get<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%" rspace="4.2pt">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We would like this synaptic change to optimally reduce a measure of mismatch between the output and the desired target as measured by a loss function. A generic smooth loss function <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be quadratically approximated when <inline-formula><mml:math id="inf217"><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> is sufficiently close to the target <inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. With this in mind, we consider a quadratic loss<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>a</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mpadded width="+1.7pt"><mml:msup><mml:mrow><mml:mo maxsize="120%" minsize="120%">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>t</mml:mi></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The loss function would decrease monotonically during learning if synaptic weights changed in proportion to the negative gradient of <inline-formula><mml:math id="inf219"><mml:mi>L</mml:mi></mml:math></inline-formula>:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf220"><mml:mi>γ</mml:mi></mml:math></inline-formula> is a learning rate. This implies<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>a</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%" rspace="4.2pt">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Using again <inline-formula><mml:math id="inf221"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, we obtain<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we used the notation from <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> for the motor error at student neuron <inline-formula><mml:math id="inf222"><mml:mi>j</mml:mi></mml:math></inline-formula>.</p><p>We now set <xref ref-type="disp-formula" rid="equ16 equ20">Equations (16) and (20)</xref> equal to each other. If the conductor fires densely in time, we need the equality to hold for all times, and we thus get a differential equation for the tutor signal <inline-formula><mml:math id="inf223"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This identifies the tutor signal that leads to gradient descent learning as a function of the motor error <inline-formula><mml:math id="inf224"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref> (with the notation <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-4"><title>Spiking simulations</title><p>We used spiking models that were based on leaky integrate-and-fire neurons with current-based dynamics for the synaptic inputs. The magnitude of synaptic potentials generated by the conductor–student synapses was independent of the membrane potential, approximating AMPA receptor dynamics, while the synaptic inputs from the tutor to the student were based on a mixture of AMPA and NMDA dynamics. Specifically, the equations describing the dynamics of the spiking model were:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>inh</mml:mtext></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mtext> conductor \#</mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>w</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>inh</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mtext>inh</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext> student</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>inh</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mtext>student</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mtext>[Mg]</mml:mtext><mml:mrow><mml:mn>3.57</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>16.13</mml:mn><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf226"><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is the membrane potential of the <inline-formula><mml:math id="inf227"><mml:msup><mml:mi>j</mml:mi><mml:mtext>th</mml:mtext></mml:msup></mml:math></inline-formula> student neuron and <inline-formula><mml:math id="inf228"><mml:msub><mml:mi>V</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> is the resting potential, as well as the potential to which the membrane was reset after a spike. Spikes were registered whenever the membrane potential went above a threshold <inline-formula><mml:math id="inf229"><mml:msub><mml:mi>V</mml:mi><mml:mtext>th</mml:mtext></mml:msub></mml:math></inline-formula>, after which a refractory period <inline-formula><mml:math id="inf230"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>ref</mml:mtext></mml:msub></mml:math></inline-formula> ensued. Apart from excitatory AMPA and NMDA inputs modeled by the <inline-formula><mml:math id="inf231"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mtext>AMPA</mml:mtext></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf232"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mtext>NMDA</mml:mtext></mml:msubsup></mml:math></inline-formula> variables in our model, we also included a global inhibitory signal <inline-formula><mml:math id="inf233"><mml:msub><mml:mi>V</mml:mi><mml:mtext>inh</mml:mtext></mml:msub></mml:math></inline-formula> which is proportional to the overall activity of student neurons averaged over a timescale <inline-formula><mml:math id="inf234"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>inh</mml:mtext></mml:msub></mml:math></inline-formula>. The averaging is performed using the auxiliary variables <inline-formula><mml:math id="inf235"><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> which are convolutions of student spike trains with an exponential kernel. These can be thought of as a simple model for the activities of inhibitory interneurons in the student.</p><p><xref ref-type="table" rid="tbl1">Table 1</xref> gives the values of the parameters we used in the simulations. These values were chosen to match the firing statistics of neurons in bird RA, as described below.<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.015</object-id><label>Table 1.</label><caption><p>Values for parameters used in the spiking simulations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.015">http://dx.doi.org/10.7554/eLife.20944.015</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>Symbol</th><th>Value</th><th>Parameter</th><th>Symbol</th><th>Value</th></tr></thead><tbody><tr><td>No. of conductor neurons</td><td/><td><inline-formula><mml:math id="inf236"><mml:mn mathbackground="#FFFFFF">300</mml:mn></mml:math></inline-formula></td><td>No. of student neurons</td><td/><td><inline-formula><mml:math id="inf237"><mml:mn mathbackground="#FFFFFF">80</mml:mn></mml:math></inline-formula></td></tr><tr><td>Reset potential</td><td><inline-formula><mml:math id="inf238"><mml:msub><mml:mi mathbackground="#DFDFDF">V</mml:mi><mml:mi mathbackground="#DFDFDF">R</mml:mi></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf239"><mml:mrow><mml:mo mathbackground="#DFDFDF">-</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#DFDFDF">72.3</mml:mn></mml:mpadded><mml:mo mathbackground="#DFDFDF">⁢</mml:mo><mml:mi mathbackground="#DFDFDF">mV</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>Input resistance</td><td><inline-formula><mml:math id="inf240"><mml:mi mathbackground="#DFDFDF">R</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf241"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#DFDFDF">353</mml:mn></mml:mpadded><mml:mo mathbackground="#DFDFDF">⁢</mml:mo><mml:mi mathbackground="#DFDFDF" mathvariant="normal">M</mml:mi><mml:mo mathbackground="#DFDFDF">⁢</mml:mo><mml:mi mathbackground="#DFDFDF" mathvariant="normal">Ω</mml:mi></mml:mrow></mml:math></inline-formula></td></tr><tr><td>Threshold potential</td><td><inline-formula><mml:math id="inf242"><mml:msub><mml:mi mathbackground="#FFFFFF">V</mml:mi><mml:mtext mathbackground="#FFFFFF">th</mml:mtext></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf243"><mml:mrow><mml:mo mathbackground="#FFFFFF">-</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#FFFFFF">48.6</mml:mn></mml:mpadded><mml:mo mathbackground="#FFFFFF">⁢</mml:mo><mml:mi mathbackground="#FFFFFF">mV</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>Strength of inhibition</td><td><inline-formula><mml:math id="inf244"><mml:msub><mml:mi mathbackground="#FFFFFF">g</mml:mi><mml:mtext mathbackground="#FFFFFF">inh</mml:mtext></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf245"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#FFFFFF">1.80</mml:mn></mml:mpadded><mml:mo mathbackground="#FFFFFF">⁢</mml:mo><mml:mi mathbackground="#FFFFFF">mV</mml:mi></mml:mrow></mml:math></inline-formula></td></tr><tr><td>Membrane time constant</td><td><inline-formula><mml:math id="inf246"><mml:msub><mml:mi mathbackground="#DFDFDF">τ</mml:mi><mml:mi mathbackground="#DFDFDF">m</mml:mi></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf247"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#DFDFDF">24.5</mml:mn></mml:mpadded><mml:mo mathbackground="#DFDFDF">⁢</mml:mo><mml:mi mathbackground="#DFDFDF">ms</mml:mi></mml:mrow></mml:math></inline-formula></td><td>Fraction NMDA receptors</td><td><inline-formula><mml:math id="inf248"><mml:mi mathbackground="#DFDFDF">r</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf249"><mml:mn mathbackground="#DFDFDF">0.9</mml:mn></mml:math></inline-formula></td></tr><tr><td>Refractory period</td><td><inline-formula><mml:math id="inf250"><mml:msub><mml:mi mathbackground="#FFFFFF">τ</mml:mi><mml:mtext mathbackground="#FFFFFF">ref</mml:mtext></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf251"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#FFFFFF">1.1</mml:mn></mml:mpadded><mml:mo mathbackground="#FFFFFF">⁢</mml:mo><mml:mi mathbackground="#FFFFFF">ms</mml:mi></mml:mrow></mml:math></inline-formula></td><td>Strength of synapses from tutor</td><td><inline-formula><mml:math id="inf252"><mml:mi mathbackground="#FFFFFF">w</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf253"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#FFFFFF">100</mml:mn></mml:mpadded><mml:mo mathbackground="#FFFFFF">⁢</mml:mo><mml:mi mathbackground="#FFFFFF">nA</mml:mi></mml:mrow></mml:math></inline-formula></td></tr><tr><td>AMPA time constant</td><td><inline-formula><mml:math id="inf254"><mml:msub><mml:mi mathbackground="#DFDFDF">τ</mml:mi><mml:mtext mathbackground="#DFDFDF">AMPA</mml:mtext></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf255"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#DFDFDF">6.3</mml:mn></mml:mpadded><mml:mo mathbackground="#DFDFDF">⁢</mml:mo><mml:mi mathbackground="#DFDFDF">ms</mml:mi></mml:mrow></mml:math></inline-formula></td><td>No. of conductor synapses per student neuron</td><td/><td><inline-formula><mml:math id="inf256"><mml:mn mathbackground="#DFDFDF">148</mml:mn></mml:math></inline-formula></td></tr><tr><td>NMDA time constant</td><td><inline-formula><mml:math id="inf257"><mml:msub><mml:mi mathbackground="#FFFFFF">τ</mml:mi><mml:mtext mathbackground="#FFFFFF">NMDA</mml:mtext></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf258"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#FFFFFF">81.5</mml:mn></mml:mpadded><mml:mo mathbackground="#FFFFFF">⁢</mml:mo><mml:mi mathbackground="#FFFFFF">ms</mml:mi></mml:mrow></mml:math></inline-formula></td><td>Mean strength of synapses from conductor</td><td/><td><inline-formula><mml:math id="inf259"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#FFFFFF">32.6</mml:mn></mml:mpadded><mml:mo mathbackground="#FFFFFF">⁢</mml:mo><mml:mi mathbackground="#FFFFFF">nA</mml:mi></mml:mrow></mml:math></inline-formula></td></tr><tr><td>Time constant for global inhibition</td><td><inline-formula><mml:math id="inf260"><mml:msub><mml:mi mathbackground="#DFDFDF">τ</mml:mi><mml:mtext mathbackground="#DFDFDF">inh</mml:mtext></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf261"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#DFDFDF">20</mml:mn></mml:mpadded><mml:mo mathbackground="#DFDFDF">⁢</mml:mo><mml:mi mathbackground="#DFDFDF">ms</mml:mi></mml:mrow></mml:math></inline-formula></td><td>Standard deviation of conductor–student weights</td><td/><td><inline-formula><mml:math id="inf262"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#DFDFDF">17.4</mml:mn></mml:mpadded><mml:mo mathbackground="#DFDFDF">⁢</mml:mo><mml:mi mathbackground="#DFDFDF">nA</mml:mi></mml:mrow></mml:math></inline-formula></td></tr><tr><td>Conductor firing rate during bursts</td><td/><td><inline-formula><mml:math id="inf263"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn mathbackground="#FFFFFF">632</mml:mn></mml:mpadded><mml:mo mathbackground="#FFFFFF">⁢</mml:mo><mml:mi mathbackground="#FFFFFF">Hz</mml:mi></mml:mrow></mml:math></inline-formula></td><td/><td/><td/></tr></tbody></table></table-wrap></p><p>The voltage dynamics for conductor and tutor neurons was not simulated explicitly. Instead, each conductor neuron was assumed to fire a burst at a fixed time during the simulation. The onset of each burst had additive timing jitter of <inline-formula><mml:math id="inf264"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>0.3</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and each spike in the burst had a jitter of <inline-formula><mml:math id="inf265"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>0.2</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This modeled the uncertainty in spike times that is observed in <italic>in vivo</italic> recordings in birdsong (<xref ref-type="bibr" rid="bib15">Hahnloser et al., 2002</xref>). Tutor neurons fired Poisson spikes with a time-dependent firing rate that was set as described in the main text.</p><p>The initial connectivity between conductor and student neurons was chosen to be sparse (see <xref ref-type="table" rid="tbl1">Table 1</xref>). The initial distribution of synaptic weights was log-normal, matching experimentally measured values for zebra finches (<xref ref-type="bibr" rid="bib14">Garst-Orozco et al., 2014</xref>). Since these measurements are done in the slice, the absolute number of HVC synapses per RA neuron is likely to have been underestimated. The number of conductor–student synapses we start with in our simulations is thus chosen to be higher than the value reported in that paper (see <xref ref-type="table" rid="tbl1">Table 1</xref>), and is allowed to change during learning. We checked that the learning paradigm described here is robust to substantial changes in these parameters, but we have chosen values that are faithful to birdsong experiments and which are thus able to imitate the RA spiking statistics during song.</p><p>The synapses projecting onto each student neuron from the tutor have a weight that is fixed during our simulations reflecting the finding in <xref ref-type="bibr" rid="bib14">Garst-Orozco et al. (2014)</xref> that the average strength of LMAN–RA synapses for zebra finches does not change with age. There is some evidence that individual LMAN–RA synapses undergo plasticity concurrently with the HVC–RA synapses (<xref ref-type="bibr" rid="bib27">Mehaffey and Doupe, 2015</xref>) but we did not seek to model this effect. There are also developmental changes in the kinetics of NMDA-mediated synaptic currents in both HVC–RA and LMAN–RA synapses which we do not model (<xref ref-type="bibr" rid="bib34">Stark and Perkel, 1999</xref>). These, however, happen early in development, and thus are unlikely to have an effect on song crystallization, which is what our model focuses on. <xref ref-type="bibr" rid="bib34">Stark and Perkel, 1999</xref> also observed changes in the relative contribution of NMDA to AMPA responses in the HVC–RA synapses. We do not incorporate such effects in our model since we do not explicitly model the dynamics of HVC neurons in this paper. However, this is an interesting avenue for future work, especially since there is evidence that area HVC can also contribute to learning, in particular in relation to the temporal structure of song (<xref ref-type="bibr" rid="bib1">Ali et al., 2013</xref>).</p></sec><sec id="s4-5"><title>Matching spiking statistics with experimental data</title><p>We used an optimization technique to choose parameters to maximize the similarity between the statistics of spiking in our simulations and the firing statistics observed in neural recordings from the songbird. The comparison was based on several descriptive statistics: the average firing rate; the coefficient of variation and skewness of the distribution of inter-spike intervals; the frequency and average duration of bursts; and the firing rate during bursts. For calculating these statistics, bursts were defined to start if the firing rate went above 80 Hz and last until the rate decreased below 40 Hz.</p><p>To carry out such optimizations in the stochastic context of our simulations, we used an evolutionary algorithm—the covariance matrix adaptation evolution strategy (CMA-ES) (<xref ref-type="bibr" rid="bib17">Hansen, 2006</xref>). The objective function was based on the relative error between the simulation statistics <inline-formula><mml:math id="inf266"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mtext>sim</mml:mtext></mml:msubsup></mml:math></inline-formula> and the observed statistics <inline-formula><mml:math id="inf267"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mtext>obs</mml:mtext></mml:msubsup></mml:math></inline-formula>,<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:mtext>error</mml:mtext><mml:mo>=</mml:mo><mml:mpadded width="+1.7pt"><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mtext>sim</mml:mtext></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mtext>obs</mml:mtext></mml:msubsup></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mpadded></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Equal weight was placed on optimizing the firing statistics in the juvenile (based on a recording from a 43 dph bird) and optimizing firing in the adult (based on a recording from a 160 dph bird). In this optimization there was no learning between the juvenile and adult stages. We simply required that the number of HVC synapses per RA neuron, and the mean and standard deviation of the corresponding synaptic weights were in the ranges seen in the juvenile and adult by <xref ref-type="bibr" rid="bib14">Garst-Orozco et al. (2014)</xref>. The optimization was carried out in Python (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008394">SCR_008394</ext-link>), using code from <ext-link ext-link-type="uri" xlink:href="https://www.lri.fr/~hansen/cmaes_inmatlab.html">https://www.lri.fr/~hansen/cmaes_inmatlab.html</ext-link>. The results fixed the parameter choices in <xref ref-type="table" rid="tbl1">Table 1</xref> which were then used to study our learning paradigm. While these choices are important for achieving firing statistics that are similar to those seen in recordings from the bird, our learning paradigm is robust to large variations in the parameters in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></sec><sec id="s4-6"><title>Software and data</title><p>We used custom-built Python (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008394">SCR_008394</ext-link>) code for simulations and data analysis. The software and data that we used can be accessed online on GitHub (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002630">SCR_002630</ext-link>) at <ext-link ext-link-type="uri" xlink:href="https://github.com/ttesileanu/twostagelearning">https://github.com/ttesileanu/twostagelearning</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Serena Bradde for fruitful discussions during the early stages of this work. We also thank Xuexin Wei and Christopher Glaze for useful discussions. We are grateful to Timothy Otchy for providing us with some of the data we used in this paper. During this work VB was supported by NSF grant PHY-1066293 at the Aspen Center for Physics and by NSF Physics of Living Systems grant PHY-1058202. TT was supported by the Swartz Foundation.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>TT, Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>BÖ, Conceptualization, Supervision, Investigation, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>VB, Conceptualization, Formal analysis, Supervision, Funding acquisition, Investigation, Writing—original draft, Writing—review and editing</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ali</surname><given-names>F</given-names></name><name><surname>Otchy</surname><given-names>TM</given-names></name><name><surname>Pehlevan</surname><given-names>C</given-names></name><name><surname>Fantana</surname><given-names>AL</given-names></name><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The basal ganglia is necessary for learning spectral, but not temporal, features of birdsong</article-title><source>Neuron</source><volume>80</volume><fpage>494</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.049</pub-id><pub-id pub-id-type="pmid">24075977</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andalman</surname><given-names>AS</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A basal ganglia-forebrain circuit in the songbird biases motor output to avoid vocal errors</article-title><source>PNAS</source><volume>106</volume><fpage>12518</fpage><lpage>12523</lpage><pub-id pub-id-type="doi">10.1073/pnas.0903214106</pub-id><pub-id pub-id-type="pmid">19597157</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basista</surname><given-names>MJ</given-names></name><name><surname>Elliott</surname><given-names>KC</given-names></name><name><surname>Wu</surname><given-names>W</given-names></name><name><surname>Hyson</surname><given-names>RL</given-names></name><name><surname>Bertram</surname><given-names>R</given-names></name><name><surname>Johnson</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Independent premotor encoding of the sequence and structure of birdsong in avian cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>16821</fpage><lpage>16834</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1940-14.2014</pub-id><pub-id pub-id-type="pmid">25505334</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canady</surname><given-names>RA</given-names></name><name><surname>Burd</surname><given-names>GD</given-names></name><name><surname>DeVoogd</surname><given-names>TJ</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Effect of testosterone on input received by an identified neuron type of the canary song system: a golgi/electron microscopy/degeneration study</article-title><source>Journal of Neuroscience</source><volume>8</volume><fpage>3770</fpage><lpage>3784</lpage><pub-id pub-id-type="pmid">2461435</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chistiakova</surname><given-names>M</given-names></name><name><surname>Bannon</surname><given-names>NM</given-names></name><name><surname>Bazhenov</surname><given-names>M</given-names></name><name><surname>Volgushev</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Heterosynaptic plasticity: multiple mechanisms and multiple roles</article-title><source>The Neuroscientist : A Review Journal Bringing Neurobiology, Neurology and Psychiatry</source><volume>20</volume><fpage>483</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1177/1073858414529829</pub-id><pub-id pub-id-type="pmid">24727248</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chistiakova</surname><given-names>M</given-names></name><name><surname>Volgushev</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Heterosynaptic plasticity in the neocortex</article-title><source>Experimental Brain Research</source><volume>199</volume><fpage>377</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1859-5</pub-id><pub-id pub-id-type="pmid">19499213</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><chapter-title>A computational model of avian song learning</chapter-title><person-group person-group-type="editor"><name><surname>Gazzaniga</surname> <given-names>M. S</given-names></name></person-group><source>The New Cognitive Neurosciences</source><publisher-name>arXiv:1011.1669v3</publisher-name><fpage>469</fpage><lpage>482</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farries</surname><given-names>MA</given-names></name><name><surname>Fairhall</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Reinforcement learning with modulated spike timing dependent synaptic plasticity</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>3648</fpage><lpage>3665</lpage><pub-id pub-id-type="doi">10.1152/jn.00364.2007</pub-id><pub-id pub-id-type="pmid">17928565</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fee</surname><given-names>MS</given-names></name><name><surname>Goldberg</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A hypothesis for basal ganglia-dependent reinforcement learning in the songbird</article-title><source>Neuroscience</source><volume>198</volume><fpage>152</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2011.09.069</pub-id><pub-id pub-id-type="pmid">22015923</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>2038</fpage><lpage>2057</lpage><pub-id pub-id-type="doi">10.1152/jn.01311.2006</pub-id><pub-id pub-id-type="pmid">17652414</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Gradient learning in spiking neural networks by dynamic perturbation of conductances</article-title><source>Physical Review Letters</source><volume>97</volume><elocation-id>048104</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.97.048104</pub-id><pub-id pub-id-type="pmid">16907616</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname><given-names>PW</given-names></name><name><surname>Bontempi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The organization of recent and remote memories</article-title><source>Nature Reviews Neuroscience</source><volume>6</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1038/nrn1607</pub-id><pub-id pub-id-type="pmid">15685217</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gadagkar</surname><given-names>V</given-names></name><name><surname>Puzerey</surname><given-names>PA</given-names></name><name><surname>Chen</surname><given-names>R</given-names></name><name><surname>Baird-Daniel</surname><given-names>E</given-names></name><name><surname>Farhang</surname><given-names>AR</given-names></name><name><surname>Goldberg</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dopamine neurons encode performance error in singing birds</article-title><source>Science</source><volume>354</volume><fpage>1278</fpage><lpage>1282</lpage><pub-id pub-id-type="doi">10.1126/science.aah6837</pub-id><pub-id pub-id-type="pmid">27940871</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garst-Orozco</surname><given-names>J</given-names></name><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A neural circuit mechanism for regulating motor variability during skill learning</article-title><source>eLife</source><volume>3</volume><elocation-id>e03697</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03697</pub-id><pub-id pub-id-type="pmid">4290448</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahnloser</surname><given-names>RH</given-names></name><name><surname>Kozhevnikov</surname><given-names>AA</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>An ultra-sparse code underlies the generation of neural sequences in a songbird</article-title><source>Nature</source><volume>419</volume><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/nature00974</pub-id><pub-id pub-id-type="pmid">12214232</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamaguchi</surname><given-names>K</given-names></name><name><surname>Tanaka</surname><given-names>M</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A distributed recurrent network contributes to temporally precise vocalizations</article-title><source>Neuron</source><volume>91</volume><fpage>680</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.06.019</pub-id><pub-id pub-id-type="pmid">27397518</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2006">2006</year><chapter-title>The CMA Evolution Strategy: A Comparing Review</chapter-title><source>Towards a New Evolutionary Computation. Advances on Estimation of Distribution Algorithms</source><fpage>75</fpage><lpage>102</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>K</given-names></name><name><surname>Arnold</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The development of afferent projections to the robust archistriatal nucleus in male zebra finches: a quantitative Electron microscopic study</article-title><source>Journal of Neuroscience</source><volume>11</volume><fpage>2063</fpage><lpage>2074</lpage><pub-id pub-id-type="pmid">2066775</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffmann</surname><given-names>LA</given-names></name><name><surname>Saravanan</surname><given-names>V</given-names></name><name><surname>Wood</surname><given-names>AN</given-names></name><name><surname>He</surname><given-names>L</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dopaminergic contributions to vocal learning</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>2176</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3883-15.2016</pub-id><pub-id pub-id-type="pmid">26888928</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Immelmann</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1969">1969</year><chapter-title>Song development in the zebra finch and other estrildid finches</chapter-title><person-group person-group-type="editor"><name><surname>Hinde</surname> <given-names>R. A</given-names></name></person-group><source>Bird Vocalizations</source><fpage>61</fpage><lpage>74</lpage></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>MH</given-names></name><name><surname>Wright</surname><given-names>BD</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neurons in a forebrain nucleus required for vocal plasticity rapidly switch between precise firing and variable bursting depending on social context</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>13232</fpage><lpage>13247</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2250-08.2008</pub-id><pub-id pub-id-type="pmid">19052215</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawai</surname><given-names>R</given-names></name><name><surname>Markman</surname><given-names>T</given-names></name><name><surname>Poddar</surname><given-names>R</given-names></name><name><surname>Ko</surname><given-names>R</given-names></name><name><surname>Fantana</surname><given-names>AL</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Motor cortex is required for learning but not for executing a motor skill</article-title><source>Neuron</source><volume>86</volume><fpage>800</fpage><lpage>812</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.024</pub-id><pub-id pub-id-type="pmid">25892304</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kojima</surname><given-names>S</given-names></name><name><surname>Kao</surname><given-names>MH</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Task-related &quot;cortical&quot; bursting depends critically on basal ganglia input and is linked to vocal plasticity</article-title><source>PNAS</source><volume>110</volume><fpage>4756</fpage><lpage>4761</lpage><pub-id pub-id-type="doi">10.1073/pnas.1216308110</pub-id><pub-id pub-id-type="pmid">23449880</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubikova</surname><given-names>L</given-names></name><name><surname>Kostál</surname><given-names>L</given-names></name><name><surname>KošÅ¥ál</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dopaminergic system in Birdsong learning and maintenance</article-title><source>Journal of Chemical Neuroanatomy</source><volume>39</volume><fpage>112</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.jchemneu.2009.10.004</pub-id><pub-id pub-id-type="pmid">19900537</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonardo</surname><given-names>A</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Ensemble coding of vocal control in birdsong</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>652</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3036-04.2005</pub-id><pub-id pub-id-type="pmid">15659602</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynch</surname><given-names>GF</given-names></name><name><surname>Okubo</surname><given-names>TS</given-names></name><name><surname>Hanuschkin</surname><given-names>A</given-names></name><name><surname>Hahnloser</surname><given-names>RH</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rhythmic Continuous-Time coding in the songbird analog of vocal motor cortex</article-title><source>Neuron</source><volume>90</volume><fpage>877</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.021</pub-id><pub-id pub-id-type="pmid">27196977</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehaffey</surname><given-names>WH</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Naturalistic stimulation drives opposing heterosynaptic plasticity at two inputs to songbird cortex</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1272</fpage><lpage>1280</lpage><pub-id pub-id-type="doi">10.1038/nn.4078</pub-id><pub-id pub-id-type="pmid">26237364</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Memmesheimer</surname><given-names>RM</given-names></name><name><surname>Rubin</surname><given-names>R</given-names></name><name><surname>Olveczky</surname><given-names>BP</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning precisely timed spikes</article-title><source>Neuron</source><volume>82</volume><fpage>925</fpage><lpage>938</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.03.026</pub-id><pub-id pub-id-type="pmid">24768299</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perkel</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Origin of the anterior forebrain pathway</article-title><source>Annals of the New York Academy of Sciences</source><volume>1016</volume><fpage>736</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1196/annals.1298.039</pub-id><pub-id pub-id-type="pmid">15313803</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picardo</surname><given-names>MA</given-names></name><name><surname>Merel</surname><given-names>J</given-names></name><name><surname>Katlowitz</surname><given-names>KA</given-names></name><name><surname>Vallentin</surname><given-names>D</given-names></name><name><surname>Okobi</surname><given-names>DE</given-names></name><name><surname>Benezra</surname><given-names>SE</given-names></name><name><surname>Clary</surname><given-names>RC</given-names></name><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Long</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Population-Level representation of a temporal sequence underlying song production in the zebra finch</article-title><source>Neuron</source><volume>90</volume><fpage>866</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.016</pub-id><pub-id pub-id-type="pmid">27196976</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission</article-title><source>Neuron</source><volume>40</volume><fpage>1063</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00761-X</pub-id><pub-id pub-id-type="pmid">14687542</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simpson</surname><given-names>HB</given-names></name><name><surname>Vicario</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Brain pathways for learned and unlearned vocalizations differ in zebra finches</article-title><source>Journal of Neuroscience</source><volume>10</volume><fpage>1541</fpage><lpage>1556</lpage><pub-id pub-id-type="pmid">2332796</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiro</surname><given-names>JE</given-names></name><name><surname>Dalva</surname><given-names>MB</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Long-range inhibition within the zebra finch song nucleus RA can coordinate the firing of multiple projection neurons</article-title><source>Journal of Neurophysiology</source><volume>81</volume><fpage>3007</fpage><lpage>3020</lpage><pub-id pub-id-type="pmid">10368416</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stark</surname><given-names>LL</given-names></name><name><surname>Perkel</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Two-stage, input-specific synaptic maturation in a nucleus essential for vocal production in the zebra finch</article-title><source>Journal of Neuroscience</source><volume>19</volume><fpage>9107</fpage><lpage>9116</lpage><pub-id pub-id-type="pmid">10516328</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Teşileanu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>Code and data for paper on efficient two-stage learning in songbirds and beyond</data-title><source>GitHub</source><version>9c9739cf30a850a59545fe80d33ceaa65f25b230</version><uri xlink:href="https://github.com/ttesileanu/twostagelearning">https://github.com/ttesileanu/twostagelearning</uri></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tumer</surname><given-names>EC</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Performance variability enables adaptive plasticity of 'crystallized' adult birdsong</article-title><source>Nature</source><volume>450</volume><fpage>1240</fpage><lpage>1244</lpage><pub-id pub-id-type="doi">10.1038/nature06390</pub-id><pub-id pub-id-type="pmid">18097411</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>RS</given-names></name><name><surname>Desmurget</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Basal ganglia contributions to motor control: a vigorous tutor</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.08.022</pub-id><pub-id pub-id-type="pmid">20850966</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veit</surname><given-names>L</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Learning to breathe and sing: development of respiratory-vocal coordination in young songbirds</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>1747</fpage><lpage>1765</lpage><pub-id pub-id-type="doi">10.1152/jn.00247.2011</pub-id><pub-id pub-id-type="pmid">21697438</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>TL</given-names></name><name><surname>Tumer</surname><given-names>EC</given-names></name><name><surname>Charlesworth</surname><given-names>JD</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mechanisms and time course of vocal learning and consolidation in the adult songbird</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>1806</fpage><lpage>1821</lpage><pub-id pub-id-type="doi">10.1152/jn.00311.2011</pub-id><pub-id pub-id-type="pmid">21734110</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>AC</given-names></name><name><surname>Margoliash</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Temporal hierarchical control of singing in birds</article-title><source>Science</source><volume>273</volume><fpage>1871</fpage><lpage>1875</lpage><pub-id pub-id-type="doi">10.1126/science.273.5283.1871</pub-id><pub-id pub-id-type="pmid">8791594</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ölveczky</surname><given-names>BP</given-names></name><name><surname>Andalman</surname><given-names>AS</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Vocal experimentation in the juvenile songbird requires a basal ganglia circuit</article-title><source>PLoS Biology</source><volume>3</volume><elocation-id>e153</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030153</pub-id><pub-id pub-id-type="pmid">15826219</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ölveczky</surname><given-names>BP</given-names></name><name><surname>Otchy</surname><given-names>TM</given-names></name><name><surname>Goldberg</surname><given-names>JH</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Changes in the neural control of a complex motor sequence during learning</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>386</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1152/jn.00018.2011</pub-id><pub-id pub-id-type="pmid">21543758</pub-id></element-citation></ref></ref-list><app-group><app id="app1"><title>Appendix 1</title><boxed-text><sec id="s18" sec-type="appendix"><title>Effect of nonlinearities</title><p>We can generalize the model from <xref ref-type="disp-formula" rid="equ8">Equation (8)</xref> by using a nonlinear transfer function from student activities to motor output, and a nonlinear activation function for student neurons:<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>inh</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Suppose further that we use a general loss function,<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo maxsize="120%" minsize="120%" rspace="4.2pt">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>t</mml:mi></mml:mpadded></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Following the same argument as that from section &quot;Derivation of the matching tutor signal&quot;, the gradient descent condition, <xref ref-type="disp-formula" rid="equ18">Equation (18)</xref>, implies<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The departure from the quadratic loss function, <inline-formula><mml:math id="inf268"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>≠</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>a</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and the nonlinearities in the output, <inline-formula><mml:math id="inf269"><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula>, have the effect of redefining the motor error,<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>A proper loss function will be such that the derivatives <inline-formula><mml:math id="inf270"><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> vanish when <inline-formula><mml:math id="inf271"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and so the motor error <inline-formula><mml:math id="inf272"><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> as defined here is zero when the rendition is perfect, as expected. If we use a tutor that ignores the nonlinearities in a nonlinear system, <italic>i.e.,</italic> if we use <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> instead of <xref ref-type="disp-formula" rid="equ26">Equation (26)</xref> to calculate the tutor signal that is plugged into <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref>, we still expect successful learning provided that <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and that <inline-formula><mml:math id="inf274"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:math></inline-formula> is itself an increasing function of <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (see section &quot;Effect of different output functions&quot;). This is because replacing <xref ref-type="disp-formula" rid="equ26">Equation (26)</xref> with <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> would affect the magnitude of the motor error without significantly changing its direction. In more complicated scenarios, if the transfer function to the output is not monotonic, there is the potential that using <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref> would push the system away from convergence instead of towards it. In such a case, an adaptive mechanism, such as the reinforcement rules from <xref ref-type="disp-formula" rid="equ7">Equations (6) or (7)</xref> can be used to adapt to the local values of the derivatives <inline-formula><mml:math id="inf276"><mml:msubsup><mml:mi>N</mml:mi><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf277"><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Finally, the nonlinear activation function <inline-formula><mml:math id="inf278"><mml:mi>F</mml:mi></mml:math></inline-formula> introduces a dependence on the student output <inline-formula><mml:math id="inf279"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ25">Equation (25)</xref>, since <inline-formula><mml:math id="inf280"><mml:msup><mml:mi>F</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> is evaluated at <inline-formula><mml:math id="inf281"><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To obtain a good match between the student and the tutor in this context, we can modify the student plasticity rule (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>) by adding a dependence on the postsynaptic activity,<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>η</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In general, synaptic plasticity has been observed to indeed depend on postsynaptic activity (<xref ref-type="bibr" rid="bib5">Chistiakova et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Chistiakova and Volgushev, 2009</xref>). Our derivation suggests that the effectiveness of learning could be improved by tuning this dependence of synaptic change on postsynaptic activity to the activation function of postsynaptic neurons, according to <xref ref-type="disp-formula" rid="equ27">Equation (27)</xref>. It would be interesting to check whether such tuning occurs in real neurons.</p></sec><sec id="s19" sec-type="appendix"><title>Effect of different output functions</title><p>In the main text, we assumed a linear mapping between student activities and motor output. Moreover, we assumed a myotopic organization, in which each student neuron projected to a single muscle, leading to a student–output assignment matrix <inline-formula><mml:math id="inf282"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in which each column had a single non-zero entry. We also assumed that student neurons only contributed additively to the outputs, with no inhibitory activity. Here we show that our results hold for other choices of student–output mappings.</p><p>For example, assume a push-pull architecture, in which half of the student neurons controlling one output are excitatory and half are inhibitory. This can be used to decouple the overall firing rate in the student from the magnitude of the outputs. Learning works just as effectively as in the case of the purely additive student–output mapping when using matched tutors, <xref ref-type="fig" rid="fig7">Appendix 1—figures 1A and 1B</xref>. The consequences of mismatching student and tutor circuits are also not significantly changed, <xref ref-type="fig" rid="fig7">Appendix 1—figures 1C and 1D</xref>.</p><p>We can also consider nonlinear mappings between the student activity and the final output. If there is a monotonic output nonlinearity, as in <xref ref-type="disp-formula" rid="equ23">Equation (23)</xref> with <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the tutor signal derived for the linear case, <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref>, can still achieve convergence, though at a slower rate and with a somewhat lower accuracy (see <xref ref-type="fig" rid="fig7">Appendix 1—figure 1E</xref> for the case of a sigmoidal nonlinearity). For non-monotonic nonlinearities, the direction from which the optimum is approached can be crucial, as learning can get stuck in local minima of the loss function (we thank Josh Gold for this observation). Studying this might provide an interesting avenue to test whether learning in songbirds is based on a gradient descent-type rule or on a more sophisticated optimization technique.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.016</object-id><label>Appendix 1—figure 1.</label><caption><title>Robustness of learning.</title><p>(<bold>A</bold>) Error trace showing how average motor error evolves with repetitions of the motor program for rate-based plasticity paired with a matching tutor, when the student–output mapping has a push-pull architecture. The inset shows the final motor output (thick red line) compared to the target output (dotted black line). The output on the first rendition and at two other stages of learning are also shown. (<bold>B</bold>) The error trace and final motor output shown for timing-based plasticity matched by a tutor with a long integration timescale. (<bold>C</bold>) Effects of mismatch between student and tutor on reproduction accuracy when using a push-pull architecture for the student–output mapping. The heatmap shows the final reproduction error of the motor output after 1000 learning cycles when a student with plasticity parameters <inline-formula><mml:math id="inf284"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf285"><mml:mi>β</mml:mi></mml:math></inline-formula> is paired with a tutor with memory timescale <inline-formula><mml:math id="inf286"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf287"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf288"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>40</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>D</bold>) Error evolution curves as a function of the mismatch between student and tutor. Each plot shows how the error in the motor program changes during 1000 learning cycles for the same conditions as those shown in the heatmap. The region shaded in light pink shows simulations where the mismatch between student and tutor leads to a deteriorating instead of improving performance during learning. (<bold>E</bold>) Convergence in the rate-based model with a linear-nonlinear controller that uses a sigmoidal nonlinearity. (<bold>F</bold>) Convergence in the spiking model when inhibition is constant instead of activity-dependent (<inline-formula><mml:math id="inf289"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mtext>inh</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>constant</mml:mtext></mml:mrow></mml:math></inline-formula>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.016">http://dx.doi.org/10.7554/eLife.20944.016</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-app1-fig1-v1"/></fig></p></sec><sec id="s20" sec-type="appendix"><title>Different inhibition models</title><p>In the spiking model, we used an activity-dependent inhibitory signal that was proportional to the average student activity. Using a constant inhibition instead, <inline-formula><mml:math id="inf290"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mtext>inh</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>constant</mml:mtext></mml:mrow></mml:math></inline-formula>, does not significantly change the results: see <xref ref-type="fig" rid="fig7">Appendix 1—figure 1F</xref> for an example.</p></sec><sec id="s21" sec-type="appendix"><title>Effect of changing plasticity kernels</title><p>In the main text, we used exponential kernels with <inline-formula><mml:math id="inf291"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf292"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>40</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for the smoothing of the conductor signal that enters the synaptic plasticity rule, <xref ref-type="disp-formula" rid="equ10">Equation (10)</xref>. We can generalize this in two ways: we can use different timescales <inline-formula><mml:math id="inf293"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf294"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, or we can use a different functional form for the kernels. (Note that in the main text we showed the effects of varying the parameters <inline-formula><mml:math id="inf295"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf296"><mml:mi>β</mml:mi></mml:math></inline-formula> in the plasticity rule, while the timescales <inline-formula><mml:math id="inf297"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf298"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> were kept fixed.)</p><p>The values for the timescales <inline-formula><mml:math id="inf299"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> were chosen to roughly match the shape of the plasticity curve measured in slices of zebra finch RA (<xref ref-type="bibr" rid="bib27">Mehaffey and Doupe, 2015</xref>) (see <xref ref-type="fig" rid="fig1">Figure 1C and D</xref>). The main predictions of our model, that learning is most effective when the tutor signal is matched to the student plasticity rule, and that large mismatches between tutor and student lead to impaired learning, hold well when the student timescales change: see <xref ref-type="fig" rid="fig8">Appendix 1—figure 2A</xref> for the case when <inline-formula><mml:math id="inf300"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>20</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf301"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>10</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In the main text we saw that the negative effects of tutor–student mismatch diminish for timescales that are shorter than <inline-formula><mml:math id="inf302"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig8">Appendix 1—figure 2A</xref>, the range of timescales where a precise matching is not essential becomes very small because the student timescales are short.<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.017</object-id><label>Appendix 1—figure 2.</label><caption><title>Effect of changing conductor smoothing kernels in the plasticity rule.</title><p>(<bold>A</bold>) Matrix showing learning accuracy when using different timescales for the student plasticity rule. Each entry in the heatmap shows the average rendition error after 1000 learning steps when pairing a tutor with timescale <inline-formula><mml:math id="inf303"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> with a non-matched student. Here the kernels are exponential, with timescales <inline-formula><mml:math id="inf304"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>20</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf305"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>10</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) Evolution of motor error with learning using kernels <inline-formula><mml:math id="inf306"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf307"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, instead of the two exponentials used in the main text. The tutor signal is as before, <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref>. The inset shows the final output for the trained model, for one of the two output channels. Learning is as effective and fast as before.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.017">http://dx.doi.org/10.7554/eLife.20944.017</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-app1-fig2-v1"/></fig></p><p>Another generalization of our plasticity rule can be obtained by changing the functional form of the kernels used to smooth the conductor input. As an example, suppose <inline-formula><mml:math id="inf308"><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is kept exponential, while <inline-formula><mml:math id="inf309"><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is replaced by<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mtext>else.</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>An example of learning using an STDP rule based on kernels <inline-formula><mml:math id="inf310"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf311"><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> where <inline-formula><mml:math id="inf312"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is shown in <xref ref-type="fig" rid="fig8">Appendix 1—figure 2B</xref>. The matching tutor has the same form as before, <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref> with timescale <inline-formula><mml:math id="inf313"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> given by <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>, but with <inline-formula><mml:math id="inf314"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. We can see that learning is as effective as in the case of purely exponential kernels.</p></sec><sec id="s22" sec-type="appendix"><title>More general conductor patterns</title><p>In the main text, we have focused on a conductor whose activity matches that observed in area HVC of songbirds (<xref ref-type="bibr" rid="bib15">Hahnloser et al., 2002</xref>): each neuron fires a single burst during the motor program. Our model, however, is not restricted to this case. We generated alternative conductor patterns by using arbitrarily-placed bursts of activity, as in <xref ref-type="fig" rid="fig9">Appendix 1—figure 3A</xref>. The model converges to a good rendition of the target program, <xref ref-type="fig" rid="fig9">Appendix 1—figure 3B</xref>. Learning is harder in this case because many conductor neurons can be active at the same time, and the weight updates affect not only the output of the system at the current position in the motor program, but also at all the other positions where the conductor neurons fire. This is in contrast to the HVC-like conductor, where each neuron fires at a single point in the motor program, and thus the effect of weight updates is better localized. More generally, simulations show that the sparser the conductor firing, the faster the convergence (data not shown). The accuracy of the final rendition of the motor program (<xref ref-type="fig" rid="fig9">Appendix 1—figure 3B</xref>, inset) is also not as good as before.<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.018</object-id><label>Appendix 1—figure 3.</label><caption><title>Learning with arbitrary conductor activity.</title><p>(<bold>A</bold>). Typical activity of conductor neurons. 20 of the 100 neurons included in the simulation are shown. The activity pattern is chosen so that about 10% of the neurons are active at any given time. The pattern is chosen randomly but is fixed during learning. Each conductor burst lasts <inline-formula><mml:math id="inf315"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>30</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>. (<bold>B</bold>) Convergence curve and final rendition of the motor program (in inset). Learning included two output channels but the final output is shown for only one of them.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.018">http://dx.doi.org/10.7554/eLife.20944.018</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-app1-fig3-v1"/></fig></p></sec><sec id="s23" sec-type="appendix"><title>Edge effects</title><p>In our derivation of the matching tutor rule, we assumed that the system has enough time to integrate all the synaptic weight changes from <xref ref-type="disp-formula" rid="equ10">Equation (10)</xref>. However, some of these changes occur tens or hundreds of milliseconds after the inputs that generated them, due to the timescales used in the plasticity kernel. Since our simulations are only run for a finite amount of time, there will in general be edge effects, where periods of the motor program towards the end of the simulations will have difficulty converging. To offset such numerical issues, we ran the simulations for a few hundred milliseconds longer than the duration of the motor program, and ignored the data from this extra period. Our simulations typically run for <inline-formula><mml:math id="inf316"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>600</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>, and the time reserved for relaxation after the end of the program was set to <inline-formula><mml:math id="inf317"><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>1200</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:math></inline-formula>. The long relaxation time was chosen to allow for cases where the tutor was chosen to have a very long memory timescale.</p></sec><sec id="s24" sec-type="appendix"><title>Parameter optimization for reproducing juvenile and adult spiking statistics</title><p>We set the parameters in our simulations to reproduce spiking statistics from recordings in zebra finch RA as closely as possible. <xref ref-type="fig" rid="fig10">Appendix 1—figure 4</xref> shows how the distribution of summary statistics obtained from 50 runs of the simulation compares to the distributions calculated from recordings in birds at various developmental stages. Each plot shows a standard box and whisker plot superimposed over a kernel-density estimate of the distribution of a given summary statistic, either over simulation runs or over recordings from birds at various stages of song learning. We ran two sets of simulations, one for a bird with juvenile-like connectivity between HVC and RA, and one with adult-like connectivity (see Materials and methods). In these simulations there was no learning to match the timecourse of songs—the goal was simply to identify parameters that lead to birdsong-like firing statistics.<fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.20944.019</object-id><label>Appendix 1—figure 4.</label><caption><title>Violin plots showing how the spiking statistics from our simulation compared to the statistics obtained from neural recordings.</title><p>Each violin shows a kernel-density estimate of the distribution that a particular summary statistic had in either several runs of a simulation, or in several recordings from behaving birds. The circle and the box within each violin show the median and the interquartile range.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.20944.019">http://dx.doi.org/10.7554/eLife.20944.019</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-20944-app1-fig4-v1"/></fig></p><p>The qualitative match between our simulations and recordings is good, but the simulations are less variable than the measurements. This may be due to sources of variability that we have ignored—for example, all our simulated neurons had exactly the same membrane time constants, refractory periods, and threshold potentials, which is not the case for real neurons. Another reason might be that in our simulations, all the runs were performed for the same network, while the measurements are from different cells in different birds.</p></sec><sec id="s25" sec-type="appendix"><title>Effect of spiking stochasticity on learning</title><p>As pointed out in the main text, learning is affected in the spiking simulations when the tutor error integration timescale <inline-formula><mml:math id="inf318"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> becomes very long. More specifically, two distinct effects occur. First, the fluctuations in the motor output increase, leading to a poorer match to the shape of the target motor program. And second, the whole output gets shifted up, towards higher muscle activation values. Both of these effects can be traced back to the stochasticity of the tutor signal.</p><p>In the spiking simulations, tutor neurons are assumed to fire Poisson spikes following a time-dependent firing rate that obeys <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref>. By the nature of the Poisson process, the tutor output in this case will contain fluctuations around the mean, <inline-formula><mml:math id="inf319"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Recall that the scale of <inline-formula><mml:math id="inf320"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is set by the threshold <inline-formula><mml:math id="inf321"><mml:mi>θ</mml:mi></mml:math></inline-formula> and thus, since this is a Poisson process, so is the scale of the variability <inline-formula><mml:math id="inf322"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>As long as the tutor error integration timescale is not very long, <inline-formula><mml:math id="inf323"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> roughly corresponds to a smoothed version of the motor error <inline-formula><mml:math id="inf324"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<italic>cf.</italic> <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). However, as <inline-formula><mml:math id="inf325"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula> grows past the duration <inline-formula><mml:math id="inf326"><mml:mi>T</mml:mi></mml:math></inline-formula> of the motor program, the exponential term in <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref> becomes essentially constant, leading to a tutor signal <inline-formula><mml:math id="inf327"><mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> whose departures from the center value <inline-formula><mml:math id="inf328"><mml:mi>θ</mml:mi></mml:math></inline-formula> decrease in proportion to the timescale <inline-formula><mml:math id="inf329"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:math></inline-formula>. As far as the student is concerned, the relevant signal is <inline-formula><mml:math id="inf330"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), and thus, when <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>tutor</mml:mtext></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the signal-to-noise ratio in the tutor guiding signal starts to decrease as <inline-formula><mml:math id="inf332"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>. This ultimately leads to a very noisy rendition of the target program. One way to improve this would be to increase the gain factor <inline-formula><mml:math id="inf333"><mml:mi>ζ</mml:mi></mml:math></inline-formula> that controls the relation between the motor error and the tutor signal (see <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). This improves the ability of the system to converge onto its target in the late stages of learning. In the early stages of learning, however, this could lead to saturation problems. One way to fix this would be to use a variable gain factor <inline-formula><mml:math id="inf334"><mml:mi>ζ</mml:mi></mml:math></inline-formula> that ensures the whole range of tutor firing rates is used without generating too much saturation. This would be an interesting avenue for future research.</p><p>Reducing the fluctuations in the tutor signal also decreases the fluctuations in the conductor–student synaptic weights, which leads to fewer weights being clamped at 0 because of the positivity constraint. This reduces the shift between the learned motor program and the target. As mentioned in the main text, another approach to reducing or eliminating this shift is to allow for negative weights or (more realistically) to use a push-pull mechanism, in which the activity of some student neurons acts to increase muscle output, while the activity of other student neurons acts as an inhibition on muscle output.</p></sec></boxed-text></app><app id="app2"><title>Appendix 2</title><boxed-text><sec id="s26" sec-type="appendix"><title>Plasticity parameter values</title><p>In the heatmaps that appear in many of the figures in the main text and in the supplementary information, we kept the timescales <inline-formula><mml:math id="inf335"><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf336"><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> constant while varying <inline-formula><mml:math id="inf337"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf338"><mml:mi>β</mml:mi></mml:math></inline-formula> to modify the student plasticity rule. Since the overall scale of <inline-formula><mml:math id="inf339"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf340"><mml:mi>β</mml:mi></mml:math></inline-formula> is inconsequential as it can be absorbed into the learning rate (as explained in the section &quot;Learning in a rate-based model&quot;), we imposed the further constraint <inline-formula><mml:math id="inf341"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. This implies that we effectively focused on a one-parameter family of student plasticity rule, as identified by the value of <inline-formula><mml:math id="inf342"><mml:mi>α</mml:mi></mml:math></inline-formula> (and the corresponding value for <inline-formula><mml:math id="inf343"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). In the figures, we expressed this instead in terms of the timescale of the optimally-matching tutor, <inline-formula><mml:math id="inf344"><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula>, as defined in <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>.</p><p>Below we give the explicit values of <inline-formula><mml:math id="inf345"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf346"><mml:mi>β</mml:mi></mml:math></inline-formula> that we used for each row in the heatmaps. These can be calculated by solving for <inline-formula><mml:math id="inf347"><mml:mi>α</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>, using <inline-formula><mml:math id="inf348"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and assuming that <inline-formula><mml:math id="inf349"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>80</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf350"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>40</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><table-wrap id="tblu1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th><inline-formula><mml:math id="inf351"><mml:msubsup><mml:mi>τ</mml:mi><mml:mtext>tutor</mml:mtext><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> (ms)</th><th><inline-formula><mml:math id="inf352"><mml:mi>α</mml:mi></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf353"><mml:mi>β</mml:mi></mml:math></inline-formula></th></tr></thead><tbody><tr><td>10</td><td><inline-formula><mml:math id="inf354"><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf355"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.75</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>20</td><td><inline-formula><mml:math id="inf356"><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf357"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>40</td><td><inline-formula><mml:math id="inf358"><mml:mn>0.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf359"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td>80</td><td><inline-formula><mml:math id="inf360"><mml:mn>1.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf361"><mml:mn>0.0</mml:mn></mml:math></inline-formula></td></tr><tr><td>160</td><td><inline-formula><mml:math id="inf362"><mml:mn>3.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf363"><mml:mn>2.0</mml:mn></mml:math></inline-formula></td></tr><tr><td>320</td><td><inline-formula><mml:math id="inf364"><mml:mn>7.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf365"><mml:mn>6.0</mml:mn></mml:math></inline-formula></td></tr><tr><td>640</td><td><inline-formula><mml:math id="inf366"><mml:mn>15.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf367"><mml:mn>14.0</mml:mn></mml:math></inline-formula></td></tr><tr><td>1280</td><td><inline-formula><mml:math id="inf368"><mml:mn>31.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf369"><mml:mn>30.0</mml:mn></mml:math></inline-formula></td></tr><tr><td>2560</td><td><inline-formula><mml:math id="inf370"><mml:mn>63.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf371"><mml:mn>62.0</mml:mn></mml:math></inline-formula></td></tr><tr><td>5120</td><td><inline-formula><mml:math id="inf372"><mml:mn>127.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf373"><mml:mn>126.0</mml:mn></mml:math></inline-formula></td></tr><tr><td>10240</td><td><inline-formula><mml:math id="inf374"><mml:mn>255.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf375"><mml:mn>254.0</mml:mn></mml:math></inline-formula></td></tr><tr><td>20480</td><td><inline-formula><mml:math id="inf376"><mml:mn>511.0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf377"><mml:mn>510.0</mml:mn></mml:math></inline-formula></td></tr></tbody></table></table-wrap></p></sec></boxed-text></app></app-group></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.20944.020</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing editor</role><aff id="aff6"><institution>Brown University</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Matching tutor to student: rules and mechanisms for efficient two-stage learning in neural circuits&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by Michael Frank as the Reviewing Editor and Andrew King as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is a careful and well-designed study of the role of the matching between plasticity rules and instructive signals for efficient learning in a two-stage model, wherein the reinforcement signal is processed by one circuit that in turn provides input to another circuit that drives the actual behavior. This is particularly applicable to recent detailed findings in the birdsong field. Several important results are laid out in the manuscript: (1) matching is important for fast learning, (2) matching is not as important when the tutor timescale is shorter than STDP timescales (3) similar results are obtained for rate-based or spiking-based models, (4) learning drives student neurons to become more bursty, (5) learning drives pruning of connections in spiking networks, and (6) sequential learning is obtained when the tutor memory is long.</p><p>Essential revisions:</p><p>All involved found the work to be of high quality and of broad interest to researchers in sensorimotor and reinforcement learning, as well as being of particular interest to those working on neural recording in the songbird brain.</p><p>1) However, an invigorating consultation session among the reviewers was needed to get down to the bottom of whether some of the main modeling results were circular/trivial or not. It was determined that in fact they are not, and that rather some of them show that the model is robust to the precise form of the motor error. But the reason for this initial difference in opinion could be traced to a lack of clarity in the manuscript over the definition of tau_tutor. One reviewer originally understood it as the timescale over which the firing rate of the tutor neurons is modulated, which led to the conclusion that g(t) is constrained to have a timescale of tau_tutor, which would mean that the main finding that your results &quot;predict the temporal structure of the [LMAN signal] given a plasticity rule&quot; – would be a trivial consequence of the plasticity rule….</p><p>However, as explained in a small sentence above Equation 1, what tau_tutor actually refers to is the timescale over which motor errors are integrated into the firing of tutor neurons. This definition of tau_tutor is specific to the Taylor series approximation taken in A.8 and hence, their matching results point to the robustness of the model. One way to summarize the finding is if you want to use an STDP rule in motor learning, you should convolve your motor error signal with a particular exponential smoothing function, with a particular timescale related to the plasticity rule, independent of the form of the motor error signal.</p><p>Another interesting observation is that their derivation for the timescale tau_tutor is not valid when tau_tutor is of a comparable magnitude to tau1 and tau2. Stated simply, they make an assumption in their derivation that tau_tutor &gt;&gt; tau1 and tau2. Therefore, they don't see clear matching in areas where this assumption is violated. A discussion of this in subsection “Match vs. unmatched learning” may also improve the readability of the paper.</p><p>The authors should comment on what they would conclude if one observed a different form for g(t) in the songbird brain. It's a subtle point – a g(t) with the \tau^* they predict would certainly shore up their theory, but one with a different form might mean either a) that their theory is wrong, or b) that their approximation (A.8) is wrong.</p><p>2) Both reviewers strongly agreed that more is needed to motivate what the model predicts. In the Abstract and Discussion, a claim is made that the results presented here make clear and testable predictions for experimentalists. The manuscript would be greatly improved if this were made more explicit and specific. Indeed, explicitly connecting the model to falsifiable experimental predictions would make the difference between this being a paper of truly broad interest.</p><p>Perhaps the authors envision revealing the precise form of the plasticity rule by looking at the structure of the tutor signal. Perhaps they envision inferring the form of the motor error signal. Perhaps they will test coarser effects that result from this kind of tutor signal, like the progressive clumping of student responses into burst-like events during learning. If these predictions were laid out more clearly, concerns about what exactly is being claimed through the calculation in Appendix 3 would be ameliorated.</p><p>A couple of other suggestions for how the manuscript might be revised are given here:</p><p>– The point \α = \β is the case that corresponds to the study by Mehaffey and Doupe, so a detailed discussion of the tutor \tau's that yield reasonable learning in that part of parameter space would be useful. How big is the range of \tau_{tutor}'s in that case? Does it span many orders of magnitude? How well does the model activity at these points align with real LMAN activity in the developing bird?</p><p>– What precise features of the firing of LMAN neurons are predicted for the models that yield efficient learning and match known plasticity features? What should an experimentalist measure in their spike trains? Putting some numbers to the predictions would be great.</p><p>3) One reviewer thought that <xref ref-type="fig" rid="fig3">Figure 3C</xref> is not very convincing that learning is good only along the diagonal. It does not seem to matter at short timescales (which the authors discuss briefly), where performance is equivalent regardless of whether timescales are matched or unmatched (e.g. 10 ms vs 80 ms). But even at timescales &gt;80 ms, the performance does not seem strictly diagonal, and the asymptotic performance of the model is significantly worse than at shorter timescales. Therefore it seems like the two key findings – timescale matching and the ability to learn – do not coexist. The other reviewer noted that the error is on a log scale is very important here. There is a steep valley sloping towards the diagonal. The larger error reached at longer timescales was thus less worrisome. 3B shows that this amount of error doesn't stem from huge differences in the output time course. We would therefore like to ask the authors for another plot like B at yet longer timescales to see how and if things break down. Also, 250 iterations isn't that many in terms of number of renditions sung during development, and we might want to see what the error surface looks like after a few thousand. Finally, the paper does discuss the flattening of the valley at short timescales, but this could be expanded.</p><p>4. It seems misleading to call τ1 and τ2 the timescales of synaptic plasticity of the student. They are simply timescales of two exponential kernels that are linearly combined to create the final filtering kernel. If the authors want to relate τ1 and τ2 to the timescales of synaptic plasticity, they should include a discussion of how and why they consider them to be related. In general it would be helpful if the authors are more clear in defining their variables through the paper – especially since tau1 and tau2 are very different from tau_tutor.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Rules and mechanisms for efficient two-stage learning in neural circuits&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Andrew King (Senior editor), Reviewing editor Michael Frank, and one external reviewer.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined clearly by the reviewer below:</p><p>By and large we are happy with the changes the authors have made.</p><p>However, there are still a few important aspects of the paper that need to be significantly revised to improve clarity and address some issues of interpretation:</p><p>1) The (qualitative) definition of tau_tutor could still be much clearer. When we first read the initial submission, we mistakenly thought that tau_tutor represented the timescale over which the firing rate of the tutor neurons is modulated, rather than (as is actually the case) the timescale over which motor errors are integrated into the firing of tutor neurons. Based on our comments from last time, the authors have somewhat clarified this, however this could still be more clear (in fact, I got confused in the same way reading the revised paper). We think this could be easily clarified with a couple of minor changes</p><p>a) Just above Equation 4, revise to &quot;Moreover, for effective learning, the timescale t_tutor – ***which quantifies the timescale on which error information is integrated into the tutor signal*** – appearing in Equation 3…&quot; (or something similar).</p><p>b) Be a bit more specific when talking about &quot;matching&quot;. For example, the first sentence in 2.3 simply reads &quot;…when the tutor is matched to the student…&quot;. I think it would be much better (especially for less quantitative readers) to explicitly state what is being matched by expanding this to read something like &quot;…when the ***timescale on which error information is integrated into the tutor signal (tau_tutor) is matched to the student plasticity rule***…&quot;.</p><p>c) More generally, throughout the paper replace the term &quot;tutor timescale&quot; – which many biologists will misinterpret as the timescale on which tutor activity varies – with &quot;tutor error integration timescale&quot; or something like this.</p><p>2) We appreciate the authors expanding the duration of simulations and range of timescales tested in the simulations shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>. However, the authors should be more explicit (and consistent) in explaining how these simulations are conducted. My understanding is that in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, tau_1 and tau_2 are fixed at [80 40]msec, and to obtain a particular t_tutor*, plasticity parameters α and β are adjusted for each simulation (subject to the constraint α-β=1). Provided this is correct, the fact that α and β were different for different simulations (i.e. for different squares in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, <xref ref-type="fig" rid="fig5">5D</xref>) should be explicitly stated in the results text and/or legend to <xref ref-type="fig" rid="fig3">Figure 3</xref>. Similarly, as far as I can tell a similar approach (fix tau_1,2, vary α/β to get a particular tau_tutor*) was used in the analysis shown in <xref ref-type="fig" rid="fig5">Figure 5D</xref>. However, this is not made clear in the text/caption (I find the caption to 5D especially confusing). Authors should explicitly state the procedures for 5D and explain any differences in the general methods used in 3C. Furthermore, the authors should present (maybe as a supplemental figure) the values of α or β used for all simulations (e.g. a heatmap of α values to go along with <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>).</p><p>3) The following is a somewhat subtle point, so I'll leave it up to the Authors to deal with as they see fit.</p><p>As noted above, tau_tutor is the timescale over which motor errors are integrated into the firing of tutor neurons. Furthermore, the various models (linear rate, spiking, etc.) can work over a big range of tau_tutors, say from 10ms-1280msec, as shown in <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>. In the revised Discussion, the authors correctly assert that LMAN/tutor firing &quot;should reflect the integral of the motor error with the timescale predicted by the model&quot;. The authors also correctly point out that we don't really know what the motor error signal looks like.</p><p>However – a key physiological fact is that firing in the &quot;tutor&quot; brain area – LMAN – consists of both short bursts (~15 msec duration) as well as single spikes whose overall rate varies much more slowly. So, although we don't know how fast the error signal varies, even if the error signal were a pulse that only lasted, say, 1 msec, it would affect the firing of the LMAN neuron on timescale tau_tutor – e.g. over &gt;100 msec if tau_tutor=100. So, if I'm getting this right, it seems implausible that a 15 msec burst in LMAN could possibly reflect motor error – even an infinitely brief motor error – if tau_tutor were longer than, say, 50 msec. The authors may want to discuss this issue, especially whether their model suggests that only slow changes in the rate of single-spikes, and not bursts, are carrying error information? I think that such an implication is both important for understanding the implications of the model and for guiding future work in the system.</p><p>Additionally, as indicated just below Equation 4, the model assumes that tau_tutor&gt;&gt;tau_1,2. In many simulations (definitely in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, also I think in <xref ref-type="fig" rid="fig5">Figure 5D</xref>, see my question above), tau_1 and tau_2 have values of 40 and 80 msec, which are values derived from the Mehaffey and Doupe STDP paper. This would suggest that tau_tutor should be &gt;&gt;80 msec, and that cases with tau_tutor and tau_tutor* less than this value (which make up a significant region of the parameter space shown in <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>) are biologically implausible. This seems to me to be something the authors should address.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.20944.021</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>All involved found the work to be of high quality and of broad interest to researchers in sensorimotor and reinforcement learning, as well as being of particular interest to those working on neural recording in the songbird brain.</italic> </p><p><italic>1) However, an invigorating consultation session among the reviewers was needed to get down to the bottom of whether some of the main modeling results were circular/trivial or not. It was determined that in fact they are not, and that rather some of them show that the model is robust to the precise form of the motor error. But the reason for this initial difference in opinion could be traced to a lack of clarity in the manuscript over the definition of tau_tutor. One reviewer originally understood it as the timescale over which the firing rate of the tutor neurons is modulated, which led to the conclusion that g(t) is constrained to have a timescale of tau_tutor, which would mean that the main finding that your results &quot;predict the temporal structure of the [LMAN signal] given a plasticity rule&quot; – would be a trivial consequence of the plasticity rule….</italic> </p><p><italic>However, as explained in a small sentence above Equation 1, what tau_tutor actually refers to is the timescale over which motor errors are integrated into the firing of tutor neurons. This definition of tau_tutor is specific to the Taylor series approximation taken in A.8 and hence, their matching results point to the robustness of the model. One way to summarize the finding is if you want to use an STDP rule in motor learning, you should convolve your motor error signal with a particular exponential smoothing function, with a particular timescale related to the plasticity rule, independent of the form of the motor error signal.</italic> </p><p>We are grateful for this suggestion. Indeed, upon re-reading we realized that the text could have been clearer on this point. Your suggested explanation is indeed clear and accurate and we have incorporated a version of this text below Equation 4.</p><p><italic>Another interesting observation is that their derivation for the timescale tau_tutor is not valid when tau_tutor is of a comparable magnitude to tau1 and tau2. Stated simply, they make an assumption in their derivation that tau_tutor &gt;&gt; tau1 and tau2. Therefore, they don't see clear matching in areas where this assumption is violated. A discussion of this in subsection “Match vs. unmatched learning” may also improve the readability of the paper.</italic> </p><p>We have added a discussion of these assumptions in our derivation at the end of section “Learning in a rate-based model”. Since the plasticity rule smooths conductor inputs over timescales of order tau_1 and tau_2, tutor signals that vary on shorter timescales do not have a strong effect on learning, justifying the approximation. It is also possible to improve the approximation we are making by including more terms in the Taylor expansion that we use in section A.3. We added a note about this in the Discussion.</p><p><italic>The authors should comment on what they would conclude if one observed a different form for g(t) in the songbird brain. It's a subtle point – a g(t) with the \tau^* they predict would certainly shore up their theory, but one with a different form might mean either a) that their theory is wrong, or b) that their approximation (A.8) is wrong.</italic> </p><p>This is a very good point, and we thank the reviewers for encouraging us to discuss this issue. We added a few sentences pointing out these different ways of interpreting a potential mismatch between theory and experiment in this case (Discussion section). We have also mentioned that we could improve the approximation by going to the next order in the Taylor series expansion that we used to derive the matched tutor.</p><p><italic>2) Both reviewers strongly agreed that more is needed to motivate what the model predicts. In the Abstract and Discussion, a claim is made that the results presented here make clear and testable predictions for experimentalists. The manuscript would be greatly improved if this were made more explicit and specific. Indeed, explicitly connecting the model to falsifiable experimental predictions would make the difference between this being a paper of truly broad interest.</italic> </p><p><italic>Perhaps the authors envision revealing the precise form of the plasticity rule by looking at the structure of the tutor signal. Perhaps they envision inferring the form of the motor error signal. Perhaps they will test coarser effects that result from this kind of tutor signal, like the progressive clumping of student responses into burst-like events during learning. If these predictions were laid out more clearly, concerns about what exactly is being claimed through the calculation in Appendix 3 would be ameliorated.</italic> </p><p>Our model provides a general method for calculating the tutor signal needed by a student circuit during learning, under the assumption that the student and tutor circuits have evolved together to maximize learning efficiency. This can be used to make predictions in several ways, as we now explain in the Discussion: (a) we could use recordings from tutor neurons together with measurements of motor error to infer the student plasticity rule; (b) we could conversely use the plasticity rule with the motor error to predict tutor spiking statistics; (c) we could electrically or optogenetically stimulate tutor neurons and test whether the student learns as predicted by our model.</p><p><italic>A couple of other suggestions for how the manuscript might be revised are given here:</italic> </p><p><italic>– The point \α = \β is the case that corresponds to the study by Mehaffey and Doupe, so a detailed discussion of the tutor \tau's that yield reasonable learning in that part of parameter space would be useful. How big is the range of \tau_{tutor}'s in that case? Does it span many orders of magnitude? How well does the model activity at these points align with real LMAN activity in the developing bird?</italic> </p><p>We added a comment about the range of tau_tutor’s that allow effective learning to the bottom of section “Matched vs. unmatched learning”. Because of the normalization we use, which sets α – β = 1, we cannot directly address the question of what happens when α = β. However, the Mehaffey &amp; Doupe data can be explained just as well using a plasticity model that has α large, α&gt;&gt;1, which would set α and β approximately equal to each other (leading to a large tau_tutor^*). This implies that the Mehaffey &amp; Doupe case corresponds to the bottom part of the plots in <xref ref-type="fig" rid="fig3">Figures 3C and D</xref>.</p><p>A precise comparison of LMAN activity and our model would require identifying the specific muscle group to which the recorded LMAN neurons refer, which is feasible but hasn't been done yet. The relation between the muscle activations and song features (such as pitch) would also have to be known – again, this is possible in principle, but goes beyond the scope of our paper.</p><p><italic>– What precise features of the firing of LMAN neurons are predicted for the models that yield efficient learning and match known plasticity features? What should an experimentalist measure in their spike trains? Putting some numbers to the predictions would be great.</italic> </p><p>Our model can predict how the average firing rate of LMAN neurons should vary during song production, provided we have a good measure of motor error and can estimate how specific LMAN neurons contribute to this error. Measurements of the latter kind are necessary for making specific quantitative comparisons, which thus lie beyond the scope of the present paper. Please also see the response to question (2.1), and the revised Discussion.</p><p><italic>3) One reviewer thought that <xref ref-type="fig" rid="fig3">Figure 3C</xref> is not very convincing that learning is good only along the diagonal. It does not seem to matter at short timescales (which the authors discuss briefly), where performance is equivalent regardless of whether timescales are matched or unmatched (e.g. 10 ms vs 80 ms). But even at timescales &gt;80 ms, the performance does not seem strictly diagonal, and the asymptotic performance of the model is significantly worse than at shorter timescales. Therefore it seems like the two key findings – timescale matching and the ability to learn – do not coexist. The other reviewer noted that the error is on a log scale is very important here. There is a steep valley sloping towards the diagonal. The larger error reached at longer timescales was thus less worrisome. 3B shows that this amount of error doesn't stem from huge differences in the output time course. We would therefore like to ask the authors for another plot like B at yet longer timescales to see how and if things break down. Also, 250 iterations isn't that many in terms of number of renditions sung during development, and we might want to see what the error surface looks like after a few thousand. Finally, the paper does discuss the flattening of the valley at short timescales, but this could be expanded.</italic> </p><p>We updated the heatmaps extending the range of timescales. Note that the timescales are chosen in geometric progression, so the four bins we added to each row and column of the heatmap extended the maximum tutor timescale by a factor of 2^4 = 16, to over 20 seconds. This is much longer than our typical simulation, which ran for about 1 second. We also ran the simulations for 1000 steps now instead of 250 in the previous version, and we used the same color scale throughout the paper to make it easier to make comparisons.</p><p>The new plots for the rate-based simulations show that both effective learning when the student matches the tutor, and impaired learning when there is a mismatch, persist even when the tutor timescales are very long. A more interesting effect occurs for the spiking simulations, where convergence breaks down for very long tutor memories. When the tutor timescale is significantly longer than the duration of the motor program, the tutor firing rates tend to stay close to the threshold theta. In the spiking case, the fluctuations due to Poisson spiking introduce noise in the tutor guiding signal, and this noise is more damaging to learning the less the tutor rates vary. This is one of the reasons for the failure in convergence in spiking simulations when the tutor timescale is very long. Another issue is related to the constraint that requires conductor-student synaptic weights to be positive. A consequence of this constraint is that negative fluctuations in the synaptic weights (which happen due to the stochasticity of the tutor signal) are sometimes clamped (when the weights reach zero), and so there is a net positive trend on the weights. This leads to a residual positive shift in the motor program that does not go away with learning.</p><p>Both effects can be reduced – increasing the gain that relates the motor error to the tutor signal reduces the effect of the fluctuations, and using a push-pull mechanism in which student neurons can either act to increase or decrease muscle activity effectively eliminates the residual shift in the motor program.</p><p>In the first version of the manuscript we had chosen to keep the tutor timescales in a range that didn’t significantly exceed the typical duration of the motor program, both because that seems to be the more relevant scenario (longer memory timescales do not have enough time to take full effect during the motor program), and to avoid the complications discussed above. We now included a discussion of these issues in section “Spiking neurons and birdsong”, and in the Supplementary Information, in section “Effect of spiking stochasticity on learning”.</p><p><italic>4. It seems misleading to call τ1 and τ2 the timescales of synaptic plasticity of the student. They are simply timescales of two exponential kernels that are linearly combined to create the final filtering kernel. If the authors want to relate τ1 and τ2 to the timescales of synaptic plasticity, they should include a discussion of how and why they consider them to be related. In general it would be helpful if the authors are more clear in defining their variables through the paper – especially since tau1 and tau2 are very different from tau_tutor.</italic> </p><p>Indeed, we did not mean to imply that tau_1 and tau_2 are directly related to particular details of the plasticity mechanism, such as the duration it takes for plasticity to occur. In our model, tau_1 and tau_2 are simply two parameters that allow us to fit the observed dynamics of synaptic plasticity in some cases. We extended the description of our rule in section “Learning in a rate-based model” to make this clearer, and also to emphasize the relation between our rule and standard STDP rules. Rather than referring to these parameters as “plasticity timescales”, we now call them “student timescales”. We also added the equation defining the plasticity rule to subsection “Learning in a rate-based model” (Equation 1) to avoid confusion regarding the meaning of the different parameters used by the plasticity rule. We further adopted the notation tau_tutor to refer to the exponential kernel used to convolve the motor error in defining the tutor signal (Equation 2), and tau_tutor^* to refer to the timescale matched to a particular student.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p><italic>However, there are still a few important aspects of the paper that need to be significantly revised to improve clarity and address some issues of interpretation:</italic> </p><p><italic>1) The (qualitative) definition of tau_tutor could still be much clearer. When we first read the initial submission, we mistakenly thought that tau_tutor represented the timescale over which the firing rate of the tutor neurons is modulated, rather than (as is actually the case) the timescale over which motor errors are integrated into the firing of tutor neurons. Based on our comments from last time, the authors have somewhat clarified this, however this could still be more clear (in fact, I got confused in the same way reading the revised paper). We think this could be easily clarified with a couple of minor changes</italic> </p><p><italic>a) Just above Equation 4, revise to &quot;Moreover, for effective learning, the timescale t_tutor – ***which quantifies the timescale on which error information is integrated into the tutor signal*** – appearing in Equation 3…&quot; (or something similar).</italic> </p><p>We added the suggested clarification above Equation 4.</p><p> <italic>b) Be a bit more specific when talking about &quot;matching&quot;. For example, the first sentence in 2.3 simply reads &quot;…when the tutor is matched to the student…&quot;. I think it would be much better (especially for less quantitative readers) to explicitly state what is being matched by expanding this to read something like &quot;…when the ***timescale on which error information is integrated into the tutor signal (tau_tutor) is matched to the student plasticity rule***…&quot;.</italic> </p><p>We added the extended explanation of the matching at the top of subsection “Matched vs. unmatched learning”.</p><p> <italic>c) More generally, throughout the paper replace the term &quot;tutor timescale&quot; – which many biologists will misinterpret as the timescale on which tutor activity varies – with &quot;tutor error integration timescale&quot; or something like this.</italic> </p><p>We replaced the phrase “tutor timescale” by “tutor error integration timescale” throughout the manuscript.</p><p><italic>2) We appreciate the authors expanding the duration of simulations and range of timescales tested in the simulations shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>. However, the authors should be more explicit (and consistent) in explaining how these simulations are conducted. My understanding is that in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, tau_1 and tau_2 are fixed at [80 40]msec, and to obtain a particular t_tutor*, plasticity parameters α and β are adjusted for each simulation (subject to the constraint α-β=1). Provided this is correct, the fact that α and β were different for different simulations (i.e. for different squares in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, <xref ref-type="fig" rid="fig5">5D</xref>) should be explicitly stated in the results text and/or legend to <xref ref-type="fig" rid="fig3">Figure 3</xref>. Similarly, as far as I can tell a similar approach (fix tau_1,2, vary α/β to get a particular tau_tutor*) was used in the analysis shown in <xref ref-type="fig" rid="fig5">Figure 5D</xref>. However, this is not made clear in the text/caption (I find the caption to 5D especially confusing). Authors should explicitly state the procedures for 5D and explain any differences in the general methods used in 3C. Furthermore, the authors should present (maybe as a supplemental figure) the values of α or β used for all simulations (e.g. a heatmap of α values to go along with <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>).</italic> </p><p>Indeed, as the referee states, for the heatmaps the timescales tau_1 and tau_2 are kept fixed while α and β are varied, while keeping α-β = 1. This was already explained in the text in the Results section. In addition, we followed the referee’s suggestions and explicitly stated this again in the captions of <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>. We further added a more detailed description of the values of α and β that we used in the supplementary information.</p><p><italic>3) The following is a somewhat subtle point, so I'll leave it up to the Authors to deal with as they see fit.</italic> </p><p><italic>As noted above, tau_tutor is the timescale over which motor errors are integrated into the firing of tutor neurons. Furthermore, the various models (linear rate, spiking, etc.) can work over a big range of tau_tutors, say from 10ms-1280msec, as shown in <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>. In the revised Discussion, the authors correctly assert that LMAN/tutor firing &quot;should reflect the integral of the motor error with the timescale predicted by the model&quot;. The authors also correctly point out that we don't really know what the motor error signal looks like.</italic> </p><p><italic>However – a key physiological fact is that firing in the &quot;tutor&quot; brain area – LMAN – consists of both short bursts (~15 msec duration) as well as single spikes whose overall rate varies much more slowly. So, although we don't know how fast the error signal varies, even if the error signal were a pulse that only lasted, say, 1 msec, it would affect the firing of the LMAN neuron on timescale tau_tutor – e.g. over &gt;100 msec if tau_tutor=100. So, if I'm getting this right, it seems implausible that a 15 msec burst in LMAN could possibly reflect motor error – even an infinitely brief motor error – if tau_tutor were longer than, say, 50 msec. The authors may want to discuss this issue, especially whether their model suggests that only slow changes in the rate of single-spikes, and not bursts, are carrying error information? I think that such an implication is both important for understanding the implications of the model and for guiding future work in the system.</italic> </p><p>If we have understood this comment correctly, the referee is asking whether variations of the LMAN signal over short timescales are informative about motor error when tau_tutor is long. While it is true that the timing of a single spike or short burst in any given trial is noisy and thus cannot hold precise information about rapid variations in the motor error, we assume that the effects of learning are averaged over many repetitions of the motor program. In this sense, learning ends up depending on average firing rates and not on the precise moments when spikes or bursts occur. Recordings suggest that the average firing rates are similar for spikes and bursts (Kao, Wright and Doupe, 2008). It is also true that short pulses in the error signal get convolved with an exponential decay with timescale tau_tutor in the tutor signal. However, a student circuit that is matched to the tutor circuit effectively performs the appropriate deconvolution to pick out the faster variations of the motor error even from slowly-varying tutor signals. This is assured by our matching condition from Equation 4, and is demonstrated by the results of our spiking simulations.</p><p>For the purposes of this manuscript, we chose to not explicitly take into account how the tutor signal is split between isolated spikes and bursts. This is because we focused on the effects from average tutor firing rates, which are independent of this distinction, and also because our model is applicable to systems other than the vocal production mechanism in songbirds, where the precise balance between isolated spiking and bursting may be different. It would nevertheless be very interesting to study the way in which tutor bursting affects learning in a model such as ours. We hope to look into these topics in future work.</p><p><italic>Additionally, as indicated just below Equation 4, the model assumes that tau_tutor&gt;&gt;tau_1,2. In many simulations (definitely in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, also I think in <xref ref-type="fig" rid="fig5">Figure 5D</xref>, see my question above), tau_1 and tau_2 have values of 40 and 80 msec, which are values derived from the Mehaffey and Doupe STDP paper. This would suggest that tau_tutor should be &gt;&gt;80 msec, and that cases with tau_tutor and tau_tutor* less than this value (which make up a significant region of the parameter space shown in <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig5">5D</xref>) are biologically implausible. This seems to me to be something the authors should address.</italic></p><p>Indeed the values of tau_1 and tau_2 were chosen here to qualitatively match the STDP curve from Mehaffey and Doupe when the ratio of α and β is approximately equal to 1. However, our model applies more generally, and hence we explore a wider range of parameters. Our analysis also applies to systems where two-stage learning happens in which the student plasticity can be modeled with different values for these timescales, or with a different ratio of α and β. This is why we consider a wide range of values for these parameters in our simulations.</p><p>Further, although our analytical derivation relies on the assumption tau_tutor&gt;&gt;tau_1,2, this is simply an approximation needed to make the calculations tractable. By running the simulations in parameter ranges in which this and other assumptions of our derivation are not valid, we can check the robustness of our matching condition beyond the range in which the strict derivation held true. Indeed, we find that many of the assumptions can apparently be relaxed without affecting the matching condition (such as replacing the rate-based dynamics by spiking), while others are more rigid (such as the condition tau_tutor&gt;&gt;tau_1,2). We have added a paragraph explaining these points towards the beginning of subsection “Matched vs. unmatched learning”.</p></body></sub-article></article>