<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53900</article-id><article-id pub-id-type="doi">10.7554/eLife.53900</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Distinct neural contributions to metacognition for detecting, but not discriminating visual stimuli</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-165989"><name><surname>Mazor</surname><given-names>Matan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3601-0644</contrib-id><email>mtnmzor@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-11070"><name><surname>Friston</surname><given-names>Karl J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7984-8909</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-79903"><name><surname>Fleming</surname><given-names>Stephen M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0233-4891</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Wellcome Centre for Human Neuroimaging, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Max Planck UCL Centre for Computational Psychiatry and Aging Research, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Department of Experimental Psychology, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>04</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53900</elocation-id><history><date date-type="received" iso-8601-date="2019-11-23"><day>23</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-03-24"><day>24</day><month>03</month><year>2020</year></date></history><permissions><copyright-statement>Â© 2020, Mazor et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Mazor et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53900-v1.pdf"/><abstract><p>Being confident in whether a stimulus is present or absent (a detection judgment) is qualitatively distinct from being confident in the identity of that stimulus (a discrimination judgment). In particular, in detection, evidence can only be available for the presence, not the absence, of a target object. This asymmetry suggests that higher-order cognitive and neural processes may be required for confidence in detection, and more specifically, in judgments about absence. In a within-subject, pre-registered and performance-matched fMRI design, we observed quadratic confidence effects in frontopolar cortex for detection but not discrimination. Furthermore, in the right temporoparietal junction, confidence effects were enhanced for judgments of target absence compared to judgments of target presence. We interpret these findings as reflecting qualitative differences between a neural basis for metacognitive evaluation of detection and discrimination, potentially in line with counterfactual or higher-order models of confidence formation in detection.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>metacognition</kwd><kwd>self-monitoring</kwd><kwd>decision-making</kwd><kwd>confidence</kwd><kwd>signal detection theory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship 206648/Z/17/Z</award-id><principal-award-recipient><name><surname>Fleming</surname><given-names>Stephen M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship 206648/Z/17/Z</award-id><principal-award-recipient><name><surname>Fleming</surname><given-names>Stephen M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Combining psychophysics and functional MRI reveals a qualitative asymmetry in neural engagement when reflecting on whether a stimulus is seen (detection) compared to reflecting on what a stimulus is (discrimination).</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When foraging for berries, one first needs to decide whether a certain bush bears fruit or not. Only if berries are detected, can one proceed to examine and classify them into a category - are these raspberries or blackberries? The first is a <italic>detection</italic> task: a decision about whether something is there or not, and the second is a <italic>discrimination</italic> task: a decision about which item is there. For these types of decisions, it is important not only to understand the decision process that leads to deciding present or absent, or raspberries or blackberries, but also our ability to reflect on and estimate the quality of the decision, known as metacognition. For instance, two foragers working together may want to share their confidence in deciding which bush to tackle next (<xref ref-type="bibr" rid="bib3">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="bib22">Frith, 2012</xref>).</p><p>There is an increasing understanding of the neural basis of confidence in simple decisions, with a network of prefrontal and parietal regions being identified as important for tracking metacognitive beliefs about the accuracy of both perceptual and value-based decisions (see <xref ref-type="bibr" rid="bib14">Domenech and Koechlin, 2015</xref>; <xref ref-type="bibr" rid="bib46">Meyniel et al., 2015</xref>, for reviews). Accordingly, neuropsychological data in humans suggest that damage or impairment of prefrontal function can lead to metacognitive impairments such as noisy or inappropriate confidence judgments (see <xref ref-type="bibr" rid="bib52">Rouault et al., 2018</xref>, for a review). However, in a majority of these cases, the study of confidence has been restricted to discrimination, or deciding whether a stimulus is from category A or B. Despite their ubiquity and importance in decision-making, much less is known about how confidence is formed in detection settings, in which subjects are asked to make a judgment about whether a target stimulus is present or not.</p><p>Computational considerations and behavioral findings suggest that computing confidence in detection judgments may differ from computing confidence in the more commonly studied discrimination tasks. In particular, detection is unique in the landscape of perceptual tasks in that evidence can only be available to support the presence, not the absence, of a target object. This makes confidence ratings in judgments about absence a unique case, where confidence is decoupled from the amount of supporting perceptual evidence. Accordingly, behavioral evidence indicates that metacognitive sensitivity, or the alignment between subjective confidence and objective performance, for judgments about absence is typically impaired compared to metacognitive sensitivity for judgments about presence (<xref ref-type="bibr" rid="bib45">Meuwese et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Kanai et al., 2010</xref>).</p><p>Under one family of models (<italic>first-order models</italic>), confidence in detection judgments is formed in the same way as confidence in discrimination judgments. For example, in evidence-accumulation models, confidence can be evaluated as the distance of the losing accumulator from the threshold at the time of decision (<xref ref-type="bibr" rid="bib61">Vickers, 1979</xref>;Â <xref ref-type="bibr" rid="bib44">Merkle and Van Zandt, 2006</xref>). Similarly, in models of discrimination confidence based on <italic>Signal Detection Theory</italic> (SDT), decision confidence is assumed to be proportional to the strength of the available evidence supporting the decision, which is modeled as the distance of the perceptual sample from the decision criterion on a strength-of-evidence axis (<xref ref-type="bibr" rid="bib62">Wickens, 2002</xref>, section 5.2). While first-order models are traditionally symmetric, they can be adapted to account for the asymmetry between judgments about presence and absence. For example, <italic>unequal-variance (uv-SDT)</italic>Â and <italic>multi-dimensional SDT models</italic> account for the inherent difference between presence and absence by making the signal distribution wider than the noise distribution (<xref ref-type="bibr" rid="bib62">Wickens, 2002</xref>, section 3.4), or by assuming a high-dimensional stimulus space, in which the absence of a signal is represented as a distribution centered around the origin (<xref ref-type="bibr" rid="bib34">King and Dehaene, 2014</xref>; <xref ref-type="bibr" rid="bib62">Wickens, 2002</xref>, section 7.2). Importantly, first-order models treat the process of metacogntive evaluation of detection and discrimination as qualitatively similar, with any differences between detection and discrimination emerging from differences in the underlying distributions (uv-SDT), or the mapping between stimulus features and responses (two-dimensional SDT).</p><p>In contrast with first-order models of detection confidence, <italic>higher-order models</italic> treat confidence in judgments about target absence as emerging from a distinct, higher-order cognitive process. For instance, in one version of the higher-order approach, confidence in judgments about absence is assumed to be based on counterfactual estimation of the likelihood of a hypothetical stimulus to be detected, if presented. In other words, subjects may be more confident in the absence of a target object when they believe they would not have missed it, based on their global estimation of task difficulty, or on their current level of attention. A similar type of modeling has been successfully employed in studies of memory, to explain how participants form judgments that an item was not presented during the preceding learning phase, based on their counterfactual expectations about remembering an item (<xref ref-type="bibr" rid="bib25">Glanzer and Adams, 1990</xref>). When applied to the comparison of detection and discrimination, this approach predicts that qualitatively distinct cognitive and neural resources will be recruited when judging confidence in detection responses, due to the additional demand on counterfactual and self-monitoring processes, and that this recruitment will be most pronounced for confidence about absence. In particular, the counterfactual account predicts that responses in the frontopolar cortex, a region which has been shown to track counterfactual world states (<xref ref-type="bibr" rid="bib6">Boorman et al., 2009</xref>), will show specificity for confidence judgements when inferring the absence of a target.</p><p>To test for such qualitative differences, here we set out to directly compare the neural basis of metacognitive evaluation of detection and discrimination responses within two similar low-level perceptual tasks, while controlling for differences in task performance. In a pre-registered design, we asked whether parametric relationships between subjective confidence ratings and the blood-oxygenation-level-dependent (BOLD) signal in a set of predefined prefrontal and parietal regions of interests (ROIs) would show systematic interaction with task (detection/discrimination) and, within detection, type of response (present/absent). To anticipate our results, we observed a quadratic effect of confidence on regional responses in frontopolar cortex for detection, but not for discrimination judgments. In further whole-brain exploratory analyses, we found stronger confidence-related effects for judgments of absence compared to presence in right temporoparietal junction.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>A total of 35 participants performed two perceptual decision-making tasks while being scanned in a 3T MRI scanner: an orientation discrimination task (<italic>âwas the grating tilted clockwise or anticlockwise?â</italic>), and a detection task (<italic>âwas any grating presented at all?'</italic>;Â seeÂ <xref ref-type="fig" rid="fig1">Figure 1</xref>). The discrimination and detection tasks were performed in separate blocks each lasting 40 trials. At the end of each trial, participants rated their confidence in the accuracy of their decision on a 6-point scale. We adjusted the difficulty of the two tasks in a preceding behavioral session to achieve equal performance of around 70% accuracy. At scanning, 10 discrimination and detection blocks were presented in 5 scanner runs.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design for discrimination and detection trials.</title><p>Perceptual decisions were reported using the right index and middle fingers, and confidence ratings were reported using the left thumb.Â (<bold>A</bold>) In discrimination blocks, participants indicated the orientation of a visual grating (<sc>clockwise</sc> or <sc>anticlockwise</sc>). (<bold>B</bold>) In detection blocks, participants indicated whether a grating was embedded in the random noise, or not (<sc>yes</sc> or <sc>no</sc>). Confidence ratings were made by varying the size and color of a circle, with 6 options ranging from small and red to big and blue. For half of the subjects, high confidence was mapped to a small, red circle. For the other half, high confidence was mapped to a big, blue circle. The initial size and color of the circle was determined randomly at the beginning of the confidence rating phase. Participants performed 10 interleaved 40-trial detection and discrimination blocks inside a 3T MRI scanner.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-fig1-v1.tif"/></fig><sec id="s2-1"><title>Behavioral results</title><p>Task performance was similar for detection (75% accuracy, dâ=1.48) and discrimination blocks (76% accuracy, dâ=1.50). Repeated measures t-tests failed to detect a difference between tasks both in mean accuracy (t(34) = â0.90, p=0.37, <italic>BF</italic><sub>01Â </sub>= 5.15), and dâ ( t(34) = â0.30, p=0.76, <italic>BF</italic><sub>01</sub>=7.29), indicating that performance was well matched. Responses were also balanced for the two tasks. The probability of responding <sc>yes</sc> (target present) in the detection task was 0.49Â Â±Â 0.11, and not significantly different from 0.5 (t(34) = â0.39, p=0.70,Â <italic>BF</italic><sub>01</sub>=7.07). The probability of responding <sc>clockwise</sc> in the discrimination task was 0.50Â Â±Â 0.08, and not significantly different from 0.5 (t(34) = 0.22, p=0.83, <italic>BF</italic><sub>01</sub>=7.43).</p><p>The distribution of confidence ratings was generally similar between the two tasks and four responses. For all four responses, participants were most likely to report the highest confidence rating compared to any other option. Within detection, a significant difference in mean confidence was observed between <sc>yes</sc> (target present) and <sc>no</sc> (target absent) responses, such that participants were more confident in their <sc>yes</sc> responses (t(34) = â4.85, p&lt;0.0001; see <xref ref-type="fig" rid="fig2">Figure 2</xref>). This difference in mean confidence was mostly driven by the higher proportion of maximum confidence ratings in <sc>yes</sc> responses compared to <sc>no</sc> responses (46% of all <sc>yes</sc> responses compared to 26% of all <sc>no</sc> responses, t(34)=5.63, p&lt;0.00001), but persisted even when ignoring the highest ratings (t(34)=2.39, p&lt;0.05).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Upper panels: response conditional type-2 ROC curves.</title><p>In parentheses: the mean area under the curve.Â Lower panels: distribution of confidence ratings for the two tasks and four responses. Right panel: Mean accuracy for both tasks. Error bars represent the standard error of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-fig2-v1.tif"/></fig><p>Metacognitive sensitivity, quantified as the area under the type-II ROC curve, was significantly higher for <sc>yes</sc> compared to <sc>no</sc> responses (t(34) = 7.83, p&lt;10â8; see <xref ref-type="fig" rid="fig2">Figure 2</xref>), as expected (<xref ref-type="bibr" rid="bib45">Meuwese et al., 2014</xref>). In other words, confidence ratings about the presence of a target stimulus were more diagnostic of accuracy than ratings about target absence, even though both sets of ratings tended to cover the full range of the scale, from low to high confidence. Taking metacognitive sensitivity following discrimination responses as a baseline, we found that this effect was driven by a decrease in metacognitive sensitivity for <sc>no</sc> responses (t(34) = â4.89, p&lt;0.0001), whereas a quantitative increase in metacognitive sensitivity for <sc>yes</sc> responses compared to discrimination was not significant (t(34)=1.84, p=0.07). NoÂ difference was observed in metacognitive sensitivity between the two discrimination responses (<sc>clockwise</sc> and <sc>anticlockwise</sc>; t(34) = 0.06, p=0.95, <italic>BF</italic><sub>01</sub>=7.6). Taken together, these results are consistent with the previously reported selective asymmetry in the fidelity of metacognitive evaluation following judgments about target absence (<xref ref-type="bibr" rid="bib45">Meuwese et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Kanai et al., 2010</xref>).</p><p>Response times were faster on average for correct responses (849Â Â±Â 79 milliseconds) compared to incorrect responses (938Â Â±Â 95 milliseconds; t(34)=10.59, p&lt;10<sup>-11Â </sup>for a paired t-test on the log-transformed response times). Within the detection task, <sc>yes</sc> responses were significantly faster than <sc>no</sc> responses (850Â Â±Â 90 milliseconds and 896Â Â±Â 103 milliseconds, respectively; t(34)=3.16, p&lt;0.005 for a paired t-test on the log-transformed response times).</p></sec><sec id="s2-2"><title>Imaging results</title><sec id="s2-2-1"><title>Parametric effect of confidence</title><p>We next turned to our fMRI data to ask whether confidence-related responses were similar or distinct across tasks (detection/discrimination) and response (target present: <sc>yes</sc>/target absent: <sc>no</sc>). We first established the presence of linear confidence-related effects in our a priori ROIs, both across tasks and response types and across correct and incorrect responses, in line with previous findings of âgenericâ or task-invariant confidence signals in these regions (<xref ref-type="bibr" rid="bib47">Morales et al., 2018</xref>). Specifically, higher confidence ratings were associated with increased activation in the ventromedial prefrontal cortex (vmPFC), the ventral striatum, and the precuneus. Conversely, activations in the posterior medial frontal cortex (pMFC) were negatively correlated with confidence (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). For the confidence effect pattern obtained from the Global-Confidence Design Matrix (GC-DM), see <xref ref-type="fig" rid="app3fig1">Appendix 3âfigure 1</xref>.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Univariate parametric effect of confidence.</title><p>(<bold>a</bold>) Glass brain visualization of global effect of confidence, thresholded at the single voxel level for visualization (p&lt;0.001, uncorrected). Negative confidence effect appears in blue, and positive effect in red. (<bold>b</bold>) Whole brain contrast between confidence in âtarget presentâ (<sc>yes</sc>) and âtarget absentâ (<sc>no</sc>) detection responses, corrected for family-wise error rate at the cluster level (p&lt;0.05) with a cluster defining threshold of p&lt;0.001, uncorrected. (<bold>c</bold>) Upper panel: BOLD signal in the rTPJ cluster from panel b as a function of response and confidence. lower panel: mean coefficients of response- and subject-specific multiple linear regression models, predicting rTPJ activation as a linear and quadratic function of confidence. * - p&lt;0.05; uncorrected for multiple comparisons across the four tests. Comparison lines above and below the x axis indicate main effect of response and task, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-fig3-v1.tif"/></fig></sec><sec id="s2-2-2"><title>Interaction of linear confidence effects with task and response</title><p>We next asked whether the linear parametric relationship between confidence and BOLD activity differed as a function of task (discrimination vs. detection) and response type (<sc>yes</sc> vs. <sc>no</sc> in detection). In the pMFC, vmPFC, ventral striatum and precuneus ROIs, the parametric effect of confidence failed to show a significant difference between the two tasks (all p-values&gt;0.3), between the two discrimination responses (all p-values&gt;0.24), or between the two detection responses (all p-values&gt;0.09). Similarly, noÂ cluster within the pre-specified frontopolar ROI showed a differential effect of confidence as a function of task or response. We show below that this absence of a linear interaction should not be taken as evidence of absence of differences between detection and discrimination, due to the presence of nonlinear interaction effects. In the next section we first explain the analysis steps we took to uncover nonlinear effects of confidence.</p></sec><sec id="s2-2-3"><title>Interaction of nonlinear confidence effects with task and response</title><p>An exploratory whole brain analysis (p&lt;0.05, corrected for multiple comparisons at the cluster-level) revealed no differential confidence effect as a function of task anywhere in the brain. However, within detection, whole-brain analysis revealed that the linear effect of confidence was significantly more negative for <sc>no</sc> compared to <sc>yes</sc> responses in the right temporo-parietal junction (rTPJ: 101 voxels, peak voxel: [54,-46, 26], zÂ =Â 5.10). To further characterize the nature of the interaction between confidence and response in the rTPJ, we fitted a new design matrix for each task (Categorical-Confidence Design Matrices (post-hoc analysis; CC-DM)) where confidence was represented as a categorical variable with 6 levels instead of one parametric modulator. In contrast to our original design matrix (Main Design Matrix (DM-1)) that assumed a linear effect of confidence, this analysis is agnostic as to the functional form of the confidence effect. We then plotted the mean activation level for each combination of response and confidence level in the rTPJ cluster (see <xref ref-type="fig" rid="fig3">Figure 3</xref>, panel c).</p><p>The categorical-confidence design matrix revealed a positive quadratic effect of confidence on activation levels in the rTPJ, with stronger activation levels for the two extremities of the confidence scale. We confirmed the presence of a significant quadratic effect of confidence in this region by fitting a second-order polynomial to the response-specific confidence curve of each participant (see Materials and methods). This analysis revealed a main quadratic effect of confidence in this region (t(34) = 5.21, p&lt;0.00001), an effect which was stronger in detection compared to discrimination (t(34)=2.06, p&lt;0.05, dÂ =Â 0.35). Importantly, the linear interaction of confidence with detection responses remained significant for this quadratic model, establishing that this response-specific effect is not explained by an overall quadratic pattern (t(33)=2.09, p&lt;0.05, dÂ =Â 0.36 ; see <xref ref-type="fig" rid="fig3">Figure 3</xref>). More generally, these analyses make clear that linear effects of parametric modulators and their interactions are not exhaustive in their characterization of the confidence-related BOLD response â in this region and potentially in our other ROIs too.</p><p>To formally test for such nonlinear differences in the activation profile of other ROIs, we extracted the coefficients from the categorical model for each ROI, and fitted a second-order polynomial to the ensuing confidence-related response. Within our a priori ROIs, noÂ quadratic effect of confidence was observed in the pMFC, the precuneus, the ventral striatum, or the vmPFC (<xref ref-type="fig" rid="app5fig1">Appendix 5âfigure 1</xref>). In contrast, in all three anatomical subregions of the frontopolar cortex, we found a positive quadratic effect of confidence, with stronger activations for the two extremities of the confidence scale. Strikingly, in both the FPl and the FPm, this positive quadratic effect of confidence was entirely driven by the detection task (FPm: t(34)=3.04, p&lt;0.005, dÂ =Â 0.51; FPl: t(34)=3.90, p&lt;0.001, dÂ =Â 0.66; see <xref ref-type="fig" rid="fig4">Figure 4</xref>). Confidence ratings for the discrimination task however showed a quadratic effect that was not statistically different from zero (FPm: t(34)=-0.54, p=0.59, dÂ =Â â0.09, <italic>BF</italic><sub>01</sub>=6.61; FPl: t(34)=1.42, p=0.16, dÂ =Â 0.24, <italic>BF</italic><sub>01</sub>=2.92). In the FPm, the linear effect of confidence was more negative for detection than for discrimination (t(34) = â2.11, dÂ =Â â0.36, p&lt;0.05), and within detection, more negative for confidence in judgments about absence (<sc>no</sc> responses; t(34) = 2.10, dÂ =Â â0.36, p&lt;0.05).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Confidence effect as a function of response in the frontopolar cortex separated into its three anatomical subcomponents: FPm, FPl, and BA 46.</title><p>Same conventions as in <xref ref-type="fig" rid="fig3">Figure 3c</xref>. * - p&lt;0.05; uncorrected for multiple comparisons. Comparison lines above and below the x axis indicate main effect of response and task, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-fig4-v1.tif"/></fig><p>Finally, to test for similar quadratic effects of confidence at the whole-brain level, we constructed a new design matrix (in a departure to our pre-registered analysis plan) in which confidence was modeled by a parametric modulator with a polynomial expansion of 2 (Quadratic-Confidence Design Matrix (post-hoc analysis; QC-DM)). Three clustersÂ showed a significantly stronger quadratic effect of confidence in detection compared to discrimination (<xref ref-type="fig" rid="fig5">Figure 5</xref>). These were located in the right superior temporal sulcus (72 voxels, peak voxel: [60,-43,2], ZÂ =Â 3.99), pre-SMA (130 voxels, peak voxel: [0,35,47], ZÂ =Â 4.07), and right frontopolar cortex, overlapping with our FPl and FPm frontopolar anatomical subregions (51 voxels, peak voxel: [9,65,-10], ZÂ =Â 4.00). Importantly, noÂ region showed stronger quadratic effects of confidence in discrimination compared to detection.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Left, top panel: a glass-brain representation of a contrast between the quadratic effects of confidence in detection and in discrimination, whole-brain corrected for family-wise error rate at the cluster-level (p&lt;0.05) with a cluster-defining threshold of p&lt;0.001, uncorrected.</title><p>Remaining panels: mean betas from the categorical model for each of the four responses and six confidence ratings, for the three indicated clusters. The second-order polynomial coefficients for these estimates are presented below each plot. Significance is only indicated for the linear effects, which are orthogonal to the quadratic contrast used to select the clusters. * - p&lt;0.05; ** - p&lt;0.01. Comparison lines above and below the x axis indicate main effect of response and task, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-fig5-v1.tif"/></fig><p>To visualize activity patterns in these regions, we extracted the mean coefficients from the categorical model for these three clusters, and fitted a second-order polynomial separately to each response estimate (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). In addition to the effect of task on the quadratic effect of confidence in all three clusters, the linear effect of confidence in the right frontopolar cluster was significantly more negative for detection, compared to discrimination (t(34)=-3.13, dÂ =Â â0.53, p&lt;0.005). For both tasks, inter-subject variability in metacognitive efficiency (measured as meta-dâ/dâ; <xref ref-type="bibr" rid="bib39">Maniscalco and Lau, 2012</xref>) was not reliably correlated with linear or quadratic parametric effect of confidence in any of the three regions (see Appendix 7).</p></sec></sec><sec id="s2-3"><title>Computational models</title><p>We next considered alternative computational-level explanations for the detection-specific quadratic activation profile. Specifically, we evaluated how latent model variables or belief states change non-linearly as a function of confidence in three candidate model architectures (see <xref ref-type="fig" rid="fig6">Figure 6</xref>): a static âSignal Detectionâ model, a âDynamic Criterionâ model where policy changes as a function of previous perceptual samples, and an âAttention Monitoringâ model in which beliefs about fluctuations in attention inform decisions and confidence judgments. A detailed formal description of the three models is available in the appendix (sections 9,Â 10 and 11).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The three models (left) and their prediction for confidence effects (right).</title><p>Top panel: In Signal Detection Theory, perceptual decisions and confidence ratings are generated by comparing the sensory evidence to a fixed set of criteria. In detection the âsignalâ distribution is assumed to have higher variance. Plotting the absolute value of the log likelihood ratio as a function of decision and confidence results in a linear curve for discrimination, and a pronounced quadratic effect for <sc>yes</sc> responses in detection, an effect that is specific to unequal-variance SDT. Middle panel: In a Dynamic Criterion model beliefs about the mean and variance of the perceptual distributions are updated as a function of incoming samples (plotted as circles) and the decision criterion is shifted accordingly. Plotting the absolute change in criterion placement as a function of decision and confidence results in a quadratic effect of confidence for detection responses only. Bottom: In the Attention Monitoring model, beliefs about overall attentiveness (âonTaskâ node) probabilistically reflect sensory precision. Plotting beliefs about overall attentiveness as a function of decision and confidence results in an overall quadratic effect of confidence, and an interaction between <sc>yes</sc> and <sc>no</sc> responses in detection. For a detailed specification of all three models see appendix sections 9, 10 and 11.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-fig6-v1.tif"/></fig><p>First, we consider the static Signal Detection Theory (SDT) model. In SDT models of confidence formation, the log likelihood-ratio between the two competing hypotheses (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>) is a useful measure for determining the certainty with which one should commit to a choice. The mapping between the perceptual sample <inline-formula><mml:math id="inf2"><mml:mi>x</mml:mi></mml:math></inline-formula> and the LLR is linear for equal-variance SDT, which is often used to model discrimination, but quadratic for unequal-variance SDT, which is often used to model detection. It then follows that if confidence is proportional to the distance of the sample <inline-formula><mml:math id="inf3"><mml:mi>x</mml:mi></mml:math></inline-formula> from the decision criterion, neuronal populations that represent the relative likelihood of a choice being correct (be it LLR or an analogue quantity) will show a quadratic tuning function of confidence in detection and a linear tuning function in discrimination, similar to that observed in FPC, pre-SMA and STS. However, LLR is also expected to scale more strongly with confidence in <sc>yes</sc> responses (see simulation results in <xref ref-type="fig" rid="fig6">Figure 6</xref>, upper panel), which was not observed in these brain regions. This model also predicts a stronger quadratic effect of confidence in participants for which the variance ratio between the signal and noise distributions is particularly high. However, the variance ratio was not significantly correlated with the quadratic effect of confidence in any of these regions, as would be expected if they were representing LLR or a similar quantity (see <xref ref-type="fig" rid="app6fig1">Appendix 6âfigure 1</xref>).</p><p>For the next two models, confidence was assumed to be directly proportional to the LLR, with the measured signal representing internal beliefs about hidden model parameters. In the âDynamic Criterionâ model, we considered whether a quadratic effect of confidence in detection may reflect the active tuning of decision policy in the absence of explicit feedback (<xref ref-type="bibr" rid="bib27">Guggenmos et al., 2016</xref>; <xref ref-type="bibr" rid="bib35">Ko and Lau, 2012</xref>). In the model, beliefs about the underlying distributions are updated on a trial-to-trial basis, and in turn affect the placement of decision criterion (for a formal description of the model, see Appendix section 10). The Dynamic Criterion model predicts that the magnitude of shift in decision criterion will display a positive quadratic relation to confidence (LLR) in detection but not discrimination (see simulation results in <xref ref-type="fig" rid="fig6">Figure 6</xref>, middle panel). This is because the problem is asymmetric in detection, and decision policy should depend on beliefs about both sensory precision (or the relative variance of the noise and signal distribution) and expected signal strength (mean of the signal distribution), which is not the case for a symmetric discrimination problem.</p><p>Notably, the pattern of criterion shifts in the Dynamic Criterion model resembled the task-specific effect of confidence in the FPC, STS and pre-SMA. As a post-hoc test of a role for these regions in criterion adjustment, we examined sequential pairs of trials of the same stimulus category (for example, a signal present trial that was followed by a signal present trial), and contrasted ârepeatâ trials with âswitchâ trials (for example, [<sc>yes</sc>, <sc>yes</sc>] vs. [<sc>yes</sc>,Â <sc>no</sc>]). The Dynamic Criterion model predicts stronger activation in switch compared to stay trials in both detection and discrimination. The FPl showed a weak effect in this direction (tÂ =Â 2.03, p=0.05, dÂ =Â 0.34), whereas FPm, pre-SMA, right BA10 and STS did not (all p-values&gt;0.15).</p><p>Finally, we considered a higher-order âAttention Monitoringâ model in which beliefs about oneâs current attentional state (precision or inverse variance in SDT) are taken into account when making perceptual decisions and confidence ratings on detection trials. This model formalizes the notion that after not detecting a target the participant may ask <italic>âGiven my current attentional state, would I have missed the target?'</italic>. The Attention Monitoring model thus makes different predictions for confidence in detection âtarget absentâ (<sc>no</sc>) responses, where the participant is assumed to reflect on the detection-likelihood of hypothetical targets, compared to âtarget presentâ (<sc>yes</sc>) responses, similar to the activation profile observed in the rTPJ. However, this model also predicts a pronounced quadratic confidence profile for all four responses, which we do not see in our data.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Previous studies of the neural basis of human perceptual decision-making have tended to focus on discrimination judgments, such as sorting stimuli into category A or B. The general computational architecture supporting discrimination judgments can be naturally extended to support detection (for instance, within signal detection theory). However, computational considerations and behavioral findings suggest that forming confidence in detection judgments may rest on qualitatively distinct cognitive and neural processes in comparison to generating confidence in discrimination judgments.</p><p>To test for such differences, here we acquired functional MRI data from 35 participants who reported their subjective confidence in judgments about stimulus type (discrimination), and target presence or absence (detection). These judgments were given on separate trials that were well-matched for stimulus characteristics, response requirements and task difficulty. Across both tasks, we found the expected linear effects of confidence in our pre-specified regions of interest in the prefrontal and parietal cortex. Specifically, in the precuneus, vmPFC, pMFC and ventral striatum, the effect of confidence was invariant to task and response. In contrast, having adjusted our planned design matrix to be sensitive to non-monotonic effects of confidence, we observed a quadratic effect of confidence in detection judgments in the frontopolar cortex (medial and lateral surfaces of BA10), that was absent for discrimination judgments. Similar quadratic activation profiles were observed for both <sc>yes</sc> and <sc>no</sc> responses. Whole-brain analysis revealed a similar effect of task on the quadratic effect of confidence in the right STS and the pre-SMA. Since task performance was matched across the two tasks and since we did not observe overall differences in activation between detection and discrimination (see <xref ref-type="fig" rid="app4fig1">Appendix 4âfigure 1</xref>), these differences in confidence profiles are unlikely to originate from experimental confounds such as task difficulty, but instead indicate a unique neurocognitive contribution to metacognition of detection judgments. In what follows we will unpack what this contribution might be.</p><p>The three regions that showed an interaction of the quadratic expansion of confidence with task in our whole-brain analysis (right frontopolar cortex, right STS, and pre-SMA), as well as two anatomical subcomponents of our frontopolar ROI (FPl and FPm), all shared a very similar activation profile. In detection, the quadratic effect of confidence was positive, but was almost entirely absent for the discrimination task. Follow-up analysis confirmed that this difference was not driven by motor aspects of the confidence rating procedure, such as the number of increase or decrease confidence steps taken to reach the desired confidence level, which was similar for the two tasks (see <xref ref-type="fig" rid="app1fig1">Appendix 1âfigure 1</xref>). Ours is not the first report of a quadratic relation between activation in prefrontal cortical structures and different subjective ratings. For example, in a study by <xref ref-type="bibr" rid="bib8">Christensen et al. (2006)</xref>, participants were presented with masked stimuli and gave subjective visibility ratings on a three-point scale. The right frontopolar cortex showed decreased activation for âclear perceptionâ and âno perceptionâ categories relative to a middle âvague perceptionâ category. Similarly, <xref ref-type="bibr" rid="bib12">De Martino et al. (2017)</xref> reported a quadratic effect of product desirability in the pMFC. However, for both of the above cases, a quadratic effect can reflect a monotonic relationship with an implicit representation of subjective confidence (<xref ref-type="bibr" rid="bib37">Lebreton et al., 2015</xref>). For example, participants may be more confident in the âclear perceptionâ and âno perceptionâ responses compared to the âvague perceptionâ option, or more confident about liking or not liking a product, compared to when using the middle parts of the liking scale. This explanation cannot account for the observed quadratic trend in our case, where in addition to strong activation levels for the highest confidence ratings in target presence and absence, we also find strong activation levels for the lowest levels of confidence.</p><p>We are unable to determine whether this effect originates from one homogeneous population of neurons that shows a quadratic effect of detection confidence, or from two overlapping populations that show nonlinear positive and negative effects of detection confidence â summing to an overall quadratic effect at the voxel level (similar to positive and negative confidence-selective neurons in the human posterior parietal cortex; <xref ref-type="bibr" rid="bib55">Rutishauser et al., 2018</xref>)Â Addressing this question would require higher spatial resolution, for example using single-cell recordings in patients. Furthermore, because confidence judgments were always preceded by perceptual decisions in our design, we cannot determine whether the observed effects reflect an implicit representation of uncertainty, computed in parallel with the perceptual decision itself, or a higher-order representation that emerges at the explicit confidence rating phase. Future studies which use model-based estimates of covert decision confidence (<xref ref-type="bibr" rid="bib4">Bang and Fleming, 2018</xref>) or EEG-informed fMRI to resolve early and late processing stages (<xref ref-type="bibr" rid="bib24">Gherman and Philiastides, 2018</xref>) may answer this question.</p><p>We considered three alternative computational models that were able to account for asymmetries between detection and discrimination activation profiles. An unequal variance signal detection theory model provided a simple account of the asymmetry between detection and discrimination, but could not account for the similar quadratic profiles observed for <sc>yes</sc> and <sc>no</sc> responses. A more direct test of the proposal that a detection-specific quadratic effect of confidence originates from the unequal-variance properties of stimulus distributions in detection would be to test for similar effects in a discrimination task in which one category of stimuli is of higher variance (e.g., <xref ref-type="bibr" rid="bib13">Denison et al., 2018</xref>). In contrast, the Dynamic Criterion model provided good qualitative accounts for distinct regional activation profiles, and the Attention Monitoring account predicted an interaction between confidence in judgments about presence and absence. However, the Attention Monitoring model also predicted a quadratic effect in discrimination, which we did not see.</p><p>Notably, both of these models share the need to learn (in the Dynamic Criterion model) or estimate (in the Attention Monitoring model) the current level of precision (inverse variance) in detection. Such online precision estimation evinces a profound asymmetry between detection and discrimination tasks: in discrimination tasks, one simply has to evaluate the relative evidence for different causes of sensory samples, under some prior belief about sensory precision; namely, the precision of the likelihood that any particular cause (e.g., <sc>clockwise</sc> or <sc>anticlockwise</sc> orientation) would generate sensory samples. In contrast, detection presents a difficult (ill-posed, dual estimation) problem. When assessing the evidence for the absence of a target, there could be no sensory evidence because the target is not there or because precision is low (or both). This puts pressure on the estimation of precision to resolve conditional dependencies between posterior beliefs about target presence and the precision with which it can be detected. In short, two things have to be estimated; the posterior expectation about the target and posterior beliefs about precision (<xref ref-type="bibr" rid="bib9">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib17">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="bib28">Haarsma et al., 2018</xref>; <xref ref-type="bibr" rid="bib50">Palmer et al., 2019</xref>; <xref ref-type="bibr" rid="bib51">Parr et al., 2018</xref>).</p><p>In line with a role in monitoring of attention or precision, right TPJ showed a negative effect of confidence that was stronger for âtarget absentâ responses compared to âtarget presentâ responses in detection. This cluster was closest to the posterior subdivision of the right TPJ (TPJp-R; <xref ref-type="bibr" rid="bib30">IgelstrÃ¶m et al., 2015</xref>), which is most strongly associated with reasoning about othersâ beliefs (<xref ref-type="bibr" rid="bib31">IgelstrÃ¶m et al., 2016</xref>). In addition to its role in Theory of Mind (<xref ref-type="bibr" rid="bib56">Saxe and Wexler, 2005</xref>; <xref ref-type="bibr" rid="bib38">Lee and McCarthy, 2016</xref>), previous work has highlighted the importance of the rTPJ in controlling attention (<xref ref-type="bibr" rid="bib40">Marois et al., 2004</xref>; <xref ref-type="bibr" rid="bib23">Geng and Vossel, 2013</xref>; <xref ref-type="bibr" rid="bib38">Lee and McCarthy, 2016</xref>; <xref ref-type="bibr" rid="bib16">DuguÃ© et al., 2018</xref>) and filtering distractors in visual search (<xref ref-type="bibr" rid="bib57">Shulman et al., 2007</xref>). Furthermore, damage to the rTPJ can result in visual hemineglect: a condition in which stimuli in the left visual hemifield fail to reach awareness (<xref ref-type="bibr" rid="bib10">Corbetta et al., 2005</xref>). Together, these observations have led to a proposal (the âAttention SchemaÂ Theoryâ) that the rTPJ is maintaining a simplified representation of oneâs own and othersâ attentional states, and that this function makes this region essential for maintaining conscious awareness (<xref ref-type="bibr" rid="bib26">Graziano and Webb, 2015</xref>).</p><p>The current Attention Monitoring model fits well with the Attention SchemaÂ Theory. A representation of oneâs current attentional state is a useful source of information for determining confidence in detection judgments, because stimuli are more likely to be missed when participants are not paying careful attention. This will be specifically useful for judgments about stimulus absence: if a target was not observed, the participant may reason something along the lines of <italic>'given my current state of attention, I was not very likely to miss a target, therefore I can be very confident that a target was not presented</italic>â. In support of this idea, the typically poor metacognitive evaluations of decisions about stimulus absence are partially recovered when task difficulty is controlled by manipulating attention rather than stimulus visibility (<xref ref-type="bibr" rid="bib32">Kanai et al., 2010</xref>; <xref ref-type="bibr" rid="bib33">Kellij et al., 2018</xref>), suggesting that subjects may harness information about their attentional state to inform their confidence judgments. Interestingly, the frontopolar cortex, which showed a detection-specific quadratic effect of confidence in our experiment, has also been implicated in attentional control via the gating of internal and external modes of attention (<xref ref-type="bibr" rid="bib7">Burgess et al., 2007</xref>) and in discriminating between imagined and externally perceived memory items (<xref ref-type="bibr" rid="bib58">Simons et al., 2006</xref>; <xref ref-type="bibr" rid="bib60">Turner et al., 2008</xref>). Together, the engagement of this set of regions in detection confidence hints at a potential role for self-monitoring of attention in metacognition of detection.</p><p>To conclude, we find a quadratic effect of confidence in detection judgments in several brain regions, including the frontopolar cortex and rTPJ. In the frontopolar cortex, this quadratic effect was not seen for discrimination judgments. In the rTPJ, we also found a linear effect of confidence that was more negative for judgments about stimulus absence compared to judgments about stimulus presence. We consider three computational accounts of our results, two of which implicate the learning and estimation of signal-to-noise statistics as promising accounts of the observed detection-specific activation profiles. However, while each of these accounts could explain some of our findings, none of the models could provide a complete account of the data. Further work is needed to decide between these alternatives, or to suggest new ones.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All design and analysis details were pre-registered before data acquisition and time-locked using pre-RNG randomization (<xref ref-type="bibr" rid="bib41">Mazor et al., 2019</xref>). The time-locked protocol folder is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/matanmazor/detectionVsDiscrimination_fMRI">https://github.com/matanmazor/detectionVsDiscrimination_fMRI</ext-link> (<xref ref-type="bibr" rid="bib42">Mazor, 2020</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/detectionVsDiscrimination_fMRI">https://github.com/elifesciences-publications/detectionVsDiscrimination_fMRI</ext-link>).Â The entire set of pre-registered analyses resultsÂ is available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/98mv4/">https://osf.io/98mv4/</ext-link>.</p><sec id="s4-1"><title>Participants</title><p>46 participants took part in the study (ages 18â36, meanÂ =Â 24Â Â±Â 4; 29 females). 35 participants met our pre-specified inclusion criteria (ages 18â36, meanÂ =Â 24Â Â±Â 4; 20 females). After applying our run-wise exclusion criteria to the data of the remaining 35 participants, our dataset consisted of 5 usable experimental runs from 15 participants, 4 usable experimental runs from 14 participants, 3 usable experimental runs from 5 participants, and 2 usable experimental runs from one participant. We pre-specified a sample-size of 35, balancing statistical power and resource considerations.</p></sec><sec id="s4-2"><title>Design and procedure</title><p>After a temporally jittered rest period of 500â4000 milliseconds, each trial started with a fixation cross (500 milliseconds), followed by a presentation of a target for 33 milliseconds. In discrimination trials, the target was a circle of diameter 3Â° containing randomly generated white noise, merged with a sinusoidal grating (2 cycles per degree; oriented 45Â° or â45Â°). In half of the detection trials, targets did not contain a sinusoidal grating and consisted of random noise only. After stimulus offset, participants used their right-hand index and middle fingers to make a perceptual decision about the orientation of the grating (discrimination blocks), or about the presence or absence of a grating (detection blocks). The response mapping was counterbalanced between blocks, such that an index finger press was used to indicate a <sc>clockwise</sc> tilt on half of the trials, and an <sc>anticlockwise</sc> tilt on the other half. Similarly, in half of the detection trials the index finger was mapped to a <sc>yes</sc> (âtarget presentâ) response, and on the other half to a <sc>no</sc> (âtarget absentâ) response.</p><p>Immediately after making a decision, participants rated their confidence on a 6-point scale by using two keys to increase and decrease their reported confidence level with their left-hand thumb. Confidence levels were indicated by the size and color of a circle presented at the center of the screen. The initial size and color of the circle was determined randomly at the beginning of the confidence rating phase, to decorrelate the number of button presses and the final confidence rating. The mapping between color and size to confidence was counterbalanced between participants: for half of the participants high confidence was mapped to small, red circles, and for the other half high confidence was mapped to large, blue circles. This counterbalancing was employed to isolate confidence-related activations from activations that originate from the perceptual properties of the confidence scale or from differences in the motor requirement to press the upper and lower buttons. The perceptual decision and the confidence rating phases were restricted to 1500 and 2500 milliseconds, respectively. No feedback was delivered to subjects about their performance.</p><p>Participants were acquainted with the task in a preceding behavioral session. During this session, task difficulty was adjusted independently for detection and for discrimination, targeting around 70% accuracy on both tasks. We achieved this by adaptively controlling the stimulus signal-to-noise ratio (SNR) once in every 10 trials: increasing the SNR when accuracy fell below 60%, and decreasing it when accuracy exceeded 80%. Performance on the detection and discrimination task was further calibrated to the scanner environment at the beginning of the scanning session, during the acquisition of anatomical (MP-RAGE and fieldmap) images. After completing the calibration phase, participants underwent five ten-minute functional scanner runs, each comprising one detection and one discrimination block of 40 trials each, presented in random order.</p><p>To avoid stimulus-driven fluctuations in confidence, grating SNR was fixed within each experimental block. Nevertheless, following experimental blocks with markedly bad (â¤Â 52.5%) or good (â¥Â 85%) accuracy, grating SNR was adjusted for the next block of the same task (SNR level was divided or multiplied by a factor of 0.9 for bad and good performance, respectively). Finally, grating SNR was adjusted for both tasks following runs in which the difference in performance between the two tasks exceeded 16.25% (SNR level was multiplied by the square root of 0.9 for the easier task and divided by the square root of 0.9 for the more difficult task).</p><p>To incentivize participants to do their best at the task and rate their confidence accurately, we offered a bonus payment according to the following payment schedule: bonus = Â£<inline-formula><mml:math id="inf4"><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>u</mml:mi><mml:mo>â¢</mml:mo><mml:mi>r</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>â</mml:mo></mml:mover><mml:mo>â</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>â</mml:mo></mml:mover></mml:mrow><mml:mn>200</mml:mn></mml:mfrac></mml:math></inline-formula> Where <inline-formula><mml:math id="inf5"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>u</mml:mi><mml:mo>â¢</mml:mo><mml:mi>r</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>â</mml:mo></mml:mover></mml:math></inline-formula> is a vector of 1 and â1 for correct and incorrect responses, and <inline-formula><mml:math id="inf6"><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>â</mml:mo></mml:mover></mml:math></inline-formula> is a vector of integers in the range of 1 to 6, representing confidence reports for all trials. We explained the payment structure to participants in the preceding behavioral session. Specifically, we advised participants that to maximize their bonus they should do their best at the main task, rate the confidence higher when they believe they are correct, and rate their confidence lower when they believe they might be wrong.</p></sec><sec id="s4-3"><title>Scanning parameters</title><p>Scanning took place at the Wellcome Centre for Human Neuroimaging, London, using a 3 Tesla Siemens Prisma MRI scanner with a 64-channel head coil. We acquired structural images using an MPRAGE sequence (1Ã1Ã1 mm voxels, 176 slices, in plane FoVÂ =Â 256Ã256 mm<sup>2</sup>), followed by a double-echo FLASH (gradient echo) sequence with TE1Â =Â 10 ms and TE2Â =Â 12.46 ms (64 slices, slice thicknessÂ =Â 2 mm, gapÂ =Â 1 mm, in plane FoVÂ =Â 192Â ÃÂ 192 mm<sup>2</sup>, resolutionÂ =Â 3Â ÃÂ 3 mm<sup>2</sup>) that was later used for field inhomogeneity correction. Functional scans were acquired using a 2D EPI sequence, optimized for regions near the orbito-frontal cortex (3Ã3Ã3 mm voxels, TRÂ =Â 3.36 s, TEÂ =Â 30 ms, 48 slices tilted by â30 degrees with respect to the TÂ &gt;Â C axis, matrix sizeÂ =Â 64Ã72, Z-shimÂ =Â â1.4).</p></sec><sec id="s4-4"><title>Analysis</title><p>The preregistered objectives of this study were to:</p><list list-type="order"><list-item><p>Replicate findings of a generic (task-invariant) confidence signal in the activity of medial prefrontal cortex (<xref ref-type="bibr" rid="bib11">De Martino et al., 2013</xref>;Â <xref ref-type="bibr" rid="bib47">Morales et al., 2018</xref>).</p></list-item><list-item><p>Test for an interaction between the parametric effect of confidence level and task (detection/discrimination) in the BOLD response in prefrontal cortex ROIs.</p></list-item><list-item><p>Within detection trials, test for an interaction between the parametric effect of confidence level and response (<sc>yes</sc>/<sc>no</sc>) in the BOLD response, specifically in the prefrontal cortex and in frontopolar regions that have previously been associated with counterfactual reasoning (<xref ref-type="bibr" rid="bib6">Boorman et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Donoso et al., 2014</xref>).</p></list-item><list-item><p>Test for relationships between fluctuations in metacognitive adequacy (a trial-by-trial measure of metacognitive sensitivity; <xref ref-type="bibr" rid="bib63">Wokke et al., 2017</xref>), and the BOLD signal separately for detection and for discrimination, and for <sc>yes</sc> and <sc>no</sc> responses within detection.</p></list-item><list-item><p>Replicate previous findings of between-subject correlations between lateral prefrontal cortex (lPFC) function and metacognitive efficiency (meta-dâ/dâ; <xref ref-type="bibr" rid="bib21">Fleming and Lau, 2014</xref>) in discrimination (<xref ref-type="bibr" rid="bib64">Yokoyama et al., 2010</xref>).</p></list-item><list-item><p>Identify between-subject functional correlates of metacognitive efficiency in detection. Specifically, ask if metacognitive efficiency in detection is predicted by activity in distinct networks compared to metacognitive efficiency in discrimination.</p></list-item></list></sec><sec id="s4-5"><title>Exclusion criteria</title><p>Subjects were excluded from all analyses for any of the following pre-specified reasons: missing more than 20% of the trials, performing one of the tasks with accuracy below 60%, exceeding the 4 mm affine motion cutoff criterion in more than 2 experimental runs, and showing a consistent response bias (i.e. using the same response in more than 75% of the trials) in at least one task. Individual scan runs were excluded from all analyses if the participant exceeded the affine motion cutoff, if more than 20% of trials were missed, if mean accuracy was below 60% or if the response bias for one of the tasks exceeded 80%.</p><p>In addition, we applied a confidence-related exclusion criterion: participants were excluded if they used the same confidence level in more than 80% of all trials globally or for a particular response, and individual scan runs were excluded if the same confidence level was used in more than 95% of the trials, either globally or for particular response types. Our preregistration document specified that the confidence exclusion criterion will be used to exclude participants from confidence-related analyses only, but we subsequently revised this plan in order to use identical design matrices for all participants.</p></sec><sec id="s4-6"><title>Behavioral analysis</title><sec id="s4-6-1"><title>Response conditional type-II ROC curves</title><p>Response conditional type-IIÂ ROC (Receiver Operating Characteristic) curves were extracted for the two discrimination and two detection responses. This was done by plotting the cumulative distribution of confidence levels in correct responses against the cumulative distribution of confidence levels in incorrect responses. As a measure of response-specific metacognitive sensitivity, we extracted the area under these curves (<italic>AUROC2</italic>). The expected AUROC2 forÂ no metacognitive insight (i.e., the confidence distributions are identical for correct and incorrect responses) is 0.5. Perfect metacognitive insight (i.e., confidence in all correct responses is higher than confidence in all incorrect responses) will result in an AUROC2 of 1.</p></sec></sec><sec id="s4-7"><title>Imaging analysis</title><sec id="s4-7-1"><title>fMRI data preprocessing</title><p>Data preprocessing followed the procedure described in <xref ref-type="bibr" rid="bib47">Morales et al. (2018)</xref>: 'Imaging analysis was performed using SPM12 (Statistical Parametric Mapping; <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">www.fil.ion.ucl.ac.uk/spm</ext-link>). The first five volumes of each run were discarded to allow for T1 stabilization. Functional images were realigned and unwarped using local field maps (<xref ref-type="bibr" rid="bib1">Andersson et al., 2001</xref>) and then slice-time corrected (<xref ref-type="bibr" rid="bib59">Sladky et al., 2011</xref>). Each participantâs structural image was segmented into gray matter, white matter, CSF, bone, soft tissue, and air/background images using a nonlinear deformation field to map it onto template tissue probability maps (<xref ref-type="bibr" rid="bib2">Ashburner and Friston, 2005</xref>). This mapping was applied to both structural and functional images to create normalized images in Montreal Neurological Institute (MNI) space. Normalized images were spatially smoothed using a Gaussian kernel (6 mm FWHM). We set a within-run 4 mm affine motion cutoff criterion'.</p><p>Preprocessing and construction of first- and second-level models used standardized pipelines and scripts available at <ext-link ext-link-type="uri" xlink:href="https://github.com/metacoglab/MetaLabCore/">https://github.com/metacoglab/MetaLabCore/</ext-link>.</p></sec></sec><sec id="s4-8"><title>Regions of interest</title><p>In addition to an exploratory whole-brain analysis (corrected for multiple comparisons at the cluster level), our analysis focused on the following a priori regions of interest, largely following the ROIs used by <xref ref-type="bibr" rid="bib20">Fleming et al. (2018)</xref>:</p><list list-type="order"><list-item><p>Frontopolar cortex (FPC, defined anatomically). We used a connectivity-based parcellation (<xref ref-type="bibr" rid="bib48">Neubert et al., 2014</xref>) to define a general FPC region of interest as the total area spanned by areas FPl, FPm and BA46. The right hemisphere mask was mirrored to create a bilateral mask.</p></list-item><list-item><p>Ventromedial prefrontal cortex (vmPFC). The vmPFC ROI was defined as a 8 mm sphere around MNI coordinates [0,46,â7], obtained from a meta-analysis of subjective-value related activations (<xref ref-type="bibr" rid="bib5">Bartra et al., 2013</xref>) and aligned to the cortical midline.</p></list-item><list-item><p>Bilateral ventral striatum. The ventral striatum ROIs was specified anatomically from the Oxford-Imanova Striatal Structural Atlas included with FSL (<ext-link ext-link-type="uri" xlink:href="http://fsl.fmrib.ox.ac.uk">http://fsl.fmrib.ox.ac.uk</ext-link>).</p></list-item><list-item><p>Posterior medial frontal cortex (pMFC). The pMFC ROI was defined as a 8 mm sphere around MNI coordinates [0, 17, 46], obtained from a functional MRI study on decision confidence and aligned to the cortical midline (<xref ref-type="bibr" rid="bib19">Fleming et al., 2012</xref>).</p></list-item><list-item><p>Precuneus. The precuneus ROI was defined as a 8 mm sphere around MNI coordinates [0,â57,18], based on Voxel- Based Morphometry studies of metacognitive efficiency (<xref ref-type="bibr" rid="bib18">Fleming et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">McCurdy et al., 2013</xref>) and aligned to the cortical midline.</p></list-item></list><p>For the general FPC ROI, small-volume correction was applied to individual voxels within the ROI for all univariate contrasts. For the multivariate analysis, we used a searchlight approach to scan for spatial patterns within the ROI, followed by a correction for multiple comparisons. For all other ROIs, a GLM was fitted to the mean time course of voxels within the region, and multivariate analysis was performed on all voxels within the ROI. While our pre-registered analysis defined the frontopolar cortex as a single region, we subsequently decided to separately analyze its 3 separate anatomical subregions identified by <xref ref-type="bibr" rid="bib48">Neubert et al. (2014)</xref> (FPl, FPm and BA46). The decision to separate the FPC ROI to its subcomponents was made after data collectionÂ and these anatomical subregions should not be taken as aÂ priori ROIs.</p></sec><sec id="s4-9"><title>Univariate analysis</title><p>Univariate analysis was based on a design matrix in which different trial types are modeled by different regressors (main design matrix, below). Additionally, to examine the global effect of confidence across trial types, a simpler design matrix was fitted to the data as a first step (global confidence design matrix, below). Experimental runs for each subject were temporally concatenated before estimating the GLM coefficients. This was done in order to maximize sensitivity to response- and task-specific modulations of confidence, given the limited and varying number of trials within each experimental run.</p><sec id="s4-9-1"><title>Main design matrix (DM-1)</title><p>The main design matrix for the univariate GLM analysis consisted of 16 regressors of interest. There was a regressor for each of the eight combinations of task x condition x response: For example, a regressor for detection trials where a signal was present and the subject reported seeing a signal with a <sc>yes</sc> response (present and present, P_P). The relevant trials were modeled by a boxcar regressor with nonzero entries at the interval starting at the offset of the stimulus and ending immediately after the confidence rating phase, convolved with the canonical hemodynamic response function (HRF). The duration of this interval wasÂ 4300Â milliseconds, and notÂ 4000Â milliseconds as mistakenly indicated in the preregistration document.Â Each of these primary regressors was accompanied by a linear parametric modulation of the confidence reported for each trial. Together, the design matrix included 16 regressors of interest (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>List of regressors in the main design matrix (DM-1).</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th>Task</th><th>Stimulus</th><th>Response</th></tr></thead><tbody><tr><td>1</td><td>CW_CW</td><td rowspan="2">Discrimination</td><td rowspan="2">Clockwise</td><td rowspan="2">Clockwise</td></tr><tr><td>2</td><td>CW_CW_conf</td></tr><tr><td>3</td><td>CW_ACW</td><td rowspan="2">Discrimination</td><td rowspan="2">Clockwise</td><td rowspan="2">Anticlockwise</td></tr><tr><td>4</td><td>CW_ACW_conf</td></tr><tr><td>5</td><td>ACW_CW</td><td rowspan="2">Discrimination</td><td rowspan="2">Anticlockwise</td><td rowspan="2">Clockwise</td></tr><tr><td>6</td><td>ACW_CW_conf</td></tr><tr><td>7</td><td>ACW_ACW</td><td rowspan="2">Discrimination</td><td rowspan="2">Anticlockwise</td><td rowspan="2">Anticlockwise</td></tr><tr><td>8</td><td>ACW_ACW_conf</td></tr><tr><td>9</td><td>P_P</td><td rowspan="2">Detection</td><td rowspan="2">Present</td><td rowspan="2">Present</td></tr><tr><td>10</td><td>P_P_conf</td></tr><tr><td>11</td><td>P_A</td><td rowspan="2">Detection</td><td rowspan="2">Present</td><td rowspan="2">Absent</td></tr><tr><td>12</td><td>P_A_conf</td></tr><tr><td>13</td><td>A_P</td><td rowspan="2">Detection</td><td rowspan="2">Absent</td><td rowspan="2">Present</td></tr><tr><td>14</td><td>A_P_conf</td></tr><tr><td>15</td><td>A_A</td><td rowspan="2">Detection</td><td rowspan="2">Absent</td><td rowspan="2">Absent</td></tr><tr><td>16</td><td>A_A_conf</td></tr></tbody></table></table-wrap><p>Trials in which the participant did not respond within the 1500 millisecond time frame were modeled by a separate regressor. The design matrix also include a run-wise constant term regressor, an instruction-screen regressor for the beginning of each block, motion regressors (the 6 motion parameters and their first derivatives as extracted by SPM in the head motion correction preprocessing phase) and regressors for physiological measures. Button presses were modeled as stick functions, convolved with the canonical HRF, in three regressors: two regressors for the right and left right-hand buttons, and one regressor for both up and down left-hand presses. We decided to have one regressor for both types of left-hand presses due to the strong positive correlation of the final confidence rating with the number of âincrease confidenceâ button presses, and the strong negative correlation with the number of âdecrease confidenceâ button presses.</p></sec><sec id="s4-9-2"><title>Global confidence design matrix (GC-DM)</title><p>The global confidence design matrix consisted of 4 regressors of interest. The first two primary regressors were âcorrect trialsâ (trials in which the participant was correct, across tasks and responses) and âincorrect trialsâ (trials in which the participant was incorrect, across tasks and responses). Single events were modeled by a boxcar regressor with nonzero entries at the 4300Â millisecondÂ interval starting at the offset of the stimulus and ending immediately after the confidence rating phase, convolved with the canonical hemodynamic response function (HRF). Additionally, the design matrix included a confidence parametric modulator for each of the first two regressors. The construction of the regressors and the additional nuisance regressors was handled similarly to the main design.</p></sec><sec id="s4-9-3"><title>Quadratic-Confidence design matrix (post-hoc analysis; QC-DM)</title><p>The quadratic-confidence design matrix for the univariate GLM analysis consisted of 12 regressors of interest. There was a regressor for each of the four responses: <sc>yes</sc>, <sc>no</sc>, <sc>clockwise</sc> and <sc>anticlockwise</sc>. Similar to the main design matrix, the relevant trials were modeled by a boxcar regressor with nonzero entries at the 4300 millisecond interval starting at the offset of the stimulus and ending immediately after the confidence rating phase, convolved with the canonical hemodynamic response function (HRF). Each of these primary regressors was accompanied by two parametric modulators, representing the linear and quadratic effects of confidence. Together, the design matrix included 12 regressors (4 responses + 4 linear confidence regressors + 4 quadratic confidence regressors). The QC-DM included the same set of nuisance regressors as the main design matrix.</p></sec><sec id="s4-9-4"><title>Categorical-Confidence design matrices (post-hoc analysis; CC-DM)</title><p>In order to better understand the nature of the linear interaction between confidence in <sc>yes</sc> and <sc>no</sc> responses, we specified a pair of design matricesâone for each taskâin which confidence level was modeled as a categorical variable. Instead of the 8 primary regressors in the main design matrix, this design matrix consisted of only one regressor of interest for all trials, modeled by a boxcar with nonzero entries at the 4300 millisecond interval starting at the offset of the stimulus and ending immediately after the confidence rating phase, convolved with the canonical hemodynamic response function (HRF). This regressor was in turn modulated by a series of 12 dummy (0/1) parametric modulators - one for every response (<sc>yes</sc> and <sc>no</sc> for detection and <sc>clockwise</sc> and <sc>anticlockwise</sc> for discrimination) and confidence rating (1â6 for both tasks). Using two design matrices instead of one allowed us to set discrimination trials to be the baseline category for detection, and detection trials as the baseline for discrimination. These design matrices included the same set of nuisance regressors as the main design matrix.</p><p>For each participant, we used the beta-estimates from the categorical-confidence design matrices as the input to four response-specific multiple linear regression models, with linear confidence and quadratic confidence as predictors, in addition to an intercept term. The subject-specific coefficients were then subjected to ordinary least squares group-level inference, to compare linear and quadratic effects of confidence between responses. The rationale for choosing this two-step approach was its indifferenceÂ to the confidence distributions for the four responses, that may bias the estimation of the quadratic and linear terms.</p></sec></sec><sec id="s4-10"><title>Multivariate analysis</title><p>Multi-voxel pattern analysis (<xref ref-type="bibr" rid="bib49">Norman et al., 2006</xref>) was used to test for consistent spatial patterns in the fMRI data. We used The Decoding Toolbox (<xref ref-type="bibr" rid="bib29">Hebart et al., 2015</xref>) and followed the procedures described by <xref ref-type="bibr" rid="bib47">Morales et al. (2018)</xref>. In order to identify brain regions that are implicated in inference about presence and absence, we trained and tested a linear classifier on detection decisions. We classified hits and correct rejections, instead of hits and misses as originally planned, due to an insufficient number of detection misses in some experimental blocks. We then compared the resulting classification accuracy with the cross-classification accuracy of training on detection responses and testing on discrimination confidence and vice versa. The purpose of this comparison was to isolate neural correlates of inference about stimulus absence or presence that should be specific to detection from more general neural correlates of stimulus visibility, that are also expected to affect confidence in discrimination judgementsÂ (seeÂ <xref ref-type="fig" rid="app8fig1">Appendix 8âfigure 1</xref>).</p><p>The other prespecified multivariate tests were designed to find universal and response-specific spatially multivariate representations of confidence. After conducting this analysis we came to realize that our experimental design was not appropriate for estimating the degree to which the representation of confidence is âresponse-generalâ. In our experimental design, confidence is confounded with visual feedback during the confidence-rating phase, such that âresponse-generalâ representations of confidence could appear if the spatial pattern of activation was sensitive to the visual feedback in the confidence rating. For completeness, we include the results of this analysis in the <ext-link ext-link-type="uri" xlink:href="https://osf.io/98mv4/">osfÂ projectÂ page</ext-link>, but do not interpret them further.</p></sec><sec id="s4-11"><title>Statistical inference</title><p>T-test and anova Bayes factors use a Jeffrey-Zellner-Siow Prior for the null distribution, with a unit prior scale (<xref ref-type="bibr" rid="bib53">Rouder et al., 2009</xref>; <xref ref-type="bibr" rid="bib54">Rouder et al., 2012</xref>). Whole-brain fMRI significance was corrected for family-wise error rate at the cluster level (p&lt;0.05), with a cluster defining threshold of p&lt;0.001.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Maayan Keshev, Dan Bang, Madeleine Scott, Peter Zeidman, NadÃ¨ge Corbin, Tim Tierney, Emma Holmes, Max Rollwage, Roni Maimon, Rani Moran, Noam Mazor and the FIL imaging team for their help in different stages of this project. The Wellcome Centre for Human Neuroimaging is supported by core funding from the Wellcome Trust (203147/Z/16/Z). SMF is supported by a Sir Henry Dale Fellowship jointly funded by the Wellcome Trust and the Royal Society (206648/Z/17/Z).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Investigation, Methodology, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: Participants gave their informed consent to take part in the experiment. The experiment was approved by the UCL ethics committee (approval numbers 8231/001 and 1260/003).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-53900-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Group statistical parametric maps are available on NeuroVault:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.collection:6065">https://identifiers.org/neurovault.collection:6065</ext-link>. Single subject behavioural data and functional data from our regions of interest is available on: <ext-link ext-link-type="uri" xlink:href="https://github.com/matanmazor/detectionVsDiscrimination_fMRI">https://github.com/matanmazor/detectionVsDiscrimination_fMRI</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/detectionVsDiscrimination_fMRI">https://github.com/elifesciences-publications/detectionVsDiscrimination_fMRI</ext-link>). Single subject raw anonymized data is available upon request.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Mazor</surname><given-names>M</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Confidence in Detection and Discrimination</data-title><source>NeuroVault</source><pub-id assigning-authority="other" pub-id-type="archive" xlink:href="https://identifiers.org/neurovault.collection:6065">6065</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Mazor</surname><given-names>M</given-names></name><name><surname>Fleming</surname><given-names>S</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Detection and Discrimination Imaging</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="doi">10.17605/OSF.IO/98MV4</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname> <given-names>JL</given-names></name><name><surname>Hutton</surname> <given-names>C</given-names></name><name><surname>Ashburner</surname> <given-names>J</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modeling geometric deformations in EPI time series</article-title><source>NeuroImage</source><volume>13</volume><fpage>903</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0746</pub-id><pub-id pub-id-type="pmid">11304086</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Unified segmentation</article-title><source>NeuroImage</source><volume>26</volume><fpage>839</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.02.018</pub-id><pub-id pub-id-type="pmid">15955494</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahrami</surname> <given-names>B</given-names></name><name><surname>Olsen</surname> <given-names>K</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Roepstorff</surname> <given-names>A</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Optimally interacting minds</article-title><source>Science</source><volume>329</volume><fpage>1081</fpage><lpage>1085</lpage><pub-id pub-id-type="doi">10.1126/science.1185718</pub-id><pub-id pub-id-type="pmid">20798320</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname> <given-names>D</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct encoding of decision confidence in human medial prefrontal cortex</article-title><source>PNAS</source><volume>115</volume><fpage>6082</fpage><lpage>6087</lpage><pub-id pub-id-type="doi">10.1073/pnas.1800795115</pub-id><pub-id pub-id-type="pmid">29784814</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartra</surname> <given-names>O</given-names></name><name><surname>McGuire</surname> <given-names>JT</given-names></name><name><surname>Kable</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The valuation system: a coordinate-based meta-analysis of BOLD fMRI experiments examining neural correlates of subjective value</article-title><source>NeuroImage</source><volume>76</volume><fpage>412</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.02.063</pub-id><pub-id pub-id-type="pmid">23507394</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boorman</surname> <given-names>ED</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How green is the grass on the other side? frontopolar cortex and the evidence in favor of alternative courses of action</article-title><source>Neuron</source><volume>62</volume><fpage>733</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.05.014</pub-id><pub-id pub-id-type="pmid">19524531</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname> <given-names>PW</given-names></name><name><surname>Gilbert</surname> <given-names>SJ</given-names></name><name><surname>Dumontheil</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Function and localization within rostral prefrontal cortex (area 10)</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>362</volume><fpage>887</fpage><lpage>899</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2095</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christensen</surname> <given-names>MS</given-names></name><name><surname>RamsÃ¸y</surname> <given-names>TZ</given-names></name><name><surname>Lund</surname> <given-names>TE</given-names></name><name><surname>Madsen</surname> <given-names>KH</given-names></name><name><surname>Rowe</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An fMRI study of the neural correlates of graded visual perception</article-title><source>NeuroImage</source><volume>31</volume><fpage>1711</fpage><lpage>1725</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.02.023</pub-id><pub-id pub-id-type="pmid">16626975</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The many faces of precision (Replies to commentaries on &quot;Whatever next? Neural prediction, situated agents, and the future of cognitive science&quot;)</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>270</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00270</pub-id><pub-id pub-id-type="pmid">23734133</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Kincade</surname> <given-names>MJ</given-names></name><name><surname>Lewis</surname> <given-names>C</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Sapir</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural basis and recovery of spatial attention deficits in spatial neglect</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1603</fpage><lpage>1610</lpage><pub-id pub-id-type="doi">10.1038/nn1574</pub-id><pub-id pub-id-type="pmid">16234807</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname> <given-names>B</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Garrett</surname> <given-names>N</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Confidence in value-based choice</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>105</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/nn.3279</pub-id><pub-id pub-id-type="pmid">23222911</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname> <given-names>B</given-names></name><name><surname>Bobadilla-Suarez</surname> <given-names>S</given-names></name><name><surname>Nouguchi</surname> <given-names>T</given-names></name><name><surname>Sharot</surname> <given-names>T</given-names></name><name><surname>Love</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Social information is integrated into value and confidence judgments according to its reliability</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6066</fpage><lpage>6074</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3880-16.2017</pub-id><pub-id pub-id-type="pmid">28566360</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denison</surname> <given-names>RN</given-names></name><name><surname>Adler</surname> <given-names>WT</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Humans incorporate attention-dependent uncertainty into perceptual decisions and confidence</article-title><source>PNAS</source><volume>115</volume><fpage>11090</fpage><lpage>11095</lpage><pub-id pub-id-type="doi">10.1073/pnas.1717720115</pub-id><pub-id pub-id-type="pmid">30297430</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Domenech</surname> <given-names>P</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Executive control and decision-making in the prefrontal cortex</article-title><source>Current Opinion in Behavioral Sciences</source><volume>1</volume><fpage>101</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2014.10.007</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoso</surname> <given-names>M</given-names></name><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human cognition. foundations of human reasoning in the prefrontal cortex</article-title><source>Science</source><volume>344</volume><fpage>1481</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1126/science.1252254</pub-id><pub-id pub-id-type="pmid">24876345</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DuguÃ©</surname> <given-names>L</given-names></name><name><surname>Merriam</surname> <given-names>EP</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name><name><surname>Carrasco</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Specific visual subregions of TPJ mediate reorienting of spatial attention</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>2375</fpage><lpage>2390</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx140</pub-id><pub-id pub-id-type="pmid">28981585</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname> <given-names>H</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention, uncertainty, and free-energy</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><elocation-id>215</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00215</pub-id><pub-id pub-id-type="pmid">21160551</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Weil</surname> <given-names>RS</given-names></name><name><surname>Nagy</surname> <given-names>Z</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Relating introspective accuracy to individual differences in brain structure</article-title><source>Science</source><volume>329</volume><fpage>1541</fpage><lpage>1543</lpage><pub-id pub-id-type="doi">10.1126/science.1191883</pub-id><pub-id pub-id-type="pmid">20847276</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Huijgen</surname> <given-names>J</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prefrontal contributions to metacognition in perceptual decision making</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>6117</fpage><lpage>6125</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6489-11.2012</pub-id><pub-id pub-id-type="pmid">22553018</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>van der Putten</surname> <given-names>EJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural mediators of changes of mind about perceptual decisions</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>617</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0104-6</pub-id><pub-id pub-id-type="pmid">29531361</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Lau</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How to measure metacognition</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>443</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00443</pub-id><pub-id pub-id-type="pmid">25076880</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of metacognition in human social interactions</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>367</volume><fpage>2213</fpage><lpage>2223</lpage><pub-id pub-id-type="doi">10.1098/rstb.2012.0123</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geng</surname> <given-names>JJ</given-names></name><name><surname>Vossel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Re-evaluating the role of TPJ in attentional control: contextual updating?</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>37</volume><fpage>2608</fpage><lpage>2620</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2013.08.010</pub-id><pub-id pub-id-type="pmid">23999082</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gherman</surname> <given-names>S</given-names></name><name><surname>Philiastides</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Human VMPFC encodes early signatures of confidence in perceptual decisions</article-title><source>eLife</source><volume>7</volume><elocation-id>e38293</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38293</pub-id><pub-id pub-id-type="pmid">30247123</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glanzer</surname> <given-names>M</given-names></name><name><surname>Adams</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>The mirror effect in recognition memory: data and theory</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>16</volume><fpage>5</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.16.1.5</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname> <given-names>MSA</given-names></name><name><surname>Webb</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The attention schema theory: a mechanistic account of subjective awareness</article-title><source>Frontiers in Psychology</source><volume>06</volume><elocation-id>500</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00500</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname> <given-names>M</given-names></name><name><surname>Wilbertz</surname> <given-names>G</given-names></name><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mesolimbic confidence signals guide perceptual learning in the absence of external feedback</article-title><source>eLife</source><volume>5</volume><elocation-id>e13388</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.13388</pub-id><pub-id pub-id-type="pmid">27021283</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Haarsma</surname> <given-names>J</given-names></name><name><surname>Fletcher</surname> <given-names>PC</given-names></name><name><surname>Ziauddeen</surname> <given-names>H</given-names></name><name><surname>Spencer</surname> <given-names>TJ</given-names></name><name><surname>Diederen</surname> <given-names>KM</given-names></name><name><surname>Murray</surname> <given-names>GK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Precision weighting of cortical unsigned prediction errors is mediated by dopamine and benefits learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/288936</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>GÃ¶rgen</surname> <given-names>K</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The decoding toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id><pub-id pub-id-type="pmid">25610393</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>IgelstrÃ¶m</surname> <given-names>KM</given-names></name><name><surname>Webb</surname> <given-names>TW</given-names></name><name><surname>Graziano</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural processes in the human temporoparietal cortex separated by localized independent component analysis</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>9432</fpage><lpage>9445</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0551-15.2015</pub-id><pub-id pub-id-type="pmid">26109666</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>IgelstrÃ¶m</surname> <given-names>KM</given-names></name><name><surname>Webb</surname> <given-names>TW</given-names></name><name><surname>Kelly</surname> <given-names>YT</given-names></name><name><surname>Graziano</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Topographical organization of attentional, social, and memory processes in the human temporoparietal cortex</article-title><source>Eneuro</source><volume>3</volume><elocation-id>ENEURO.0060-16.2016</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0060-16.2016</pub-id><pub-id pub-id-type="pmid">27280153</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanai</surname> <given-names>R</given-names></name><name><surname>Walsh</surname> <given-names>V</given-names></name><name><surname>Tseng</surname> <given-names>CH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Subjective discriminability of invisibility: a framework for distinguishing perceptual and attentional failures of awareness</article-title><source>Consciousness and Cognition</source><volume>19</volume><fpage>1045</fpage><lpage>1057</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2010.06.003</pub-id><pub-id pub-id-type="pmid">20598906</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kellij</surname> <given-names>S</given-names></name><name><surname>Fahrenfort</surname> <given-names>J</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name><name><surname>Peters</surname> <given-names>MA</given-names></name><name><surname>Odegaard</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The foundations of introspective access: how the relative precision of target encoding influences metacognitive performance</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/xky38</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>J-R</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A model of subjective report and objective discrimination as categorical decisions in a vast representational space</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130204</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0204</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ko</surname> <given-names>Y</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A detection theoretic explanation of blindsight suggests a link between conscious perception and metacognition</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>367</volume><fpage>1401</fpage><lpage>1411</lpage><pub-id pub-id-type="doi">10.1098/rstb.2011.0380</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A higher order bayesian decision theory of consciousness</article-title><source>Progress in Brain Research</source><volume>168</volume><fpage>35</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(07)68004-2</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebreton</surname> <given-names>M</given-names></name><name><surname>Abitbol</surname> <given-names>R</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Pessiglione</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automatic integration of confidence in the brain valuation signal</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1159</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1038/nn.4064</pub-id><pub-id pub-id-type="pmid">26192748</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>SM</given-names></name><name><surname>McCarthy</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional heterogeneity and convergence in the right temporoparietal junction</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>1108</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu292</pub-id><pub-id pub-id-type="pmid">25477367</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maniscalco</surname> <given-names>B</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings</article-title><source>Consciousness and Cognition</source><volume>21</volume><fpage>422</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2011.09.021</pub-id><pub-id pub-id-type="pmid">22071269</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marois</surname> <given-names>R</given-names></name><name><surname>Yi</surname> <given-names>DJ</given-names></name><name><surname>Chun</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The neural fate of consciously perceived and missed events in the attentional blink</article-title><source>Neuron</source><volume>41</volume><fpage>465</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(04)00012-1</pub-id><pub-id pub-id-type="pmid">14766184</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazor</surname> <given-names>M</given-names></name><name><surname>Mazor</surname> <given-names>N</given-names></name><name><surname>Mukamel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A novel tool for time-locking study plans to results</article-title><source>The European Journal of Neuroscience</source><volume>49</volume><fpage>1149</fpage><lpage>1156</lpage><pub-id pub-id-type="doi">10.1111/ejn.14278</pub-id><pub-id pub-id-type="pmid">30462871</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mazor</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Confidence in Detection and Discrimination</data-title><source>GitHub</source><version designator="943ea96">943ea96</version><ext-link ext-link-type="uri" xlink:href="https://github.com/matanmazor/detectionVsDiscrimination_fMRI">https://github.com/matanmazor/detectionVsDiscrimination_fMRI</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCurdy</surname> <given-names>LY</given-names></name><name><surname>Maniscalco</surname> <given-names>B</given-names></name><name><surname>Metcalfe</surname> <given-names>J</given-names></name><name><surname>Liu</surname> <given-names>KY</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Anatomical coupling between distinct metacognitive systems for memory and visual perception</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>1897</fpage><lpage>1906</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1890-12.2013</pub-id><pub-id pub-id-type="pmid">23365229</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merkle</surname> <given-names>EC</given-names></name><name><surname>Van Zandt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An application of the Poisson race model to confidence calibration</article-title><source>Journal of Experimental Psychology: General</source><volume>135</volume><fpage>391</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.135.3.391</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meuwese</surname> <given-names>JDI</given-names></name><name><surname>van Loon</surname> <given-names>AM</given-names></name><name><surname>Lamme</surname> <given-names>VAF</given-names></name><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The subjective experience of object recognition: comparing metacognition for object detection and object categorization</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>61</volume><fpage>1057</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.3758/s13414-014-0643-1</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyniel</surname> <given-names>F</given-names></name><name><surname>Sigman</surname> <given-names>M</given-names></name><name><surname>Mainen</surname> <given-names>ZF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Confidence as bayesian probability: from neural origins to behavior</article-title><source>Neuron</source><volume>88</volume><fpage>78</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.039</pub-id><pub-id pub-id-type="pmid">26447574</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morales</surname> <given-names>J</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Domain-General and Domain-Specific patterns of activity supporting metacognition in human prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>3534</fpage><lpage>3546</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2360-17.2018</pub-id><pub-id pub-id-type="pmid">29519851</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neubert</surname> <given-names>FX</given-names></name><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Thomas</surname> <given-names>AG</given-names></name><name><surname>Sallet</surname> <given-names>J</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparison of human ventral frontal cortex Areas for cognitive control and language with Areas in monkey frontal cortex</article-title><source>Neuron</source><volume>81</volume><fpage>700</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.012</pub-id><pub-id pub-id-type="pmid">24485097</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Polyn</surname> <given-names>SM</given-names></name><name><surname>Detre</surname> <given-names>GJ</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>424</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id><pub-id pub-id-type="pmid">16899397</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname> <given-names>CE</given-names></name><name><surname>Auksztulewicz</surname> <given-names>R</given-names></name><name><surname>Ondobaka</surname> <given-names>S</given-names></name><name><surname>Kilner</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sensorimotor beta power reflects the precision-weighting afforded to sensory prediction errors</article-title><source>NeuroImage</source><volume>200</volume><fpage>59</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.034</pub-id><pub-id pub-id-type="pmid">31226494</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname> <given-names>T</given-names></name><name><surname>Benrimoh</surname> <given-names>DA</given-names></name><name><surname>Vincent</surname> <given-names>P</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Precision and false perceptual inference</article-title><source>Frontiers in Integrative Neuroscience</source><volume>12</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2018.00039</pub-id><pub-id pub-id-type="pmid">30294264</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouault</surname> <given-names>M</given-names></name><name><surname>McWilliams</surname> <given-names>A</given-names></name><name><surname>Allen</surname> <given-names>MG</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Human metacognition across domains: insights from individual differences and neuroimaging</article-title><source>Personality Neuroscience</source><volume>1</volume><elocation-id>e17</elocation-id><pub-id pub-id-type="doi">10.1017/pen.2018.16</pub-id><pub-id pub-id-type="pmid">30411087</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouder</surname> <given-names>JN</given-names></name><name><surname>Speckman</surname> <given-names>PL</given-names></name><name><surname>Sun</surname> <given-names>D</given-names></name><name><surname>Morey</surname> <given-names>RD</given-names></name><name><surname>Iverson</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian t tests for accepting and rejecting the null hypothesis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>225</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.2.225</pub-id><pub-id pub-id-type="pmid">19293088</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouder</surname> <given-names>JN</given-names></name><name><surname>Morey</surname> <given-names>RD</given-names></name><name><surname>Speckman</surname> <given-names>PL</given-names></name><name><surname>Province</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Default bayes factors for ANOVA designs</article-title><source>Journal of Mathematical Psychology</source><volume>56</volume><fpage>356</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2012.08.001</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutishauser</surname> <given-names>U</given-names></name><name><surname>Aflalo</surname> <given-names>T</given-names></name><name><surname>Rosario</surname> <given-names>ER</given-names></name><name><surname>Pouratian</surname> <given-names>N</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Single-Neuron representation of memory strength and recognition confidence in left human posterior parietal cortex</article-title><source>Neuron</source><volume>97</volume><fpage>209</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.029</pub-id><pub-id pub-id-type="pmid">29249283</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname> <given-names>R</given-names></name><name><surname>Wexler</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Making sense of another mind: the role of the right temporo-parietal junction</article-title><source>Neuropsychologia</source><volume>43</volume><fpage>1391</fpage><lpage>1399</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.02.013</pub-id><pub-id pub-id-type="pmid">15936784</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shulman</surname> <given-names>GL</given-names></name><name><surname>Astafiev</surname> <given-names>SV</given-names></name><name><surname>McAvoy</surname> <given-names>MP</given-names></name><name><surname>d'Avossa</surname> <given-names>G</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Right TPJ deactivation during visual search: functional significance and support for a filter hypothesis</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2625</fpage><lpage>2633</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl170</pub-id><pub-id pub-id-type="pmid">17264254</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simons</surname> <given-names>JS</given-names></name><name><surname>Davis</surname> <given-names>SW</given-names></name><name><surname>Gilbert</surname> <given-names>SJ</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name><name><surname>Burgess</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Discriminating imagined from perceived information engages brain Areas implicated in schizophrenia</article-title><source>NeuroImage</source><volume>32</volume><fpage>696</fpage><lpage>703</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.04.209</pub-id><pub-id pub-id-type="pmid">16797186</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sladky</surname> <given-names>R</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>TrÃ¶stl</surname> <given-names>J</given-names></name><name><surname>Cunnington</surname> <given-names>R</given-names></name><name><surname>Moser</surname> <given-names>E</given-names></name><name><surname>Windischberger</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Slice-timing effects and their correction in functional MRI</article-title><source>NeuroImage</source><volume>58</volume><fpage>588</fpage><lpage>594</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.06.078</pub-id><pub-id pub-id-type="pmid">21757015</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>MS</given-names></name><name><surname>Simons</surname> <given-names>JS</given-names></name><name><surname>Gilbert</surname> <given-names>SJ</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name><name><surname>Burgess</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Distinct roles for lateral and medial rostral prefrontal cortex in source monitoring of perceived and imagined events</article-title><source>Neuropsychologia</source><volume>46</volume><fpage>1442</fpage><lpage>1453</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.12.029</pub-id><pub-id pub-id-type="pmid">18294660</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vickers</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1979">1979</year><chapter-title>Confidence</chapter-title><source>Decision Processes in Visual Perception</source><publisher-name>Academic Press</publisher-name><fpage>171</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-721550-1.50011-9</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickens</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Elementary Signal Detection Theory</source><publisher-loc>USA</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195092509.001.0001</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wokke</surname> <given-names>ME</given-names></name><name><surname>Cleeremans</surname> <given-names>A</given-names></name><name><surname>Ridderinkhof</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sure I'm sure: prefrontal oscillations support metacognitive monitoring of decision making</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>781</fpage><lpage>789</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1612-16.2016</pub-id><pub-id pub-id-type="pmid">28123015</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yokoyama</surname> <given-names>O</given-names></name><name><surname>Miura</surname> <given-names>N</given-names></name><name><surname>Watanabe</surname> <given-names>J</given-names></name><name><surname>Takemoto</surname> <given-names>A</given-names></name><name><surname>Uchida</surname> <given-names>S</given-names></name><name><surname>Sugiura</surname> <given-names>M</given-names></name><name><surname>Horie</surname> <given-names>K</given-names></name><name><surname>Sato</surname> <given-names>S</given-names></name><name><surname>Kawashima</surname> <given-names>R</given-names></name><name><surname>Nakamura</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Right frontopolar cortex activity correlates with reliability of retrospective rating of confidence in short-term recognition memory performance</article-title><source>Neuroscience Research</source><volume>68</volume><fpage>199</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2010.07.2041</pub-id><pub-id pub-id-type="pmid">20688112</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Confidence button presses</title><fig id="app1fig1" position="float"><label>Appendix 1âfigure 1.</label><caption><title>Average number of button presses for each confidence level, as a function of task.</title><p>More button presses were needed on average to reach the extreme confidence ratings, hence the quadratic shape. No difference between the two tasks was observed in the mean number of button presses for any of the confidence levels. Error bars represent the standard error of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-app1-fig1-v1.tif"/></fig></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s9" sec-type="appendix"><title>zROCÂ curves</title><fig id="app2fig1" position="float"><label>Appendix 2âfigure 1.</label><caption><title>mean zROC curves for the discrimination and detection tasks.</title><p>As expected in a uv-SDT setting, the discrimination curve is approximately linear with a slope of 1, and the detection curve is approximately linear with a shallower slope. Error bars represent the standard error of the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-app2-fig1-v1.tif"/></fig></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><sec id="s10" sec-type="appendix"><title>Global confidence design matrix</title><p>From our pre-specified ROIs, only the vmPFC and BA46 ROIs showed a significant linear effect of confidence in correct responses, in the opposite direction to what we expected based on previous studies. This is likely to be due to the differences in confidence profiles between the detection and discrimination tasks:</p><fig id="app3fig1" position="float"><label>Appendix 3âfigure 1.</label><caption><title>Effect of confidence in correct responses, from the global-confidence design matrix.</title><p>Uncorrected, thresholded at p&lt;0.001. Left: glass brain visualization of the whole brain contrast. Right: yellow-red represent a positive correlation with subjective confidence ratings, and green-blue represent a negative correlation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-app3-fig1-v1.tif"/></fig><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th/><th>Average beta</th><th>T value</th><th>P value</th><th>Standard deviation</th></tr></thead><tbody><tr><td>vmPFC</td><td>-0.35</td><td>-3.06</td><td>4 Ã 10<sup>-3</sup></td><td>0.67</td></tr><tr><td>pMFC</td><td>-0.31</td><td>-2.48</td><td>0.02</td><td>0.74</td></tr><tr><td>precuneus</td><td>0.25</td><td>2.30</td><td>0.03</td><td>0.64</td></tr><tr><td>ventral striatum</td><td>-0.056</td><td>-1.51</td><td>0.14</td><td>0.22</td></tr><tr><td>FPl</td><td>0.16</td><td>1.52</td><td>0.14</td><td>0.64</td></tr><tr><td>FPm</td><td>-0.12</td><td>-1.46</td><td>0.16</td><td>0.48</td></tr><tr><td>BA 46</td><td>0.37</td><td>3.77</td><td>6 Ã 10<sup>-4</sup></td><td>0.57</td></tr></tbody></table></table-wrap></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><sec id="s11" sec-type="appendix"><title>Main effect of task</title><fig id="app4fig1" position="float"><label>Appendix 4âfigure 1.</label><caption><title>main effect of task, from the main design matrix.</title><p>Uncorrected, thresholded at p&lt;0.001. Left: glass brain visualization of the whole brain contrast. Right: yellow-red represent stronger activations in detection, and green-blue in discrimination. None of our ROIs showed a main effect of task (detection vs. discrimination).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-app4-fig1-v1.tif"/></fig><table-wrap id="inlinetable2" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th/><th>Average beta</th><th>T value</th><th>P value</th><th>Standard deviation</th></tr></thead><tbody><tr><td>vmPFC</td><td>-0.01</td><td>-0.05</td><td>0.96</td><td>1.64</td></tr><tr><td>pMFC</td><td>0.15</td><td>0.60</td><td>0.55</td><td>1.45</td></tr><tr><td>precuneus</td><td>-0.04</td><td>-0.16</td><td>0.87</td><td>1.65</td></tr><tr><td>ventral striatum</td><td>0.09</td><td>0.77</td><td>0.45</td><td>0.72</td></tr><tr><td>FPl</td><td>0.28</td><td>1.08</td><td>0.29</td><td>1.55</td></tr><tr><td>FPm</td><td>5 ÃÂ 10<sup>-3</sup></td><td>0.02</td><td>0.98</td><td>1.22</td></tr><tr><td>BA 46</td><td>0.38</td><td>1.19</td><td>0.24</td><td>1.89</td></tr></tbody></table></table-wrap></sec></boxed-text></app><app id="appendix-5"><title>Appendix 5</title><boxed-text><sec id="s12" sec-type="appendix"><title>Effect of confidence in our pre-specified ROIs</title><fig id="app5fig1" position="float"><label>Appendix 5âfigure 1.</label><caption><title>Effect of confidence in all 4 ROIs, as a function of task and response, as extracted from the categorical design matrix.</title><p>No significant interaction between the linear or quadratic effects and task or response was observed in any of the ROIs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-app5-fig1-v1.tif"/></fig></sec></boxed-text></app><app id="appendix-6"><title>Appendix 6</title><boxed-text><sec id="s13" sec-type="appendix"><title>SDT variance ratio correlation with the quadratic confidence effect</title><fig id="app6fig1" position="float"><label>Appendix 6âfigure 1.</label><caption><title>Inter-subject correlation between the quadratic effect in the right hemisphere clusters and the ratio between the detection (top panel) and discrimination (lower panel) distribution variances, as estimated from the zROC curve slopes in the two tasks.</title><p>Marker color indicates the goodness of fit of the second-order polynomial model to the BOLD data. All Spearman correlation coefficients areÂ &lt;0.25.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-app6-fig1-v1.tif"/></fig></sec></boxed-text></app><app id="appendix-7"><title>Appendix 7</title><boxed-text><sec id="s14" sec-type="appendix"><title>Correlation of metacognitive efficiency with linear and quadratic confidence effects</title><fig id="app7fig1" position="float"><label>Appendix 7âfigure 1.</label><caption><title>Inter-subject correlation between the linear (upper panel) and quadratic (lower panel) effects in the right hemisphere clusters and the metacognitive efficiency scores (measured as MÂ ratioÂ =Â meta-d'/d', <xref ref-type="bibr" rid="bib39">Maniscalco and Lau, 2012</xref>).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-app7-fig1-v1.tif"/></fig></sec></boxed-text></app><app id="appendix-8"><title>Appendix 8</title><boxed-text><sec id="s15" sec-type="appendix"><title>Confidence-decision cross classification</title><fig id="app8fig1" position="float"><label>Appendix 8âfigure 1.</label><caption><title>Accuracy minus chance for classification of response in detection (<sc>yes</sc> vs. <sc>no</sc>; blue), and from a cross-classification between tasks: confidence in detection and confidence in discrimination (gray), and confidence in discrimination and decision in detection (pink).</title><p>In order to dissociate between brain regions that encode stimulus visibility and brain regions that encode decision confidence, we performed a multivariate cross-classification analysis. We trained a linear classifier on detection decisions (<sc>yesÂ </sc>andÂ <sc>no</sc>), and tested it on discrimination confidence (high and low), and vice versa. Shared information content between detection responses and confidence in discrimination is expected in brain regions that encode stimulus visibility, rather than accuracy estimation. In detection,Â <sc>yesÂ </sc>responses are associated with higher stimulus visibility compared toÂ <sc>noÂ </sc>responses (regardless of decision confidence), and in discrimination high confidence trials are associated with higher visibility than low confidence trials (regardless of subjective confidence).</p><p>Presented cross classification scores are the mean of cross classification accuracies in both directions. Detection-response and discrimination-confidence cross-classification was significantly above chance in in the pMFC (t(29)=2.76, p&lt;0.05, corrected for family-wise error across the four ROIs), and in the BA46 anatomical subregion of the frontopolar ROI (t(29)=2.64, p&lt;0.05, corrected).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53900-app8-fig1-v1.tif"/></fig></sec></boxed-text></app><app id="appendix-9"><title>Appendix 9</title><boxed-text><sec id="s16" sec-type="appendix"><title>Static signal detection theory</title><sec id="s16-1"><title>Discrimination</title><sec id="s16-1-1"><title>Generative model</title><p>According to SDT, a decision variable <inline-formula><mml:math id="inf7"><mml:mi>x</mml:mi></mml:math></inline-formula> is sampled from one of two distributions on each experimental trial.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>if {CW}</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>if {ACW}</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s16-1-2"><title>Inference</title><p><inline-formula><mml:math id="inf8"><mml:mi>x</mml:mi></mml:math></inline-formula> is compared against a criterion to generate a decision about which of the two distributions was most likely, given the sample. For a discrimination task with equallyÂ likelyÂ symmetric distributions around 0, the optimal placement for a criterion is at 0.<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd columnalign="left"><mml:mtext>CW</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>ifÂ </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>ACW</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>else</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In standard discrimination tasks, a common assumption is that the two distributions are Gaussian with equal variance. This assumption has a convenient computational consequence: the log-likelihood ratio (LLR), a quantity that reflects the degree to which the sample is more likely under one distribution or another, is linear with respect to <inline-formula><mml:math id="inf9"><mml:mi>x</mml:mi></mml:math></inline-formula>. Confidence is then assumed to be proportional to the distance of <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> from the decision criterion.</p><p>In what follows <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>Î¼</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the likelihood of observing x when sampling from a normal distribution with mean Î¼ and standard deviation <inline-formula><mml:math id="inf12"><mml:mi>Ï</mml:mi></mml:math></inline-formula>.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="s16-2"><title>Detection</title><sec id="s16-2-1"><title>Generative model</title><p>A common assumption is that in detection the signal distribution is wider than the noise distribution (unequal-variance SDT; <xref ref-type="bibr" rid="bib62">Wickens, 2002</xref>, section 3.4).<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1.3</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if P</mml:mtext></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if A</mml:mtext></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if P</mml:mtext></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if A</mml:mtext></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s16-2-2"><title>Inference</title><p>Here <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the median sensory sample <inline-formula><mml:math id="inf14"><mml:mi>x</mml:mi></mml:math></inline-formula>. This criterion was chosen to ensure that detection responses are balanced.<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd columnalign="left"><mml:mtext>P</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>ifÂ </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mtext>A</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>else</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Importantly, in uv-SDT, LLR is quadratic in x.<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>1.3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec></sec></boxed-text></app><app id="appendix-10"><title>Appendix 10</title><boxed-text><sec id="s17" sec-type="appendix"><title>Dynamic criterion</title><p>In SDT, task performance depends on the degree of overlap between the underlying distributions (dâ) and on the positioning of the decision criterion (c). Participants may optimize criterion placement based on their changing beliefs about the underlying distributions (<xref ref-type="bibr" rid="bib36">Lau, 2007</xref>; <xref ref-type="bibr" rid="bib35">Ko and Lau, 2012</xref>). To model this dynamic process of criterion setting we simulated a model where beliefs about the underlying distributions are the Maximum Likelihood Estimates of the mean and standard deviation, based on the last 5 samples that were (correctly or not) categorized.</p><sec id="s17-1"><title>Discrimination</title><sec id="s17-1-1"><title>Generative Model</title><p>As in the Static Signal Detection model.</p></sec><sec id="s17-1-2"><title>Inference</title><p>Means and standard deviations of the two distributions are estimated based on the last 5 samples in each category. To model prior beliefs about these parameters, each participant starts the task with 5 imaginary samples from the veridical distributions. Means and standard deviations are then extracted from these imaginary samples. In what follows, <inline-formula><mml:math id="inf15"><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo stretchy="false">â</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mover accent="true"><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo stretchy="false">â</mml:mo></mml:mover></mml:math></inline-formula> are vectors with entries corresponding to the last 5 samples that were (correcly or not) labelled as <sc>clockwise</sc> and <sc>anticlockwise</sc>, respectively. <inline-formula><mml:math id="inf17"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> correspond to the sample means of these vectors. <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> correspond to their standard deviations.<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Decisions and confidence are extracted from the <italic>LLR</italic> as in the Static Signal Detection model.</p></sec></sec><sec id="s17-2"><title>Detection</title><sec id="s17-2-1"><title>Generative Model</title><p>As in the Static Signal Detection model.</p></sec><sec id="s17-2-2"><title>Inference</title><p>As in discrimination. In what follows, <inline-formula><mml:math id="inf21"><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">â</mml:mo></mml:mover></mml:math></inline-formula> are vectors with entries corresponding to the last 5 samples that were (correcly or not) labelled as âsignal absentâ and âsignal presentâ, respectively. <inline-formula><mml:math id="inf23"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> correspond to the sample means of these vectors. <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>Ï</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> correspond to their standard deviations.<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>g</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In detection, <italic>LLR</italic>Â =Â 0 at two points (see <xref ref-type="fig" rid="fig6">Figure 6</xref>). The decision criterion <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is chosen to coincide with the rightmost point, which is positioned between the Signal and Noise distribution means.<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mtext>P</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>ifÂ </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>A</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mtext>else</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec></sec></boxed-text></app><app id="appendix-11"><title>Appendix 11</title><boxed-text><sec id="s18" sec-type="appendix"><title>Attention monitoring</title><p>Similar to the Dynamic Criterion model, in the Attention Monitoring model participants adjust a decision criterion based on changing beliefs about the underlying distributions. However, unlike the Dynamic Criterion model, here beliefs change not as a function of recent perceptual samples, but as a function of access to an internal variable that represents the expected sensory precision (attention).</p><sec id="s18-1"><title>Discrimination</title><sec id="s18-1-1"><title>Generative model</title><p>In our schematic formulation of this model, participants have a true attentional state, which for simplicity we treat as either being on (1) or off (0). When attending, participatns enjoy higher sensitivity than when they areÂ notÂ attending.<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></disp-formula></p><p>The attentional state determines the means of sensory distributions.<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if {CW} andÂ </mml:mtext><mml:mrow><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if {ACW} andÂ </mml:mtext><mml:mrow><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if {CW} andÂ </mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if {ACW} andÂ </mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>However, they doÂ not have direct access to their attentional state, but only to a noisy approximation of the probability that they were attending.<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>â¼</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mtext>if </mml:mtext><mml:mrow><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>t</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s18-1-2"><title>Inference</title><p>Participants are then assumed to use their knowledge about the <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>T</mml:mi><mml:mo>â¢</mml:mo><mml:mi>a</mml:mi><mml:mo>â¢</mml:mo><mml:mi>s</mml:mi><mml:mo>â¢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> variable when making a decision and confidence estimate.<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>{CW}</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>{ACW}</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>{CW}</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>{ACW}</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mtext>{CW}</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>ifÂ </mml:mtext><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>{ACW}</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>else</mml:mtext><mml:mo>.</mml:mo><mml:mtext mathbackground="#ffff88" mathcolor="#cc0000" mathsize="100%">\par</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="s18-2"><title>Detection</title><sec id="s18-2-1"><title>Generative model</title><p>In detection, attentional states only affect the signal distribution, as noise is always centred at 0.<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if {A} andÂ </mml:mtext><mml:mrow><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if {P} andÂ </mml:mtext><mml:mrow><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if {A} andÂ </mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if {P} andÂ </mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ð©</mml:mi><mml:mo>â¢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s18-2-2"><title>Inference</title><p><disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>{P}</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The likelihood of observing <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> if no stimulus was presented is independent of the attention state.<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>{A}</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Â¬</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mtext class="ltx_font_smallcaps" mathvariant="normal">p</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mtext class="ltx_font_smallcaps" mathvariant="normal">a</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mtext>{P}</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>ifÂ </mml:mtext><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>{A}</mml:mtext><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>else</mml:mtext><mml:mo>.</mml:mo><mml:mtext mathbackground="#ffff88" mathcolor="#cc0000" mathsize="100%">\par</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Nevertheless, confidence in judgments about stimulus absence is dependent on beliefs about the attentional state. This is mediated by the effect of attention on the likelihood of observing <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> if a stimulus were present. This is the counterfactual part.<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:mi>o</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>f</mml:mi><mml:mo>â¢</mml:mo><mml:mi>i</mml:mi><mml:mo>â¢</mml:mo><mml:mi>d</mml:mi><mml:mo>â¢</mml:mo><mml:mi>e</mml:mi><mml:mo>â¢</mml:mo><mml:mi>n</mml:mi><mml:mo>â¢</mml:mo><mml:mi>c</mml:mi><mml:mo>â¢</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>L</mml:mi><mml:mo>â¢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53900.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Graziano</surname><given-names>Michael</given-names> </name><role>Reviewer</role><aff><institution>Princeton University</institution></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This study explores whether neural correlates of metacognitive judgments differ between detection and discrimination tasks. Results show that neural correlates of confidence in fronto-polar cortex differ by task, such that activity correlates non-linearly with confidence in detection tasks. In addition, the authors show that right temporoparietal junction is uniquely recruited for absence judgments.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Michael Graziano (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This study explores the neural correlates of metacognitive judgments of detection compared to discrimination. The authors find that confidence in detection judgments differs from that of discrimination in frontopolar cortex, and that right TPJ may be involved in confidence for absence judgments. Most importantly, they show that activity in detection judgements has a quadratic relationship with confidence ratings. All reviewers agreed that the paper is interesting and informative. However, there were also questions regarding the Discussion and whether alternative explanations have been adequately explored.</p><p>Essential revisions:</p><p>1) More needs to be said in the Discussion about what the quadratic confidence effects really mean. Do these signals originate from neurons that show a quadratic relationship between firing and confidence, or do they come from two overlapping populations that show positive and negative linear relationships with confidence, respectively? In the former case, what do neurons that respond to only to very high and very low confidence really signal. To these neurons encode confidence as such or a function of confidence? For instance, such a function could be the confidence of the confidence rating, which, as for any subjective report, should be higher at the extremes of the scale. However, in this case, it is the confidence about the confidence rating not the confidence about the perceptual judgement. The question is whether you would still observe quadratic confidence effects in detection task if there was no confidence rating? The authors bring up the related concept of meta-confidence as it can be derived from the Bayesian inference model. But it is not made clear enough that here confidence is on precision not on the detection judgement itself. These issues of meta-confidence need to be discussed more expansively and also independently of the Bayesian model.</p><p>2) Related to this, in several areas the quadratic effect is stronger in the detection than in the discrimination task. So this says that these regions care about whether you're really confident, in which case you should probably learn something about your environment based on feedback regarding the outcome of your choice (e.g. Guggenmos et al., 2016), or you're really not confident, in which case you should not update your model of the world at all. So what are these areas coding for? How much you care to update your world model based on the outcome of a choice? What does the quadratic relationship buy the organism?</p><p>3) Still related to the interpretation of the quadratic effect, it would be helpful to expand on the models and mechanisms that can give rise to nonlinear confidence effects. For instance, the suggestion that equal vs. unequal variance of the sample in the SDT model can explain linear vs. nonlinear effects is very interesting but this needs to be unpacked and explained in much more detail. The same is true (though to a lesser degree) for the Bayesian model. It may be helpful to include a figure illustrating how different models can explain nonlinear confidence effects. In general, these theoretical/conceptual considerations are an essential part of the current study and it would be important for the authors to expand on this.</p><p>4) The difference in response-conditional metacognitive sensitivity between yes and no responses, as shown also by Meuwese et al., and the lack of difference between YES-conditional metacognitive sensitivity and discrimination-based metacognitive sensitivity, suggests that the difference in metacognitive sensitivity may result from difference in variance of the underlying internal response distributions. If YES- and discrimination-dependent metacognitive sensitivity is similar, and the only one that is different is no-dependent metacognitive sensitivity, doesn't that imply simply that the signal distribution has larger variance than the noise across both tasks? Some simulations could be done to evaluate this possibility. This would be in line with a report from Kellij et al. (2018) on Psyarxiv (doi: 10.31234/osf.io/xky38), suggesting that asymmetric variance is wholly responsible for differences in t2AUC.</p><p>5) The exploratory analysis sought regions in which the quadratic effect of confidence was stronger in the detection than the discrimination task. What about the other way around? Were there ROIs where the quadratic effect of confidence was stronger for discrimination than detection?</p><p>6) Was the difference between quadratic effects in detection versus discrimination related to the difference in the (negative) linear effects, in any of these ROIs? In other words, is this just a regression to the mean problem, where you're having trouble finding areas that show either linear or quadratic effects of confidence? It seems that the explanation offered in the Discussion may be of utility here, such that if one specifies the SDT system in LLR space there is a quadratic relationship between the internal estimate x and the LLR is quadratic for unequal variance systems (Discussion, eighth paragraph). Although the measure of variance inequality across individuals was not correlated with the quadratic effect of confidence in the reported ROIs, I wonder if this might be mediated by the SNR of the BOLD signal in each individual, which could maybe be informed by the relationship between the linear differences between detection and discrimination and their quadratic relationship difference. In other words, if the linear relationship is weak in a given person, that could also imply a weaker quadratic relationship due to irrelevant factors such as SNR of the BOLD signal. Unless I am missing a point, this could destroy cross-subject relationships between zROC-based estimates of variance inequality and quadratic magnitude. At the least, the authors could test whether the variance imbalanced revealed by the slope of the zROC is related to any of these measures.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53900.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) More needs to be said in the Discussion about what the quadratic confidence effects really mean. Do these signals originate from neurons that show a quadratic relationship between firing and confidence, or do they come from two overlapping populations that show positive and negative linear relationships with confidence, respectively? In the former case, what do neurons that respond to only to very high and very low confidence really signal. To these neurons encode confidence as such or a function of confidence? For instance, such a function could be the confidence of the confidence rating, which, as for any subjective report, should be higher at the extremes of the scale. However, in this case, it is the confidence about the confidence rating not the confidence about the perceptual judgement. The question is whether you would still observe quadratic confidence effects in detection task if there was no confidence rating? The authors bring up the related concept of meta-confidence as it can be derived from the Bayesian inference model. But it is not made clear enough that here confidence is on precision not on the detection judgement itself. These issues of meta-confidence need to be discussed more expansively and also independently of the Bayesian model.</p></disp-quote><p>We thank the reviewers for prompting further reflection on the interpretation of the confidence response profiles here. The observed activation profile in the FPC, STS and pre-SMA could indeed originate from one homogeneous population of neurons that shows a quadratic effect of confidence, or from two overlapping populations that show a nonlinear positive and negative effects of confidence â summing to an overall quadratic effect at the voxel level. It is difficult to evaluate these alternatives with the current design, as they do not make different predictions at the level of the BOLD signal. We are also unable to tell whether these activations would continue to be observed in the absence of an explicit confidence rating, given that a rating was always required in the current design. However, the quadratic response profile cannot be explained as a result of simply using a confidence scale: if this were the case, we would have observed the same pattern in the discrimination trials which also required an explicit subjective judgment. We have now added the following section to the Discussion to make clear where further work is needed to evaluate these alternatives:</p><p>âWe are unable to determine whether this effect originates from one homogeneous population of neurons that shows a quadratic effect of detection confidence, or from two overlapping populations that show nonlinear positive and negative effects of detection confidence â summing to an overall quadratic effect at the voxel level (similar to positive and negative confidence-selective neurons in the human posterior parietal cortex; Rutishauser et al., 2015). [â¦] Future studies which use model-based estimates of covert decision confidence (Bang and Fleming, 2018) or EEG-informed fMRI to resolve early and late processing stages (Gherman and Philiastides, 2018) may answer this question.â</p><p>In response to point (3) below, we now also unpack in greater detail the possible computational mechanisms that may have given rise to a quadratic profile.</p><disp-quote content-type="editor-comment"><p>2) Related to this, in several areas the quadratic effect is stronger in the detection than in the discrimination task. So this says that these regions care about whether you're really confident, in which case you should probably learn something about your environment based on feedback regarding the outcome of your choice (e.g. Guggenmos et al., 2016), or you're really not confident, in which case you should not update your model of the world at all. So what are these areas coding for? How much you care to update your world model based on the outcome of a choice? What does the quadratic relationship buy the organism?</p></disp-quote><p>We agree with the reviewers that this asymmetry in the quadratic profile with respect to confidence is the key result of our paper. The notion that this is related to updating of a model of the task, or learning, is interesting to pursue â and forms the basis of new simulations that we present in response to point (3) below (please see below).</p><disp-quote content-type="editor-comment"><p>3) Still related to the interpretation of the quadratic effect, it would be helpful to expand on the models and mechanisms that can give rise to nonlinear confidence effects. For instance, the suggestion that equal vs. unequal variance of the sample in the SDT model can explain linear vs. nonlinear effects is very interesting but this needs to be unpacked and explained in much more detail. The same is true (though to a lesser degree) for the Bayesian model. It may be helpful to include a figure illustrating how different models can explain nonlinear confidence effects. In general, these theoretical/conceptual considerations are an essential part of the current study and it would be important for the authors to expand on this.</p></disp-quote><p>We agree and are glad of the opportunity to expand on potential models of the quadratic effect. We now simulate three different models that predict detection-specific non-linear effects of confidence: a Signal Detection model, a Dynamic Criterion model, and an Attention Monitoring model. We include simulations from the three models in the appendix and as part of our GitHub repository here: https://github.com/matanmazor/detectionVsDiscrimination_fMRI/tree/master/simulation</p><p>We describe the three models and their predictions in a new section of the Results section (subsection âComputational modelsâ).and include Figure 6 for clarity.</p><disp-quote content-type="editor-comment"><p>4) The difference in response-conditional metacognitive sensitivity between yes and no responses, as shown also by Meuwese et al., and the lack of difference between YES-conditional metacognitive sensitivity and discrimination-based metacognitive sensitivity, suggests that the difference in metacognitive sensitivity may result from difference in variance of the underlying internal response distributions. If YES- and discrimination-dependent metacognitive sensitivity is similar, and the only one that is different is no-dependent metacognitive sensitivity, doesn't that imply simply that the signal distribution has larger variance than the noise across both tasks? Some simulations could be done to evaluate this possibility. This would be in line with a report from Kellij et al. (2018) on Psyarxiv (doi: 10.31234/osf.io/xky38), suggesting that asymmetric variance is wholly responsible for differences in t2AUC.</p></disp-quote><p>We thank the reviewer for suggesting this analysis. Because we used two independent staircase procedures, stimulus visibility (SNR) in detection âsignalâ trials was generally higher than in discrimination. For this reason, signal variance in detection cannot be directly extrapolated from signal variance in discrimination in the same way as Kellij et al. However, we carried out an additional analysis of the detection and discrimination zROC curves to evaluate extent of variance asymmetries, which we now include as Appendix 2âfigure 1. For both tasks the zROC curves were relatively linear, showing a good fit to the SDT assumptions. The discrimination task zROC had a linear slope of approximately ~1, supporting an equal-variance model, whereas the detection task zROC showed a shallower slope of &lt; 1, consistent with an uv-SDT model. However, uv-SDT does not preclude interpretations of the metacognitive disparity effect at the decision-making or metacognitive levels, especially in light of the sensitivity of this effect to the means by which stimuli are made difficult to perceive (Kanai et al., 2011; Kellij et al., 2018).</p><disp-quote content-type="editor-comment"><p>5) The exploratory analysis sought regions in which the quadratic effect of confidence was stronger in the detection than the discrimination task. What about the other way around? Were there ROIs where the quadratic effect of confidence was stronger for discrimination than detection?</p></disp-quote><p>We did not find any brain region that showed stronger effects of confidence (linear or quadratic) for discrimination over detection. This point is now clarified in the manuscript:</p><p>ââ¦voxels, peak voxel: [9,65,-10], Z=4.00). Importantly, no region showed stronger quadratic effects of confidence in discrimination compared to detection.â</p><disp-quote content-type="editor-comment"><p>6) Was the difference between quadratic effects in detection versus discrimination related to the difference in the (negative) linear effects, in any of these ROIs? In other words, is this just a regression to the mean problem, where you're having trouble finding areas that show either linear or quadratic effects of confidence? It seems that the explanation offered in the Discussion may be of utility here, such that if one specifies the SDT system in LLR space there is a quadratic relationship between the internal estimate x and the LLR is quadratic for unequal variance systems (Discussion, eighth paragraph). Although the measure of variance inequality across individuals was not correlated with the quadratic effect of confidence in the reported ROIs, I wonder if this might be mediated by the SNR of the BOLD signal in each individual, which could maybe be informed by the relationship between the linear differences between detection and discrimination and their quadratic relationship difference. In other words, if the linear relationship is weak in a given person, that could also imply a weaker quadratic relationship due to irrelevant factors such as SNR of the BOLD signal. Unless I am missing a point, this could destroy cross-subject relationships between zROC-based estimates of variance inequality and quadratic magnitude. At the least, the authors could test whether the variance imbalanced revealed by the slope of the zROC is related to any of these measures.</p></disp-quote><p>We thank the reviewer for suggesting this useful control analysis. We agree that cross-subject correlation may indeed have been masked by differences in overall noisiness of the signal between participants, especially with this relatively modest sample size for examining between-subject effects. We have now tackled this potential concern in a new analysis. To estimate the noisiness of single subject data, we extracted subject-wise R-squared for the second-order polynomial model predicting BOLD signal from confidence level and response. We replotted Appendix 6âfigure 1, this time with a color-code that indicates this goodness of fit for each subject. If the relation between zROC slope and the quadratic coefficient is masked by variability in overall data quality, a correlation should be unmasked when focusing on only the participants with high R-squared scores. We did not see evidence for such an effect in any of the three clusters â instead, the relationship with log(SD ratio) seemed similar for different subgroups of subjects.</p></body></sub-article></article>