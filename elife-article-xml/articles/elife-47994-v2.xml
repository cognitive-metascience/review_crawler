<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47994</article-id><article-id pub-id-type="doi">10.7554/eLife.47994</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-119284"><name><surname>Graving</surname><given-names>Jacob M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5826-467X</contrib-id><email>jgraving@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140474"><name><surname>Chae</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154182"><name><surname>Naik</surname><given-names>Hemal</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154183"><name><surname>Li</surname><given-names>Liang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2447-6295</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund12"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140477"><name><surname>Koger</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154184"><name><surname>Costelloe</surname><given-names>Blair R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5291-788X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund11"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-154185"><name><surname>Couzin</surname><given-names>Iain D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8556-4558</contrib-id><email>icouzin@ab.mpg.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Collective Behaviour</institution><institution>Max Planck Institute of Animal Behavior</institution><addr-line><named-content content-type="city">Konstanz</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Biology</institution><institution>University of Konstanz</institution><addr-line><named-content content-type="city">Konstanz</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Centre for the Advanced Study of Collective Behaviour</institution><institution>University of Konstanz</institution><addr-line><named-content content-type="city">Konstanz</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Computer Science</institution><institution>Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Chair for Computer Aided Medical Procedures</institution><institution>Technische Universität München</institution><addr-line><named-content content-type="city">Munich</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Baldwin</surname><given-names>Ian T</given-names></name><role>Senior Editor</role><aff><institution>Max Planck Institute for Chemical Ecology</institution><country>Germany</country></aff></contrib><contrib contrib-type="editor"><name><surname>Shaevitz</surname><given-names>Josh W</given-names></name><role>Reviewing Editor</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>01</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e47994</elocation-id><history><date date-type="received" iso-8601-date="2019-04-26"><day>26</day><month>04</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-09-18"><day>18</day><month>09</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Graving et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Graving et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47994-v2.pdf"/><abstract><p>Quantitative behavioral measurements are important for answering questions across scientific disciplines—from neuroscience to ecology. State-of-the-art deep-learning methods offer major advances in data quality and detail by allowing researchers to automatically estimate locations of an animal’s body parts directly from images or videos. However, currently available animal pose estimation methods have limitations in speed and robustness. Here, we introduce a new easy-to-use software toolkit, <italic>DeepPoseKit</italic>, that addresses these problems using an efficient multi-scale deep-learning model, called <italic>Stacked DenseNet</italic>, and a fast GPU-based peak-detection algorithm for estimating keypoint locations with subpixel precision. These advances improve processing speed &gt;2x with no loss in accuracy compared to currently available methods. We demonstrate the versatility of our methods with multiple challenging animal pose estimation tasks in laboratory and field settings—including groups of interacting individuals. Our work reduces barriers to using advanced tools for measuring behavior and has broad applicability across the behavioral sciences.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Studying animal behavior can reveal how animals make decisions based on what they sense in their environment, but measuring behavior can be difficult and time-consuming. Computer programs that measure and analyze animal movement have made these studies faster and easier to complete. These tools have also made more advanced behavioral experiments possible, which have yielded new insights about how the brain organizes behavior.</p><p>Recently, scientists have started using new machine learning tools called deep neural networks to measure animal behavior. These tools learn to measure animal posture – the positions of an animal’s body parts in space – directly from real data, such as images or videos, without being explicitly programmed with instructions to perform the task. This allows deep learning algorithms to automatically track the locations of specific animal body parts in videos faster and more accurately than previous techniques. This ability to learn from images also removes the need to attach physical markers to animals, which may alter their natural behavior.</p><p>Now, Graving et al. have created a new deep learning toolkit for measuring animal behavior that combines components from previous tools with the latest advances in computer science. Simple modifications to how the algorithms are trained can greatly improve their performance. For example, adding connections between layers, or ‘neurons’, in the deep neural network and training the algorithm to learn the full geometry of the body – by drawing lines between body parts – both enhance its accuracy. As a result of adding these changes, the new toolkit can measure an animal's pose from previously unseen images with high speed and accuracy, after being trained on just 100 examples. Graving et al. tested their model on videos of fruit flies, zebras and locusts, and found that, after training, it was able to accurately track the animals’ movements. The new toolkit has an easy-to-use software interface and is freely available for other scientists to use and build on.</p><p>The new toolkit may help scientists in many fields including neuroscience and psychology, as well as other computer scientists. For example, companies like Google and Apple use similar algorithms to recognize gestures, so making those algorithms faster and more efficient may make them more suitable for mobile devices like smartphones or virtual-reality headsets. Other possible applications include diagnosing and tracking injuries, or movement-related diseases in humans and livestock.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Grévy's zebra</kwd><kwd>desert locust</kwd><kwd><italic>D. melanogaster</italic></kwd><kwd><italic>Equus grevyi</italic></kwd><kwd><italic>Schistocerca gregaria</italic></kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>IOS-1355061</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-09-1-1074</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-14-1-0635</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000183</institution-id><institution>Army Research Office</institution></institution-wrap></funding-source><award-id>W911NG-11-1-0385</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000183</institution-id><institution>Army Research Office</institution></institution-wrap></funding-source><award-id>W911NF14-1-0431</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>DFG Centre of Excellence 2117</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010583</institution-id><institution>University of Konstanz</institution></institution-wrap></funding-source><award-id>Zukunftskolleg Investment Grant</award-id><principal-award-recipient><name><surname>Costelloe</surname><given-names>Blair R</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003542</institution-id><institution>Ministry of Science, Research and Art Baden-Württemberg</institution></institution-wrap></funding-source><award-id>The Strukture-und Innovations fonds fur die Forschung of the State of Baden-Wurttemberg</award-id><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max Planck Society</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Couzin</surname><given-names>Iain D</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>Marie Sklodowska-Curie grant agreement No. 748549</award-id><principal-award-recipient><name><surname>Costelloe</surname><given-names>Blair R</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007065</institution-id><institution>Nvidia</institution></institution-wrap></funding-source><award-id>GPU Grant</award-id><principal-award-recipient><name><surname>Costelloe</surname><given-names>Blair R</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution>Nvidia</institution></institution-wrap></funding-source><award-id>GPU Grant</award-id><principal-award-recipient><name><surname>Li</surname><given-names>Liang</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new deep-learning software toolkit with general-purpose methods for quickly and reliably measuring the full body posture of animals directly from images or videos without physical markers.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Understanding the relationships between individual behavior, brain activity (reviewed by <xref ref-type="bibr" rid="bib64">Krakauer et al., 2017</xref>), and collective and social behaviors (<xref ref-type="bibr" rid="bib90">Rosenthal et al., 2015</xref>; <xref ref-type="bibr" rid="bib99">Strandburg-Peshkin et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Jolles et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Klibaite et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Klibaite and Shaevitz, 2019</xref>) is a central goal of the behavioral sciences—a field that spans disciplines from neuroscience to psychology, ecology, and genetics. Measuring and modelling behavior is key to understanding these multiple scales of complexity, and, with this goal in mind, researchers in the behavioral sciences have begun to integrate theory and methods from physics, computer science, and mathematics (<xref ref-type="bibr" rid="bib4">Anderson and Perona, 2014</xref>; <xref ref-type="bibr" rid="bib14">Berman, 2018</xref>; <xref ref-type="bibr" rid="bib18">Brown and de Bivort, 2018</xref>). A cornerstone of this interdisciplinary revolution is the use of state-of-the-art computational tools, such as computer vision algorithms, to automatically measure locomotion and body posture (<xref ref-type="bibr" rid="bib27">Dell et al., 2014</xref>). Such a rich description of animal movement then allows for modeling, from first principles, the full behavioral repertoire of animals (<xref ref-type="bibr" rid="bib97">Stephens et al., 2011</xref>; <xref ref-type="bibr" rid="bib12">Berman et al., 2014b</xref>; <xref ref-type="bibr" rid="bib13">Berman et al., 2016</xref>; <xref ref-type="bibr" rid="bib112">Wiltschko et al., 2015</xref>; <xref ref-type="bibr" rid="bib50">Johnson et al., 2016b</xref>; <xref ref-type="bibr" rid="bib102">Todd et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Klibaite et al., 2017</xref>; <xref ref-type="bibr" rid="bib71">Markowitz et al., 2018</xref>; <xref ref-type="bibr" rid="bib63">Klibaite and Shaevitz, 2019</xref>; <xref ref-type="bibr" rid="bib25">Costa et al., 2019</xref>). Tools for automatically measuring animal movement represent a vital first step toward developing unified theories of behavior across scales (<xref ref-type="bibr" rid="bib14">Berman, 2018</xref>; <xref ref-type="bibr" rid="bib18">Brown and de Bivort, 2018</xref>). Therefore, technical factors like scalability, robustness, and usability are issues of critical importance, especially as researchers across disciplines begin to increasingly rely on these methods.</p><p>Two of the latest contributions to the growing toolbox for quantitative behavioral analysis are from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref>, who make use of a popular type of machine learning model called <italic>convolutional neural networks</italic>, or <italic>CNNs</italic> (<xref ref-type="bibr" rid="bib68">LeCun et al., 2015</xref>; Appendix 2), to automatically measure detailed representations of animal posture—structural <italic>keypoints</italic>, or <italic>joints</italic>, on the animal’s body—directly from images and without markers. While these methods offer a major advance over conventional methods with regard to data quality and detail, they have disadvantages in terms of speed and robustness, which may limit their practical applications. To address these problems, we introduce a new software toolkit, called <italic>DeepPoseKit</italic>, with methods that are fast, robust, and easy-to-use. We run experiments using multiple datasets to compare our new methods with those from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref>, and we find that our approach offers considerable improvements. These results also demonstrate the flexibility of our toolkit for both laboratory and field situations and exemplify the wide applicability of our methods across a range of species and experimental conditions.</p><sec id="s1-1"><title>Measuring animal movement with computer vision</title><p>Collecting high-quality behavioral data is a challenging task, and while direct observations are important for gathering qualitative data about a study system, a variety of automated methods for quantifying movement have become popular in recent years (<xref ref-type="bibr" rid="bib27">Dell et al., 2014</xref>; <xref ref-type="bibr" rid="bib4">Anderson and Perona, 2014</xref>; <xref ref-type="bibr" rid="bib55">Kays et al., 2015</xref>). Methods like video monitoring and recording help to accelerate data collection and reduce the effects of human intervention, but the task of manually scoring videos is time consuming and suffers from the same limitations as direct observation, namely observer bias and mental fatigue. Additionally, due to limitations of human observers’ ability to process information, many studies that rely on manual scoring use relatively small datasets to estimate experimental effects, which can lead to increased rates of statistical errors. Studies that lack the statistical resolution to robustly test hypotheses (commonly called 'power' in frequentist statistics) also raise concerns about the use of animals for research, as statistical errors caused by sparse data can impact researchers’ ability to accurately answer scientific questions. These limitations have led to the development of automated methods for quantifying behavior using advanced imaging technologies (<xref ref-type="bibr" rid="bib27">Dell et al., 2014</xref>) as well as sophisticated tags and collars with GPS, accelerometry, and acoustic-recording capabilities (<xref ref-type="bibr" rid="bib55">Kays et al., 2015</xref>). Tools for automatically measuring the behavior of individuals now play a central role in our ability to study the neurobiology and ecology of animals, and reliance on these technologies for studying animal behavior will only increase in the future.</p><p>The rapid development of computer vision hardware and software in recent years has allowed for the use of automated image-based methods for measuring behavior across many experimental contexts (<xref ref-type="bibr" rid="bib27">Dell et al., 2014</xref>). Early methods for quantifying movement with these techniques required highly controlled laboratory conditions. However, because animals exhibit different behaviors depending on their surroundings (<xref ref-type="bibr" rid="bib101">Strandburg-Peshkin et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Francisco et al., 2019</xref>; <xref ref-type="bibr" rid="bib2">Akhund-Zade et al., 2019</xref>), laboratory environments are often less than ideal for studying many natural behaviors. Most conventional computer vision methods are also limited in their ability to accurately track groups of individuals over time, but nearly all animals are social at some point in their life and exhibit specialized behaviors when in the presence of conspecifics (<xref ref-type="bibr" rid="bib99">Strandburg-Peshkin et al., 2013</xref>; <xref ref-type="bibr" rid="bib90">Rosenthal et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Jolles et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Klibaite et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Klibaite and Shaevitz, 2019</xref>; <xref ref-type="bibr" rid="bib33">Francisco et al., 2019</xref>; <xref ref-type="bibr" rid="bib108">Versace et al., 2019</xref>). These methods also commonly track only the animal’s center of mass, which reduces the behavioral output of an individual to a two-dimensional or three-dimensional particle-like trajectory. While trajectory data are useful for many experimental designs, the behavioral repertoire of an animal cannot be fully described by its aggregate locomotory output. For example, stationary behaviors, like grooming and antennae movements, or subtle differences in walking gaits cannot be reliably detected by simply tracking an animal’s center of mass (<xref ref-type="bibr" rid="bib12">Berman et al., 2014b</xref>; <xref ref-type="bibr" rid="bib112">Wiltschko et al., 2015</xref>).</p><p>Together these factors have driven the development of software that can accurately track the positions of marked (<xref ref-type="bibr" rid="bib26">Crall et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Graving, 2017</xref>; <xref ref-type="bibr" rid="bib111">Wild et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Boenisch et al., 2018</xref>) or unmarked (<xref ref-type="bibr" rid="bib81">Pérez-Escudero et al., 2014</xref>; <xref ref-type="bibr" rid="bib88">Romero-Ferrero et al., 2019</xref>) individuals as well as methods that can quantify detailed descriptions of an animal’s posture over time (<xref ref-type="bibr" rid="bib97">Stephens et al., 2011</xref>; <xref ref-type="bibr" rid="bib12">Berman et al., 2014b</xref>; <xref ref-type="bibr" rid="bib112">Wiltschko et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>). Recently, these advancements have been further improved through the use of deep learning, a class of machine learning algorithms that learn complex statistical relationships from data (<xref ref-type="bibr" rid="bib68">LeCun et al., 2015</xref>). Deep learning has opened the door to accurately tracking large groups of marked (<xref ref-type="bibr" rid="bib111">Wild et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Boenisch et al., 2018</xref>) or unmarked (<xref ref-type="bibr" rid="bib88">Romero-Ferrero et al., 2019</xref>) individuals and has made it possible to measure the body posture of animals in nearly any context—including 'in the wild' (<xref ref-type="bibr" rid="bib78">Nath et al., 2019</xref>)—by tracking the positions of user-defined body parts (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>). These advances have drastically increased the quality and quantity, as well as the diversity, of behavioral data that are potentially available to researchers for answering scientific questions.</p></sec><sec id="s1-2"><title>Animal pose estimation using deep learning</title><p>In the past, conventional methods for measuring posture with computer vision relied on species-specific algorithms (<xref ref-type="bibr" rid="bib104">Uhlmann et al., 2017</xref>), highly specialized or restrictive experimental setups (<xref ref-type="bibr" rid="bib74">Mendes et al., 2013</xref>; <xref ref-type="bibr" rid="bib54">Kain et al., 2013</xref>), attaching intrusive physical markers to the study animal (<xref ref-type="bibr" rid="bib54">Kain et al., 2013</xref>), or some combination thereof. These methods also typically required expert computer-vision knowledge to use, were limited in the number or type of body parts that could be tracked (<xref ref-type="bibr" rid="bib74">Mendes et al., 2013</xref>), involved capturing and handling the study animals to attach markers (<xref ref-type="bibr" rid="bib54">Kain et al., 2013</xref>)—which is not possible for many species—and despite best efforts to minimize human involvement, often required manual intervention to correct errors (<xref ref-type="bibr" rid="bib104">Uhlmann et al., 2017</xref>). These methods were all built to work for a small range of conditions and typically required considerable effort to adapt to novel contexts.</p><p>In contrast to conventional computer-vision methods, modern deep-learning–﻿based methods can be used to achieve near human-level accuracy in almost any scenario by manually annotating data (<xref ref-type="fig" rid="fig1">Figure 1</xref>)—known as a <italic>training set</italic>—and training a general-purpose image-processing algorithm—a convolutional neural network or CNN—to automatically estimate the locations of an animal’s body parts directly from images (<xref ref-type="fig" rid="fig2">Figure 2</xref>). State-of-the-art machine learning methods, like CNNs, use these training data to parameterize a model describing the statistical relationships between a set of input data (i.e., images) and the desired output distribution (i.e., posture keypoints). After adequate training, a model can be used to make predictions on previously-unseen data from the same dataset—inputs that were not part of the training set, which is known as <italic>inference</italic>. In other words, these models are able to generalize human-level expertise at scale after having been trained on only a relatively small number of examples. We provide more detailed background information on using CNNs for pose estimation in Appendices 2–6.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>An illustration of the workflow for DeepPoseKit.</title><p>Multi-individual images are localized, tracked, and preprocessed into individual images, which is not required for single-individual image datasets. An initial image set is sampled, annotated, and then iteratively updated using the active learning approach described by <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> (see Appendix 3). As annotations are made, the model is trained (<xref ref-type="fig" rid="fig2">Figure 2</xref>) with the current training set and keypoint locations are initialized for unannotated data to reduce the difficulty of further annotations. This is repeated until there is a noticeable improvement plateau for the initialized data—where the annotator is providing only minor corrections—and for the validation error when training the model (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). New data from the full dataset are evaluated with the model, and the training set is merged with new examples that are sampled based on the model’s predictive performance, which can be assessed with techniques described by <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref> for identifying outlier frames and minimizing extreme prediction errors—shown here as the distribution of confidence scores predicted by the model and predicted body part positions with large temporal derivatives, indicating extreme errors. This process is repeated as necessary until performance is adequate when evaluating new data. The pose estimation model can then be used to make predictions for the full data set, and the data can be used for further analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-fig1-v2.tif"/></fig><media id="fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-47994-fig1-video1.mp4"><label>Figure 1—video 1.</label><caption><title>A visualization of the posture data output for a group of locusts (5× speed).</title></caption></media></fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>An illustration of the model training process for our Stacked DenseNet model in DeepPoseKit (see Appendix 2 for details about training models).</title><p>Input images <inline-formula><mml:math id="inf1"><mml:mi>x</mml:mi></mml:math></inline-formula> (top-left) are augmented (bottom-left) with various spatial transformations (rotation, translation, scale, etc.) followed by noise transformations (dropout, additive noise, blurring, contrast, etc.) to improve the robustness and generalization of the model. The ground truth annotations are then transformed with matching spatial augmentations (not shown for the sake of clarity) and used to draw the confidence maps <inline-formula><mml:math id="inf2"><mml:mi>y</mml:mi></mml:math></inline-formula> for the keypoints and hierarchical posture graph (top-right). The images <inline-formula><mml:math id="inf3"><mml:mi>x</mml:mi></mml:math></inline-formula> are then passed through the network to produce a multidimensional array <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>—a stack of images corresponding to the keypoint and posture graph confidence maps for the ground truth <inline-formula><mml:math id="inf5"><mml:mi>y</mml:mi></mml:math></inline-formula>. Mean squared error between the outputs for both networks <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the ground truth data <inline-formula><mml:math id="inf8"><mml:mi>y</mml:mi></mml:math></inline-formula> is then minimized (bottom-right), where <inline-formula><mml:math id="inf9"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates a subset of the output from <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>—only those feature maps being optimized to reproduce the confidence maps for the purpose of intermediate supervision (Appendix 5). The loss function is minimized until the validation loss stops improving—indicating that the model has converged or is starting to overfit to the training data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-fig2-v2.tif"/></fig><p>Similar to conventional pose estimation methods, the task of implementing deep-learning models in software and training them on new data is complex and requires expert knowledge. However, in most cases, once the underlying model and training routine are implemented, a high-accuracy pose estimation model for a novel context can be built with minimal modification—often just by changing the training data. With a simplified toolkit and high-level software interface designed by an expert, even scientists with limited computer-vision knowledge can begin to apply these methods to their research. Once the barriers for implementing and training a model are sufficiently reduced, the main bottleneck for using these methods becomes collecting an adequate training set—a labor-intensive task made less time-consuming by techniques described in Appendix 3.</p><p><xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> were the first to popularize the use of CNNs for animal pose estimation. These researchers built on work from the human pose estimation literature (e.g., <xref ref-type="bibr" rid="bib5">Andriluka et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib79">Newell et al., 2016</xref>) using a type of <italic>fully-convolutional neural network</italic> or <italic>F-CNN</italic> (<xref ref-type="bibr" rid="bib70">Long et al., 2015</xref>; Appendix 4) often referred to as an <italic>encoder-decoder</italic> model (Appendix 4: 'Encoder-decoder models'). These models are used to measure animal posture by training the network to transform images into probabilistic estimates of keypoint locations, known as <italic>confidence maps</italic> (shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>), that describe the body posture for one or more individuals. These confidence maps are processed to produce the 2-D spatial coordinates of each keypoint, which can then be used for further analysis.</p><p>While deep-learning models typically need large amounts of training data, both <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> have demonstrated that near human-level accuracy can be achieved with few training examples (Appendix 3). In order to ensure generalization to large datasets, both groups of researchers introduced ideas related to iteratively refining the training set used for model fitting (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>). In particular, <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> describe a technique known as <italic>active learning</italic> where a trained model is used to initialize new training data and reduce annotation time (Appendix 3). <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> describe multiple techniques that can be used to further refine training data and minimize errors when making predictions on the full dataset. Simple methods to accomplish this include filtering data or selecting new training examples based on confidence scores or the entropy of the confidence maps from the model output. <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref> also introduced the use temporal derivatives (i.e., speed and acceleration) and autoregressive models to identify outlier frames, which can then be labeled to refine the training set or excluded from further analysis on the final dataset (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p></sec><sec id="s1-3"><title>Pose estimation models and the speed-accuracy trade-off</title><p><xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref> developed their pose estimation model, which they call <italic>DeepLabCut</italic>, by modifying a previously published model called <italic>DeeperCut</italic> (<xref ref-type="bibr" rid="bib44">Insafutdinov et al., 2016</xref>). The DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>), like the DeeperCut model, is built on the popular <italic>ResNet</italic> architecture (<xref ref-type="bibr" rid="bib40">He et al., 2016</xref>)—a state-of-the-art deep-learning model used for image classification. This choice is advantageous because the use of a popular architecture allows for incorporating a pre-trained encoder to improve performance and reduce the number of required training examples (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>), known as <italic>transfer learning</italic> (<xref ref-type="bibr" rid="bib82">Pratt, 1992</xref>; Appendix 3)—although, as will be seen, transfer learning appears to offer little improvement over a randomly initialized model. However, this choice of of a pre-trained architecture is also disadvantageous as the model is <italic>overparameterized</italic> with &gt;25 million parameters. Overparameterization allows the model to make accurate predictions, but this may come with the cost of slow inference. To alleviate these effects, work from <xref ref-type="bibr" rid="bib73">Mathis and Warren (2018)</xref> showed that inference speed for the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) can be improved by decreasing the resolution of input images, but this is achieved at the expense of accuracy.</p><p>With regard to model design, <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> implement a modified version of a model called <italic>SegNet</italic> (<xref ref-type="bibr" rid="bib9">Badrinarayanan et al., 2015</xref>), which they call <italic>LEAP</italic> (LEAP Estimates Animal Pose), that attempts to limit model complexity and overparameterization with the goal of maximizing inference speed (see Appendix 6)—however, our comparisons from this paper suggest (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) achieved only limited success compared to the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>). The LEAP model is advantageous because it is explicitly designed for fast inference but has disadvantages such as a lack of robustness to data variance, like rotations or shifts in lighting, and an inability to generalize to new experimental setups. Additionally, to achieve maximum performance, the training routine for the LEAP model introduced by <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> requires computationally expensive preprocessing that is not practical for many datasets, which makes it unsuitable for a wide range of experiments (see Appendix 6 for more details).</p><p>Together the methods from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> represent the two extremes of a phenomenon known as the <italic>speed-accuracy trade-off</italic> (<xref ref-type="bibr" rid="bib43">Huang et al., 2017b</xref>)—an active area of research in the machine learning literature. <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> prioritize accuracy over speed by using a large overparameterized model (<xref ref-type="bibr" rid="bib44">Insafutdinov et al., 2016</xref>), and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> prioritize speed over accuracy by using a smaller less-robust model. While this speed-accuracy trade-off can limit the capabilities of CNNs, there has been extensive work to make these models more efficient without impacting accuracy (e.g., <xref ref-type="bibr" rid="bib24">Chollet, 2017</xref>; <xref ref-type="bibr" rid="bib42">Huang et al., 2017a</xref>; <xref ref-type="bibr" rid="bib94">Sandler et al., 2018</xref>). To address the limitations of this trade-off, we apply recent developments from the machine learning literature and provide an effective solution to the problem.</p><p>In the case of F-CNN models used for pose estimation, improvements in efficiency and robustness have been made through the use of <italic>multi-scale inference</italic> (Appendix 4: 'Encoder-decoder models') by increasing connectivity between the model’s many layers across multiple spatial scales (<xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>) Multi-scale inference implicitly allows the model to simultaneously integrate large-scale global information, such as the lighting, image background, or the orientation of the focal individual’s body trunk; information from intermediate scales like anatomical geometry related to cephalization and bilateral symmetry; and fine-scale local information that could include differences in color, texture, or skin patterning for specific body parts. This multi-scale design gives the model capacity to learn the hierarchical relationships between different spatial scales and efficiently aggregate them into a joint representation when solving the posture estimation task (see Appendix 4: 'Encoder-decoder models' and <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref> for further discussion).</p></sec><sec id="s1-4"><title>Individual vs. multiple pose estimation</title><p>Most work on human pose estimation now focuses on estimating the pose of multiple individuals in an image (e.g. <xref ref-type="bibr" rid="bib20">Cao et al., 2017</xref>). For animal pose estimation, the methods from <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> are limited to estimating posture for single individuals—known as <italic>individual pose estimation</italic>—while the methods from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> can also be extended to estimate posture for multiple individuals simultaneously—known as <italic>multiple pose estimation</italic>. However, the majority of work on multiple pose estimation, including <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>, has not adequately solved the tracking problem of linking individual posture data across frames in a video, especially after visual occlusions, which are common in many behavioral experiments—although recent work has attempted to address this problem (<xref ref-type="bibr" rid="bib45">Iqbal et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Andriluka et al., 2018</xref>). Additionally, as the name suggests, the task of multiple pose estimation requires exhaustively annotating images of multiple individuals—where every individual in the image must be annotated to prevent the model from learning conflicting information. This type of annotation task is even more laborious and time consuming than annotations for individual pose estimation and the amount of labor increases proportionally with the number of individuals in each frame, which makes this approach intractable for many experimental systems.</p><p>Reliably tracking the position of individuals over time is important for most behavioral studies, and there are a number of diverse methods already available for solving this problem (<xref ref-type="bibr" rid="bib81">Pérez-Escudero et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Crall et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Graving, 2017</xref>; <xref ref-type="bibr" rid="bib88">Romero-Ferrero et al., 2019</xref>; <xref ref-type="bibr" rid="bib111">Wild et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Boenisch et al., 2018</xref>). Therefore, to avoid solving an already-solved problem of tracking individuals and to circumvent the cognitively complex task of annotating data for multiple pose estimation, the work we describe in this paper is purposefully limited to individual pose estimation—where each image contains only a single focal individual, which may be cropped from a larger multi-individual image after localization and tracking. We introduce a top-down posture estimation framework that can be readily adapted to existing behavioral analysis workflows, which could include any method for localizing and tracking individuals.</p><p>The additional step of localizing and tracking individuals naturally increases the processing time for producing posture data from raw image data, which varies depending on the algorithms being used and the number of individuals in each frame. While tracking and localization may not be practical for all experimental systems, which could make our methods difficult to apply 'out-of-the-box', the increased processing time from automated tracking algorithms is a reasonable trade-off for most systems given the costly alternative of increased manual labor when annotating data. This trade-off seems especially practical when considering that the posture data produced by most multiple pose estimation algorithms still need to be linked across video frames to maintain the identity of each individual, which is effectively a bottom-up method for achieving the same result. Limiting our methods to individual pose estimation also simplifies the pose detection problem as processing confidence maps produced by the model does not require computationally-expensive local peak detection and complex methods for grouping keypoints into individual posture graphs (e.g. <xref ref-type="bibr" rid="bib44">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">Cao et al., 2017</xref>; Appendix 4). Additionally, because individual pose estimation is such a well-studied problem in computer vision, we can readily build on state-of-the-art methods for this task (see Appendices 4 and 5 for details).</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><p>Here, we introduce fast, flexible, and robust pose estimation methods, with a software interface—a high-level programming interface (API) and graphical user-interface (GUI) for annotations—that emphasizes usability. Our methods build on the state-of-the-art for individual pose estimation (<xref ref-type="bibr" rid="bib79">Newell et al., 2016</xref>; Appendix 5), convolutional regression models (<xref ref-type="bibr" rid="bib48">Jégou et al., 2017</xref>; Appendix 4: 'Encoder-decoder models'), and conventional computer vision algorithms (<xref ref-type="bibr" rid="bib38">Guizar-Sicairos et al., 2008</xref>) to improve model efficiency and achieve faster, more accurate results on multiple challenging pose estimation tasks. We developed two model implementations—including a new model architecture that we call <italic>Stacked DenseNet</italic>—and a new method for processing confidence maps called <italic>subpixel maxima</italic> that provides fast and accurate peak detection for estimating keypoint locations with subpixel precision—even at low spatial resolutions. We also discuss a modification to incorporate a hierarchical posture graph for learning the multi-scale geometry between keypoints on the animal’s body, which increases accuracy when training pose estimation models. We ran experiments to optimize our approach and compared our new models to the models from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> (DeepLabCut) and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> (LEAP) in terms of speed, accuracy, training time, and generalization ability. We benchmarked these models using three image datasets recorded in the laboratory and the field—including multiple interacting individuals that were first localized and cropped from larger, multi-individual images (see 'Materials and methods’ for details).</p><sec id="s2-1"><title>An end-to-end pose estimation framework</title><p>We provide a full-featured, extensible, and easy-to-use software package that is written entirely in the Python programming language (Python Software Foundation) and is built using TensorFlow as a backend (<xref ref-type="bibr" rid="bib1">Abadi et al., 2015</xref>). Our software is a complete, end-to-end pipeline (<xref ref-type="fig" rid="fig1">Figure 1</xref>) with a custom GUI for creating annotated training data with active learning similar to <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> (Appendix 3), as well as a flexible pipeline for data augmentation (<xref ref-type="bibr" rid="bib52">Jung, 2018</xref>; Appendix 3; shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>), model training and evaluation (<xref ref-type="fig" rid="fig2">Figure 2</xref>; Appendix 2), and running inference on new data. We designed our high-level programming interface using the same guidelines from Keras (<xref ref-type="bibr" rid="bib58">keras team, 2015</xref>) to allow the user to go from idea to result as quickly as possible, and we organized our software into a Python module called <italic>DeepPoseKit</italic>. The code, documentation, and examples for our entire software package are freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/deepposekit">https://github.com/jgraving/deepposekit</ext-link> under a permissive open-source license.</p></sec><sec id="s2-2"><title>Our pose estimation models</title><p>To achieve the goal of 'fast animal pose estimation’ introduced by <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref>, while maintaining the robust predictive power of models like DeepLabCut (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>), we implemented two fast pose estimation models that extend the state-of-the-art model for individual pose estimation introduced by <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref> and the current state-of-the art for convolutional regression from <xref ref-type="bibr" rid="bib48">Jégou et al. (2017)</xref>. Our model implementations use fewer parameters than both the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) and LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) while simultaneously removing many of the limitations of these architectures.</p><p>In order to limit overparameterization while minimizing performance loss, we designed our models to allow for multi-scale inference (Appendix 4: 'Encoder-decoder models') while optimizing our model hyperparameters for efficiency. Our first model is a novel implementation of <italic>FC-DenseNet</italic> from <xref ref-type="bibr" rid="bib48">Jégou et al. (2017)</xref> (Appendix 4: 'Encoder-decoder models') arranged in a stacked configuration similar to <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref> (Appendix 5). We call this new model Stacked DenseNet, and to the best of our knowledge, this is the first implementation of this model architecture in the literature—for pose estimation or otherwise. Further details for this model are available in Appendix 8. Our second model is a modified version of the <italic>Stacked Hourglass</italic> model from <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref> (Appendix 5) with hyperparameters that allow for changing the number of filters in each convolutional block to constrain the number of parameters—rather than using 256 filters for all layers as described in <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref>.</p></sec><sec id="s2-3"><title>Subpixel keypoint prediction on the GPU allows for fast and accurate inference</title><p>In addition to implementing our efficient pose estimation models, we developed a new method to process model outputs to allow for faster, more accurate predictions. When using a fully-convolutional posture estimation model, the confidence maps produced by the model must be converted into coordinate values for the predictions to be useful, and there are typically two choices for making this conversion. The first is to move the confidence maps out of GPU memory and post-process them on the CPU. This solution allows for easy, flexible, and accurate calculation of the coordinates with subpixel precision (<xref ref-type="bibr" rid="bib44">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>). However, CPU processing is not ideal because moving large arrays of data between the GPU and CPU can be costly, and computation on the CPU is generally slower. The other option is to directly process the confidence maps on the GPU and then move the coordinate values from the GPU to the CPU. This approach usually means converting confidence maps to integer coordinates based on the row and column index of the global maximum for each confidence map (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>). However, this means that, to achieve a precise estimation, the confidence maps should be predicted at the full resolution of the input image, or larger, which slows down inference speed.</p><p>As an alternative to these two strategies, we introduce a new GPU-based convolutional layer that we call <italic>subpixel maxima</italic>. This layer uses the fast, efficient, image registration algorithm introduced by <xref ref-type="bibr" rid="bib38">Guizar-Sicairos et al. (2008)</xref> to translationally align a two-dimensional Gaussian filter to each confidence map via Fourier-based convolution. The translational shift between the filter and each confidence map allows us to calculate the coordinates of the global maxima with high-speed and subpixel precision. This technique allows for accurate predictions of keypoint locations even if the model’s confidence maps are dramatically smaller than the resolution of the input image. We compared the accuracy of our subpixel maxima layer to an integer-based maxima layer using the fly dataset from <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> (see 'Materials and methods’). We found significant accuracy improvements across every downsampling configuration (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1a</xref>). Even with confidence maps at <inline-formula><mml:math id="inf11"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> the resolution of the original image, error did not drastically increase compared to full-resolution predictions. Making predictions for confidence maps at such a downsampled resolution allows us to achieve very fast inference &gt;1000 Hz while maintaining high accuracy (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1b</xref>).</p><p>We also provide speed comparisons with the other models we tested and find that our Stacked DenseNet model with our subpixel peak detection algorithm is faster than the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) for both offline (batch size = 100) and real-time speeds (batch size = 1). While we find that our Stacked DenseNet model is faster than the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) for offline processing (batch size = 100), the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) is significantly faster for real-time processing (batch size = 1). Our Stacked Hourglass model (<xref ref-type="bibr" rid="bib79">Newell et al., 2016</xref>) is about the same or slightly faster than Stacked DenseNet for offline speeds (batch size = 100), but is much slower for real-time processing (batch size = 1). Achieving fast pose estimation using CNNs typically relies on massively parallel processing on the GPU with large batches of data or requires downsampling the images to increase speed, which increases error (<xref ref-type="bibr" rid="bib73">Mathis and Warren, 2018</xref>). These factors make fast and accurate real-time inference challenging to accomplish. Our Stacked DenseNet model, with a batch size of one, can run inference at ∼30–110 Hz—depending on the resolution of the predicted confidence maps (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1b</xref>). These speeds are faster than the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) and could be further improved by downsampling the input image resolution or reconfiguring the model with fewer parameters. This allows our methods to be flexibly used for real-time or closed-loop behavioral experiments with prediction errors similar to current state-of-the-art methods.</p></sec><sec id="s2-4"><title>Learning multi-scale geometry between keypoints improves accuracy and reduces extreme errors</title><p>Minimizing extreme prediction errors is important to prevent downstream effects on any further behavioral analysis (<xref ref-type="bibr" rid="bib96">Seethapathi et al., 2019</xref>)—especially in the case of analyses based on time-frequency transforms like those from <xref ref-type="bibr" rid="bib12">Berman et al. (2014b)</xref>, <xref ref-type="bibr" rid="bib13">Berman et al. (2016)</xref>, <xref ref-type="bibr" rid="bib62">Klibaite et al. (2017)</xref>, <xref ref-type="bibr" rid="bib102">Todd et al. (2017)</xref>, <xref ref-type="bibr" rid="bib63">Klibaite and Shaevitz (2019)</xref> and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> where high magnitude errors can cause inaccurate behavioral classifications. While effects of these extreme errors can be minimized using post-hoc filters and smoothing, these post-processing techniques can remove relevant high-frequency information from time-series data, so this solution is less than ideal. One way to minimize extreme errors when estimating posture is to incorporate multiple spatial scales when making predictions (e.g., <xref ref-type="bibr" rid="bib23">Chen et al., 2017</xref>). Our pose estimation models are implicitly capable of using information from multiple scales (see Appendix 4: 'Encoder-decoder models'), but there is no explicit signal that optimizes the model to take advantage of this information when making predictions.</p><p>To remedy this, we modified the model’s output to predict, in addition to keypoint locations, a hierarchical graph of edges describing the multi-scale geometry between keypoints—similar to the part affinity fields described by <xref ref-type="bibr" rid="bib20">Cao et al. (2017)</xref>. This was achieved by adding an extra set of confidence maps to the output where edges in the postural graph are represented by Gaussian-blurred lines the same width as the Gaussian peaks in the keypoint confidence maps. Our posture graph output then consists of four levels: (1) a set of confidence maps for the smallest limb segments in the graph (e.g. foot to ankle, knee to hip, etc.; <xref ref-type="fig" rid="fig2">Figure 2</xref>), (2) a set of confidence maps for individual limbs (e.g. left leg, right arm, etc.; Figure 4), (3) a map with the entire postural graph, and (4) a fully integrated map that incorporates the entire posture graph and confidence peaks for all of the joint locations (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Each level of the hierarchical graph is built from lower levels in the output, which forces the model to learn correlated features across multiple scales when making predictions.</p><p>We find that training our Stacked DenseNet model to predict a hierarchical posture graph reduces keypoint prediction error (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>), and because the feature maps for the posture graph can be removed from the final output during inference, this effectively improves prediction accuracy for free. Both the mean and variance of the error distributions were lower when predicting the posture graph, which suggests that learning multi-scale geometry both decreases error on average and helps to reduce extreme prediction errors. The overall effect size for this decrease in error is fairly small (&lt;1 pixel average reduction in error), but based on the results from the zebra dataset, this modification more dramatically improves performance for datasets with higher variance images and sparse posture graphs. Predicting the posture graph may be especially useful for animals with long slender appendages such as insect legs and antennae where prediction errors are likely to occur due to occlusions and natural variation in the movement of these body parts. These results also suggest that annotating multiple keypoints to incorporate an explicit signal for multi-scale information may help improve prediction accuracy for a specific body part of interest.</p></sec><sec id="s2-5"><title>Stacked DenseNet is fast and robust</title><p>We benchmarked our new model implementations against the models <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> and <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>. We find that our Stacked DenseNet model outperforms both the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) and the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) in terms of speed while also achieving much higher accuracy than the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) with similar accuracy to the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>; <xref ref-type="fig" rid="fig3">Figure 3a</xref>). We found that both the Stacked Hourglass and Stacked DenseNet models outperformed the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>). Notably our Stacked DenseNet model achieved approximately 2× faster inference speeds with 3× higher mean accuracy. Not only were our models average prediction error significantly improved, but also, importantly, the variance was lower—indicating that our models produced fewer extreme prediction errors. At <inline-formula><mml:math id="inf12"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> resolution, our Stacked DenseNet model consistently achieved prediction accuracy nearly identical to the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) while running inference at nearly 2× the speed and using only ∼5% of the parameters—1.5 million vs. ∼26 million. Detailed results of our model comparisons are shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>DeepPoseKit is fast, accurate, and easy-to-use.</title><p>Our Stacked DenseNet model estimates posture at approximately 2×—or greater—the speed of the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) and the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) while also achieving similar accuracy to the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>)—shown here as mean accuracy <inline-formula><mml:math id="inf13"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>Euclidean error</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> for our most challenging dataset of multiple interacting Grévy’s zebras (<italic>E. grevyi</italic>) recorded in the wild (<bold>a</bold>). See <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for further details. Our software interface is designed to be straightforward but flexible. We include many options for expert users to customize model training with sensible default settings to make pose estimation as easy as possible for beginners. For example, training a model and running inference on new data requires writing only a few lines of code and specifying some basic settings (<bold>b</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Euclidean error distributions for each model across our three datasets.</title><p>Letter-value plots (left) show the raw error distributions for each model. Violinplots of the posterior distributions for the mean and variance (right) show statistical differences between the error distributions. Overall the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) was the worst performer on every dataset in terms of both mean and variance. Our Stacked DenseNet model was the best performer for the fly dataset, while our Stacked DenseNet model and the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) both performed equally well on the locust and zebra datasets. The posteriors for the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) and our Stacked DenseNet model are highly overlapping for these datasets, which suggests they are not statistically discernible from one another. Our Stacked Hourglass model (<xref ref-type="bibr" rid="bib79">Newell et al., 2016</xref>) performed equally to the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) and our Stacked DenseNet model for the locust dataset but performed slightly worse for the fly and zebra datasets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-fig3-figsupp1-v2.tif"/></fig></fig-group><p>While the Stacked DenseNet model used for comparisons is already fast, inference speed could be further improved by using a <inline-formula><mml:math id="inf14"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> output without much increase in error (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>) or by further adjusting the hyperparameters to constrain the size of the model. Our Stacked Hourglass implementation followed closely behind the performance of our Stacked DenseNet model and the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) but consistently performed more poorly than our Stacked DenseNet model in terms of prediction accuracy, so we excluded this model from further analysis. We were also able to reproduce the results reported by <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> that the LEAP model and the Stacked Hourglass model (<xref ref-type="bibr" rid="bib79">Newell et al., 2016</xref>) have similar average prediction error for the fly dataset. However, we also find that the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) has much higher variance, which suggests it is more prone to extreme prediction errors—a problem for further data analysis.</p></sec><sec id="s2-6"><title>Stacked DenseNet trains quickly and requires few training examples</title><p>To further compare models, we used our zebra dataset to assess the training time needed for our Stacked DenseNet model, the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>), and the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) to reach convergence (i.e., complete training) as well as the amount of training data needed for each model to generalize to new data from outside the training set. We find that our Stacked DenseNet model, the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>), and the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) all fully converge in just a few hours and reach reasonably high accuracy after only an hour of training (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). However, it appears that our Stacked DenseNet model tends to converge to a good minimum faster than both the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) and the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>).</p><p>We also show that our Stacked DenseNet model achieves good generalization with few training examples and without the use of transfer learning (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). These results demonstrate that, when combined with data augmentation, as few as five training examples can be used as an initial training set for labelling keypoints with active learning (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Additionally, because our analysis shows that generalization to new data plateaus after approximately 100 labeled training examples, it appears that 100 training examples is a reasonable minimum size for a training set—although the exact number will likely change depending the variance of the image data being annotated. To further examine the effect of transfer learning on model generalization, we compared performance between the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) initialized with weights pretrained on the ImageNet database (<xref ref-type="bibr" rid="bib28">Deng et al., 2009</xref>) vs. the same model with randomly initialized weights (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). As postulated by <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>, we find that transfer learning does provide some benefit to the DeepLabCut model’s ability to generalize. However, the effect size of this improvement is small with a mean reduction in Euclidean error of &lt;0.5 pixel. Together these results indicate that transfer learning is helpful, but not required, for deep learning models to achieve good generalization with limited training data.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we have presented a new software toolkit, called DeepPoseKit, for estimating animal posture using deep learning models. We built on the state-of-the-art for individual pose estimation using convolutional neural networks to achieve fast inference without reducing accuracy or generalization ability. Our new pose estimation model, called Stacked DenseNet, offers considerable improvements (<xref ref-type="fig" rid="fig3">Figure 3a</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) over the models from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> (DeepLabCut) and <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> (LEAP), and our software framework also provides a simplified interface (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) for using these advanced tools to measure animal behavior and locomotion. We tested our methods across a range of datasets from controlled laboratory environments with single individuals to challenging field situations with multiple interacting individuals and variable lighting conditions. We found that our methods perform well for all these situations and require few training examples to achieve good predictive performance on new data—without the use of transfer learning. We ran experiments to optimize our approach and discovered that some straightforward modifications can greatly improve speed and accuracy. Additionally, we demonstrated that these modifications improve not the just the average error but also help to reduce extreme prediction errors—a key determinant for the reliability of subsequent statistical analysis.</p><p>While our results offer a good-faith comparison of the available methods for animal pose estimation, there is inherent uncertainty that we have attempted to account for but may still bias our conclusions. For example, deep learning models are trained using stochastic optimization algorithms that give different results with each replicate, and the Bayesian statistical methods we use for comparison are explicitly probabilistic in nature. There is also great variability across hardware and software configurations when using these models in practice (<xref ref-type="bibr" rid="bib73">Mathis and Warren, 2018</xref>), so performance may change across experimental setups and datasets. Additionally, we demonstrated that some models may perform better than others for specific applications (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), and to account for this, our toolkit offers researchers the ability to choose the model that best suits their requirements—including the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) and the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>).</p><p>We highlighted important considerations when using CNNs for pose estimation and reviewed the progress of fully convolutional regression models from the literature. The latest advancements for these models have been driven mostly by a strategy of adding more connections between layers to increase performance and efficiency (e.g., <xref ref-type="bibr" rid="bib48">Jégou et al., 2017</xref>). Future progress for this class of models may require better loss functions (<xref ref-type="bibr" rid="bib34">Goodfellow et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Johnson et al., 2016a</xref>; <xref ref-type="bibr" rid="bib23">Chen et al., 2017</xref>; <xref ref-type="bibr" rid="bib113">Zhang et al., 2018</xref>), models that more explicitly incorporate the spatial dependencies within a scene (<xref ref-type="bibr" rid="bib107">Van den Oord et al., 2016b</xref>), and temporal structure of the data (<xref ref-type="bibr" rid="bib96">Seethapathi et al., 2019</xref>), as well as more mathematically principled approaches (e.g., <xref ref-type="bibr" rid="bib109">Weigert et al., 2018</xref>; <xref ref-type="bibr" rid="bib91">Roy et al., 2019</xref>) such as the application of formal probabilistic concepts (<xref ref-type="bibr" rid="bib57">Kendall and Gal, 2017</xref>) and Bayesian inference at scale (<xref ref-type="bibr" rid="bib103">Tran et al., 2018</xref>).</p><p>Measuring behavior is a critical factor for many studies in neuroscience (<xref ref-type="bibr" rid="bib64">Krakauer et al., 2017</xref>). Understanding the connections between brain activity and behavioral output requires detailed and objective descriptions of body posture that match the richness and resolution neural measurement technologies have provided for years (<xref ref-type="bibr" rid="bib4">Anderson and Perona, 2014</xref>; <xref ref-type="bibr" rid="bib14">Berman, 2018</xref>; <xref ref-type="bibr" rid="bib18">Brown and de Bivort, 2018</xref>), which our methods and other deep-learning–﻿based tools provide (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>). We have also demonstrated the possibility that our toolkit could be used for real-time inference, which allows for closed-loop experiments where sensory stimuli or optogenetic stimulation are controlled in response to behavioral measurements (e.g., <xref ref-type="bibr" rid="bib10">Bath et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Stowers et al., 2017</xref>). Using real-time measurements in conjunction with optogenetics or thermogenetics may be key to disentangling the causal structure of motor output from the brain—especially given that recent work has shown an animal’s response to optogenetic stimulation can differ depending on the behavior it is currently performing (<xref ref-type="bibr" rid="bib19">Cande et al., 2018</xref>). Real-time behavioral quantification is also particularly important as closed-loop virtual reality is quickly becoming an indispensable tool for studying sensorimotor relationships in individuals and collectives (<xref ref-type="bibr" rid="bib98">Stowers et al., 2017</xref>).</p><p>Quantifying individual movement is essential for revealing the genetic (<xref ref-type="bibr" rid="bib53">Kain et al., 2012</xref>; <xref ref-type="bibr" rid="bib17">Brown et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Ayroles et al., 2015</xref>) and environmental (<xref ref-type="bibr" rid="bib15">Bierbach et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Akhund-Zade et al., 2019</xref>; <xref ref-type="bibr" rid="bib108">Versace et al., 2019</xref>) underpinnings of phenotypic variation in behavior—as well as the phylogeny of behavior (e.g., <xref ref-type="bibr" rid="bib11">Berman et al., 2014a</xref>). Measuring individual behavioral phenotypes requires tools that are robust, scaleable, and easy-to-use, and our approach offers the ability to quickly and accurately quantify the behavior of many individuals in great detail. When combined with tools for genetic manipulations (<xref ref-type="bibr" rid="bib85">Ran et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Doudna and Charpentier, 2014</xref>), high-throughput behavioral experiments (<xref ref-type="bibr" rid="bib3">Alisch et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Javer et al., 2018</xref>; <xref ref-type="bibr" rid="bib110">Werkhoven et al., 2019</xref>), and behavioral analysis (e.g., <xref ref-type="bibr" rid="bib12">Berman et al., 2014b</xref>; <xref ref-type="bibr" rid="bib112">Wiltschko et al., 2015</xref>), our methods could help to provide the data resolution and statistical power needed for dissecting the complex relationships between genes, environment, and behavioral variation.</p><p>When used together with other tools for localization and tracking (e.g., <xref ref-type="bibr" rid="bib81">Pérez-Escudero et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Crall et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Graving, 2017</xref>; <xref ref-type="bibr" rid="bib88">Romero-Ferrero et al., 2019</xref>; <xref ref-type="bibr" rid="bib111">Wild et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Boenisch et al., 2018</xref>), our methods are capable of reliably measuring posture for multiple interacting individuals. The importance of measuring detailed representations of individual behavior when studying animal collectives has been well established (<xref ref-type="bibr" rid="bib99">Strandburg-Peshkin et al., 2013</xref>; <xref ref-type="bibr" rid="bib90">Rosenthal et al., 2015</xref>; <xref ref-type="bibr" rid="bib100">Strandburg-Peshkin et al., 2015</xref>; <xref ref-type="bibr" rid="bib101">Strandburg-Peshkin et al., 2017</xref>). Estimating body posture is an essential first step for unraveling the sensory networks that drive group coordination, such as vision-based networks measured via raycasting (<xref ref-type="bibr" rid="bib99">Strandburg-Peshkin et al., 2013</xref>; <xref ref-type="bibr" rid="bib90">Rosenthal et al., 2015</xref>). Additionally, using body pose estimation in combination with computational models of behavior (e.g., <xref ref-type="bibr" rid="bib25">Costa et al., 2019</xref>; <xref ref-type="bibr" rid="bib112">Wiltschko et al., 2015</xref>) and unsupervised behavioral classification methods (e.g., <xref ref-type="bibr" rid="bib12">Berman et al., 2014b</xref>; <xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) may allow for further dissection of how information flows through groups by revealing the networks of behavioral contagion across multiple timescales and sensory modalities. While we have provided a straightforward solution for applying existing pose estimation methods to measure collective behavior, there still remain many challenging scenarios where these methods would fail. For example, tracking posture in a densely packed bee hive or school of fish would require novel solutions to deal with the 3-D nature of individual movement, which includes maintaining individual identities and dealing with the resulting occlusions that go along with imaging these types of biological systems.</p><p>When combined with unmanned aerial vehicles (UAVs; <xref ref-type="bibr" rid="bib95">Schiffman, 2014</xref>) or other field-based imaging (<xref ref-type="bibr" rid="bib33">Francisco et al., 2019</xref>), applying these methods to the study of individuals and groups in the wild can provide high-resolution behavioral data that goes beyond the capabilities of current GPS and accelerometry-based technologies (<xref ref-type="bibr" rid="bib76">Nagy et al., 2010</xref>; <xref ref-type="bibr" rid="bib77">Nagy et al., 2013</xref>; <xref ref-type="bibr" rid="bib55">Kays et al., 2015</xref>; <xref ref-type="bibr" rid="bib100">Strandburg-Peshkin et al., 2015</xref>; <xref ref-type="bibr" rid="bib101">Strandburg-Peshkin et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Flack et al., 2018</xref>)—especially for species that are impractical to study with tags or collars. Additionally, by applying these methods in conjunction with 3-D habitat reconstruction—using techniques from photogrammetry (<xref ref-type="bibr" rid="bib101">Strandburg-Peshkin et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Francisco et al., 2019</xref>)—field-based studies can begin to integrate fine-scale behavioral measurements with the full 3-D environment in which the behavior evolved. Future advances will likely allow for the calibration and synchronizaton of imaging devices across multiple UAVs (e.g., <xref ref-type="bibr" rid="bib84">Price et al., 2018</xref>; <xref ref-type="bibr" rid="bib93">Saini et al., 2019</xref>). This would make it possible to measure the full 3-D posture of wild animals (e.g., <xref ref-type="bibr" rid="bib115">Zuffi et al., 2019</xref>) in scenarios where fixed camera systems (e.g., <xref ref-type="bibr" rid="bib78">Nath et al., 2019</xref>) would not be tractable, such as during migratory or predation events. When combined, these technologies could allow researchers to address questions about the behavioral ecology of animals that were previously impossible to answer.</p><p>Computer vision algorithms for measuring behavior at the scale of posture have rapidly advanced in a very short time; nevertheless, the task of pose estimation is far from solved. There are hard limitations to this current generation of pose estimation methods that are primarily related to the requirement for human annotations and user-defined keypoints—both in terms of the number of keypoints, the specific body parts being tracked, and the inherent difficulty of incorporating temporal information into the annotation and training procedures. Often the body parts chosen for annotation are an obvious fit for the experimental design and have reliably visible reference points on the animal’s body that make them easy to annotate. However, in many cases the required number and type of body parts needed for data analysis may not be so obvious—such as in the case of unsupervised behavior classification methods (<xref ref-type="bibr" rid="bib12">Berman et al., 2014b</xref>; <xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>). Additionally, the reference points for labeling images with keypoints can be hard to define and consistently annotate across images, which is often the case for soft or flexible-bodied animals like worms and fish. Moreover, due to the laborious nature of annotating keypoints, the current generation of methods also rarely takes into account the natural temporal structure of the data, instead treating each video frame as a statistically independent event, which can lead to extreme prediction errors (reviewed by <xref ref-type="bibr" rid="bib96">Seethapathi et al., 2019</xref>). Extending these methods to track the full three-dimensional posture of animals also typically requires the use of multiple synchronized cameras (<xref ref-type="bibr" rid="bib78">Nath et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Günel et al., 2019</xref>), which increases the cost and complexity of creating an experimental setup, as well as the manual labor required for annotating a training set, which must include labeled data from every camera view.</p><p>These limitations make it clear that fundamentally-different methods may be required to move the field forward. New pose estimation methods are already replacing human annotations with fully articulated volumetric 3-D models of the animal’s body (e.g., the SMAL model from <xref ref-type="bibr" rid="bib114">Zuffi et al., 2017</xref> or the SMALST model from <xref ref-type="bibr" rid="bib115">Zuffi et al., 2019</xref>), and the 3-D scene can be estimated using unsupervised, semi-supervised, or weakly-supervised methods (e.g., <xref ref-type="bibr" rid="bib46">Jaques et al., 2019</xref>; <xref ref-type="bibr" rid="bib115">Zuffi et al., 2019</xref>), where the shape, position, and posture of the animal’s body, the camera position and lens parameters, and the background environment and lighting conditions are jointly learned directly from 2-D images by a deep-learning model (<xref ref-type="bibr" rid="bib105">Valentin et al., 2019</xref>; <xref ref-type="bibr" rid="bib115">Zuffi et al., 2019</xref>). These <italic>inverse graphics models</italic> (<xref ref-type="bibr" rid="bib66">Kulkarni et al., 2015</xref>; <xref ref-type="bibr" rid="bib92">Sabour et al., 2017</xref>; <xref ref-type="bibr" rid="bib105">Valentin et al., 2019</xref>) take advantage of recently developed differentiable graphics engines that allow 3-D rendering parameters to be controlled using standard optimization methods (<xref ref-type="bibr" rid="bib115">Zuffi et al., 2019</xref>; <xref ref-type="bibr" rid="bib105">Valentin et al., 2019</xref>). After optimization, the volumetric 3-D timeseries data predicted by the deep learning model could be used directly for behavioral analysis or specific keypoints or body parts could be selected for analysis post-hoc. In order to more explicitly incorporate the natural statistical properties of the data, these models also apply perceptual loss functions (<xref ref-type="bibr" rid="bib49">Johnson et al., 2016a</xref>; <xref ref-type="bibr" rid="bib113">Zhang et al., 2018</xref>; <xref ref-type="bibr" rid="bib115">Zuffi et al., 2019</xref>) and could be extended to use adversarial (<xref ref-type="bibr" rid="bib34">Goodfellow et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Chen et al., 2017</xref>) loss functions, both of which incorporate spatial dependencies within the scene rather than modelling each video frame as a set of statistically independent pixel distributions—as is the case with current methods that use likelihood functions such as pixel-wise mean squared error (e.g., <xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) or cross-entropy loss (e.g., <xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>). Because there is limited or no requirement for human-provided labels with these new methods, these models could also be easily modified to incorporate the temporal structure of the data using autoregressive representations (e.g., <xref ref-type="bibr" rid="bib106">Van den Oord et al., 2016a</xref>; <xref ref-type="bibr" rid="bib107">Van den Oord et al., 2016b</xref>; <xref ref-type="bibr" rid="bib67">Kumar et al., 2019</xref>), rather than modeling the scene in each video frame as a statistically independent event. Together these advances could lead to larger, higher-resolution, more reliable behavioral datasets that could revolutionize our understanding of relationships between behavior, the brain, and the environment.</p><p>In conclusion, we have presented a new toolkit, called DeepPoseKit, for automatically measuring animal posture from images. We combined recent advances from the literature to create methods that are fast, robust, and widely applicable to a range of species and experimental conditions. When designing our framework we emphasized usability across the entire software interface, which we expect will help to make these advanced tools accessible to a wider range of researchers. The fast inference and real-time capabilities of our methods should also help further reduce barriers to previously intractable questions across many scientific disciplines—including neuroscience, ethology, and behavioral ecology—both in the laboratory and the field.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>We ran three main experiments to test and optimize our approach. First, we compared our new subpixel maxima layer to an integer-based global maxima with downsampled outputs ranging from 1× to <inline-formula><mml:math id="inf15"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>16</mml:mn></mml:mfrac><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> the input resolution using our Stacked DenseNet model. Next, we tested if training our Stacked DenseNet model to predict the multi-scale geometry of the posture graph improves accuracy. Finally, we compared our model implementations of Stacked Hourglass and Stacked DenseNet to the models from <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> (LEAP) and <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> (DeepLabCut), which we also implemented in our framework (see Appendix 8 for details). We assessed both the inference speed and prediction accuracy of each model as well as training time and generalization ability. When comparing these models we incorporated the relevant improvements from our experiments—including subpixel maxima and predicting multi-scale geometry between keypoints—unless otherwise noted (see Appendix 8).</p><p>While we do make comparisons to the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) we do not use the same training routine as <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref>, who use binary cross-entropy loss for optimizing the confidence maps in addition to the location refinement maps described by <xref ref-type="bibr" rid="bib44">Insafutdinov et al. (2016)</xref>. We made this modification in order to hold the training routine constant for each model while only varying the model itself. However, we find that these differences between training routines effectively have no impact on performance when the models are trained using the same dataset and data augmentations (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1</xref>). We also provide qualitative comparisons to demonstrate that, when trained with our DeepPoseKit framework, our implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) appears to produce fewer prediction errors than the original implementation from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref> when applied to a novel video (<xref ref-type="fig" rid="app8fig1s1">Appendix 8—figure 1—figure supplements 1</xref> and <xref ref-type="fig" rid="app8fig1s2">2</xref>; <xref ref-type="video" rid="app8fig1video1">Appendix 8—figure 1—video 1</xref>).</p><sec id="s4-1"><title>Datasets</title><p>We performed experiments using the vinegar or 'fruit’ fly (<italic>Drosophila melanogaster</italic>) dataset (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref>) provided by <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref>, and to demonstrate the versatility of our methods we also compared model performance across two previously unpublished posture data sets from groups of desert locusts (<italic>Schistocerca gregaria</italic>) recorded in a laboratory setting (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="video" rid="fig4video2">Figure 4—video 2</xref>), and herds of Grévy’s zebras (<italic>Equus grevyi</italic>) recorded in the wild (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="video" rid="fig4video3">Figure 4—video 3</xref>). The locust and zebra datasets are particularly challenging for pose estimation as they feature multiple interacting individuals—with focal individuals centered in the frame—and the latter with highly-variable environments and lighting conditions. These datasets are freely-available from <ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/deepposekit-data">https://github.com/jgraving/deepposekit-data</ext-link> (<xref ref-type="bibr" rid="bib37">Graving et al., 2019</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/DeepPoseKit-Data">https://github.com/elifesciences-publications/DeepPoseKit-Data</ext-link>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Datasets used for evaluation.</title><p>A visualization of the datasets we used to evaluate our methods (<xref ref-type="table" rid="table1">Table 1</xref>). For each dataset, confidence maps for the keypoints (bottom-left) and posture graph (top-right) are illustrated using different colors for each map. These outputs are from our Stacked DenseNet model at <inline-formula><mml:math id="inf16"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> resolution.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-fig4-v2.tif"/></fig><media id="fig4video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-47994-fig4-video1.mp4"><label>Figure 4—video 1.</label><caption><title>A video of a behaving fly from <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> with pose estimation outputs visualized.</title></caption></media><media id="fig4video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-47994-fig4-video2.mp4"><label>Figure 4—video 2.</label><caption><title>A video of a behaving locust with pose estimation outputs visualized.</title></caption></media><media id="fig4video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-47994-fig4-video3.mp4"><label>Figure 4—video 3.</label><caption><title>A video of a behaving Grévy’s zebra with pose estimation outputs visualized.</title></caption></media></fig-group><p>Our locust dataset consisted of a group of 100 locusts in a circular plastic arena 1 m in diameter. The locust group was recorded from above using a high-resolution camera (Basler ace acA2040-90umNIR) and video recording system (Motif, loopbio GmbH). Locusts were localized and tracked using 2-D barcode markers (<xref ref-type="bibr" rid="bib36">Graving, 2017</xref>) attached to the thorax with cyanoacrylate glue, and any missing localizations (&lt;0.02% of the total dataset) between successful barcode reads were interpolated with linear interpolation. Our zebra dataset consisted of variably sized groups in the wild recorded from above using a commercially available quadcopter drone (DJI Phantom 4 Pro). Individual zebra were localized using custom deep-learning software based on Faster R-CNN (<xref ref-type="bibr" rid="bib86">Ren et al., 2015</xref>) for predicting bounding boxes. The positions of each zebra were then tracked across frames using a linear assignment algorithm (<xref ref-type="bibr" rid="bib75">Munkres, 1957</xref>) and data were manually verified for accuracy.</p><p>After positional tracking, the videos were then cropped using the egocentric coordinates of each individual and saved as separate videos—one for each individual. The images used for each training set were randomly selected using the k-means sampling procedure (with k = 10) described by <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> (Appendix 3) to reduce correlation between sampled images. After annotating the images with keypoints, we rotationally and translationally aligned the images and keypoints using the central body axis of the animal in each labeled image. This step allowed us to more easily perform data augmentations (see 'Model training’) that allow the model to make accurate predictions regardless of the animal’s body size and orientation (see Appendix 6). However, this preprocessing step is not a strict requirement for training, and there is no requirement for this preprocessing step when making predictions on new unlabeled data, such as with the methods described by <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> (Appendix 6). Before training each model we split each annotated dataset into randomly selected training and validation sets with 90% training examples and 10% validation examples, unless otherwise noted. The details for each dataset are described in <xref ref-type="table" rid="table1">Table 1</xref>.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Datasets used for model comparisons.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>Species</th><th>Resolution</th><th># Images</th><th># Keypoints</th><th>Individuals</th><th>Source</th></tr></thead><tbody><tr><td>Vinegar fly</td><td><italic>Drosophila melanogaster</italic></td><td>192 × 192</td><td>1500</td><th>32</th><td>Single</td><td><xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref></td></tr><tr><td>Desert locust</td><td><italic>Schistocerca gregaria</italic></td><td>160 × 160</td><td>800</td><td>35</td><td>Multiple</td><td>This paper</td></tr><tr><td>Grévy’s zebra</td><td><italic>Equus grevyi</italic></td><td>160 × 160</td><td>900</td><td>9</td><td>Multiple</td><td>This paper</td></tr></tbody></table></table-wrap></sec><sec id="s4-2"><title>Model training</title><p>For each experiment, we set our model hyperparameters to the same configuration for our Stacked DenseNet and Stacked Hourglass models. Both models were trained with <inline-formula><mml:math id="inf17"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> resolution outputs and a stack of two networks with two outputs where loss was applied (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Although our model hyperparameters could be infinitely adjusted to trade off between speed and accuracy, we compared only one configuration for each of our model implementations. These results are not meant to be an exhaustive search of model configurations as the best configuration will depend on the application. The details of the hyperparameters we used for each model are described in Appendix 8.</p><p>To make our posture estimation tasks closer to realistic conditions, incorporate prior information (Appendix 3), and properly demonstrate the robustness of our methods to rotation, translation, scale, and noise, we applied various augmentations to each data set during training (<xref ref-type="fig" rid="fig2">Figure 2</xref>). All models were trained using data augmentations that included random flipping, or mirroring, along both the horizontal and vertical image axes with each axis being independently flipped by drawing from a Bernoulli distribution (with <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>); random rotations around the center of the image drawn from a uniform distribution in the range [−180°, +180°); random scaling drawn from a uniform distribution in the range [90%, 110%] for flies and locusts and [75%, 125%] for zebras (to account for greater size variation in the data set); and random translations along the horizontal and vertical axis independently drawn from a uniform distribution with the range [−5%, +5%]—where percentages are relative to the original image size. After performing these spatial augmentations we also applied a variety of noise augmentations that included additive noise (i.e., adding or subtracting randomly-selected values to pixels); dropout (i.e., setting individual pixels or groups of pixels to a randomly-selected value); blurring or sharpening (i.e., changing the composition of spatial frequencies); and contrast ratio augmentations—(i.e., changing the ratio between the highest pixel value and lowest pixel value in the image). These augmentations help to further ensure robustness to shifts in lighting, noise, and occlusions. See Appendix 3 for further discussion on data augmentation.</p><p>We trained our models (<xref ref-type="fig" rid="fig2">Figure 2</xref>) using mean squared error loss optimized using the ADAM optimizer (<xref ref-type="bibr" rid="bib60">Kingma and Ba, 2014</xref>) with a learning rate of 1 × 10<sup>-3</sup> and a batch size of 16. We lowered the learning rate by a factor of five each time the validation loss did not improve by more than 1 × 10<sup>-3</sup> for 10 epochs. We considered models to be converged when the validation loss stopped improving for 50 epochs, and we calculated validation error as the Euclidean distance between predicted and ground-truth image coordinates for only the best performing version of the model, which we evaluated at the end of each epoch during optimization. We performed this procedure five times for each experiment and randomly selected a new training and validation set for each replicate.</p></sec><sec id="s4-3"><title>Model evaluation</title><p>Machine learning models are typically evaluated for their ability to generalize to new data, known as <italic>predictive performance</italic>, using a held-out <italic>test set</italic>—a subsample of annotated data that is not used for training or validation. However, due to the small size of the datasets used for making comparisons, we elected to use only a validation set for model evaluation, as using an overly small training or test set can bias assessments of a model’s predictive performance (<xref ref-type="bibr" rid="bib65">Kuhn and Johnson, 2013</xref>). Generally a test set is used to avoid biased performance measures caused by overfitting the model hyperparameters to the validation set. However, we did not adjust our model architecture to achieve better performance on our validation set—only to achieve fast inference speeds. While we did use validation error to decide when to lower the learning rate during training and when to stop training, lowering the learning rate in this way should have no effect on the generalization ability of the model, and because we heavily augment our training set during optimization—forcing the model to learn a much larger data distribution than what is included in the training and validation sets—overfitting to the validation set is unlikely. We also demonstrate the generality of our results for each experiment by randomly selecting a new validation set with each replicate. All these factors make the Euclidean error for the unaugmented validation set a reasonable measure of the predictive performance for each model.</p><p>The inference speed for each model was assessed by running predictions on 100,000 randomly generated images with a batch size of 1 for real-time speeds and a batch size of 100 for offline speeds, unless otherwise noted. Our hardware consisted of a Dell Precision Tower 7910 workstation (Dell, Inc) running Ubuntu Linux v18.04 with 2× Intel Xeon E5-2623 v3 CPUs (8 cores, 16 threads at 3.00 GHz), 64 GB of RAM, a Quadro P6000 GPU and a Titan Xp GPU (NVIDIA Corporation). We used both GPUs (separately) for training models and evaluating predictive performance, but we only used the faster Titan Xp GPU for benchmarking inference speeds and training time. While the hardware we used for development and testing is on the high-end of the current performance spectrum, there is no requirement for this level of performance, and our software can easily be run on lower-end hardware. We evaluated inference speeds on multiple consumer-grade desktop computers and found similar performance (±10%) when using the same GPU; however, training speed depends more heavily other hardware components like the CPU and hard disk.</p></sec><sec id="s4-4"><title>Assessing prediction accuracy with Bayesian inference</title><p>To more rigorously assess performance differences between models, we parameterized the Euclidean error distribution for each experiment by fitting a Bayesian linear model with a Gamma-distributed likelihood function. This model takes the form:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>∼</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>α</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>β</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>μ</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ϕ</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf19"><mml:mi>X</mml:mi></mml:math></inline-formula> is the design matrix composed of binary indicator variables for each pose estimation model, <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>θ</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> are vectors of intercepts, <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the softplus function (<xref ref-type="bibr" rid="bib31">Dugas et al., 2001</xref>)—or <inline-formula><mml:math id="inf23"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mi mathvariant="bold">𝐱</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>—used to enforce positivity of <inline-formula><mml:math id="inf24"><mml:mi>μ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf26"><mml:mi>y</mml:mi></mml:math></inline-formula> is the Euclidean error of the pose estimation model. Parameterizing our error distributions in this way allows us to calculate the posterior distributions for the mean <inline-formula><mml:math id="inf27"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>≡</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf28"><mml:mrow><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>≡</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:math></inline-formula>. This parameterization then provides us with a statistically rigorous way to assess differences in model accuracy in terms of both central tendency and spread—accounting for both epistemic uncertainty (unknown unknowns; e.g., parameter uncertainty) and aleatoric uncertainty (known unknowns; e.g., data variance). Details of how we fitted these models can be found in Appendix 7.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We are indebted to Talmo Pereira et al. and A Mathis et al. for making their software open-source and freely-available—this project would not have been possible without them. We also thank M Mathis and A Mathis for their comments, which greatly improved the manuscript. We thank François Chollet, the Keras and TensorFlow teams, and Alexander Jung for their open source contributions, which provided the core programming interface for our work. We thank A Strandburg-Peshkin, Vivek H Sridhar, Michael L Smith, and Joseph B Bak-Coleman for their helpful discussions on the project and comments on the manuscript. We also thank MLS for the use of his GPU. We thank Felicitas Oehler for annotating the zebra posture data and Chiara Hirschkorn for assistance with filming the locusts and annotating the locust posture data. We thank Alex Bruttel, Christine Bauer, Jayme Weglarski, Dominique Leo, Markus Miller and loobio GmbH for providing technical support. We acknowledge the NVIDIA Corporation for their generous donations to our research. This project received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 748549. BRC acknowledges support from the University of Konstanz Zukunftskolleg’s Investment Grant program. IDC acknowledges support from NSF Grant IOS-1355061, Office of Naval Research Grants N00014-09-1-1074 and N00014-14-1-0635, Army Research Office Grants W911NG-11-1-0385 and W911NF14-1-0431, the Struktur-und Innovationsfonds fur die Forschung of the State of Baden-Württemberg, the DFG Centre of Excellence 2117 'Centre for the Advanced Study of Collective Behaviour’ (ID: 422037984), and the Max Planck Society.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Validation, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Data curation, Investigation, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Data curation, Supervision, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: All procedures for collecting the zebra (<italic>E. grevyi</italic>) dataset were reviewed and approved by Ethikrat, the independent Ethics Council of the Max Planck Society. The zebra dataset was collected with the permission of Kenya's National Commission for Science, Technology and Innovation (NACOSTI/P/17/59088/15489 and NACOSTI/P/18/59088/21567) using drones operated by BRC with the permission of the Kenya Civil Aviation Authority (authorization numbers: KCAA/OPS/2117/4 Vol. 2 (80), KCAA/OPS/2117/4 Vol. 2 (81), KCAA/OPS/2117/5 (86) and KCAA/OPS/2117/5 (87); RPAS Operator Certificate numbers: RPA/TP/0005 AND RPA/TP/000-0009).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-47994-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data used and generated for experiments and model comparisons are included in the supporting files. Posture datasets can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/deepposekit-data">https://github.com/jgraving/deepposekit-data</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/DeepPoseKit-Data">https://github.com/elifesciences-publications/DeepPoseKit-Data</ext-link>). The code for DeepPoseKit is publicly available at the URL we provided in the paper: <ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/deepposekit/">https://github.com/jgraving/deepposekit/</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/DeepPoseKit">https://github.com/elifesciences-publications/DeepPoseKit</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Koger</surname><given-names>B</given-names></name><name><surname>Costelloe</surname><given-names>BR</given-names></name><name><surname>Couzin</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Example Datasets for DeepPoseKit (Version v0.1-doi) [Data set].</data-title><source>Zenodo</source><pub-id assigning-authority="Zenodo" pub-id-type="doi">10.5281/zenodo.3366908</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Willmore</surname><given-names>L</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Fast animal pose estimation using deep neural networks</data-title><source>DataSpace</source><pub-id assigning-authority="other" pub-id-type="archive" xlink:href="https://dataspace.princeton.edu/jspui/handle/88435/dsp01pz50gz79z">dsp01pz50gz79z</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Abadi</surname> <given-names>M</given-names></name><name><surname>Agarwal</surname> <given-names>A</given-names></name><name><surname>Barham</surname> <given-names>P</given-names></name><name><surname>Brevdo</surname> <given-names>E</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Citro</surname> <given-names>C</given-names></name><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Davis</surname> <given-names>A</given-names></name><name><surname>Dean</surname> <given-names>J</given-names></name><name><surname>Devin</surname> <given-names>M</given-names></name><name><surname>Ghemawat</surname> <given-names>S</given-names></name><name><surname>Goodfellow</surname> <given-names>I</given-names></name><name><surname>Harp</surname> <given-names>A</given-names></name><name><surname>Irving</surname> <given-names>G</given-names></name><name><surname>Isard</surname> <given-names>M</given-names></name><name><surname>Jia</surname> <given-names>Y</given-names></name><name><surname>Jozefowicz</surname> <given-names>R</given-names></name><name><surname>Kaiser</surname> <given-names>L</given-names></name><name><surname>Kudlur</surname> <given-names>M</given-names></name><name><surname>Levenberg</surname> <given-names>J</given-names></name><name><surname>Mané</surname> <given-names>D</given-names></name><name><surname>Monga</surname> <given-names>R</given-names></name><name><surname>Moore</surname> <given-names>S</given-names></name><name><surname>Murray</surname> <given-names>D</given-names></name><name><surname>Olah</surname> <given-names>C</given-names></name><name><surname>Schuster</surname> <given-names>M</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Steiner</surname> <given-names>B</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Talwar</surname> <given-names>K</given-names></name><name><surname>Tucker</surname> <given-names>P</given-names></name><name><surname>Vanhoucke</surname> <given-names>V</given-names></name><name><surname>Vasudevan</surname> <given-names>V</given-names></name><name><surname>Viégas</surname> <given-names>F</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name><name><surname>Warden</surname> <given-names>P</given-names></name><name><surname>Wattenberg</surname> <given-names>M</given-names></name><name><surname>Wicke</surname> <given-names>M</given-names></name><name><surname>Yu</surname> <given-names>Y</given-names></name><name><surname>Zheng</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>TensorFlow: Large-scale machine learning on heterogeneous systems</data-title><ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org">https://www.tensorflow.org</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akhund-Zade</surname> <given-names>J</given-names></name><name><surname>Ho</surname> <given-names>S</given-names></name><name><surname>O'Leary</surname> <given-names>C</given-names></name><name><surname>de Bivort</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The effect of environmental enrichment on behavioral variability depends on genotype, behavior, and type of enrichment</article-title><source>The Journal of Experimental Biology</source><volume>222</volume><elocation-id>jeb.202234</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.202234</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alisch</surname> <given-names>T</given-names></name><name><surname>Crall</surname> <given-names>JD</given-names></name><name><surname>Kao</surname> <given-names>AB</given-names></name><name><surname>Zucker</surname> <given-names>D</given-names></name><name><surname>de Bivort</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>MAPLE (modular automated platform for large-scale experiments), a robot for integrated organism-handling and phenotyping</article-title><source>eLife</source><volume>7</volume><elocation-id>e37166</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37166</pub-id><pub-id pub-id-type="pmid">30117804</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>DJ</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Toward a science of computational ethology</article-title><source>Neuron</source><volume>84</volume><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.005</pub-id><pub-id pub-id-type="pmid">25277452</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Andriluka</surname> <given-names>M</given-names></name><name><surname>Pishchulin</surname> <given-names>L</given-names></name><name><surname>Gehler</surname> <given-names>P</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>2d human pose estimation: new benchmark and state of the art analysis</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><pub-id pub-id-type="doi">10.1109/cvpr.2014.471</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Andriluka</surname> <given-names>M</given-names></name><name><surname>Iqbal</surname> <given-names>U</given-names></name><name><surname>Insafutdinov</surname> <given-names>E</given-names></name><name><surname>Pishchulin</surname> <given-names>L</given-names></name><name><surname>Milan</surname> <given-names>A</given-names></name><name><surname>Gall</surname> <given-names>J</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Posetrack: a benchmark for human pose estimation and tracking</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>5167</fpage><lpage>5176</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2018.00542</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ayinde</surname> <given-names>BO</given-names></name><name><surname>Zurada</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Building efficient convnets using redundant feature pruning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.07653">https://arxiv.org/abs/1802.07653</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayroles</surname> <given-names>JF</given-names></name><name><surname>Buchanan</surname> <given-names>SM</given-names></name><name><surname>O'Leary</surname> <given-names>C</given-names></name><name><surname>Skutt-Kakaria</surname> <given-names>K</given-names></name><name><surname>Grenier</surname> <given-names>JK</given-names></name><name><surname>Clark</surname> <given-names>AG</given-names></name><name><surname>Hartl</surname> <given-names>DL</given-names></name><name><surname>de Bivort</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Behavioral idiosyncrasy reveals genetic control of phenotypic variability</article-title><source>PNAS</source><volume>112</volume><fpage>6706</fpage><lpage>6711</lpage><pub-id pub-id-type="doi">10.1073/pnas.1503830112</pub-id><pub-id pub-id-type="pmid">25953335</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Badrinarayanan</surname> <given-names>V</given-names></name><name><surname>Kendall</surname> <given-names>A</given-names></name><name><surname>Cipolla</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Segnet: a deep convolutional encoder-decoder architecture for image segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.00561">https://arxiv.org/abs/1511.00561</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bath</surname> <given-names>DE</given-names></name><name><surname>Stowers</surname> <given-names>JR</given-names></name><name><surname>Hörmann</surname> <given-names>D</given-names></name><name><surname>Poehlmann</surname> <given-names>A</given-names></name><name><surname>Dickson</surname> <given-names>BJ</given-names></name><name><surname>Straw</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>FlyMAD: rapid thermogenetic control of neuronal activity in freely walking Drosophila</article-title><source>Nature Methods</source><volume>11</volume><fpage>756</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2973</pub-id><pub-id pub-id-type="pmid">24859752</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Berman</surname> <given-names>GJ</given-names></name><name><surname>Choi</surname> <given-names>DM</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Mapping the structure of drosophilid behavior</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/002873</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname> <given-names>GJ</given-names></name><name><surname>Choi</surname> <given-names>DM</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title><source>Journal of the Royal Society Interface</source><volume>11</volume><elocation-id>20140672</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2014.0672</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname> <given-names>GJ</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Predictability and hierarchy in Drosophila behavior,</article-title><source>PNAS</source><volume>11</volume><elocation-id>948</elocation-id><pub-id pub-id-type="doi">10.1101/052928</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Measuring behavior across scales</article-title><source>BMC Biology</source><volume>16</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1186/s12915-018-0494-7</pub-id><pub-id pub-id-type="pmid">29475451</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bierbach</surname> <given-names>D</given-names></name><name><surname>Laskowski</surname> <given-names>KL</given-names></name><name><surname>Wolf</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behavioural individuality in clonal fish arises despite near-identical rearing conditions</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15361</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15361</pub-id><pub-id pub-id-type="pmid">28513582</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boenisch</surname> <given-names>F</given-names></name><name><surname>Rosemann</surname> <given-names>B</given-names></name><name><surname>Wild</surname> <given-names>B</given-names></name><name><surname>Dormagen</surname> <given-names>D</given-names></name><name><surname>Wario</surname> <given-names>F</given-names></name><name><surname>Landgraf</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Tracking all members of a honey bee colony over their lifetime using learned models of correspondence</article-title><source>Frontiers in Robotics and AI</source><volume>5</volume><elocation-id>35</elocation-id><pub-id pub-id-type="doi">10.3389/frobt.2018.00035</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname> <given-names>AE</given-names></name><name><surname>Yemini</surname> <given-names>EI</given-names></name><name><surname>Grundy</surname> <given-names>LJ</given-names></name><name><surname>Jucikas</surname> <given-names>T</given-names></name><name><surname>Schafer</surname> <given-names>WR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A dictionary of behavioral motifs reveals clusters of genes affecting Caenorhabditis elegans locomotion</article-title><source>PNAS</source><volume>110</volume><fpage>791</fpage><lpage>796</lpage><pub-id pub-id-type="doi">10.1073/pnas.1211447110</pub-id><pub-id pub-id-type="pmid">23267063</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname> <given-names>AEX</given-names></name><name><surname>de Bivort</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ethology as a physical science</article-title><source>Nature Physics</source><volume>14</volume><fpage>653</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1038/s41567-018-0093-0</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cande</surname> <given-names>J</given-names></name><name><surname>Namiki</surname> <given-names>S</given-names></name><name><surname>Qiu</surname> <given-names>J</given-names></name><name><surname>Korff</surname> <given-names>W</given-names></name><name><surname>Card</surname> <given-names>GM</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name><name><surname>Stern</surname> <given-names>DL</given-names></name><name><surname>Berman</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Optogenetic dissection of descending behavioral control in <italic>Drosophila</italic></article-title><source>eLife</source><volume>7</volume><elocation-id>e34275</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34275</pub-id><pub-id pub-id-type="pmid">29943729</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cao</surname> <given-names>Z</given-names></name><name><surname>Simon</surname> <given-names>T</given-names></name><name><surname>Wei</surname> <given-names>S-E</given-names></name><name><surname>Sheikh</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Realtime Multi-Person 2d pose estimation using part affinity fields,</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>7291</fpage><lpage>7299</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2017.143</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname> <given-names>B</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Brubaker</surname> <given-names>MA</given-names></name><name><surname>Riddell</surname> <given-names>A</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Goodrich</surname> <given-names>B</given-names></name><name><surname>Guo</surname> <given-names>J</given-names></name><name><surname>Hoffman</surname> <given-names>M</given-names></name><name><surname>Betancourt</surname> <given-names>M</given-names></name><name><surname>Li</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stan: a probabilistic programming language</article-title><source>Journal of Statistical Software</source><volume>76</volume><elocation-id>v076i01</elocation-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cauchy</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1847">1847</year><article-title>Méthode générale pour la résolution des systemes d’équations simultanées</article-title><source>Comp Rend Sci Paris</source><volume>25</volume><fpage>536</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1017/cbo9780511702396.063</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Shen</surname> <given-names>C</given-names></name><name><surname>Wei</surname> <given-names>X-S</given-names></name><name><surname>Liu</surname> <given-names>L</given-names></name><name><surname>Yang</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adversarial posenet: a structure-aware convolutional network for human pose estimation,</article-title><conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name><fpage>1221</fpage><lpage>1230</lpage><pub-id pub-id-type="doi">10.1109/iccv.2017.137</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chollet</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Xception: deep learning with depthwise separable convolutions</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>1251</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2017.195</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname> <given-names>AC</given-names></name><name><surname>Ahamed</surname> <given-names>T</given-names></name><name><surname>Stephens</surname> <given-names>GJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptive, locally linear models of complex dynamics</article-title><source>PNAS</source><volume>116</volume><fpage>1501</fpage><lpage>1510</lpage><pub-id pub-id-type="doi">10.1073/pnas.1813476116</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crall</surname> <given-names>JD</given-names></name><name><surname>Gravish</surname> <given-names>N</given-names></name><name><surname>Mountcastle</surname> <given-names>AM</given-names></name><name><surname>Combes</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>BEEtag: a Low-Cost, Image-Based tracking system for the study of animal behavior and locomotion</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0136487</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0136487</pub-id><pub-id pub-id-type="pmid">26332211</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dell</surname> <given-names>AI</given-names></name><name><surname>Bender</surname> <given-names>JA</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name><name><surname>de Polavieja</surname> <given-names>GG</given-names></name><name><surname>Noldus</surname> <given-names>LP</given-names></name><name><surname>Pérez-Escudero</surname> <given-names>A</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name><name><surname>Straw</surname> <given-names>AD</given-names></name><name><surname>Wikelski</surname> <given-names>M</given-names></name><name><surname>Brose</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Automated image-based tracking and its application in ecology</article-title><source>Trends in Ecology &amp; Evolution</source><volume>29</volume><fpage>417</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2014.05.004</pub-id><pub-id pub-id-type="pmid">24908439</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname> <given-names>J</given-names></name><name><surname>Dong</surname> <given-names>W</given-names></name><name><surname>Socher</surname> <given-names>R</given-names></name><name><surname>Li</surname> <given-names>K</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Imagenet: a large-scale hierarchical image database</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doudna</surname> <given-names>JA</given-names></name><name><surname>Charpentier</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Genome editing. The new frontier of genome engineering with CRISPR-Cas9</article-title><source>Science</source><volume>346</volume><elocation-id>1258096</elocation-id><pub-id pub-id-type="doi">10.1126/science.1258096</pub-id><pub-id pub-id-type="pmid">25430774</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duane</surname> <given-names>S</given-names></name><name><surname>Kennedy</surname> <given-names>AD</given-names></name><name><surname>Pendleton</surname> <given-names>BJ</given-names></name><name><surname>Roweth</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Hybrid monte carlo</article-title><source>Physics Letters B</source><volume>195</volume><fpage>216</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/0370-2693(87)91197-X</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dugas</surname> <given-names>C</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Bélisle</surname> <given-names>F</given-names></name><name><surname>Nadeau</surname> <given-names>C</given-names></name><name><surname>Garcia</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Incorporating second-order functional knowledge for better option pricing</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>472</fpage><lpage>478</lpage><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/2024-incorporating-invariances-in-non-linear-support-vector-machines">http://papers.nips.cc/paper/2024-incorporating-invariances-in-non-linear-support-vector-machines</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flack</surname> <given-names>A</given-names></name><name><surname>Nagy</surname> <given-names>M</given-names></name><name><surname>Fiedler</surname> <given-names>W</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name><name><surname>Wikelski</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>From local collective behavior to global migratory patterns in white storks</article-title><source>Science</source><volume>360</volume><fpage>911</fpage><lpage>914</lpage><pub-id pub-id-type="doi">10.1126/science.aap7781</pub-id><pub-id pub-id-type="pmid">29798883</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Francisco</surname> <given-names>FA</given-names></name><name><surname>Nührenberg</surname> <given-names>P</given-names></name><name><surname>Jordan</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A low-cost, open-source framework for tracking and behavioural analysis of animals in aquatic ecosystems</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/571232</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodfellow</surname> <given-names>I</given-names></name><name><surname>Pouget-Abadie</surname> <given-names>J</given-names></name><name><surname>Mirza</surname> <given-names>M</given-names></name><name><surname>Xu</surname> <given-names>B</given-names></name><name><surname>Warde-Farley</surname> <given-names>D</given-names></name><name><surname>Ozair</surname> <given-names>S</given-names></name><name><surname>Courville</surname> <given-names>A</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Generative adversarial nets</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/5423-generative-adversarial-nets">http://papers.nips.cc/paper/5423-generative-adversarial-nets</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname> <given-names>I</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Courville</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Deep Learning</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Graving</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Pinpoint: behavioral tracking using 2D barcode tags v0.0.1-alpha</data-title><publisher-name>Zenodo</publisher-name><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3366908">https://doi.org/10.5281/zenodo.3366908</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Graving</surname> <given-names>JM</given-names></name><name><surname>Chae</surname> <given-names>D</given-names></name><name><surname>Naik</surname> <given-names>H</given-names></name><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>Koger</surname> <given-names>B</given-names></name><name><surname>Costelloe</surname> <given-names>BR</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Example datasets for DeepPoseKit</data-title><source>Github</source><version designator="c6964d2">c6964d2</version><ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/deepposekit-data">https://github.com/jgraving/deepposekit-data</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guizar-Sicairos</surname> <given-names>M</given-names></name><name><surname>Thurman</surname> <given-names>ST</given-names></name><name><surname>Fienup</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Efficient subpixel image registration algorithms</article-title><source>Optics Letters</source><volume>33</volume><fpage>156</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1364/OL.33.000156</pub-id><pub-id pub-id-type="pmid">18197224</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Günel</surname> <given-names>S</given-names></name><name><surname>Rhodin</surname> <given-names>H</given-names></name><name><surname>Morales</surname> <given-names>D</given-names></name><name><surname>Campagnolo</surname> <given-names>JH</given-names></name><name><surname>Ramdya</surname> <given-names>P</given-names></name><name><surname>Fua</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepFly3D, a deep learning-based approach for 3D limb and appendage tracking in tethered, adult <italic>Drosophila</italic></article-title><source>eLife</source><volume>8</volume><elocation-id>e48571</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48571</pub-id><pub-id pub-id-type="pmid">31584428</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname> <given-names>MD</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The No-U-Turn sampler: adaptively setting path lengths in hamiltonian monte carlo</article-title><source>Journal of Machine Learning Research</source><volume>15</volume><fpage>1593</fpage><lpage>1623</lpage></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>G</given-names></name><name><surname>Liu</surname> <given-names>Z</given-names></name><name><surname>Van Der Maaten</surname> <given-names>L</given-names></name><name><surname>Weinberger</surname> <given-names>KQ</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Densely connected convolutional networks</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>4700</fpage><lpage>4708</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2017.243</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>J</given-names></name><name><surname>Rathod</surname> <given-names>V</given-names></name><name><surname>Sun</surname> <given-names>C</given-names></name><name><surname>Zhu</surname> <given-names>M</given-names></name><name><surname>Korattikara</surname> <given-names>A</given-names></name><name><surname>Fathi</surname> <given-names>A</given-names></name><name><surname>Fischer</surname> <given-names>I</given-names></name><name><surname>Wojna</surname> <given-names>Z</given-names></name><name><surname>Song</surname> <given-names>Y</given-names></name><name><surname>Guadarrama</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Speed/accuracy trade-offs for modern convolutional object detectors</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>7310</fpage><lpage>7311</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2017.351</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Insafutdinov</surname> <given-names>E</given-names></name><name><surname>Pishchulin</surname> <given-names>L</given-names></name><name><surname>Andres</surname> <given-names>B</given-names></name><name><surname>Andriluka</surname> <given-names>M</given-names></name><name><surname>Schiele</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deepercut: a deeper, stronger, and faster multi-person poseestimation model</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>34</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-46466-4_3</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Iqbal</surname> <given-names>U</given-names></name><name><surname>Milan</surname> <given-names>A</given-names></name><name><surname>Gall</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Posetrack: joint multi-person pose estimation and tracking</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>2011</fpage><lpage>2020</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2017.495</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jaques</surname> <given-names>M</given-names></name><name><surname>Burke</surname> <given-names>M</given-names></name><name><surname>Hospedales</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Physics-as-inverse-graphics: joint unsupervised learning of objects and physics from video</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.11169">https://arxiv.org/abs/1905.11169</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javer</surname> <given-names>A</given-names></name><name><surname>Currie</surname> <given-names>M</given-names></name><name><surname>Lee</surname> <given-names>CW</given-names></name><name><surname>Hokanson</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>K</given-names></name><name><surname>Martineau</surname> <given-names>CN</given-names></name><name><surname>Yemini</surname> <given-names>E</given-names></name><name><surname>Grundy</surname> <given-names>LJ</given-names></name><name><surname>Li</surname> <given-names>C</given-names></name><name><surname>Ch'ng</surname> <given-names>Q</given-names></name><name><surname>Schafer</surname> <given-names>WR</given-names></name><name><surname>Nollen</surname> <given-names>EAA</given-names></name><name><surname>Kerr</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>AEX</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An open-source platform for analyzing and sharing worm-behavior data</article-title><source>Nature Methods</source><volume>15</volume><fpage>645</fpage><lpage>646</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0112-1</pub-id><pub-id pub-id-type="pmid">30171234</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jégou</surname> <given-names>S</given-names></name><name><surname>Drozdzal</surname> <given-names>M</given-names></name><name><surname>Vázquez</surname> <given-names>D</given-names></name><name><surname>Romero</surname> <given-names>A</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition Workshops</conf-name><fpage>1175</fpage><lpage>1183</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>J</given-names></name><name><surname>Alahi</surname> <given-names>A</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Perceptual losses for real-time style transfer and super-resolution</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage> 694</fpage><lpage> 711</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-46475-6_43</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>M</given-names></name><name><surname>Duvenaud</surname> <given-names>DK</given-names></name><name><surname>Wiltschko</surname> <given-names>A</given-names></name><name><surname>Adams</surname> <given-names>RP</given-names></name><name><surname>Datta</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Composing graphical models with neural networks for structured representations and fast inference</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage> 2946</fpage><lpage> 2954</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/6379-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-inference">https://papers.nips.cc/paper/6379-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-inference</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolles</surname> <given-names>JW</given-names></name><name><surname>Boogert</surname> <given-names>NJ</given-names></name><name><surname>Sridhar</surname> <given-names>VH</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name><name><surname>Manica</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Consistent individual differences drive collective behavior and group functioning of schooling fish</article-title><source>Current Biology</source><volume>27</volume><fpage>2862</fpage><lpage>2868</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.08.004</pub-id><pub-id pub-id-type="pmid">28889975</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jung</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>imgaug</data-title><ext-link ext-link-type="uri" xlink:href="https://github.com/aleju/imgaug">https://github.com/aleju/imgaug</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kain</surname> <given-names>JS</given-names></name><name><surname>Stokes</surname> <given-names>C</given-names></name><name><surname>de Bivort</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Phototactic personality in fruit flies and its suppression by serotonin and white</article-title><source>PNAS</source><volume>109</volume><fpage>19834</fpage><lpage>19839</lpage><pub-id pub-id-type="doi">10.1073/pnas.1211988109</pub-id><pub-id pub-id-type="pmid">23150588</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kain</surname> <given-names>J</given-names></name><name><surname>Stokes</surname> <given-names>C</given-names></name><name><surname>Gaudry</surname> <given-names>Q</given-names></name><name><surname>Song</surname> <given-names>X</given-names></name><name><surname>Foley</surname> <given-names>J</given-names></name><name><surname>Wilson</surname> <given-names>R</given-names></name><name><surname>de Bivort</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Leg-tracking and automated behavioural classification in Drosophila</article-title><source>Nature Communications</source><volume>4</volume><elocation-id>1910</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms2908</pub-id><pub-id pub-id-type="pmid">23715269</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kays</surname> <given-names>R</given-names></name><name><surname>Crofoot</surname> <given-names>MC</given-names></name><name><surname>Jetz</surname> <given-names>W</given-names></name><name><surname>Wikelski</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ECOLOGY. Terrestrial animal tracking as an eye on life and planet</article-title><source>Science</source><volume>348</volume><elocation-id>aaa2478</elocation-id><pub-id pub-id-type="doi">10.1126/science.aaa2478</pub-id><pub-id pub-id-type="pmid">26068858</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ke</surname> <given-names>L</given-names></name><name><surname>Chang</surname> <given-names>M-C</given-names></name><name><surname>Qi</surname> <given-names>H</given-names></name><name><surname>Lyu</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multi-scale structure-aware network for human pose estimation</article-title><conf-name>The European Conference on Computer Vision (ECCV)</conf-name><fpage>731</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-01216-8_44</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kendall</surname> <given-names>A</given-names></name><name><surname>Gal</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>What uncertainties do we need in bayesian deep learning for computer vision?</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>5574</fpage><lpage>5584</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision">https://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="software"><person-group person-group-type="author"><collab>keras team</collab></person-group><year iso-8601-date="2015">2015</year><data-title>Keras</data-title><ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiefer</surname> <given-names>J</given-names></name><name><surname>Wolfowitz</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>Stochastic estimation of the maximum of a regression function</article-title><source>The Annals of Mathematical Statistics</source><volume>23</volume><fpage>462</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177729392</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Klambauer</surname> <given-names>G</given-names></name><name><surname>Unterthiner</surname> <given-names>T</given-names></name><name><surname>Mayr</surname> <given-names>A</given-names></name><name><surname>Hochreiter</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Self-normalizing neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage> 971</fpage><lpage> 980</lpage><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/6698-self-normalizing-neural-networks">http://papers.nips.cc/paper/6698-self-normalizing-neural-networks</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klibaite</surname> <given-names>U</given-names></name><name><surname>Berman</surname> <given-names>GJ</given-names></name><name><surname>Cande</surname> <given-names>J</given-names></name><name><surname>Stern</surname> <given-names>DL</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An unsupervised method for quantifying the behavior of paired animals</article-title><source>Physical Biology</source><volume>14</volume><elocation-id>015006</elocation-id><pub-id pub-id-type="doi">10.1088/1478-3975/aa5c50</pub-id><pub-id pub-id-type="pmid">28140374</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Klibaite</surname> <given-names>U</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Interacting fruit flies synchronize behavior</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/545483</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname> <given-names>JW</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name><name><surname>Gomez-Marin</surname> <given-names>A</given-names></name><name><surname>MacIver</surname> <given-names>MA</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: correcting a reductionist Bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuhn</surname> <given-names>M</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Applied Predictive Modeling</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4614-6849-3</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kulkarni</surname> <given-names>TD</given-names></name><name><surname>Whitney</surname> <given-names>WF</given-names></name><name><surname>Kohli</surname> <given-names>P</given-names></name><name><surname>Tenenbaum</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep convolutional inverse graphics network</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>2539</fpage><lpage>2547</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/5851-deep-convolutional-inverse-graphics-network">https://papers.nips.cc/paper/5851-deep-convolutional-inverse-graphics-network</ext-link></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kumar</surname> <given-names>M</given-names></name><name><surname>Babaeizadeh</surname> <given-names>M</given-names></name><name><surname>Erhan</surname> <given-names>D</given-names></name><name><surname>Finn</surname> <given-names>C</given-names></name><name><surname>Levine</surname> <given-names>S</given-names></name><name><surname>Dinh</surname> <given-names>L</given-names></name><name><surname>Kingma</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Videoflow: a flow-based generative model for video</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1903.01434">https://arxiv.org/abs/1903.01434</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>H</given-names></name><name><surname>Xu</surname> <given-names>Z</given-names></name><name><surname>Taylor</surname> <given-names>G</given-names></name><name><surname>Studer</surname> <given-names>C</given-names></name><name><surname>Goldstein</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visualizing the loss landscape of neural nets</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>6391</fpage><lpage>6401</lpage><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets">http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets</ext-link></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>J</given-names></name><name><surname>Shelhamer</surname> <given-names>E</given-names></name><name><surname>Darrell</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fully convolutional networks for semantic segmentation,</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>3431</fpage><lpage>3440</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2015.7298965</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname> <given-names>JE</given-names></name><name><surname>Gillis</surname> <given-names>WF</given-names></name><name><surname>Beron</surname> <given-names>CC</given-names></name><name><surname>Neufeld</surname> <given-names>SQ</given-names></name><name><surname>Robertson</surname> <given-names>K</given-names></name><name><surname>Bhagat</surname> <given-names>ND</given-names></name><name><surname>Peterson</surname> <given-names>RE</given-names></name><name><surname>Peterson</surname> <given-names>E</given-names></name><name><surname>Hyun</surname> <given-names>M</given-names></name><name><surname>Linderman</surname> <given-names>SW</given-names></name><name><surname>Sabatini</surname> <given-names>BL</given-names></name><name><surname>Datta</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The striatum organizes 3D behavior via Moment-to-Moment action selection</article-title><source>Cell</source><volume>174</volume><fpage>44</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.04.019</pub-id><pub-id pub-id-type="pmid">29779950</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Warren</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>On the inference speed and video-compression robustness of DeepLabCut</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/457242</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendes</surname> <given-names>CS</given-names></name><name><surname>Bartos</surname> <given-names>I</given-names></name><name><surname>Akay</surname> <given-names>T</given-names></name><name><surname>Márka</surname> <given-names>S</given-names></name><name><surname>Mann</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Quantification of gait parameters in freely walking wild type and sensory deprived Drosophila Melanogaster</article-title><source>eLife</source><volume>2</volume><elocation-id>e00231</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.00231</pub-id><pub-id pub-id-type="pmid">23326642</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munkres</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>Algorithms for the assignment and transportation problems</article-title><source>Journal of the Society for Industrial and Applied Mathematics</source><volume>5</volume><fpage>32</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1137/0105003</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagy</surname> <given-names>M</given-names></name><name><surname>Akos</surname> <given-names>Z</given-names></name><name><surname>Biro</surname> <given-names>D</given-names></name><name><surname>Vicsek</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hierarchical group dynamics in pigeon flocks</article-title><source>Nature</source><volume>464</volume><fpage>890</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1038/nature08891</pub-id><pub-id pub-id-type="pmid">20376149</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagy</surname> <given-names>M</given-names></name><name><surname>Vásárhelyi</surname> <given-names>G</given-names></name><name><surname>Pettit</surname> <given-names>B</given-names></name><name><surname>Roberts-Mariani</surname> <given-names>I</given-names></name><name><surname>Vicsek</surname> <given-names>T</given-names></name><name><surname>Biro</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent hierarchies in pigeons</article-title><source>PNAS</source><volume>110</volume><fpage>13049</fpage><lpage>13054</lpage><pub-id pub-id-type="doi">10.1073/pnas.1305552110</pub-id><pub-id pub-id-type="pmid">23878247</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname> <given-names>T</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>AC</given-names></name><name><surname>Patel</surname> <given-names>A</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title><source>Nature Protocols</source><volume>14</volume><fpage>2152</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id><pub-id pub-id-type="pmid">31227823</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Newell</surname> <given-names>A</given-names></name><name><surname>Yang</surname> <given-names>K</given-names></name><name><surname>Deng</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stacked hourglass networks for human pose estimation</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>483</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-46484-8_29</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>TD</given-names></name><name><surname>Aldarondo</surname> <given-names>DE</given-names></name><name><surname>Willmore</surname> <given-names>L</given-names></name><name><surname>Kislin</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>SS</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pérez-Escudero</surname> <given-names>A</given-names></name><name><surname>Vicente-Page</surname> <given-names>J</given-names></name><name><surname>Hinz</surname> <given-names>RC</given-names></name><name><surname>Arganda</surname> <given-names>S</given-names></name><name><surname>de Polavieja</surname> <given-names>GG</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>idTracker: tracking individuals in a group by automatic identification of unmarked animals</article-title><source>Nature Methods</source><volume>11</volume><fpage>743</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2994</pub-id><pub-id pub-id-type="pmid">24880877</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pratt</surname> <given-names>LY</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Discriminability-based transfer between neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>204</fpage><lpage>211</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/641-discriminability-based-transfer-between-neural-networks">https://papers.nips.cc/paper/641-discriminability-based-transfer-between-neural-networks</ext-link></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prechelt</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Automatic early stopping using cross validation: quantifying the criteria</article-title><source>Neural Networks</source><volume>11</volume><fpage>761</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(98)00010-0</pub-id><pub-id pub-id-type="pmid">12662814</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname> <given-names>E</given-names></name><name><surname>Lawless</surname> <given-names>G</given-names></name><name><surname>Ludwig</surname> <given-names>R</given-names></name><name><surname>Martinovic</surname> <given-names>I</given-names></name><name><surname>Bulthoff</surname> <given-names>HH</given-names></name><name><surname>Black</surname> <given-names>MJ</given-names></name><name><surname>Ahmad</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep neural Network-Based cooperative visual tracking through multiple micro aerial vehicles</article-title><source>IEEE Robotics and Automation Letters</source><volume>3</volume><fpage>3193</fpage><lpage>3200</lpage><pub-id pub-id-type="doi">10.1109/LRA.2018.2850224</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ran</surname> <given-names>FA</given-names></name><name><surname>Hsu</surname> <given-names>PD</given-names></name><name><surname>Wright</surname> <given-names>J</given-names></name><name><surname>Agarwala</surname> <given-names>V</given-names></name><name><surname>Scott</surname> <given-names>DA</given-names></name><name><surname>Zhang</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Genome engineering using the CRISPR-Cas9 system</article-title><source>Nature Protocols</source><volume>8</volume><fpage>2281</fpage><lpage>2308</lpage><pub-id pub-id-type="doi">10.1038/nprot.2013.143</pub-id><pub-id pub-id-type="pmid">24157548</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Faster R-CNN: towards real-time object detection with region proposal networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>91</fpage><lpage>99</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks">https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robbins</surname> <given-names>H</given-names></name><name><surname>Monro</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1951">1951</year><article-title>A stochastic approximation method</article-title><source>The Annals of Mathematical Statistics</source><volume>22</volume><fpage>400</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177729586</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romero-Ferrero</surname> <given-names>F</given-names></name><name><surname>Bergomi</surname> <given-names>MG</given-names></name><name><surname>Hinz</surname> <given-names>RC</given-names></name><name><surname>Heras</surname> <given-names>FJH</given-names></name><name><surname>de Polavieja</surname> <given-names>GG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Idtracker.ai: tracking all individuals in small or large collectives of unmarked animals</article-title><source>Nature Methods</source><volume>16</volume><fpage>179</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0295-5</pub-id><pub-id pub-id-type="pmid">30643215</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>U-net: convolutional networks for biomedical image segmentation</article-title><conf-name>International Conference on Medical Image Computing and Computer-Assisted Intervention</conf-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal</surname> <given-names>SB</given-names></name><name><surname>Twomey</surname> <given-names>CR</given-names></name><name><surname>Hartnett</surname> <given-names>AT</given-names></name><name><surname>Wu</surname> <given-names>HS</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Revealing the hidden networks of interaction in mobile animal groups allows prediction of complex behavioral contagion</article-title><source>PNAS</source><volume>112</volume><fpage>4690</fpage><lpage>4695</lpage><pub-id pub-id-type="doi">10.1073/pnas.1420068112</pub-id><pub-id pub-id-type="pmid">25825752</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname> <given-names>AG</given-names></name><name><surname>Conjeti</surname> <given-names>S</given-names></name><name><surname>Navab</surname> <given-names>N</given-names></name><name><surname>Wachinger</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian QuickNAT: model uncertainty in deep whole-brain segmentation for structure-wise quality control</article-title><source>NeuroImage</source><volume>195</volume><fpage>11</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.042</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sabour</surname> <given-names>S</given-names></name><name><surname>Frosst</surname> <given-names>N</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic routing between capsules</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>3856</fpage><lpage>3866</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules">https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules</ext-link></element-citation></ref><ref id="bib93"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Saini</surname> <given-names>N</given-names></name><name><surname>Price</surname> <given-names>E</given-names></name><name><surname>Tallamraju</surname> <given-names>R</given-names></name><name><surname>Enficiaud</surname> <given-names>R</given-names></name><name><surname>Ludwig</surname> <given-names>R</given-names></name><name><surname>Martinovia</surname> <given-names>I</given-names></name><name><surname>Ahmad</surname> <given-names>A</given-names></name><name><surname>Black</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Markerless outdoor human motion capture using multiple autonomous micro aerial vehicles</article-title><conf-name>In: International Conference on Computer Vision</conf-name></element-citation></ref><ref id="bib94"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sandler</surname> <given-names>M</given-names></name><name><surname>Howard</surname> <given-names>A</given-names></name><name><surname>Zhu</surname> <given-names>M</given-names></name><name><surname>Zhmoginov</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>L-C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mobilenetv2: inverted residuals and linear bottlenecks,</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>4510</fpage><lpage>4520</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2018.00474</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiffman</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Drones flying high as new tool for field biologists</article-title><source>Science</source><volume>344</volume><fpage>459</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1126/science.344.6183.459</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Seethapathi</surname> <given-names>N</given-names></name><name><surname>Wang</surname> <given-names>S</given-names></name><name><surname>Saluja</surname> <given-names>R</given-names></name><name><surname>Blohm</surname> <given-names>G</given-names></name><name><surname>Kording</surname> <given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Movement science needs different pose tracking algorithms</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1907.10226">https://arxiv.org/abs/1907.10226</ext-link></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname> <given-names>GJ</given-names></name><name><surname>Bueno de Mesquita</surname> <given-names>M</given-names></name><name><surname>Ryu</surname> <given-names>WS</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Emergence of long timescales and stereotyped behaviors in Caenorhabditis elegans</article-title><source>PNAS</source><volume>108</volume><fpage>7286</fpage><lpage>7289</lpage><pub-id pub-id-type="doi">10.1073/pnas.1007868108</pub-id><pub-id pub-id-type="pmid">21502536</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowers</surname> <given-names>JR</given-names></name><name><surname>Hofbauer</surname> <given-names>M</given-names></name><name><surname>Bastien</surname> <given-names>R</given-names></name><name><surname>Griessner</surname> <given-names>J</given-names></name><name><surname>Higgins</surname> <given-names>P</given-names></name><name><surname>Farooqui</surname> <given-names>S</given-names></name><name><surname>Fischer</surname> <given-names>RM</given-names></name><name><surname>Nowikovsky</surname> <given-names>K</given-names></name><name><surname>Haubensak</surname> <given-names>W</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name><name><surname>Tessmar-Raible</surname> <given-names>K</given-names></name><name><surname>Straw</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Virtual reality for freely moving animals</article-title><source>Nature Methods</source><volume>14</volume><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id><pub-id pub-id-type="pmid">28825703</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strandburg-Peshkin</surname> <given-names>A</given-names></name><name><surname>Twomey</surname> <given-names>CR</given-names></name><name><surname>Bode</surname> <given-names>NW</given-names></name><name><surname>Kao</surname> <given-names>AB</given-names></name><name><surname>Katz</surname> <given-names>Y</given-names></name><name><surname>Ioannou</surname> <given-names>CC</given-names></name><name><surname>Rosenthal</surname> <given-names>SB</given-names></name><name><surname>Torney</surname> <given-names>CJ</given-names></name><name><surname>Wu</surname> <given-names>HS</given-names></name><name><surname>Levin</surname> <given-names>SA</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual sensory networks and effective information transfer in animal groups</article-title><source>Current Biology</source><volume>23</volume><fpage>R709</fpage><lpage>R711</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.07.059</pub-id><pub-id pub-id-type="pmid">24028946</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strandburg-Peshkin</surname> <given-names>A</given-names></name><name><surname>Farine</surname> <given-names>DR</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name><name><surname>Crofoot</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>GROUP DECISIONS. Shared decision-making drives collective movement in wild baboons</article-title><source>Science</source><volume>348</volume><fpage>1358</fpage><lpage>1361</lpage><pub-id pub-id-type="doi">10.1126/science.aaa5099</pub-id><pub-id pub-id-type="pmid">26089514</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strandburg-Peshkin</surname> <given-names>A</given-names></name><name><surname>Farine</surname> <given-names>DR</given-names></name><name><surname>Crofoot</surname> <given-names>MC</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Habitat and social factors shape individual decisions and emergent group structure during baboon collective movement</article-title><source>eLife</source><volume>6</volume><elocation-id>e19505</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19505</pub-id><pub-id pub-id-type="pmid">28139196</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todd</surname> <given-names>JG</given-names></name><name><surname>Kain</surname> <given-names>JS</given-names></name><name><surname>de Bivort</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Systematic exploration of unsupervised methods for mapping behavior</article-title><source>Physical Biology</source><volume>14</volume><elocation-id>015002</elocation-id><pub-id pub-id-type="doi">10.1088/1478-3975/14/1/015002</pub-id><pub-id pub-id-type="pmid">28166059</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tran</surname> <given-names>D</given-names></name><name><surname>Hoffman</surname> <given-names>MW</given-names></name><name><surname>Moore</surname> <given-names>D</given-names></name><name><surname>Suter</surname> <given-names>C</given-names></name><name><surname>Vasudevan</surname> <given-names>S</given-names></name><name><surname>Radul</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Simple, distributed, and accelerated probabilistic programming.</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/7987-simple-distributed-and-accelerated-probabilistic-programming">https://papers.nips.cc/paper/7987-simple-distributed-and-accelerated-probabilistic-programming</ext-link></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhlmann</surname> <given-names>V</given-names></name><name><surname>Ramdya</surname> <given-names>P</given-names></name><name><surname>Delgado-Gonzalo</surname> <given-names>R</given-names></name><name><surname>Benton</surname> <given-names>R</given-names></name><name><surname>Unser</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>FlyLimbTracker: an active contour based approach for leg segment tracking in unmarked, freely behaving Drosophila</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0173433</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0173433</pub-id><pub-id pub-id-type="pmid">28453566</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Valentin</surname> <given-names>J</given-names></name><name><surname>Keskin</surname> <given-names>C</given-names></name><name><surname>Pidlypenskyi</surname> <given-names>P</given-names></name><name><surname>Makadia</surname> <given-names>A</given-names></name><name><surname>Sud</surname> <given-names>A</given-names></name><name><surname>Bouaziz</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Tensorflow graphics: Computer graphics meets deep learning</data-title><ext-link ext-link-type="uri" xlink:href="https://github.com/tensorflow/graphics">https://github.com/tensorflow/graphics</ext-link></element-citation></ref><ref id="bib106"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Van den Oord</surname> <given-names>A</given-names></name><name><surname>Dieleman</surname> <given-names>S</given-names></name><name><surname>Zen</surname> <given-names>H</given-names></name><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name><name><surname>Graves</surname> <given-names>A</given-names></name><name><surname>Kalchbrenner</surname> <given-names>N</given-names></name><name><surname>Senior</surname> <given-names>A</given-names></name><name><surname>Kavukcuoglu</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Wavenet: a generativemodel for raw audio</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.03499">https://arxiv.org/abs/1609.03499</ext-link></element-citation></ref><ref id="bib107"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Van den Oord</surname> <given-names>A</given-names></name><name><surname>Kalchbrenner</surname> <given-names>N</given-names></name><name><surname>Espeholt</surname> <given-names>L</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name><name><surname>Graves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Conditional image generation with pixelcnn decoders</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>4790</fpage><lpage>4798</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders">https://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders</ext-link></element-citation></ref><ref id="bib108"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Versace</surname> <given-names>E</given-names></name><name><surname>Caffini</surname> <given-names>M</given-names></name><name><surname>Werkhoven</surname> <given-names>Z</given-names></name><name><surname>de Bivort</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Individual, but not population asymmetries, are modulated by social environment and genotype in Drosophila Melanogaster</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/694901</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weigert</surname> <given-names>M</given-names></name><name><surname>Schmidt</surname> <given-names>U</given-names></name><name><surname>Boothe</surname> <given-names>T</given-names></name><name><surname>Müller</surname> <given-names>A</given-names></name><name><surname>Dibrov</surname> <given-names>A</given-names></name><name><surname>Jain</surname> <given-names>A</given-names></name><name><surname>Wilhelm</surname> <given-names>B</given-names></name><name><surname>Schmidt</surname> <given-names>D</given-names></name><name><surname>Broaddus</surname> <given-names>C</given-names></name><name><surname>Culley</surname> <given-names>S</given-names></name><name><surname>Rocha-Martins</surname> <given-names>M</given-names></name><name><surname>Segovia-Miranda</surname> <given-names>F</given-names></name><name><surname>Norden</surname> <given-names>C</given-names></name><name><surname>Henriques</surname> <given-names>R</given-names></name><name><surname>Zerial</surname> <given-names>M</given-names></name><name><surname>Solimena</surname> <given-names>M</given-names></name><name><surname>Rink</surname> <given-names>J</given-names></name><name><surname>Tomancak</surname> <given-names>P</given-names></name><name><surname>Royer</surname> <given-names>L</given-names></name><name><surname>Jug</surname> <given-names>F</given-names></name><name><surname>Myers</surname> <given-names>EW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Content-aware image restoration: pushing the limits of fluorescence microscopy</article-title><source>Nature Methods</source><volume>15</volume><fpage>1090</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0216-7</pub-id><pub-id pub-id-type="pmid">30478326</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Werkhoven</surname> <given-names>Z</given-names></name><name><surname>Rohrsen</surname> <given-names>C</given-names></name><name><surname>Qin</surname> <given-names>C</given-names></name><name><surname>Brembs</surname> <given-names>B</given-names></name><name><surname>de Bivort</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>MARGO (Massively automated Real-time GUI for Object-tracking), aplatform for high-throughput ethology</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/593046</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wild</surname> <given-names>B</given-names></name><name><surname>Sixt</surname> <given-names>L</given-names></name><name><surname>Landgraf</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Automatic localization and decoding of honeybee markers using deep convolutional neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.04557">https://arxiv.org/abs/1802.04557</ext-link></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname> <given-names>AB</given-names></name><name><surname>Johnson</surname> <given-names>MJ</given-names></name><name><surname>Iurilli</surname> <given-names>G</given-names></name><name><surname>Peterson</surname> <given-names>RE</given-names></name><name><surname>Katon</surname> <given-names>JM</given-names></name><name><surname>Pashkovski</surname> <given-names>SL</given-names></name><name><surname>Abraira</surname> <given-names>VE</given-names></name><name><surname>Adams</surname> <given-names>RP</given-names></name><name><surname>Datta</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping Sub-Second Structure in Mouse Behavior</article-title><source>Neuron</source><volume>88</volume><fpage>1121</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id><pub-id pub-id-type="pmid">26687221</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>R</given-names></name><name><surname>Isola</surname> <given-names>P</given-names></name><name><surname>Efros</surname> <given-names>AA</given-names></name><name><surname>Shechtman</surname> <given-names>E</given-names></name><name><surname>Wang</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The unreasonable effectiveness of deep features as a perceptual metric</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>586</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2018.00068</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zuffi</surname> <given-names>S</given-names></name><name><surname>Kanazawa</surname> <given-names>A</given-names></name><name><surname>Jacobs</surname> <given-names>DW</given-names></name><name><surname>Black</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>3D menagerie: modeling the 3D shape and pose of animals,</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>6365</fpage><lpage>6373</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2017.586</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zuffi</surname> <given-names>S</given-names></name><name><surname>Kanazawa</surname> <given-names>A</given-names></name><name><surname>Berger-Wolf</surname> <given-names>T</given-names></name><name><surname>Black</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Three-D safari: learning to estimate zebra pose, shape, and texture from images&quot; In the Wild</article-title><conf-name>International Conference on Computer Vision</conf-name></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Our subpixel maxima algorithm increases speed without decreasing accuracy.</title><p>Prediction accuracy on the fly dataset is maintained across downsampling configurations (<bold>a</bold>). Letter-value plots (a-top) show the raw error distributions for each configuration. Visualizations of the credible intervals (99% highest-density region) of the posterior distributions for the mean and variance (a-bottom) illustrate statistical differences between the error distributions, where using subpixel maxima decreases both the mean and variance of the error distribution. Inference speed is fast and can be run in real-time on single images (batch size = 1) at ~30–110 Hz or offline (batch size = 100) upwards of 1000 Hz (<bold>b</bold>). Plots show the inference speeds for our Stacked DenseNet model across downsampling configurations as well as the other models we tested for each of our datasets.</p><p><supplementary-material id="app1fig1sdata1"><label>Appendix 1—figure 1—source data 1.</label><caption><title>Raw prediction errors for experiments in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1a</xref>.</title><p>See 'Materials and methods’ for details.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47994-app1-fig1-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app1-fig1-v2.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Predicting the multi-scale geometry of the posture graph reduces error.</title><p>Letter-value plots (top) show the raw error distributions for each experiment. Visualizations of the posterior distributions for the mean and variance (bottom) show statistical differences between the error distributions. Predicting the posture graph decreases both the mean and variance of the error distribution.</p><p><supplementary-material id="app1fig2sdata1"><label>Appendix 1—figure 2—source data 1.</label><caption><title>Raw prediction errors for experiments in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>.</title><p>See 'Materials and methods’ for details.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47994-app1-fig2-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app1-fig2-v2.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Training time required for our Stacked DenseNet model, the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>), and the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) (n = 15 per model) using our zebra dataset.</title><p>Boxplots and swarm plots (left) show the total training time to convergence (&lt;0.001 improvement in validation loss for 50 epochs). Line plots (right) illustrate the Euclidean error of the validation set during training, where error bars show bootstrapped (n = 1000) 99% confidence intervals of the mean. Fully training models to convergence requires only a few hours of optimization (left) with reasonable accuracy reached after only 1 hr (right) for our Stacked DenseNet model.</p><p><supplementary-material id="app1fig3sdata1"><label>Appendix 1—figure 3—source data 1.</label><caption><title>Total training time for each model in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47994-app1-fig3-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="app1fig3sdata2"><label>Appendix 1—figure 3—source data 2.</label><caption><title>Mean euclidean error as a function of training time for each model in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47994-app1-fig3-data2-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app1-fig3-v2.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>A comparison of prediction accuracy with different numbers of training examples from our zebra dataset.</title><p>The error distributions shown as letter-value plots (top) illustrate the Euclidean error for the remainder of the dataset not used for training—with a total of 900 labeled examples in the dataset. Line plots (bottom) show posterior credible intervals (99% highest-density region) for the mean and variance of the error distributions. We tested our Stacked DenseNet model; the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) with transfer learning—that is with weights pretrained on ImageNet (<xref ref-type="bibr" rid="bib28">Deng et al., 2009</xref>); the same model without transfer learning—that is with randomly-initialized weights; and the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>). Our Stacked DenseNet model achieves high accuracy using few training examples without the use the transfer learning. Using pretrained weights does slightly decrease overall prediction error for the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>), but the effect size is relatively small.</p><p><supplementary-material id="app1fig4sdata4"><label>Appendix 1—figure 4—source data 1.</label><caption><title>Raw prediction errors for experiments in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>.</title><p>See 'Materials and methods’ for details.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47994-app1-fig4-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app1-fig4-v2.tif"/></fig></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s8" sec-type="appendix"><title>Convolutional neural networks (CNNs)</title><p><italic>Artificial neural networks</italic> like CNNs are complex, non-linear regression models that 'learn’ a hierarchically-organized set of parameters from real-world data via optimization. These machine learning models are now commonplace in science and industry and have proven to be surprisingly effective for a large number of applications where more conventional statistical models have failed (<xref ref-type="bibr" rid="bib68">LeCun et al., 2015</xref>). For computer vision tasks, CNN parameters typically take the form of two-dimensional convolutional filters that are optimized to detect spatial features needed to model relationships between high-dimensional image data and some related variable(s) of interest, such as locations in space—for example posture keypoints—or semantic labels (<xref ref-type="bibr" rid="bib70">Long et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Badrinarayanan et al., 2015</xref>).</p><p>Once a training set is generated (Appendix 3), a CNN model must be selected and optimized to perform the prediction task. CNNs are incredibly flexible with regard to how models are specified and trained, which is both an advantage and a disadvantage. This flexibility means models can be adapted to almost any computer vision task, but it also means the number of possible model architectures and optimization schemes is very large. This can make selecting an architecture and specifying hyperparameters a challenging process. However, most research on pose estimation has converged on a set of models that generally work well for this task (Appendix 4).</p><p>After selecting an architecture, the parameters of the model are set to an initial value and then iteratively updated to minimize some objective function, or <italic>loss function</italic>, that describes the difference between the model’s predictive distribution and the true distribution of the data—in other words, the likelihood of the model’s output is maximized. These parameter updates are performed using a modified version of the gradient descent algorithm (<xref ref-type="bibr" rid="bib22">Cauchy, 1847</xref>) known as <italic>mini-batch stochastic gradient descent</italic>—often referred to as simply <italic>stochastic gradient descent</italic> or <italic>SGD</italic> (<xref ref-type="bibr" rid="bib87">Robbins and Monro, 1951</xref>; <xref ref-type="bibr" rid="bib59">Kiefer and Wolfowitz, 1952</xref>). SGD iteratively optimizes the model parameters using small randomly-selected subsamples, or <italic>batches</italic>, of training data. Using SGD allows the model to be trained on extremely large datasets in an iterative 'online’ fashion without the need to load the entire dataset into memory. The model parameters are updated with each batch by adjusting the parameter values in a direction that minimizes the error—where one round of training on the full dataset is commonly referred to as an <italic>epoch</italic>. The original SGD algorithm requires careful selection and tuning of hyperparameters to successfully optimize a model, but modern versions of the algorithm, such as <italic>ADAM</italic> (<xref ref-type="bibr" rid="bib60">Kingma and Ba, 2014</xref>), automatically tune these hyperparameters, which makes optimization more straightforward.</p><p>The model parameters are optimized until they reach a convergence criterion, which is some measure of performance that indicates the model has reached a good location in parameter space. The most commonly used convergence criterion is a measure of predictive accuracy—often the loss function used for optimization—on a held-out <italic>validation set</italic>—a subsample of the training data not used for optimization—that evaluates the model’s ability to generalize to new 'out-of-sample’ data. The model is typically evaluated at the end of each training epoch to assess performance on the validation set. Once performance on the validation set stops improving, training is usually stopped to prevent the model from overfitting to the training set—a technique known as <italic>early stopping</italic> (<xref ref-type="bibr" rid="bib83">Prechelt, 1998</xref>).</p></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><sec id="s9" sec-type="appendix"><title>Collecting training data</title><p>Depending on the variability of the data, CNNs usually require thousands or tens of thousands of manually-annotated examples in order to reach human-level accuracy. However, in laboratory settings, sources of image variation like lighting and spatial scale can be more easily controlled, which minimizes the number of training examples needed to achieve accurate predictions.</p><p>This need for a large training set can be further reduced in a number of ways. Two commonly used methods include (1) <italic>transfer learning</italic>—using a model with parameters that are pre-trained on a larger set of images, such as the ImageNet database (<xref ref-type="bibr" rid="bib28">Deng et al., 2009</xref>), containing diverse features (<xref ref-type="bibr" rid="bib82">Pratt, 1992</xref>; <xref ref-type="bibr" rid="bib44">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>)— and (2) <italic>augmentation</italic>— artificially increasing data variance by applying spatial and noise transformations such as flipping (mirroring), rotating, scaling, and adding different forms of noise or artificial occlusions. Both of these methods act as useful forms of <italic>regularization</italic>—incorporating a prior distribution—that allows the model to generalize well to new data even when the training set is small. Transfer learning incorporates prior information that images from the full dataset should contain statistical features similar to other images of the natural world, while augmentation incorporates prior knowledge that animals are bilaterally symmetric, can vary in their body size, position, and orientation, and that noise and occlusions sometimes occur.</p><p><xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> introduced two especially clever solutions for collecting an adequate training set. First, they cluster unannotated images based on pixel variance and uniformly sample images from each cluster, which reduces correlation between training examples and ensures the training data are representative of the entire distribution of possible images. Second, they use <italic>active learning</italic> where a CNN is trained on a small number of annotated examples and is then used to initialize keypoint locations for a larger set of unannotated data. These pre-initialized data are then manually corrected by the annotator, the model is retrained, and the unannotated data are re-initialized. The annotator applies this process iteratively as the training set grows larger until they are providing only minor adjustments to the pre-initialized data. This 'human-in-the-loop’-style annotation expedites the process of generating an adequately large training set by reducing the cognitive load on the annotator—where the pose estimation model serves as a 'cognitive partner’. Such a strategy also allows the annotator to automatically select new training examples based on the performance of the current iteration—where low-confidence predictions indicate examples that should be annotated for maximum improvement (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>Of course, annotating image data requires software made for this purpose. <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> provide a custom annotation GUI written in MATLAB specifically designed for annotating posture using an active learning strategy. recently <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> added a Python-based GUI in an updated version of their software—including active learning and image sampling methods (see <xref ref-type="bibr" rid="bib78">Nath et al., 2019</xref>). Our framework also includes a Python-based GUI for annotating data with similar features.</p></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><sec id="s10" sec-type="appendix"><title>Fully-convolutional regression</title><p>For the task of pose estimation, a CNN is optimized to predict the locations of postural keypoints in an image. One approach is to use a CNN to directly predict the numerical value of each keypoint coordinate as an output. However, making predictions in this way removes real-world constraints on the model’s predictive distribution by destroying spatial relationships within images, which negates many of the advantages of using CNNs in the first place.</p><p>CNNs are particularly good at transforming one image to produce another related image, or set of images, while preserving spatial relationships and allowing for translation-invariant predictions—a configuration known as a <italic>fully-convolutional neural network</italic> or <italic>F-CNN</italic> (<xref ref-type="bibr" rid="bib70">Long et al., 2015</xref>). Therefore, instead of directly regressing images to coordinate values, a popular solution (<xref ref-type="bibr" rid="bib79">Newell et al., 2016</xref>; <xref ref-type="bibr" rid="bib44">Insafutdinov et al., 2016</xref>; <xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) is to optimize a F-CNN that transforms images to predict a stack of output images known as <italic>confidence maps</italic>—one for each keypoint. Each confidence map in the output volume contains a single, two-dimensional, symmetric Gaussian indicating the location of each joint, and the scalar value of the peak indicates the confidence score of the prediction—typically a value between 0 and 1. The confidence maps are then processed to produce the coordinates of each keypoint.</p><p>In the case of <italic>multiple pose estimation</italic> where an image contains many individuals, the global geometry of the posture graph is also predicted by training the model to produce <italic>part affinity fields</italic> (<xref ref-type="bibr" rid="bib20">Cao et al., 2017</xref>)— directional vector fields drawn between joints in the posture graph—or <italic>pairwise terms</italic> (<xref ref-type="bibr" rid="bib44">Insafutdinov et al., 2016</xref>)—vector fields of the conditional distributions between posture keypoints (e.g., <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>foot</mml:mtext><mml:mo stretchy="false">|</mml:mo><mml:mtext>head</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This allows multiple posture graphs to be disentangled from the image using graph partitioning as the vector fields indicate the probability of the connection between joints (see <xref ref-type="bibr" rid="bib20">Cao et al., 2017</xref> for details).</p><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>An illustration showing the progression of encoder-decoder architectures from the literature—ordered by performance from top to bottom (see Appendix 4: 'Encoder-decoder models' for further details).</title><p>Most advances in performance have come from adding connections between layers in the network, culminating in FC-DenseNet from <xref ref-type="bibr" rid="bib48">Jégou et al. (2017)</xref>. Lines in each illustration indicate connections between convolutional blocks with the thickness of the line indicating the magnitude of information flow between layers in the network. The size of each convolution block indicates the relative number of feature maps (width) and spatial scale (height). The callout for FC-DenseNet (<xref ref-type="bibr" rid="bib48">Jégou et al., 2017</xref>; bottom-left) shows the elaborate set of skip connections within each densely-connected convolutional block as well as our additions of bottleneck and compression layers (described by <xref ref-type="bibr" rid="bib42">Huang et al., 2017a</xref>) to increase efficiency (Appendix 8).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app4-fig1-v2.tif"/></fig></sec><sec id="s11" sec-type="appendix"><title>Encoder-decoder models</title><p>A popular type of F-CNN (Appendix 4) for solving posture regression problems is known as an <italic>encoder-decoder</italic> model (<xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref>), which first gained popularity for the task of <italic>semantic segmentation</italic>—a supervised computer vision problem where each pixel in an image is classified into a one of several labeled categories like 'dog’, 'tree’, or 'road’ (<xref ref-type="bibr" rid="bib70">Long et al., 2015</xref>). This model is designed to repeatedly convolve and downsample input images in the bottom-up <italic>encoder</italic> step and then convolve and upsample the encoder’s output in the top-down <italic>decoder</italic> step to produce the final output. Repeatedly applying convolutions and non-linear functions, or <italic>activations</italic>, to the input images transforms pixel values into higher-order spatial features, while downsampling and upsampling respectively increases and decreases the scale and complexity of these features.</p><fig id="app4fig2" position="float"><label>Appendix 4—figure 2.</label><caption><title>An illustration of the basic encoder-decoder design.</title><p>The encoder converts the input images into spatial features, and the decoder transforms spatial features to the desired output.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app4-fig2-v2.tif"/></fig><p><xref ref-type="bibr" rid="bib9">Badrinarayanan et al. (2015)</xref> were the first to popularize a form of this model —known as <italic>SegNet</italic>— for semantic segmentation. However, this basic design is inherently limited because the decoder relies solely on the downsampled output from the encoder, which restricts the features used for predictions to those with the largest spatial scale and highest complexity. For example, a very deep network might learn a complex spatial pattern for predicting 'grass’ or 'trees’, but because it cannot directly access information from the earliest layers of the network, it cannot use the simplest features that plants are green and brown. Subsequent work by <xref ref-type="bibr" rid="bib89">Ronneberger et al. (2015)</xref> improved on these problems with the addition of <italic>skip connections</italic> between the encoder and decoder, where feature maps from encoder layers are concatenated to those decoder layers with the same spatial scale. This set of connections then allows the optimizer, rather than the user, to select the most relevant spatial scale(s) for making predictions.</p><p><xref ref-type="bibr" rid="bib48">Jégou et al. (2017)</xref> are the latest to advance the encoder-decoder paradigm. These researchers introduced a fully-convolutional version of <xref ref-type="bibr" rid="bib42">Huang et al. (2017a)</xref> <italic>DenseNet</italic> architecture known as a <italic>fully-convolutional DenseNet</italic>, or <italic>FC-DenseNet</italic>. FC-DenseNet's key improvement is an elaborate set of feed-forward residual connections where the input to each convolutional layer is a concatenated stack of feature maps from all previous layers. This densely-connected design was motivated by the insight that many state-of-the-art models learn a large proportion of redundant features. Most CNNs are not designed so that the final output layers can access all feature maps in the network simultaneously, and this limitation causes these networks to 'forget’ and 'relearn’ important features as the input images are transformed to produce the output. In the case of the incredibly popular ResNet-101 (<xref ref-type="bibr" rid="bib40">He et al., 2016</xref>) nearly 40% of the features can be classified as redundant (<xref ref-type="bibr" rid="bib7">Ayinde and Zurada, 2018</xref>). A densely-connected architecture has the advantages of reduced feature redundancy, increased feature reuse, enhanced feature propagation from early layers to later layers, and subsequently, a substantial reduction in the number of parameters needed to achieve state-of-the-art results (<xref ref-type="bibr" rid="bib42">Huang et al., 2017a</xref>). Recent work has also shown that DenseNet's elaborate set of skip connections have the pleasant side-effect of convexifying the loss landscape during optimization (<xref ref-type="bibr" rid="bib69">Li et al., 2018</xref>), which allows for faster optimization and increases the likelihood of reaching a good optimum.</p></sec></boxed-text></app><app id="appendix-5"><title>Appendix 5</title><boxed-text><sec id="s12" sec-type="appendix"><title>The state of the art for individual pose estimation</title><p>Many of the current state-of-the-art models for individual posture estimation are based on the design from <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref> (e.g., <xref ref-type="bibr" rid="bib56">Ke et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Chen et al., 2017</xref>; also see benchmark results from <xref ref-type="bibr" rid="bib5">Andriluka et al. (2014)</xref>, but employ various modifications that increase complexity to improve performance. <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref> employ what they call a <italic>Stacked Hourglass</italic> network (<xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>), which consists of a series of multi-scale encoder-decoder <italic>hourglass</italic> modules connected together in a feed-forward configuration (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The main novelties these researchers introduce include (1) stacking multiple hourglass networks together for repeated top-down-bottom-up inference, (2) using convolutional blocks based on the ResNet architecture (<xref ref-type="bibr" rid="bib40">He et al., 2016</xref>) with residual connections between the input and output of each block, and (3) using residual connections between the encoder and decoder (similar to <xref ref-type="bibr" rid="bib89">Ronneberger et al., 2015</xref>) with residual blocks in between. <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref> also apply a technique known as <italic>intermediate supervision</italic> (<xref ref-type="fig" rid="fig2">Figure 2</xref>) where the loss function used for model training is applied to the output of each hourglass as a way of improving optimization across the model’s many layers. Recent work by <xref ref-type="bibr" rid="bib48">Jégou et al. (2017)</xref> has further improved on this encoder-decoder design (see Appendix 4: 'Encoder-decoder models' and <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>), but to the best of our knowledge, the model introduced by <xref ref-type="bibr" rid="bib48">Jégou et al. (2017)</xref> has not been previously applied to pose estimation.</p></sec></boxed-text></app><app id="appendix-6"><title>Appendix 6</title><boxed-text><sec id="s13" sec-type="appendix"><title>Overparameterization and the limitations of LEAP</title><p>Overparameterization is a key limitation for many pose estimation methods, and addressing this problem is critical for high-performance applications. <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> approached this problem by designing their LEAP model after the model from <xref ref-type="bibr" rid="bib9">Badrinarayanan et al. (2015)</xref>, which is a straighforward encoder-decoder design (<xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>; Appendix 4: 'Encoder-decoder models'). They benchmarked their model on posture estimation tasks for laboratory animals and compared performance with the more-complex Stacked Hourglass model from <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref>. They found their smaller, simplified model achieved equal or better median accuracy with dramatic improvements in inference speed up to 185 Hz. However, <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref> first rotationally and translationally aligned each image to improve performance, and their reported inference speeds do not include this computationally expensive preprocessing step. Additionally, rotationally and translationally aligning images is not always possible when the background is complex or highly-variable—such as in field settings—or the study animal has a non-rigid body. This limitation makes the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) unsuitable in many cases. While their approach is simple and effective for a multitude of experimental setups, the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) is also implicitly limited in the same ways as <xref ref-type="bibr" rid="bib9">Badrinarayanan et al. (2015)</xref>'s SegNet model (see Appendix 4: 'Encoder-decoder models'). The LEAP model cannot make predictions using multiple spatial scales and is not robust to data variance such as rotations (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>).</p></sec></boxed-text></app><app id="appendix-7"><title>Appendix 7</title><boxed-text><sec id="s14" sec-type="appendix"><title>Linear model fitting with Stan</title><p>We estimated the joint posterior <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for each model using the No-U-Turn Sampler (NUTS; <xref ref-type="bibr" rid="bib41">Hoffman and Gelman, 2014</xref>), a self-tuning variant of the Hamiltonian Monte Carlo (HMC) algorithm (<xref ref-type="bibr" rid="bib30">Duane et al., 1987</xref>), implemented in Stan (<xref ref-type="bibr" rid="bib21">Carpenter et al., 2017</xref>). We drew HMC samples using four independent Markov chains consisting of 1000 warm-up iterations and 1000 sampling iterations for a total of 4000 sampling iterations. To speed up sampling, we randomly subsampled 20% of the data from each replicate when fitting each linear model, and we fit each model 5 times to ensure the results were consistent. All models converged without any signs of pathological behavior. We performed a posterior predictive check by visually inspecting predictive samples to assess model fit. For our priors we chose relatively uninformative distributions <inline-formula><mml:math id="inf31"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>𝐶𝑎𝑢𝑐ℎ𝑦</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>𝐶𝑎𝑢𝑐ℎ𝑦</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, but we found that the choice of prior generally did not have an effect on the final result due to the large amount of data used to fit each model.</p></sec></boxed-text></app><app id="appendix-8"><title>Appendix 8</title><boxed-text><sec id="s15" sec-type="appendix"><title>Stacked DenseNet</title><p>Our Stacked DenseNet model consists of an initial 7 × 7 convolutional layer with stride 2, to efficiently downsample the input resolution—following <xref ref-type="bibr" rid="bib79">Newell et al. (2016)</xref>—followed by a stack of densely-connected hourglass networks with intermediate supervision (Appendix 5) applied at the output of each network. We also include hyperparameters for the bottleneck and compression layers described by <xref ref-type="bibr" rid="bib42">Huang et al. (2017a)</xref> to make the model as efficient as possible. These consist of applying a 1 × 1 convolution to inexpensively compress the number of feature maps before each 3 × 3 convolution as well as when downsampling and upsampling (see <xref ref-type="bibr" rid="bib42">Huang et al., 2017a</xref> and <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref> for details).</p><fig-group><fig id="app8fig1" position="float"><label>Appendix 8—figure 1.</label><caption><title>Prediction errors for the odor-trail mouse dataset from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> using the original implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib78">Nath et al., 2019</xref>) and our modified version of this model implemented in DeepPoseKit.</title><p>Mean prediction error is slightly lower for the DeepPoseKit implementation, but there is no discernible difference in variance. These results indicate that the models achieve nearly identical prediction accuracy despite modification. We also provide qualitative comparisons of these results in <xref ref-type="fig" rid="app8fig1s1">Appendix 8—figure 1—figure supplement 1</xref> and <xref ref-type="fig" rid="app8fig1s2">2</xref>, and <xref ref-type="video" rid="app8fig1video1">Appendix 8—figure 1—video 1</xref>.</p><p><supplementary-material id="app8fig1sdata1"><label>Appendix 8—figure 1—source data 1.</label><caption><title>Raw prediction errors for our DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) reimplemented in DeepPoseKit in <xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1</xref>.</title><p>See 'Materials and methods’ for details.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47994-app8-fig1-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="app8fig1sdata2"><label>Appendix 8—figure 1—source data 2.</label><caption><title>Raw prediction errors for the original DeepLabCut model from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> in <xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1</xref>.</title><p>See 'Materials and methods’ for details.</p></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-47994-app8-fig1-data2-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app8-fig1-v2.tif"/></fig><media id="app8fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-47994-fig1-v2.mp4"><label>Appendix 8—figure 1—video 1.</label><caption><title>A video comparison of the tracking output of our implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) in DeepPoseKit vs. the original implementation from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> and <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref>.</title></caption></media><fig id="app8fig1s1" position="float" specific-use="child-fig"><label>Appendix 8—figure 1—figure supplement 1.</label><caption><title>Plots of the predicted output for <xref ref-type="video" rid="app8fig1video1">Appendix 8—figure 1—video 1</xref> comparing our implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) in DeepPoseKit <italic>vs. </italic>the original implementation from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>, and <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref>.</title><p>Note the many fast jumps in position for the original verison from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>, which indicates prediction errors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app8-fig1-figsupp1-v2.tif"/></fig><fig id="app8fig1s2" position="float" specific-use="child-fig"><label>Appendix 8—figure 1—figure supplement 2.</label><caption><title>Plots of the temporal derivatives of the predicted output for <xref ref-type="video" rid="app8fig1video1">Appendix 8—figure 1—video 1</xref> comparing our implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) in DeepPoseKit vs. the original implementation from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>, and <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref>.</title><p>Note the many fast jumps in position for the original verison from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>, which indicates prediction errors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47994-app8-fig1-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s16" sec-type="appendix"><title>Model hyperparameters</title><p>For our Stacked Hourglass model we used a block size of 64 filters (64 filters per 3 × 3 convolution) with a bottleneck factor of 2 (64/2 = 32 filters per 1 × 1 bottleneck block). For our Stacked DenseNet model we used a growth rate of 48 (48 filters per 3×3 convolution), a bottleneck factor of 1 (1 × growth rate = 48 filters per 1 × 1 bottleneck block), and a compression factor of 0.5 (feature maps compressed with 1 × 1 convolution to 0.5 m when upsampling and downsampling, where <inline-formula><mml:math id="inf33"><mml:mi>m</mml:mi></mml:math></inline-formula> is the number of feature maps). For our Stacked DenseNet model we also replaced the typical configuration of batch normalization and ReLU activations (<xref ref-type="bibr" rid="bib35">Goodfellow et al., 2016</xref>) with the more recently-developed self-normalizing SELU activation function (<xref ref-type="bibr" rid="bib61">Klambauer et al., 2017</xref>), as we found this modification increased inference speed. For the LEAP model (<xref ref-type="bibr" rid="bib80">Pereira et al., 2019</xref>) we used a 1 × resolution output with integer-based global maxima because we wanted to compare our more complex models with this model in the original configuration described by <xref ref-type="bibr" rid="bib80">Pereira et al. (2019)</xref>. The LEAP model could be modified to output smaller confidence maps and increase inference speed, but because there is no obvious 'best' way to alter the model to achieve this, we forgo any modification. Additionally, applying our subpixel maxima algorithm at high-resolution reduces inference speed compared to integer-based maxima, so this would bias our speed comparisons.</p></sec><sec id="s17" sec-type="appendix"><title>Our implementation of the DeepLabCut model</title><p>Because the DeepLabCut model from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> was not implemented in Keras (a requirement for our pose estimation framework), we re-implemented it. Implementing this model directly in our framework is important to ensure model training and data augmentation are identical when making comparisons between models. As a consequence, our version of this model does not exactly match the description in the paper but is identical except for the output. Rather than using the location refinement maps described by <xref ref-type="bibr" rid="bib44">Insafutdinov et al. (2016)</xref> and post-processing confidence maps on the CPU, our version of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) has an additional transposed convolutional layer to upsample the output to <inline-formula><mml:math id="inf34"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> resolution and uses our subpixel maxima algorithm.</p><p>To demonstrate that our implementation of the DeepLabCut model matches the performance described by <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>, we compared prediction accuracy between the two frameworks using the odor-trail mouse dataset provided by <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> (downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexEMG/DeepLabCut/">https://github.com/AlexEMG/DeepLabCut/</ext-link>). This dataset consists of 116 images of a freely moving individual mouse labeled with four keypoints describing the location of the snout, ears, and the base of the tail. See <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref> for further details on this dataset. We trained both models using 95% training and 5% validation data and applied data augmentations for both frameworks using the data augmentation procedure described by <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref>. We tried to match these data augmentations as best as possible in DeepPoseKit; however, rather than cropping images as described by <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref>, we randomly translated the images independently along the horizontal and vertical axis by drawing from a uniform distribution in the range [−100%, +100%]—where percentages are relative to the size of each axis. Translating the images in this way should serve the same purpose as cropping them.</p><p>We trained the original DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) using the default settings and recommendations from <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref> for 1 million training iterations. See <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>; <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref> for further details on the data augmentation and training routine for the original implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>). For our re-implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>), we trained the model with the same batch size and optimization scheme described in the 'Model training’ section. We then calculated the the prediction accuracy on the full data set. We repeated this procedure five times for each model and fit a Bayesian linear model to a randomly selected subset of the evaluation data to compare the results statistically (see Appendix 7).</p><p>These results demonstrate that our re-implementation of and modification to the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) have little effect on prediction accuracy (<xref ref-type="fig" rid="app8fig1">Appendix 8—figure 1</xref>). We also provide qualitative comparisons of these results in <xref ref-type="fig" rid="app8fig1s1">Appendix 8—figure 1—figure supplement 1</xref> and <xref ref-type="video" rid="app8fig1video1">Appendix 8—figure 1—video 1</xref>. For these qualitative comparisons, we also added an additional rotational augmentation (drawing from a uniform distribution in the range [−180°, +180°)) when training our implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) as we noticed this improved generalization to the video for situations where the mouse rotated its body axis. To the best of our knowledge, rotational augmentations are not currently available when using the software from <xref ref-type="bibr" rid="bib72">Mathis et al. (2018)</xref>, and <xref ref-type="bibr" rid="bib78">Nath et al. (2019)</xref>, which demonstrates the flexibility of the data augmentation pipeline (<xref ref-type="bibr" rid="bib52">Jung, 2018</xref>) for DeepPoseKit. The inference speed for the odor-trail mouse dataset using our implementation of the DeepLabCut model (<xref ref-type="bibr" rid="bib72">Mathis et al., 2018</xref>) is ∼49 Hz with a batch size of 64 (offline speeds) and ∼35 Hz with a batch size of 1 (real-time speeds) at full resolution 640×480, which matches well with results from <xref ref-type="bibr" rid="bib73">Mathis and Warren (2018)</xref> of ∼47 Hz and ∼32 Hz respectively. This suggests our modifications did not affect the speed of the model and that our speed comparisons are also reasonable. Because the training routine could be changed for any underlying model—including the new models we present in this paper—this factor is not relevant when making comparisons as long as training is identical for all models being compared, which we ensure when performing our comparisons.</p></sec></boxed-text></app><app id="appendix-9"><title>Appendix 9</title><boxed-text><sec id="s18" sec-type="appendix"><title>Depthwise-separable convolutions for memory-limited applications</title><p>In an effort to maximize model efficiency, we also experimented with replacing 3 × 3 convolutions in our model implementations with 3 × 3 depthwise-separable convolutions —first introduced by <xref ref-type="bibr" rid="bib24">Chollet (2017)</xref> and now commonly used in fast, efficient 'mobile' CNNs (e.g. <xref ref-type="bibr" rid="bib94">Sandler et al., 2018</xref>). In theory, this modification should both reduce the memory footprint of the model and increase inference speed. However we found that, while this does drastically decrease the memory footprint of our already memory-efficient models, it slightly decreases accuracy and does not improve inference speed, so we opt for a full 3 × 3 convolution instead. We suspect that this discrepancy between theory and application is due to inefficient implementations of depthwise-separable convolutions in many popular deep learning frameworks, which will hopefully improve in the near future. At the moment we include this option as a hyperparameter for our Stacked DenseNet model, but we recommend using depthwise-separable convolutions only for applications that require a small memory footprint such as training on a lower-end GPU with limited memory or running inference on a mobile device.</p></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47994.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Shaevitz</surname><given-names>Josh W</given-names></name><role>Reviewing Editor</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Shaevitz</surname><given-names>Josh W</given-names></name><role>Reviewer</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Stephens</surname><given-names>Greg</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Fast and robust animal pose estimation&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Josh W Shaevitz as a guest Reviewing Editor, and the evaluation has been overseen by Ian Baldwin as the Senior Editor. The following individual involved in review of your submission has also agreed to reveal their identity: Greg Stephens.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This is a very well written resource article that covers the role of deep-learning in animal pose estimation, develops a new method with several improvements in accuracy and speed, and compares this method to existing methods in the literature. This is a timely paper and the improvements are likely to have a significant impact on users in this field. While this field is changing extremely rapidly, the reviewers believe that this paper will both move the technology further and also provide a readable review to the field for newcomers. However, the reviewers identified several issues that need to be addressed before publication, including text not well explained or a bit exaggerated, the effect of relatively small datasets, training routine differences that might affect the comparisons made, and a lack of explanation of the data acquisition.</p><p>Essential revisions:</p><p>1) Issues with the code:</p><p>a) In the script deepposekit/augment/__init__.py, the line 'from. import augmenters' needed to be substituted by 'from imgaug import augmenters'.</p><p>b) The module imgaug had to be installed.</p><p>c) We also found that comments in code were good at the beginning but less detailed later.</p><p>d) The example notebook &quot;step5_predict.ipynb&quot; could use some more instruction. In particular, what is missing is a section of code to analyze the full video.avi file, instead of just one batch of 5000 frames, which might be confusing for a beginner.</p><p>e) One suggestion for &quot;step_4_train_model.ipynb&quot;. In the section &quot;Define callbacks to enhance model training&quot;, the kwarg for the Logger object should be renamed &quot;validation_batch_size&quot; instead of &quot;batch_size&quot;, since it is indeed using validation frames. If one labels a small number of annotated examples, then it is possible to get an error here, as the logger will try and use more validation frames than are actually available. The renaming of this variable might help any confusion.</p><p>2) Subsection “Animal pose estimation using deep learning”, fourth paragraph: I would recommend writing more text on the distinction between single and multi-animal pose estimation and tracking in the main text. This is a very important issue and I worry that the casual/uninitiated reader might be confused and not look at Appendix 4. For some systems, tracking is very difficult and it should be clear to readers that this method will be difficult to use out-of-the-box for those systems.</p><p>3) The Abstract and title do not specifically mention the key novelties of the manuscript and should be rewritten.</p><p>4) 'Further details of how these image datasets were acquired, preprocessed, and tracked before applying our pose estimation methods will be described elsewhere.' I think they need to be given here.</p><p>5) How do the presented methods differ depending on the amount of labelled data? In the subsection “Experiments and model comparisons”, the authors postulate that differences in methods depending on training routines are minimal. As you are proposing an improvement over these methods, you need to prove this. You should also add a discussion of how many frames one should annotate before starting. While this is an incremental process (using the network to initialize further annotations), how many frames should one label at first? Also, as a related point, how does the accuracy of the network depend on the number of annotations?</p><p>6) It is apparent that machine vision methods to track animal behavior on the scale of posture will continue to advance at a remarkable speed. The authors could add substantial and long-lasting value to their work by discussing some of the more general aspects of behavioral measurement. Some possibilities:</p><p>a) It was only a few years ago that most behavioral measurements focused on image centroids and it would be useful to expand on the usefulness of representing behavior through posture vs. centroid.</p><p>b) What behavioral conditions remain challenging for the current generation of pose estimation algorithms (including DeepPoseKit)? For example, it would seem that a densely-packed fish school or bee hive might require novel approaches for both individual identity, the 3D nature of the school and resulting occlusions. This is an important consideration for the comparison of techniques. For example LEAP was designed very directly for high-contrast, controlled laboratory environments and it is perhaps not surprising that LEAP fares worse under less ideal conditions.</p><p>c) Relatedly, when would we consider the &quot;pose tracking&quot; problem solved? For example, the number of body points is user- not organism-defined, when do we know that we have enough?</p><p>d) The DeepPoseKit algorithm leverages multiple spatial scales in the input image data and it would be useful to expand the discussion about why this is beneficial. For example, for the fly data, what explicitly are the multiple scales that one might want to learn from the images? Can you further discuss how exactly does multi-scale inference achieve the fast speed of LEAP without sacrificing accuracy</p><p>e) With deep learning algorithms especially, there is a danger of rare but very wrong label assignments. Since DeepPoseKit is designed for general use, including among those not experienced in such networks, it would be quite useful to emphasize post-processing analysis that can help minimize the effect of these errors.</p><p>7) The manuscript would benefit from a discussion of how long it takes to train the networks and especially interesting would be a benchmarking of the three algorithms: DeepPoseKit, LEAP and DeepLabCut.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47994.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Issues with the code:</p><p>a) In the script deepposekit/augment/__init__.py, the line 'from. import augmenters' needed to be substituted by 'from imgaug import augmenters'.</p></disp-quote><p>We thank the reviewers for pointing out this oversight. The __init__.py file has already been updated since the initial release to correct this bug. This was not actually related to imgaug (although the described substitution does solve the import error) but was related to legacy code from when we originally developed our own data augmentation pipeline before switching to the imgaug package.</p><disp-quote content-type="editor-comment"><p>b) The module imgaug had to be installed.</p></disp-quote><p>It was brought to our attention by other users that imgaug needs to be manually installed when using Anaconda on Windows (and potentially other operating systems). We have updated the README with additional details that imgaug should be manually installed when using Anaconda (<ext-link ext-link-type="uri" xlink:href="https://github.com/jgraving/deepposekit/blob/master/README.md#installation">https://github.com/jgraving/deepposekit/blob/master/README.md#installation</ext-link>). We are working to address these issues with Anaconda as best as possible. Otherwise the imgaug module should be installed automatically as a dependency when installing DeepPoseKit with pip using the README instructions, which we have tested with many other systems. This has been included in the setup.py script since the initial public release of the code.</p><disp-quote content-type="editor-comment"><p>c) We also found that comments in code were good at the beginning but less detailed later.</p></disp-quote><p>This is an excellent point. We provided only minimal documentation in order to be able to send the code to the reviewers as quickly as possible and avoid further delays with the review process. We have further updated our example notebooks with more extensive comments as suggested. We have also added more doc strings to classes and functions to improve the general documentation. Adding additional documentation to the code will take time and effort, but we are working to address this as best as possible for future updates.</p><disp-quote content-type="editor-comment"><p>d) The example notebook &quot;step5_predict.ipynb&quot; could use some more instruction. In particular, what is missing is a section of code to analyze the full video.avi file, instead of just one batch of 5000 frames, which might be confusing for a beginner.</p></disp-quote><p>We have updated this notebook with an example for processing an entire video and saving the data to disk with more extensive comments to explain the details of the code.</p><disp-quote content-type="editor-comment"><p>e) One suggestion for &quot;step_4_train_model.ipynb&quot;. In the section &quot;Define callbacks to enhance model training&quot;, the kwarg for the Logger object should be renamed &quot;validation_batch_size&quot; instead of &quot;batch_size&quot;, since it is indeed using validation frames. If one labels a small number of annotated examples, then it is possible to get an error here, as the logger will try and use more validation frames than are actually available. The renaming of this variable might help any confusion.</p></disp-quote><p>We thank the reviewers for this excellent suggestion. We have updated the code as specified.</p><disp-quote content-type="editor-comment"><p>2) Subsection “Animal pose estimation using deep learning”, fourth paragraph: I would recommend writing more text on the distinction between single and multi-animal pose estimation and tracking in the main text. This is a very important issue and I worry that the casual/uninitiated reader might be confused and not look at Appendix 4. For some systems, tracking is very difficult and it should be clear to readers that this method will be difficult to use out-of-the-box for those systems.</p></disp-quote><p>We have updated the main text to more clearly and thoroughly make the distinction between individual and multiple pose estimation (subsection “Individual vs. multiple pose estimation”). We have also added a discussion of the advantages and disadvantages of using tracking and individual annotations vs. no tracking and multiple (exhaustive) annotations of full-sized images. This should help to make clear to the reader that our method may be difficult to use for some systems where tracking is difficult or not possible.</p><disp-quote content-type="editor-comment"><p>3) The Abstract and title do not specifically mention the key novelties of the manuscript and should be rewritten.</p></disp-quote><p>We have updated the title and modified the Abstract to explicitly mention the key novelties presented in the manuscript.</p><disp-quote content-type="editor-comment"><p>4) 'Further details of how these image datasets were acquired, preprocessed, and tracked before applying our pose estimation methods will be described elsewhere.' I think they need to be given here.</p></disp-quote><p>We have updated the subsection “Datasets” to provide a more detailed description of our image acquisition, tracking, and preprocessing procedures. The tracking algorithms used for our datasets are unpublished and would take significant space to describe in full detail. Adding this description is outside the scope of this paper and would take away from the main focus of our pose estimation methods. These tracking algorithms will also be the subject of further publications and we do not wish to reduce the novelty of these publications. The details of different localization and tracking methods are not especially relevant for comparing pose estimation algorithms other than the fact that individuals are successfully localized and tracked before cropping and annotating. Any of the many already-available tracking algorithms cited in the paper could be used for this preprocessing step, and of course, each has its own set of advantages and disadvantages that are not relevant to this paper.</p><disp-quote content-type="editor-comment"><p>5) How do the presented methods differ depending on the amount of labelled data?</p><p>Also, as a related point, how does the accuracy of the network depend on the number of annotations?</p></disp-quote><p>We assume this is the same question, otherwise please let us know if these are distinct questions that should be addressed separately. We have performed additional experiments and updated the text to address these comments. Appendix 1—figure 3 shows that our methods need little training data to generalize well to new data. Subsection “Stacked DenseNet trains quickly and requires few training examples”, first paragraph in the main text provide further details of these results.</p><disp-quote content-type="editor-comment"><p>In the subsection “Experiments and model comparisons”, the authors postulate that differences in methods depending on training routines are minimal. As you are proposing an improvement over these methods, you need to prove this.</p></disp-quote><p>To address this we adapted one of the example datasets from Mathis et al., 2018 to work with DeepPoseKit and then directly compared the two training routines (from the original paper and our modified implementation). We find that there is effectively no difference in prediction accuracy between our implementation and the original implementation from Mathis et al., 2018 when data are augmented in the same way during training. We have added Appendix 8—figure 1 and discussion in Materials and methods second paragraph and Appendix 8 subsection “Our implementation of the DeepLabCut model” to address this point. Additionally we provide a video of the posture tracking output for a novel video from this dataset (Appendix 8—figure 1—video 1) and plots of the time series output (Appendix 8—figure 1—figure supplement 1) for qualitative comparison. Together these results demonstrate that our implementation of the DeepLabCut model actually generalizes slightly better to novel data.</p><disp-quote content-type="editor-comment"><p>You should also add a discussion of how many frames one should annotate before starting. While this is an incremental process (using the network to initialize further annotations), how many frames should one label at first?</p></disp-quote><p>We have added discussion of this in the last paragraph of the subsection “Stacked DenseNet trains quickly and requires few training examples” in relation to how much training data is required for the model to generalize well.</p><disp-quote content-type="editor-comment"><p>6) It is apparent that machine vision methods to track animal behavior on the scale of posture will continue to advance at a remarkable speed. The authors could add substantial and long-lasting value to their work by discussing some of the more general aspects of behavioral measurement. Some possibilities:</p><p>a) It was only a few years ago that most behavioral measurements focused on image centroids and it would be useful to expand on the usefulness of representing behavior through posture vs. centroid.</p></disp-quote><p>We have expanded on the discussion of general aspects of measuring behavior including this point in the subsection “Measuring animal movement with computer vision”.</p><disp-quote content-type="editor-comment"><p>b) What behavioral conditions remain challenging for the current generation of pose estimation algorithms (including DeepPoseKit)? For example, it would seem that a densely-packed fish school or bee hive might require novel approaches for both individual identity, the 3D nature of the school and resulting occlusions. This is an important consideration for the comparison of techniques. For example LEAP was designed very directly for high-contrast, controlled laboratory environments and it is perhaps not surprising that LEAP fares worse under less ideal conditions.</p></disp-quote><p>We have added discussion of this in the sixth and eighth paragraphs of the Discussion.</p><disp-quote content-type="editor-comment"><p>c) Relatedly, when would we consider the &quot;pose tracking&quot; problem solved? For example, the number of body points is user- not organism-defined, when do we know that we have enough?</p></disp-quote><p>We have added discussion of this in the eighth paragraph of the Discussion.</p><disp-quote content-type="editor-comment"><p>d) The DeepPoseKit algorithm leverages multiple spatial scales in the input image data and it would be useful to expand the discussion about why this is beneficial. For example, for the fly data, what explicitly are the multiple scales that one might want to learn from the images? Can you further discuss how exactly does multi-scale inference achieve the fast speed of LEAP without sacrificing accuracy.</p></disp-quote><p>We have added additional discussion of this point in the last paragraph of the subsection “Pose estimation models and the speed-accuracy trade-off” and further discussion can be found in Appendix 4.</p><disp-quote content-type="editor-comment"><p>e) With deep learning algorithms especially, there is a danger of rare but very wrong label assignments. Since DeepPoseKit is designed for general use, including among those not experienced in such networks, it would be quite useful to emphasize post-processing analysis that can help minimize the effect of these errors.</p></disp-quote><p>We have added discussion of this in in the last paragraph of the subsection “Animal pose estimation using deep learning” and in the first paragraph of the subsection “Learning multi-scale geometry between keypoints improves accuracy and reduces extreme errors” and updated Figure 1 to better emphasize this point.</p><disp-quote content-type="editor-comment"><p>7) The manuscript would benefit from a discussion of how long it takes to train the networks and especially interesting would be a benchmarking of the three algorithms: DeepPoseKit, LEAP and DeepLabCut.</p></disp-quote><p>We have performed additional experiments and added discussion of this point. See subsection “Stacked DenseNet trains quickly and requires few training examples” and Appendix 1—figure 3.</p></body></sub-article></article>