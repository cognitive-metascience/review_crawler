<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">41328</article-id><article-id pub-id-type="doi">10.7554/eLife.41328</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Individuals physically interacting in a group rapidly coordinate their movement by estimating the collective goal</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-70897"><name><surname>Takagi</surname><given-names>Atsushi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4733-7471</contrib-id><email>takagi.atsushi21@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-52423"><name><surname>Hirashima</surname><given-names>Masaya</given-names></name><email>hira@nict.go.jp</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-50588"><name><surname>Nozaki</surname><given-names>Daichi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1338-8337</contrib-id><email>nozaki@p.u-tokyo.ac.jp</email><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-121133"><name><surname>Burdet</surname><given-names>Etienne</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2123-0185</contrib-id><email>e.burdet@imperial.ac.uk</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Institute of Innovative Research</institution><institution>Tokyo Institute of Technology</institution><addr-line><named-content content-type="city">Yokohama</named-content></addr-line><country>Japan</country></aff><aff id="aff2"><label>2</label><institution>Imperial College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Center for Information and Neural Networks</institution><institution>National Institute of Information and Communications Technology</institution><addr-line><named-content content-type="city">Osaka</named-content></addr-line><country>Japan</country></aff><aff id="aff4"><label>4</label><institution>University of Tokyo</institution><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff><aff id="aff5"><label>5</label><institution>Nanyang Technological University</institution><addr-line><named-content content-type="city">Singapore</named-content></addr-line><country>Singapore</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role>Reviewing Editor</role><aff><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>12</day><month>02</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e41328</elocation-id><history><date date-type="received" iso-8601-date="2018-08-23"><day>23</day><month>08</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-01-29"><day>29</day><month>01</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Takagi et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Takagi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-41328-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.41328.001</object-id><p>How can a human collective coordinate, for example to move a banquet table, when each person is influenced by the inertia of others who may be inferior at the task? We hypothesized that large groups cannot coordinate through touch alone, accruing to a zero-sum scenario where individuals inferior at the task hinder superior ones. We tested this hypothesis by examining how dyads, triads and tetrads, whose right hands were physically coupled together, followed a common moving target. Surprisingly, superior individuals followed the target accurately even when coupled to an inferior group, and the interaction benefits increased with the group size. A computational model shows that these benefits arose as each individual uses their respective interaction force to infer the collective’s target and enhance their movement planning, which permitted coordination in seconds independent of the collective’s size. By estimating the collective’s movement goal, its individuals make physical interaction beneficial, swift and scalable.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>motor control</kwd><kwd>human-human interaction</kwd><kwd>sensorimotor integration</kwd><kwd>computational neuroscience</kwd><kwd>haptic interaction</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>ICT-644727</award-id><principal-award-recipient><name><surname>Takagi</surname><given-names>Atsushi</given-names></name><name><surname>Burdet</surname><given-names>Etienne</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>17H00874</award-id><principal-award-recipient><name><surname>Hirashima</surname><given-names>Masaya</given-names></name><name><surname>Nozaki</surname><given-names>Daichi</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>JP18K1813</award-id><principal-award-recipient><name><surname>Hirashima</surname><given-names>Masaya</given-names></name><name><surname>Nozaki</surname><given-names>Daichi</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004963</institution-id><institution>Seventh Framework Programme</institution></institution-wrap></funding-source><award-id>ICT-601003</award-id><principal-award-recipient><name><surname>Burdet</surname><given-names>Etienne</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004963</institution-id><institution>Seventh Framework Programme</institution></institution-wrap></funding-source><award-id>ICT-611626</award-id><principal-award-recipient><name><surname>Burdet</surname><given-names>Etienne</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/NO29003/1</award-id><principal-award-recipient><name><surname>Burdet</surname><given-names>Etienne</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A computational model of collective physical interaction reveals that individuals infer the collective's movement goal in order to enhance the group's overall performance and coordinate with several partners in seconds.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A recent social experiment involving the widely acclaimed Pokemon video game ignited enormous public interest, where tens of thousands of players simultaneously controlled the protagonist of the game together and successfully finished the game (<xref ref-type="bibr" rid="bib19">Zhang and Liu, 2015</xref>). Such collective behavior in humans has been researched when a collective makes a decision verbally (<xref ref-type="bibr" rid="bib18">Webb, 1991</xref>; <xref ref-type="bibr" rid="bib5">Hastie and Kameda, 2005</xref>; <xref ref-type="bibr" rid="bib1">Bahrami et al., 2010</xref>). However, the key to many great human accomplishments, such as carrying stone blocks to construct the Great Pyramids, was enabled by many individuals who needed to coordinate the forces they applied on a stone in order to guide it on top of wooden rollers and move it. Such physical coordination has been investigated in pairs or dyads in the past decade (<xref ref-type="bibr" rid="bib2">Basdogan et al., 2000</xref>; <xref ref-type="bibr" rid="bib13">Sebanz et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Reed and Peshkin, 2008</xref>; <xref ref-type="bibr" rid="bib17">van der Wel et al., 2011</xref>; <xref ref-type="bibr" rid="bib10">Malysz and Sirouspour, 2013</xref>).</p><p>Previous studies that investigated dyads found evidence of improved task performance (<xref ref-type="bibr" rid="bib2">Basdogan et al., 2000</xref>; <xref ref-type="bibr" rid="bib12">Reed and Peshkin, 2008</xref>; <xref ref-type="bibr" rid="bib10">Malysz and Sirouspour, 2013</xref>), but the underlying mechanism of physical coordination was unknown. In a recent study, we tested dyads interacting in a continuous tracking task, and found that the tracking performance of both partners improved, even when the partner was worse at the task (<xref ref-type="bibr" rid="bib4">Ganesh et al., 2014</xref>). This mutual improvement during continuous interaction is explained by a mechanism where individuals estimate the partner’s target from the interaction force to improve their prediction of the target’s motion (<xref ref-type="bibr" rid="bib15">Takagi et al., 2017</xref>). In a second study, we showed that a stronger connection yields a better estimate of the partner’s target, enabling partners to improve more from the interaction (<xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>). We speak of the partner’s target for a tracking task, but this can be generalized to estimating a partner’s <italic>movement goal</italic>, which we define as the partner’s desired state, for example a position and velocity in time.</p><p>Although the mechanism of estimating the partner’s movement goal explains coordination in dyads, it is not known whether this interaction mechanism holds for an interactive tracking task with more than one partner. The connection dynamics to multiple partners may help inferior partners in a group but will likely hinder superior partners’ task performance. The dynamics may interfere with the coordination mechanism, which in dyads enabled even the superior partner to improve during the interactive tracking task (<xref ref-type="bibr" rid="bib4">Ganesh et al., 2014</xref>). It is therefore unclear whether the interaction remains mutually beneficial for large groups.</p><p>To elucidate this question and investigate how collectives negotiate common actions, we examined a task inspired by dancing in which two, three and four partners have to control their motion while feeling forces from the soft interaction with others. We hypothesized that the stochastic summation of every partner’s actions, yielding the interaction force, would produce a noisier and poorer haptic estimate of the target as the group size increases. We also expected the connection dynamics and the collective’s inertia to have a detrimental effect on the superior partners’ performance. In such a scenario, the dynamics of being physically connected to a collective of partners may characterize the interaction behavior, similar to what was observed in joint reaching movements (<xref ref-type="bibr" rid="bib14">Takagi et al., 2016</xref>). Could the coordination mechanism proposed in earlier studies (<xref ref-type="bibr" rid="bib15">Takagi et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>) fully explain the tracking performance observed in collective interaction, or would the dynamics of the collective’s inertia outweigh the benefits of the coordination mechanism in larger groups?</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We tested interaction in dyads, triads and tetrads who tracked a common target together using their right hands, which were all joined together with virtual elastic bands with a stiffness of 100 N/m (<xref ref-type="fig" rid="fig1">Figure 1A and B</xref>). 12 fours carried out the experiment in 12 triads and 12 tetrads, and 12 dyads were tested separately (see <xref ref-type="fig" rid="fig1">Figure 1D</xref> and the Materials and methods for details on the protocol). Individuals in the collective had to control a robotic handle using their right hand, which moved a cursor on their own respective monitor, to track a moving target (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The same target was used for all individuals of the collective. Each individual saw, on their own monitor, the positions of the target and their hand, but not the partners’ cursor positions. Individual performance at the task was calculated for each 15 s trial by measuring the average distance between their cursor and the target, defined as the <italic>tracking error</italic>. Two types of trials were tested: in <italic>solo</italic> trials, each individual tracked the target alone; in <italic>connected</italic> trials, the individuals’ right hands were coupled together by elastic bands.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.41328.002</object-id><label>Figure 1.</label><caption><title>Dyads, triads and tetrads, whose right hands were connected, tracked a common target together to investigate collective physical interaction.</title><p>(<bold>A</bold>) Subjects were recruited in twos or fours (schematic shows a tetrad). Each participant held onto a robotic handle with their dominant right hand to control a cursor on a monitor and track the same randomly moving target. Only one’s own cursor and the target were displayed on the monitor. (<bold>B</bold>) Physical coordination for dyads, triads and tetrads was enabled by forces exerted through the robotic handle that elastically connected all individuals’ right hands together. The interaction was removed in some trials to measure each individual’s solo tracking error as a function of the deviation in the target spots’ velocities. (<bold>C</bold>) The target was composed of five spots were spread thinly or widely to control each individual’s tracking performance. The deviation in the spots’ velocities, which was fixed during a trial, was randomized at every trial so participants could not know their skill relative to their partners beforehand. (<bold>D</bold>) Experimental protocol for twos and fours, where each circle represents an individual denoted by color. A dash indicates the individual was connected to partners. Both twos and fours experienced 10 solo training trials to become acquainted to the task. Twos then experienced 30 pairs of trials with and without the elastic connection, that is connected-solo-connected-solo etc. for 30 repetitions, and then 10 connected trials. For fours, three individuals of a tetrad were selected, forming all four different combinations of triads, and interacted in triads for one block per combination. In the first and last of these triad blocks, triads experienced 10 pairs of connected-solo trials, while the second and third triad blocks was composed of 10 connected trials. This was done to intersperse the solo trials to have a robust measure of the relationship between visual noise level and tracking error. In the final block, all individuals interacted as a tetrad for 30 trials.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41328-fig1-v1.tif"/></fig><media id="fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-41328-fig1-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.41328.003</object-id><label>Figure 1—video 1.</label><caption><title>The video shows a 15 second trial that each subject observed on their screen during training trials when the noise was lowest.</title><p>The target is composed of five spots, which are regenerated every 400ms.</p></caption></media><media id="fig1video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-41328-fig1-video2.mp4"><object-id pub-id-type="doi">10.7554/eLife.41328.004</object-id><label>Figure 1—video 2.</label><caption><title>This video shows a 15-s trial where the visual noise was highest.</title><p>Each of the five spots generated around the actual target position rapidly spread out as their velocity is picked from a wider Gaussian distribution.</p></caption></media></fig-group><p>To test how interaction with inferior or superior partners influenced tracking performance, we <italic>manipulated the tracking ability</italic> of subjects by applying visual noise to the target (<xref ref-type="bibr" rid="bib8">Körding and Wolpert, 2004</xref>) as described in the Materials and methods (see <xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="video" rid="fig1video1">Video 1 and 2</xref> for visual noise during tracking task). The tracking error of subjects was linearly and tightly related to the standard deviation of this visual noise, such that greater visual noise resulted in larger tracking errors (see <xref ref-type="fig" rid="fig2">Figure 2B</xref> for sample subjects). A different amount of visual noise, which was randomly selected but fixed during each connected trial, was applied to each member of a collective for every trial. This enabled us to test the influence of interaction with participants of different selected tracking ability. As the visual noise was linearly related to the tracking error, we could calculate the change in each subject’s tracking error during the interaction relative to the visual noise that we applied. We dispersed the solo trials throughout the entire experiment to verify that the relationship between visual noise level and tracking error did not drift with time, for example due to fatigue, which is the rationale for having a complicated protocol for twos and fours (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.41328.005</object-id><label>Figure 2.</label><caption><title>Sample trajectories from a single trial, and how the tracking error is tightly related with the visual noise imposed by the experimenter on each individual’s target.</title><p>(<bold>A</bold>) Raw data showing the trajectories in the <italic>x</italic>-axis from a sample tetrad, where the black trace is the target and each dashed colored trace (red, green, blue and magenta) is one subject. The top panel shows the <italic>x</italic>-axis position of all subjects in a tetrad in a solo trial (all disconnected), whereas the bottom two panels are the <italic>x</italic>-axis position and force from a sample connected trial where all four subjects were coupled to each other via elastic bands. Subjects’ positions in both the sample solo and connected trials were all delayed with respect to the target due to visual feedback delays in anticipating the target’s motion. The force felt by each partner is approximately zero mean, and depended on each individual’s relative position to their partners in the group. (<bold>B</bold>) Linear fit of the standard deviation of spots’ velocities versus the tracking error from a sample tetrad. Each level of noise was tested for three trials without the elastic band to assess individual tracking error. This data was linearly regressed to estimate the expected tracking error of each individual as a function of the visual noise on the target imposed by the experimenter. This enabled us to test collectives composed of individuals with different tracking skill.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41328-fig2-v1.tif"/></fig><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> shows raw data of the <italic>x</italic>-axis positions and forces experienced by a sample tetrad in solo and connected trials. The positions of the subjects lagged the target’s motion due to visual feedback delays in anticipating the target’s movement. For each subject, we assessed the <italic>performance improvement </italic><inline-formula><mml:math id="inf1"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>e</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was an individual’s tracking error in a connected trial and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> was the same subject’s solo error, which was estimated from the visual noise applied during the connected trial. This ratio quantifies an individual’s tracking ability during the interaction relative to tracking alone. We analyzed this performance improvement as a function of the <italic>partners’ relative error</italic>, <inline-formula><mml:math id="inf4"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>e</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was the mean of the partners’ solo errors, which was also estimated from the visual noise applied to the partners in the connected trial. This ratio is a measure of how the partners’ average tracking ability compared with the individual’s. This enabled us to study how each individual’s tracking ability changed when they interacted with ‘superior’ or ‘inferior’ partners.</p><p>The results of the collective physical interaction are plotted in <xref ref-type="fig" rid="fig3">Figure 3</xref> (the data in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref> was used for all subsequent analysis). First, we assessed how the collective as a whole improved from the physical interaction, which is shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, by taking the mean performance improvement from all individuals in the collective from every connected trial, and averaging over all trials for each collective. Two-sample t-tests revealed that the collective’s mean improvement increased with its size (between dyads and triads: <italic>t</italic>(22)=2.53, p&lt;0.02; between dyads and tetrads: <italic>t</italic>(22)=6.07, p&lt;10<sup>−5</sup>), revealing the benefits of interacting in larger collectives. To observe how each individual’s improvement changed as a function of the partners’ performance, we plotted each individual’s performance improvement as a function of the partners’ mean relative error for dyads (red trace), triads (green) and tetrads (blue) in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. The data was fit using a linear mixed-effects model, where each recruited group of twos and fours were treated as a random factor to control for individual differences in their inherent ability to improve from the interaction (see <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> in the Materials and methods for details). A mixed-effects analysis showed that the collective’s size modulated the individual’s performance improvement (χ<sup>2</sup>(2)=412, p&lt;10<sup>−15</sup>, see Materials and methods for details).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.41328.006</object-id><label>Figure 3.</label><caption><title>Collective physical interaction was surprisingly beneficial with coordination emerging rapidly in seconds, with the benefit in performance increasing with the number of partners.</title><p>(<bold>A</bold>) The collective improvement for dyads, triads and tetrads increased with the collective’s size, reflecting the advantage of larger collectives. (<bold>B</bold>) Performance improvement as a function of the partners’ relative error for dyads (red trace), triads (in green) and tetrads (in blue). The solid traces come from a linear mixed-effects fit of the raw data (points come from all connected trials from all groups). Interacting with a superior group was found to improve one’s performance, which was graded by the collective’s size such that a larger collective resulted in more improvement. We expected a similar effect when interacting with an inferior collective, but interacting with more inferior partners did not degrade a superior member’s performance. (<bold>C</bold>) The performance improvement of a sample tetrad is plot in increments of 0.5 s from the start to the end of the trial as a function of the partners’ relative error. The improvement rapidly converged to the improvement curve observed in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. (<bold>D</bold>) The deviation of the improvement curve in <xref ref-type="fig" rid="fig3">Figure 3B</xref> from the final improvement curve is plotted as a function of the trial time for dyads, triads and tetrads. The solid line is the mean of all groups, and the area represents one standard error. The rate at which they deviated from the collective mean was independent of group size.</p><p><supplementary-material id="fig3sdata1"><object-id pub-id-type="doi">10.7554/eLife.41328.007</object-id><label>Figure 3—source data 1.</label><caption><title>Data of the partners’ mean relative error, improvement, group label and the group size used in the linear mixed-effects analysis.</title><p>This data was also used in <xref ref-type="fig" rid="fig4">Figure 4 and 5</xref>.</p></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-41328-fig3-data1-v1.txt"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41328-fig3-v1.tif"/></fig><p>We split the data into the <italic>superior</italic> (<inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&lt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and <italic>inferior</italic> (<inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) <italic>individuals</italic> of the collective for dyads, triads and tetrads to examine how they were affected by physically interacting with superior or inferior partners. One-sample t-tests were carried out on the inferior and superior individuals’ improvements using a Bonferroni correction of significance 0.05/6. Inferior individuals improved when coupled to a superior collective, regardless of its size (dyads: <italic>t</italic>(11)=10.8, p&lt;10<sup>−6</sup>; triads: <italic>t</italic>(11)=24.0, p&lt;10<sup>−10</sup>, <italic>t</italic>(11)=23.0; tetrads: p&lt;10<sup>−9</sup>). Surprisingly, superior individuals in dyads, triads and tetrads maintained their performance with respect to their solo error (dyads: <italic>t</italic>(11)=-2.22, p&gt;0.05; triads: <italic>t</italic>(11)=-1.53, p&gt;0.15; tetrads: <italic>t</italic>(11)=3.01, p&gt;0.012). A superior individual could sustain their tracking performance even if they were physically coupled to an inferior collective regardless of how many inferior individuals were part of the collective.</p><p>An individual’s improvement was dependent on the performance of the others in the collective, but did the performance improvement change within the 15 s trial? We examined the improvement plot of <xref ref-type="fig" rid="fig3">Figure 3B</xref> for each collective as a function of time by calculating the improvement from the start of the trial to a specific trial time in increments of 0.5 s. <xref ref-type="fig" rid="fig3">Figure 3C</xref> shows the evolution of the improvement of a sample tetrad, where each trace is a second-order polynomial fitted to the data. The improvement was observed to significantly change over time. To study the evolution of the interaction’s beneficial effect on performance, we analyzed the improvement curve’s deviation from the <italic>final improvement</italic>, defined as the improvement at the end of the 15 s trial, that is the improvement during the entire trial, for dyads, triads and tetrads. <xref ref-type="fig" rid="fig3">Figure 3D</xref> shows the Euclidean distance between the second-order polynomial fits on the data at different times and the final improvement as a function of time for dyads, triads and tetrads. The improvement increased rapidly during the 15 trials for all collectives. To compare the rate of convergence between dyads, triads and tetrads, we fitted an exponential function to each collective of the form <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi>λ</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> are parameters, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the decay constant and <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the trial time. Mann-Whitney U-tests revealed that the decay constant was similar between dyads and triads (<italic>U</italic>=122, n<sub>1</sub>=12, n<sub>2</sub>=12, p&gt;0.11 two tailed), and between dyads and tetrads (<italic>U</italic>=136, n<sub>1</sub>=12, n<sub>2</sub>=12, p&gt;0.44 two tailed). Thus, the time constant for the collective’s improvement did not depend on its size. Remarkably, it took only 7.4±0.9 s (mean ± standard error) for the collective to reach 90% of the final improvement.</p><p>The empirical data shows that the collective physical interaction was beneficial for most individuals in the collective. How could individuals cause the performance improvement during collective interaction? To determine the behavioral strategy that individuals employed during collective interaction, we compared the empirical data from collective interaction with a simulation of it using the control models represented in <xref ref-type="fig" rid="fig4">Figure 4A and C</xref> to predict the outcome of the collective interaction experiment. In the simulation, we assumed that each individual sent motor commands to their arm to minimize the distance between their hand and the moving target. Simulated individuals relied on proprioception and vision for feedback of their hand and target positions, respectively. The simulated individuals had two free parameters that controlled the jerkiness of their movement and the strength of the controller, that is the control gain to bring the hand to the target. We carried out a sensitivity analysis to find values for these parameters that explained the empirical data best for each interaction model proposed in this study (see Supplementary material for details). Two, three and four such individuals were simulated in parallel with and without the elastic coupling to measure their performance at the tracking task during interaction and solo practice.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.41328.008</object-id><label>Figure 4.</label><caption><title>Simulations of the two proposed models of collective interaction and their predictions.</title><p>(<bold>A</bold>) The <italic>no exchange</italic> model was simulated by assuming that individuals cannot interpret the interaction forces and track the target as if they were alone under the influence of the mechanics of the elastic band. (<bold>B</bold>) This model predicted a larger inferior collective to be a greater hindrance to a superior individual of the collective, unlike the data where superior individuals maintained their performance. (<bold>C</bold>) In the <italic>neuromechanical goal sharing</italic> model, each individual built a representation of the partners’ average behavior to estimate the collective target. (<bold>D</bold>) The predictions from this model best explained the improvement of both inferior and superior individuals in the collective, and its modulation with the collective’s size.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41328-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41328.009</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Results of the control experiment from a different set of subjects who tracked a haptic target without visual feedback.</title><p>This experiment estimated how the interaction mechanics of coupling in dyads, triads and tetrads affected the quality of the estimated target from haptics. The compliance in the connection for dyads implies that dyads have more difficulty in estimating the collective target compared with triads and tetrads who felt a greater force. These values were used to adjust the <italic>neuromechanical goal sharing</italic> and the <italic>source separation</italic> limiting case.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41328-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41328.010</object-id><label>Figure 4—figure supplement 2.</label><caption><title>The sum of the sum of squared error between the fits from the data and the simulation as a function of <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</title><p>The SSE is relatively insensitive to changes in the strength <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The model with the lowest SSE is the <italic>neuromechanical goal sharing</italic> model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41328-fig4-figsupp2-v1.tif"/></fig></fig-group><p>We first tested whether the performance improvements observed in groups larger than dyads can be explained by a model where the physical connection to a superior or inferior collective with greater inertia dominates the interaction outcome. This model also tests whether the averaging of multiple partners’ trajectories during the tracking task helped to reduce tracking errors due to a cancellation of tracking errors. In this <italic>no exchange</italic> model (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), individuals track a target estimated from vision whilst under the influence of the forces from the elastic bands. This model predicted an improvement that was linearly dependent on the partners’ relative error, which was different from the data (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Importantly, the model predicted that a superior individual in the collective was hindered by inferior partners, and the hindrance was greater with more inferior individuals in contrast to the data (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The mismatch between the experimental data and the <italic>no exchange</italic> model’s prediction for triads and tetrads suggests that individuals interacting in large groups use the interaction force to exchange information that is relevant to the task, as was found in dyads in our previous study (<xref ref-type="bibr" rid="bib15">Takagi et al., 2017</xref>).</p><p>What kind of information did the individuals in triads and tetrads estimate from the interaction with their partners during the tracking task? In earlier studies (<xref ref-type="bibr" rid="bib15">Takagi et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>), we showed that partners in dyads estimated each other’s target through the interaction force to improve their prediction of the target’s motion. Individuals in triads and tetrads may also extract useful information from haptics to improve tracking performance. We hypothesized that individuals interpret the summed interaction force as originating from one entity that tracks a <italic>collective target</italic>. According to this hypothesis, the individuals’ central nervous system (CNS) recognizes some correlation between the interaction force and the target motion (<xref ref-type="bibr" rid="bib11">Parise and Ernst, 2016</xref>), then builds a representation of the entity that tracks the target. We assume that every individual’s CNS in the group estimates one collective target from the summed interaction force regardless of the number of partners in the group. In this extended <italic>neuromechanical goal sharing</italic> model, we propose that individuals track the optimally weighted average of the collective target and one’s own target from vision (see <xref ref-type="fig" rid="fig4">Figure 4C</xref> for schematic of the model).</p><p>As an example, for tetrads, we simulated four connected individuals who each estimated a collective target from the three other partners, and who then integrated this haptic estimate of the target with their own visual estimate of the target’s position (see <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> in the Materials and methods). The weights between vision and haptics were assumed to be known by every partner as we were interested in comparing the steady-state predictions of the model with the data. Furthermore, we accounted for the additional haptic noise that arises due to the compliance of the spring connection. In our earlier study (<xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>), we found that a stiffer spring reduces the haptic noise when estimating the partner’s target. Mechanics tells us that an individual in a triad who is connected to two partners by a total of two springs, each of stiffness 100 N/m, feels an equivalent force to being connected to the average of the two partners’ positions by a spring of stiffness 200 N/m (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> in Materials and methods) (<xref ref-type="bibr" rid="bib3">Burdet et al., 2013</xref>). In other words, individuals in larger groups effectively feel like they are connected to the group’s average position by a stronger spring. The <italic>error due to a specific compliant connection</italic> was accounted for in the simulations as additive noise (see Materials and methods for details on the haptic tracking experiment to measure this additional error due to the compliance in the spring in dyads, triads and tetrads, and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for the results of the haptic tracking experiment).</p><p>The simulation of the <italic>neuromechanical goal sharing</italic> model predicted a performance improvement that captured the curvature of the improvement as a function of the partners’ relative error with minimal deviation from the data when tested in a sensitivity analysis (<xref ref-type="fig" rid="fig4">Figure 4D</xref> and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). The performance improvement increased supralinearly for inferior individuals, and superior individuals retained their performance even when coupled to an inferior collective. Furthermore, the improvement was correctly modulated by the collective’s size, such that tetrads improved the most, followed by triads, and then dyads. These results suggest that individuals in collectives of different sizes use the same coordination strategy of extracting a haptic estimate of the collective target position from the interaction force.</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This study tested physically interacting dyads, triads and tetrads in a tracking task to assess the effect of the group’s size and its skill on the participating individuals’ tracking performance. We found that the total group’s performance increased with the group size, where inferior individuals in the group improved incrementally more in larger groups, and superior individuals were capable of sustaining their superior tracking performance even when connected to a group of individuals with inferior performance. Contrary to the results of our previous study (<xref ref-type="bibr" rid="bib4">Ganesh et al., 2014</xref>), the superior individuals of the dyads in the current study did not improve. This discrepancy in the results is likely due to the high amount of visual noise added to the target in order to manipulate each individual’s tracking performance.</p><p>In our experiment, the performance improvements observed in dyads, triads and tetrads did not arise instantaneously, but emerged continuously during the trial such that 90% of the group’s final performance improvement (calculated over the entire trial) was reached after 7 s. As <xref ref-type="fig" rid="fig3">Figure 3C</xref> shows, the partners’ movements initially depend only on the connecting spring dynamics (compare with <xref ref-type="fig" rid="fig4">Figure 4A and B</xref>), and gradually acquire a model of the interaction dynamics enabling them to benefit from this interaction. The similarity in the adaptation rates between dyads, triads and tetrads in reaching their performance improvement at the end of the trial may indicate that the same coordination mechanism may be utilized regardless of the size of the interacting group. The similarity in these adaptation rates for physical interaction stands in contrast with verbal or gestural communication where significantly longer time is needed with more participants. This highlights the advantage of the simultaneity of haptic communication relative to the sequential exchanges in verbal and gestural communication.</p><p>In order to identify the coordination mechanism that explained the improvements from collective physical interaction, we used a computational model to test the determinants of interaction, to predict their effect on the performance improvement, and compare the predictions with the empirical data. The <italic>neuromechanical goal sharing</italic> model, which captured the improvements from the empirical data, suggests a mechanism whereby individuals extract task relevant sensory information from haptics, and integrate it with their own visual information of the target’s motion to improve tracking performance during interaction. In this model, we assumed that each individual extracts a haptic estimate of the target from the interaction force. As this haptic estimate of the target is stochastically optimally combined with the individual’s visual target, this improves their tracking performance even when connected to partners having a collectively inferior performance. The haptic estimate of the target arises from the summed interaction force, which is composed of the elastic couplings to multiple partners, that is equivalent to one elastic coupling to the average partner (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> in the Materials and methods).</p><p>If the haptic estimate of the target is extracted from summed interaction force, which is a function of the average partner’s movements, then intuitively the performance improvement from integrating this haptic signal should depend only on the average partner’s tracking error, and not on the number of partners in the collective. If so, why did the simulation in <xref ref-type="fig" rid="fig4">Figure 4D</xref> of the <italic>neuromechanical goal sharing</italic> model predict improvements that were dependent on both the average partner’s error and the size of the collective? There may be two main reasons for the graded performance improvement with group size. First, the connection dynamics alone could have graded the improvement, since the <italic>no exchange</italic> model (in <xref ref-type="fig" rid="fig4">Figure 4A and B</xref>) also predicted improvements that were graded by group size. Second, the effect of the additional noise in the haptic estimate of the target due to the compliance of the elastic coupling may explain the graded improvement (see <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> in Materials and methods). To assess the impact of these two factors on the predicted performance improvement, we simulated the <italic>neuromechanical goal sharing</italic> model (see <xref ref-type="fig" rid="fig5">Figure 5A</xref>) without the interaction spring dynamics (<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> in the Materials and methods) and without the additional noise from the elastic compliance (<inline-formula><mml:math id="inf16"><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> in the Materials and methods). As the results still exhibit improvements graded as a function of group size (see <xref ref-type="fig" rid="fig5">Figure 5B</xref>), the graded improvement was not caused by the connection dynamics nor by the additional haptic noise due to the elastic compliance.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.41328.011</object-id><label>Figure 5.</label><caption><title>Simulation to examine the mechanism underlying the graded improvement and the limits of sharing information through haptics.</title><p>(<bold>A</bold>) A schematic of the <italic>neuromechanical goal sharing</italic> model without the dynamics of the spring and without the additional haptic noise due to the compliance of the spring. The simulation of this model are shown in (<bold>B</bold>), where the improvements remained graded by group size, implying that the performance was graded for a different reason. (<bold>C</bold>) The <italic>neuromechanical goal sharing</italic> model was simulated without the dynamics of the spring, without the additional haptic noise due to the compliance of the spring, and assuming that the noise in the interaction force was the average partners’ visual tracking noise. The simulation of this model is shown in (<bold>D</bold>). Since the noise was averaged, the group size had no effect on the improvement, which was similar for dyads, triads and tetrads. Thus, the graded improvement due to group size arose from the stochastic summation in the interaction force. (<bold>E</bold>) The <italic>source separation</italic> limiting case assumes that each individual receives additional sensory information of the target position through the interaction force from every partner. (<bold>F</bold>) The <italic>source separation</italic> case significantly overestimates the improvement for dyads, triads and tetrads, indicating that the individuals could not separate the sources of the interaction force.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41328-fig5-v1.tif"/></fig><p>What explains the grading of the improvement as a function of both the average partner’s error and the collective’s size? The original intuitive premise must be questioned as to whether the improvement from interacting with a collective of partners, whose mean tracking error is <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, is the same as the improvement from interacting with one average partner who has the error <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In previous studies (<xref ref-type="bibr" rid="bib15">Takagi et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>), the noise in the haptic measurement of the target was equivalent to the partner’s visual tracking noise. So what is the noise in haptics from an interaction force during collective interaction? Although the interaction force is equivalent to one stiffer elastic coupling to the average partner (as <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> shows), the noise in this interaction force is not the average of the partners’ visual tracking noise. Instead, the stochastically summed interaction force has noise that is inversely proportional to the squared number of partners (see <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> in Materials and methods), which is different from the mean of the partners’ visual tracking noise. To illustrate this difference, we simulated the <italic>neuromechanical goal sharing</italic> model where the haptic noise was the mean of the partners’ visual tracking noise (see <xref ref-type="fig" rid="fig5">Figure 5C</xref>). To isolate the effects of the haptic measurement noise, we again removed the connection dynamics and the noise from the elastic coupling in this simulation. This model predicted similar improvements irrespective of group size (see <xref ref-type="fig" rid="fig5">Figure 5A</xref>), showing that the graded improvement as a function of group size is indeed explained by a reduction in the variance of haptics due to the averaging of the partners’ positions in the interaction force.</p><p>Does the elucidated mechanism provide maximum possible performance improvement with haptic feedback? Maximum information transfer during collective interaction can be estimated in a limiting case of the <italic>neuromechanical goal sharing</italic> model where the central nervous system is able to extract every partner’s contribution to the interaction force (instead of modeling the interaction force as coming from a single entity). This would be similar to the cocktail party effect of audition where one can isolate a conversation in a room of people talking at the same time (<xref ref-type="bibr" rid="bib6">Hawley et al., 2004</xref>). Each individual might extract multiple streams of information from the interaction force (one, two and three streams for dyads, triads and tetrads, respectively) using individual spectral characteristics, yielding the maximal possible information transfer through haptics (see <xref ref-type="fig" rid="fig5">Figure 5E</xref> for the schematic of this model). However, the predictions of this <italic>source separation</italic> limiting case (see <xref ref-type="fig" rid="fig5">Figure 5F</xref>) consistently overestimated the improvement in comparison to the data, showing that our individuals could not break down each individual source in the interaction force. Instead, the average behavior of all other individuals was identified, and their <italic>collective target</italic> was inferred. This reveals a limit in the ability to share and estimate information through haptics.</p><p>In summary, this paper presented experiments and computational modeling to understand how physically interacting human individuals coordinate their movements during the collective tracking of a common target. The results elucidate the coordination mechanism in a collective by systematically analyzing how the information from the interaction dynamics is processed by its individuals. As the interaction force is the sum of all partner’s forces, it is not possible to identify each partner’s specific contribution to it. Instead, the individuals estimate <italic>a collective target</italic> from the interaction force, which they combine with their own visual target. The performance improvement resulting from this mechanism is suboptimal relative to that allowed by a putative source separation mechanism, but it still enables the collective’s individuals to improve their tracking error when interacting with superior partners, and to not be hindered by inferior ones. This neuromechanical coordination mechanism is also scalable, as the time required to adapt to the group’s skill is independent of group size, and a group’s total performance improvement increases with its size. The surprising result that the collective’s mean improvement increases in larger groups is explained by the stochastic properties of the collective target that is extracted from the summed interaction force.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experiments</title><p>The study was conducted according to the Declaration of Helsinki and was approved by the ethics committee of the Graduate School of Education at the University of Tokyo (reference number 14-75). Each of the 72 subjects gave a written consent prior to starting with the experiments. The sample size of 12 per group of dyads, triads and tetrads was determined by a prior power analysis from a repeated-measures ANOVA within and between interaction consisting of 3 groups and the error detection parameters <inline-formula><mml:math id="inf19"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:math></inline-formula> with a medium effect size of <inline-formula><mml:math id="inf21"><mml:msup><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0.06</mml:mn></mml:math></inline-formula>.</p><p>Each subject held onto the robotic handle of the Phantom 1.5HF (Geomagic), which constrained the handle’s movement within a horizontal plane via software. The individual monitors displayed a cursor of the handle position and the target, which was composed of a dynamic cloud around the multi-sine function<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1.6</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>0.1</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>0.3</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.8</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>0.5</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>2.4</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>0.8</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.2</mml:mn><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>2.4</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.3</mml:mn><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>4</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.6</mml:mn><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>2.4</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0.8</mml:mn><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the target’s trajectory was randomized through the selection of the initial time according to a uniform stochastic distribution in the interval between 0 and 10 s.</p><p>The dynamic cloud consisted of five circular spots that were displayed every millisecond (as shown in the <xref ref-type="video" rid="fig1video1">Video 1 and 2</xref>). Each spot was regenerated one at a time every 400 ms by picking a new position and velocity. These position and velocity parameters were determined at the start of a trial from normal random distributions with a standard deviation of 0.005 m for the position, and from a set of ten equally spaced values from 0.005 m/s to 0.3 m/s for the velocity. The wider the spots were spread, the more difficult it was to follow the target as its true position was hard to guess (<xref ref-type="bibr" rid="bib8">Körding and Wolpert, 2004</xref>). Spots with low velocity noise were easy to track but high velocity noise spots spread out rapidly like fireworks. Every time a velocity noise was selected for each subject, it was removed from the set that was unique to each subject. The random selection ensured that an individual’s own performance and the others’ tracking skill were unknown a priori.</p><p>The velocity parameter enabled us to control the <italic>tracking error</italic> of each individual in a trial, which was measured as the root-mean squared distance between the target and the cursor. For each subject, the tracking error on trials without interaction was regressed with the target spot velocity noise using data from three trials per velocity noise level, giving a fit with R<sup>2</sup> = 0.80 ± 0.01 (mean ±standard error for all subjects). The spot velocity noise was used as an estimate of each individual’s tracking error (see <xref ref-type="fig" rid="fig2">Figure 2B</xref> for fits from a sample tetrad).</p><p>Subjects were instructed to follow the target as accurately as possible and were told that they would experience forces on their hand. At the end of the experiment, subjects were asked about the nature of the forces. Although some guessed that the forces originated from a partner, none of them could tell how many partners they were connected to.</p><p>The experimental protocol is described in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Twos experienced 80 total trials and fours completed 100 trials in total. Both twos and fours experienced 10 solo training trials to become acquainted to the task. After this training phase, twos and fours encountered a series of solo and connected trials. Twos then carried out 60 trials with and without the elastic connection in a series of 30 connected-solo trials, and then 10 connected trials. This ensured that solo trials were interspersed throughout the experiment. Interaction data from both triads and tetrads were collected during the experiment with fours. We collected as much data as possible from triads by testing all four combinations of triads possible from the tetrad, and collected the tetrad interaction data in the last 30 trials. Thirty solo trials were interspersed such that 10 were tested after training, 10 prior to tetrad interaction, and another 10 in each triad block where the excluded individual experienced solo trials instead of triad interaction trials. In total, 40 connected trials for dyads and triads, and 30 connected trials for tetrads.</p></sec><sec id="s4-2"><title>Analysis</title><p>A linear mixed-effects model<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>p</mml:mi><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>was employed to fit the improvement <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf23"><mml:mi>e</mml:mi></mml:math></inline-formula> is the error of an individual in a solo trial (estimated from the linear regression with the visual noise) and <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the same subject’s error on a connected trial, as a function of the partners’ relative error, <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was the partners’ mean error estimated from solo trials, and the collective’s size <inline-formula><mml:math id="inf27"><mml:mi>s</mml:mi></mml:math></inline-formula>. In this model, <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the intercept, <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mi>ρ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the parameters for each predictor and <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the unexplained variance of the improvement for each collective <inline-formula><mml:math id="inf32"><mml:mi>ρ</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-3"><title>Simulation model</title><p>A model was developed in discrete time <inline-formula><mml:math id="inf33"><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mi>k</mml:mi> <mml:mi/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> to simulate how the members of a collective connected by elastic bands plan their movement to track a randomly moving target in two dimensions. The Cartesian product of two one-dimensional models as described below was used in simulation. At every time index the target with position <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> must be estimated, then a motor command <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is generated to move the hand’s position <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to the target. First, we describe the state equation that governs the movement of the target, and then that of the hand, and combine these two equations to formulate a single state equation of the full system. The movement of the target, which is assumed to be moving randomly via Gaussian noise in its velocity <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, is described by the first-order system<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">τ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mspace width="thickmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <italic>target state</italic> and <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msubsup><mml:mi mathvariant="bold-italic">τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the covariance matrix.</p><p>The control of the hand is modelled as <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with point-mass <inline-formula><mml:math id="inf41"><mml:mi>m</mml:mi></mml:math></inline-formula> and the force <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> from one or several elastic bands. In state-space format, this yields<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where the control command <italic><inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> to move the hand towards the target is described by<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>with <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> describing the position and velocity control gains, respectively. In a collective of individuals <inline-formula><mml:math id="inf46"><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, each individual’s right hand with state <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>≡</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is connected to the <inline-formula><mml:math id="inf48"><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> other individuals’ right hands through elastics bands of stiffness <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and damping <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> that produce the force<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the average partners’ position. Importantly, as we see in the right side of this equation, the sum of the elastic coupling to all partners is equivalent to the interaction force when coupled with a more rigid and damped elastic interaction with the average of the partners’ hands. Solo trials, where the subjects are not connected, are characterized by zero force <inline-formula><mml:math id="inf52"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>≡</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> for all <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. A subject using the motor command of <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> to move their hand according to <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> to follow the target, with motion described by <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, is described by the full state equation<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">A</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mspace width="thickmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>≡</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">τ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>which is equivalent to the difference of <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> minus <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>.</p></sec><sec id="s4-4"><title>Models of interaction</title><p>Two models of interaction are described from the sensory information exchange between the partners. First, we describe the solo strategy of one subject tracking the target <inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> alone using only visual feedback. To generate the motor command according to <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>, the state describing the difference between the target and the hand is observed through<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mi/> <mml:mi/> <mml:mi/> <mml:mi/> <mml:mi mathvariant="bold"/> <mml:mi mathvariant="bold"/> <mml:mi mathvariant="bold"/> <mml:mi/></mml:math></disp-formula>where the observation <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is corrupted by Gaussian visual noise <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with variance <inline-formula><mml:math id="inf57"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>≡</mml:mo><mml:mi>E</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>E</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula>. The linear quadratic estimation is computed in discrete time using an iterative Kalman filter algorithm (<xref ref-type="bibr" rid="bib7">Kalman, 1960</xref>). Sensory delay in vision and proprioception is compensated for by integrating <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>.</p><p>Now that we have described how to visually track a target, what motion planning model could be used to track the randomly moving target whilst being physically coupled to multiple partners? In the <italic>no exchange</italic> model, each individual ignores the interaction forces and tracks the target using the visual information of the target’s position, as in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>, under the influence of the dynamics of the elastic bands described in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. The <italic>neuromechanical goal sharing</italic> model (<xref ref-type="bibr" rid="bib15">Takagi et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>) proposes that, in dyads, both individuals extract a haptic estimate of the target’s position from the interaction force with the partner, and optimally combine it with their own visual estimate of the target. Similarly, we propose that in collective interaction each individual <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> uses the interaction force to extract a haptic estimate of the target position, referred from here on as the <italic>collective target </italic><inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, such that the observation of the difference between the hand of individual <italic><inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> and the target is observed using<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>which extends the corresponding law of previous studies (<xref ref-type="bibr" rid="bib15">Takagi et al., 2017</xref>; <xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>).</p><p>What is the variance of the noise that corrupts the haptic measurement of the collective target in the extended <italic>neuromechanical goal sharing</italic> model? In previous studies (<xref ref-type="bibr" rid="bib15">Takagi et al., 2017</xref>, <xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>), the interaction; force was linearly dependent on the partner’s hand position, and so the noise in the haptic measurement of the partner’s target was the partner’s visual tracking noise. Similarly, in collective interaction, the collective target is estimated from the interaction noise, which is shown in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> to be linearly dependent on the partners’ average hand position <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>h</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Let every <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><sup>th</sup> partner’s visual measurement of the target be corrupted by Gaussian visual tracking noise with variance <inline-formula><mml:math id="inf63"><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. Then the difference between the hand and the collective target’s position <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> suffers Gaussian noise with variance<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We can assume that these measurements between partners are independent, thus <inline-formula><mml:math id="inf65"><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn> <mml:mi/><mml:mo>∀</mml:mo><mml:mi>m</mml:mi><mml:mo>≠</mml:mo><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, and the variance in the measurement of the collective target<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula>is inversely proportional to the number of partners in the collective. Therefore, the measurement noise on the collective target will reduce in larger collectives even if the partners’ average tracking noise is equivalent.</p><p>We further tested a modified version of the <italic>neuromechanical goal sharing</italic> model (see <xref ref-type="fig" rid="fig5">Figure 5C and 5D</xref>) with the intuitive, but incorrect, expectation that the interaction with multiple partners whose average error is <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is identical to interacting with one partner whose error is equivalent to <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. In this scenario, the variance of the noise in the haptic measurement of the collective target would be equal to the average of the partners’ visual tracking noise, that is <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, with a denominator different from <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>. If this were the noise in the haptic estimate of the collective target, the improvement would not change with the group’s size (as <xref ref-type="fig" rid="fig5">Figure 5D</xref> shows), which is in contrast to what is observed in the data.</p><p>How does an individual <italic><inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> estimate the collective target of <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> from the interaction force in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>? The average of the partners would use a motor command similar to <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>,<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula>where the average partner’s state is estimated through the force and the state of one’s own hand, whilst the average partner’s control law <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> is identified by letting it evolve with noise according to<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>≡</mml:mo><mml:mi mathvariant="bold-italic">λ</mml:mi><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mspace width="thickmathspace"/><mml:mtext> </mml:mtext><mml:mi mathvariant="bold-italic">λ</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>.</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, the representation of the partner includes the state of their hand, their target, their control law, one’s own hand and the elastic force to yield<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">ξ</mml:mi></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold"> </mml:mtext></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold"> </mml:mtext></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold"> </mml:mtext></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="bold"> </mml:mtext></mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This state <inline-formula><mml:math id="inf72"><mml:mi mathvariant="bold">ξ</mml:mi></mml:math></inline-formula> is described by the non-linear function <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>≡</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that is linearized at every time step to be used for linear quadratic estimation (<xref ref-type="bibr" rid="bib9">Ljung, 1979</xref>). <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is identified by minimizing the squared estimation error of the observations of one’s own target position <inline-formula><mml:math id="inf75"><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, hand position <inline-formula><mml:math id="inf76"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and force <inline-formula><mml:math id="inf77"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. Once <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is identified, the collective target <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">τ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is estimated by minimizing the squared estimation error of the observations of the average partner’s estimated control <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, one’s own hand position <inline-formula><mml:math id="inf81"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and the force <inline-formula><mml:math id="inf82"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p><p>The <italic>source separation</italic> limiting case is where every partner’s target can be estimated and combined with one’s own visual estimate of the target,<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In this limiting case, <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> observations of the partners’ target position are directly provided to each individual in the collective, who integrates the partners’ targets with their own visual estimate of the target, providing <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> total observations of the target.</p></sec><sec id="s4-5"><title>How compliance changes the quality of haptic information</title><p>In a previous study (<xref ref-type="bibr" rid="bib16">Takagi et al., 2018</xref>), we found that the strength of the elastic coupling influenced the quality of the haptic information. With a weaker elastic band, the amplitude of the interaction force is smaller, reducing the signal-to-noise ratio when measuring it through haptics. Dyads, triads and tetrads experienced different magnitudes of force due to the increasing number of elastic bands that coupled them together. The dynamics experienced by dyads, triads and tetrads can be modeled as a single elastic band of 100 N/m, 200 N/m and 300 N/m respectively, which connects each individual to the average position of the partners as shown in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>.</p><p>Another eight subjects were recruited individually to carry out a <italic>haptic tracking control experiment</italic>. The target movement was the same as in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, but without visual feedback, that is the target was invisible to the subject. Subjects tracked the haptic target for 15 s, and experienced five trials of each coupling stiffness consecutively in the order of 300 N/m, 200 N/m and 100 N/m, respectively. <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows the results of this experiment, revealing that stronger stiffness resulted in lower tracking errors. The values found in this experiment were used to alter the <italic>source separation</italic> and <italic>neuromechanical goal sharing</italic> models by changing the sensory noise in the haptic estimate of the collective target. The haptic noise from <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> has some additive noise <inline-formula><mml:math id="inf85"><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> due to the compliance of the elastic connection,<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In the haptic tracking experiment, we measured the additional error in the tracking task that arises due to the softness of the elastic spring. These error values can be used to estimate <inline-formula><mml:math id="inf86"><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, and so the haptic noise is described by<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a function that converts tracking error <inline-formula><mml:math id="inf88"><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> to an equivalent sensory noise and <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the stiffness of the elastic spring. This function changes with respect to the process noise <inline-formula><mml:math id="inf90"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> as the performance of the linear quadratic estimator is directly related to this value, and the controller strength <inline-formula><mml:math id="inf91"><mml:mi>q</mml:mi></mml:math></inline-formula> that affects how closely one can follow the estimated target trajectory.</p><p>To determine <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we simulated only the solo trials of the tracking task for each unique pair of <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf94"><mml:mi>q</mml:mi></mml:math></inline-formula>, and fitted a second order polynomial that related the standard deviation of an individual’s visual tracking noise <inline-formula><mml:math id="inf95"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></inline-formula> and tracking error <inline-formula><mml:math id="inf96"><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>,<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>γ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf97"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf99"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are fitted parameters. Since we assume that the softness of the interaction results in additive sensory noise, the haptic noise of <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> is modified to<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>e</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the average of the partners’ tracking error and <inline-formula><mml:math id="inf101"><mml:mi>E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the additional error from the interaction stiffness <inline-formula><mml:math id="inf102"><mml:mi>K</mml:mi></mml:math></inline-formula>, whose values were taken from the haptic tracking experiment.</p><p>To remove the effects of the additional noise due to the elastic coupling on the predicted performance improvement (as described in the Discussion), the compliance noise in <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> was set to <inline-formula><mml:math id="inf103"><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was partially supported by JSPS KAKENHI grant numbers JP18K18130 and 17H00874, and by EU-FP7 grants ICT-601003 BALANCE, ICT-611626 SYMBITRON, EU-H2020 ICT-644727 COGIMON, UK EPSRC MOTION grant EP/NO29003/1.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Visualization, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was conducted according to the Declaration of Helsinki, and was approved by the ethics committee of the Graduate School of Education at the University of Tokyo (reference number 14-75). Each of the 72 subjects gave a written consent prior to starting with the experiments.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.41328.012</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-41328-transrepform-v1.docx"/></supplementary-material><sec id="s8" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files.</p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahrami</surname> <given-names>B</given-names></name><name><surname>Olsen</surname> <given-names>K</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Roepstorff</surname> <given-names>A</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Optimally interacting minds</article-title><source>Science</source><volume>329</volume><fpage>1081</fpage><lpage>1085</lpage><pub-id pub-id-type="doi">10.1126/science.1185718</pub-id><pub-id pub-id-type="pmid">20798320</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basdogan</surname> <given-names>C</given-names></name><name><surname>Ho</surname> <given-names>C-H</given-names></name><name><surname>Srinivasan</surname> <given-names>MA</given-names></name><name><surname>Slater</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>An experimental study on the role of touch in shared virtual environments</article-title><source>ACM Transactions on Computer-Human Interaction</source><volume>7</volume><fpage>443</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1145/365058.365082</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Burdet</surname> <given-names>E</given-names></name><name><surname>Franklin</surname> <given-names>DW</given-names></name><name><surname>Milner</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Human Robotics: Neuromechanics and Motor Control</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganesh</surname> <given-names>G</given-names></name><name><surname>Takagi</surname> <given-names>A</given-names></name><name><surname>Osu</surname> <given-names>R</given-names></name><name><surname>Yoshioka</surname> <given-names>T</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name><name><surname>Burdet</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two is better than one: physical interactions improve motor performance in humans</article-title><source>Scientific Reports</source><volume>4</volume><elocation-id>3824</elocation-id><pub-id pub-id-type="doi">10.1038/srep03824</pub-id><pub-id pub-id-type="pmid">24452767</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hastie</surname> <given-names>R</given-names></name><name><surname>Kameda</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The robust beauty of majority rules in group decisions</article-title><source>Psychological Review</source><volume>112</volume><fpage>494</fpage><lpage>508</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.112.2.494</pub-id><pub-id pub-id-type="pmid">15783295</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawley</surname> <given-names>ML</given-names></name><name><surname>Litovsky</surname> <given-names>RY</given-names></name><name><surname>Culling</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The benefit of binaural hearing in a cocktail party: effect of location and type of interferer</article-title><source>The Journal of the Acoustical Society of America</source><volume>115</volume><fpage>833</fpage><lpage>843</lpage><pub-id pub-id-type="doi">10.1121/1.1639908</pub-id><pub-id pub-id-type="pmid">15000195</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>A new approach to linear filtering and prediction problems</article-title><source>Journal of Basic Engineering</source><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname> <given-names>KP</given-names></name><name><surname>Wolpert</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Bayesian integration in sensorimotor learning</article-title><source>Nature</source><volume>427</volume><fpage>244</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1038/nature02169</pub-id><pub-id pub-id-type="pmid">14724638</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ljung</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Asymptotic behavior of the extended Kalman filter as a parameter estimator for linear systems</article-title><source>IEEE Transactions on Automatic Control</source><volume>24</volume><fpage>36</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1109/TAC.1979.1101943</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malysz</surname> <given-names>P</given-names></name><name><surname>Sirouspour</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Task performance evaluation of asymmetric semiautonomous teleoperation of mobile twin-arm robotic manipulators</article-title><source>IEEE Transactions on Haptics</source><volume>6</volume><fpage>484</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1109/TOH.2013.23</pub-id><pub-id pub-id-type="pmid">24808400</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname> <given-names>CV</given-names></name><name><surname>Ernst</surname> <given-names>MO</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Correlation detection as a general mechanism for multisensory integration</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11543</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11543</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reed</surname> <given-names>KB</given-names></name><name><surname>Peshkin</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Physical collaboration of Human-Human and Human-Robot teams</article-title><source>IEEE Transactions on Haptics</source><volume>1</volume><fpage>108</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1109/TOH.2008.13</pub-id><pub-id pub-id-type="pmid">27788067</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sebanz</surname> <given-names>N</given-names></name><name><surname>Bekkering</surname> <given-names>H</given-names></name><name><surname>Knoblich</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Joint action: bodies and minds moving together</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>70</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.12.009</pub-id><pub-id pub-id-type="pmid">16406326</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takagi</surname> <given-names>A</given-names></name><name><surname>Beckers</surname> <given-names>N</given-names></name><name><surname>Burdet</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Motion plan changes predictably in dyadic reaching</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0167314</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0167314</pub-id><pub-id pub-id-type="pmid">27911938</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takagi</surname> <given-names>A</given-names></name><name><surname>Ganesh</surname> <given-names>G</given-names></name><name><surname>Yoshioka</surname> <given-names>T</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name><name><surname>Burdet</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Physically interacting individuals estimate the partner’s goal to enhance their movements</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0054</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-017-0054</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takagi</surname> <given-names>A</given-names></name><name><surname>Usai</surname> <given-names>F</given-names></name><name><surname>Ganesh</surname> <given-names>G</given-names></name><name><surname>Sanguineti</surname> <given-names>V</given-names></name><name><surname>Burdet</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Haptic communication between humans is tuned by the hard or soft mechanics of interaction</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1005971</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005971</pub-id><pub-id pub-id-type="pmid">29565966</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Wel</surname> <given-names>RP</given-names></name><name><surname>Knoblich</surname> <given-names>G</given-names></name><name><surname>Sebanz</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Let the force be with Us: dyads exploit haptic coupling for coordination</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>37</volume><fpage>1420</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1037/a0022337</pub-id><pub-id pub-id-type="pmid">21417545</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname> <given-names>NM</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Task-Related Verbal Interaction and Mathematics Learning in Small Groups</article-title><source>Journal for Research in Mathematics Education</source><volume>22</volume><fpage>366</fpage><lpage>389</lpage><pub-id pub-id-type="doi">10.2307/749186</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Liu</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>On crowdsourced interactive live streaming: a Twitch.Tv-based measurement study</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1502.04666">https://arxiv.org/abs/1502.04666</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s7" sec-type="appendix"><title>Sensitivity analysis</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.41328.013</object-id><p>For each proposed model, we conducted a sensitivity analysis to compare their predictive power over a parameter space. As the length of the trial is sufficiently long and the tracking task is continuous, we used an infinite-horizon optimal controller with quadratic cost. Two parameters of the model are adjustable; <inline-formula><mml:math id="inf104"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, a multiplier for the Gaussian noise <inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the velocity, and <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, a multiplier for the state cost <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in the controller. The control gain <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> will minimize the cost functional<disp-formula id="equ20"><label>(A1)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi><mml:mo>≡</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="bold">∞</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>R</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mtext mathvariant="bold"> </mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold"> </mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold"> </mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold"> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the combined state vector, <inline-formula><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the motor command, the state cost <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is positive semi-definite and control cost <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>&gt;</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p><inline-formula><mml:math id="inf113"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> is a scaling factor of the process noise that determines the frequency content of a simulated individual’s movement. <italic><inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> modulates the strength of the individual controllers. <inline-formula><mml:math id="inf115"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> was bound within a range such that the trajectories were not jerky, and <italic><inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></italic> was varied within a stable controllable range. These parameters were kept the same for all individuals in a collective.</p><p>The sum of squared error (SSE) between the fit from the data and from the simulation was used as a metric for predictive power. The <italic>neuromechanical goal sharing</italic> model exhibits less error than the <italic>no exchange</italic> model at the parameters best fitting the experimental data (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). The <italic>no exchange</italic> model appears to fit the data best for large <inline-formula><mml:math id="inf117"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> where trajectories were less smooth. However, it always yielded linear, first order improvement curves as a function of the partners’ relative error, which was different from the data that exhibited second and third order components. Thus, the <italic>neuromechanical goal sharing</italic> model that explained both the curvature of the improvement curve and the effect of the collective’s size on the improvement best explained the empirical data.</p></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.41328.015</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names></name><role>Reviewing Editor</role><aff><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Diedrichsen</surname><given-names>Jörn</given-names> </name><role>Reviewer</role><aff><institution>University of Western Ontario</institution><country>Canada</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Giese</surname><given-names>Martin A</given-names></name><role>Reviewer</role><aff><institution>University of Tübingen</institution><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Individuals physically interacting in a group rapidly coordinate their movement by estimating the collective goal&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Jörn Diedrichsen as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Richard Ivry as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Martin A Giese (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The manuscript describes an experiment that studies the haptic interaction between groups of different size within a tracking task, where multiple agents manipulate together a goal point that is elastically coupled to the individual agents. The study investigates how tracking accuracy depends on the performance of the individuals and finds that performance is mainly determined by the best members of the group, instead of representing the average. A detailed quantitative analysis is provided, how the performance error relative to the average performance changes as function of the number of group members. In addition, three quantitative control models are compared with the data, finding that a model that assumes that control is determined by an estimate of the average target position provides the best fits of the data.</p><p>Essential revisions:</p><p>1) All reviewers agreed that the manuscript could profit from a better explanation/motivation of the different models. Specifically, the neuromechanical goal sharing model should be introduced alongside the other models, as it is the most intuitive expectation of how the results should come out, based on what is known from interacting dyads. Reviewer 1 found the other two models not very informative, writing &quot;Given past work, the dyads model would seem to the main hypothesis to be tested. The two alternative hypotheses – the no exchange model and the source separation model – are not well motivated. The no-exchange model has been already rejected in the context of the dyadic interaction. So why should participants suddenly switch to being confused? Just because they know that they track together with multiple partners? The &quot;neuromechanical goal sharing&quot; model would be the logical prediction from what has been found in dyads already. The null-model spelled out in the third paragraph of the Introduction seems therefore somewhat contrived – and the real null model is exactly the &quot;neuromechnical goal sharing&quot;. Similarly, the source separation model appears contrived in that the participants simply have no information on which to base the source separation on. It would have been very surprising if they could have done this, but the finding that they simply treat the experienced forces as if these come from one individual is the expected finding.&quot;</p><p>However, the other two reviewers saw clearly value in retaining them, but also pointed out that they need to be motivated better so we recommend that you keep all three but revise the paper to provide better motivation.</p><p>2) One key result is that individuals benefit from groups of 3 and 4 more than they do from groups of 2. The nature of this benefit needs to be more fully understood. An important confounding factor is the virtual stiffness between the hand of the subject and the group-averaged position of all the other hands (Discussion paragraph three). Can this factor fully explain the group effect? The reviewers thought answering this question may demand some new data, where the group size is varied while stiffness kept constant.</p><p>It appears that the performance might strongly dependent on learning, maybe also on learning processes that are specific for the different numbers of group members, or for the presence of specific individuals in the groups. Authors should discuss clearly if/how they control for differences in such learning processes.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.41328.016</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) All reviewers agreed that the manuscript could profit from a better explanation/motivation of the different models. Specifically, the neuromechanical goal sharing model should be introduced alongside the other models, as it is the most intuitive expectation of how the results should come out, based on what is known from interacting dyads. Reviewer 1 found the other two models not very informative, writing &quot;Given past work, the dyads model would seem to the main hypothesis to be tested. The two alternative hypotheses – the no exchange model and the source separation model – are not well motivated. The no-exchange model has been already rejected in the context of the dyadic interaction. So why should participants suddenly switch to being confused? Just because they know that they track together with multiple partners? The &quot;neuro mechanical goal sharing&quot; model would be the logical prediction from what has been found in dyads already. The null-model spelled out in the third paragraph of the Introduction seems therefore somewhat contrived – and the real null model is exactly the &quot;neuromechnical goal sharing&quot;. Similarly, the source separation model appears contrived in that the participants simply have no information on which to base the source separation on. It would have been very surprising if they could have done this, but the finding that they simply treat the experienced forces as if these come from one individual is the expected finding.&quot;</p><p>However, the other two reviewers saw clearly value in retaining them, but also pointed out that they need to be motivated better so we recommend that you keep all three but revise the paper to provide better motivation.</p></disp-quote><p>We agree with the reviewers that these models should be better motivated in the manuscript. According to our paper (Takagi et al., 2017), the two individuals of a dyad combine the haptic information from the interaction with the partner with their own visual information in a stochastically optimal manner, based on an internal representation of the partner’s target as a function of their state, that they form during the interaction. The beneficial effects of the coordination mechanism could be outweighed by the interaction dynamics from a connection to multiple partners, which may help inferior partners but will likely hinder superior ones. As such, it was unclear whether the improvements from the coordination mechanism hold during collective interaction, or whether the connection dynamics to a larger inertia system begin to outweigh these benefits. This motivates a comparison between the data and the predictions from the no exchange model (considering only the effect of the connection dynamics) alongside the neuromechanical goal sharing model. We have modified the Introduction’s last two paragraphs to motivate the use of these two models correspondingly.</p><p>On the other hand, the source separation model is merely a limiting case of haptics useful to elucidate the interaction mechanism. It is now presented only the Discussion (paragraph six) in order to show that maximal information transfer cannot occur through haptics, highlighting the physical limit of haptics.</p><disp-quote content-type="editor-comment"><p>2) One key result is that individuals benefit from groups of 3 and 4 more than they do from groups of 2. The nature of this benefit needs to be more fully understood. An important confounding factor is the virtual stiffness between the hand of the subject and the group-averaged position of all the other hands (Discussion paragraph three). Can this factor fully explain the group effect? The reviewers thought answering this question may demand some new data, where the group size is varied while stiffness kept constant.</p></disp-quote><p>Thank you for this feedback that prompted us to clarify the mechanism of haptic communication in a collective. We have rewritten the Discussion to elucidate this mechanism, integrating new simulations to clarify how the partners exchange haptic information and how the interaction mechanics contribute to the collective size effect. In the model, an individual integrates the haptic estimate of the collective target by relating the interaction force and the target position through a representation of the partner. In the previous studies (Takagi et al., 2017, 2018), the noise in the haptic measurement of the partner’s target from the interaction force was the partner’s visual tracking noise. In collective interaction, the interaction force is equivalent to being connected to the average of the partners’ hand position by a stiffer spring (as shown in Equation 6). Assuming stochastically independent partners’ visual tracking, the variance in the interaction force, and subsequently the collective target estimated from this force, is inversely proportional to the squared number of partners (see Equation 10 and 11). Thus, the haptic noise on the collective target is different from averaging the partners’ visual tracking noise, and will yield different predictions depending on the group size. This is illustrated in the simulation of Figures 5C and 5D, where we modified the neuromechanical goal sharing model such that the haptic noise on the collective target was equal to the partners’ average noise (we also removed the dynamics and the compliance noise). This modified model yields similar improvements for dyads, triads and tetrads. Thus, the reduction in the variance in the interaction force due to the averaging of the noisy partners’ positions explains why the improvement is larger in bigger groups even if the partners’ average error is equivalent.</p><disp-quote content-type="editor-comment"><p>It appears that the performance might strongly dependent on learning, maybe also on learning processes that are specific for the different numbers of group members, or for the presence of specific individuals in the groups. Authors should discuss clearly if/how they control for differences in such learning processes.</p></disp-quote><p>We examined the deviation from the final improvement (shown in Figure 3D) for all individuals in every twos and fours recruited in our study as illustrated in <xref ref-type="fig" rid="respfig1">Author response image 1</xref>. The rate at which individuals reached their final improvement was highly correlated between individuals in the same group (ρ=0.995 ± 0.001 for dyads and ρ=0.996 ± 0.0009 for tetrads), making it difficult to assess individual contributions to the improvement observed in a particular dyad or tetrad. The performance improvement of the individuals in a particular group are correlated likely due to the dynamics. As such, the improvements from a group should be treated as a random factor that can modulate the group’s performance improvement. This factor is controlled for in the linear-mixed effects analysis of the performance improvement, by assuming that each recruited group of two and fours is a random factor that influences the performance improvement (refer to Equation 2 in the Materials and methods). This is now discussed in the Results when describing the linear mixed-effects analysis of the improvement curves in Figure 3B.</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Deviation from the final improvement as a function of the time during the trial for every individual (denoted by a different colour) in all twos (left panels) and fours (right panels) recruited in our study.</title><p>The rates of convergence to the final improvement differs somewhat between different groups, but it was highly correlated between individuals of the same group. As such, it is difficult to isolate the contribution of a specific individual to the improvement of a collective, and so each group of twos and fours must be treated as a random factor in the analysis.</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-41328-resp-fig1-v1"/></fig></body></sub-article></article>