<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">27203</article-id><article-id pub-id-type="doi">10.7554/eLife.27203</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Short Report</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The human auditory brainstem response to running speech reveals a subcortical mechanism for selective attention</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-84891"><name><surname>Forte</surname><given-names>Antonio Elia</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-84892"><name><surname>Etard</surname><given-names>Octave</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-27787"><name><surname>Reichenbach</surname><given-names>Tobias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3367-3511</contrib-id><email>reichenbach@imperial.ac.uk</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Bioengineering, Centre for Neurotechnology</institution><institution>Imperial College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-19576"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Reviewing Editor</role><aff><institution>Boston University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>10</day><month>10</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e27203</elocation-id><history><date date-type="received" iso-8601-date="2017-03-27"><day>27</day><month>03</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2017-09-14"><day>14</day><month>09</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Forte et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Forte et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-27203-v1.pdf"/><related-object ext-link-type="url" xlink:href="https://elifesciences.org/articles/e27203v1"><date date-type="v1" iso-8601-date="2017-10-10"><day>10</day><month>10</month><year>2017</year></date></related-object><abstract><object-id pub-id-type="doi">10.7554/eLife.27203.001</object-id><p>Humans excel at selectively listening to a target speaker in background noise such as competing voices. While the encoding of speech in the auditory cortex is modulated by selective attention, it remains debated whether such modulation occurs already in subcortical auditory structures. Investigating the contribution of the human brainstem to attention has, in particular, been hindered by the tiny amplitude of the brainstem response. Its measurement normally requires a large number of repetitions of the same short sound stimuli, which may lead to a loss of attention and to neural adaptation. Here we develop a mathematical method to measure the auditory brainstem response to running speech, an acoustic stimulus that does not repeat and that has a high ecological validity. We employ this method to assess the brainstem's activity when a subject listens to one of two competing speakers, and show that the brainstem response is consistently modulated by attention.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>auditory brainstem</kwd><kwd>auditory attention</kwd><kwd>auditory scene analysis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/M026728/1</award-id><principal-award-recipient><name><surname>Reichenbach</surname><given-names>Tobias</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>PHY-1125915</award-id><principal-award-recipient><name><surname>Reichenbach</surname><given-names>Tobias</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Selective attention to one of two speakers consistently modulates the response of the human auditory brainstem to each speaker's pitch.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>It is well known that selective attention to one of several competing acoustic signals affects the encoding of sound in the auditory cortex (<xref ref-type="bibr" rid="bib33">Shinn-Cunningham, 2008</xref>; <xref ref-type="bibr" rid="bib14">Hackley et al., 1990</xref>; <xref ref-type="bibr" rid="bib5">Choi et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Fritz et al., 2007b</xref>; <xref ref-type="bibr" rid="bib16">Hillyard et al., 1973</xref>; <xref ref-type="bibr" rid="bib39">Womelsdorf and Fries, 2007</xref>; <xref ref-type="bibr" rid="bib8">Fritz et al., 2007a</xref>; <xref ref-type="bibr" rid="bib28">Näätänen et al., 2001</xref>). Because extensive auditory centrifugal pathways carry information from central to more peripheral levels of the auditory system (<xref ref-type="bibr" rid="bib38">Winer, 2006</xref>; <xref ref-type="bibr" rid="bib29">Pickels, 1988</xref>; <xref ref-type="bibr" rid="bib36">Song et al., 2008</xref>; <xref ref-type="bibr" rid="bib2">Bajo et al., 2010</xref>), neural activity in the subcortical structures may contribute to attention as well. Previous attempts to determine an attentional modulation from recording the auditory brainstem response through scalp electrodes have, however, yielded highly inconclusive results.</p><p>In particular, one investigation found that selective attention alters the brainstem's response to the fundamental frequency of a speech signal (<xref ref-type="bibr" rid="bib11">Galbraith et al., 1998</xref>), while another study concluded that this response is modulated in an unsystematic but subject-specific manner (<xref ref-type="bibr" rid="bib24">Lehmann and Schönwiesner, 2014</xref>) and a third recent experiment did not find a significant attentional effect (<xref ref-type="bibr" rid="bib37">Varghese et al., 2015</xref>). Results on the effects of attention on the auditory-brainstem response to short clicks or pure tones are similarly inconclusive (<xref ref-type="bibr" rid="bib3">Brix, 1984</xref>; <xref ref-type="bibr" rid="bib13">Gregory et al., 1989</xref>; <xref ref-type="bibr" rid="bib17">Hoormann et al., 2000</xref>; <xref ref-type="bibr" rid="bib12">Galbraith et al., 2003</xref>). These inconsistencies may result from a main experimental limitation in these studies: because the brainstem response is tiny, its measurement requires hundred- to thousandfold repetition of the same sound. The large number of repetitions may lead to difficulties for subjects in sustaining selective attention, to adaptation in the nervous system, and to a reduction in efferent feedback (<xref ref-type="bibr" rid="bib23">Lasky, 1997</xref>; <xref ref-type="bibr" rid="bib22">Kumar Neupane et al., 2014</xref>).</p><p>To overcome this limitation, we develop here a method to measure the auditory brainstem's response to natural running speech that does not repeat. We then use this method to assess the modulation of the auditory brainstem response to one of two competing speakers by selective attention.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Assessing the brainstem's response to continuous non-repetitive speech does not allow to average over many repeated presentations of the same sound. Instead, we sought to quantify the brainstem's response to the fundamental frequency of speech. Neuronal activity in the brainstem, and in particular in the inferior colliculus, can indeed phase lock to the periodicity of voiced speech (<xref ref-type="bibr" rid="bib34">Skoe and Kraus, 2010</xref>). The fundamental frequency of running speech varies over time, however, compounding a direct read-out of the evoked brainstem response.</p><p>To overcome this difficulty, we employed empirical mode decomposition (EMD) of the speech stimuli to identify an empirical mode that, at each time instance, oscillates at the fundamental frequency of the speech signal (<xref ref-type="bibr" rid="bib18">Huang and Pan, 2006</xref>) (Materials and methods). This mode is a nonlinear and nonstationary oscillation with a temporally-varying amplitude and frequency that we refer to as the 'fundamental waveform' of the speech stimulus (<xref ref-type="fig" rid="fig1">Figure 1a</xref>).</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.27203.002</object-id><label>Figure 1.</label><caption><title>The brainstem response to running speech.</title><p>(<bold>a</bold>) Speech (black) contains voiced parts with irregular oscillations at a time-varying fundamental frequency and higher harmonics. We extract a fundamental waveform (red) that oscillates nonlinearly and nonstationary at the fundamental frequency. (<bold>b</bold>) The autocorrelation of the fundamental waveform (red) peaks when the delay vanishes and oscillates at the average fundamental frequency. The cross-correlation of the fundamental waveform with its Hilbert transform (blue) can be seen as an imaginary part of the autocorrelation. The amplitude of the resulting complex cross-correlation (black) shows a life-time of a few ms. (<bold>c</bold>) The correlation of the speech-evoked brainstem response, recorded from one subject, to the fundamental waveform of the speech signal (red) as well as to its Hilbert transform (blue) can serve as real and imaginary parts of a complex correlation function. Its amplitude (black) peaks at a latency of 9 ms. The latency of the correlation is not altered by the processing of the speech signal or of the neural recording, and contains neither a stimulus artifact nor the cochlear microphonic (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27203-fig1-v1"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.27203.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Controls for latencies induced by signal processing as well as for the source of the measured brainstem response to running speech.</title><p>(<bold>a</bold>) The cross-correlation between the original speech signal with the fundamental waveform (red) as well as with its Hilbert transform (blue) and the resulting amplitude (black) show a peak at 0 ms and no phase shift. The processing of the acoustic signal does accordingly not change the latency or phase of that signal. (<bold>b</bold>) The computation of the cross-correlation of the fundamental waveform to the neural recording involved processing of the neural signal such as through filtering. However, the cross-correlation between the recorded neural signal and the filtered version shows a peak at vanishing latency. The processing of the neural signal did therefore not alter the latency. (<bold>c</bold>) When the earphones are placed close to the ears, but not inside the ear canal, preventing a subject from hearing the speech signal, the cross-correlation between the recorded neural signal and the fundamental waveform of speech (red) as well as its Hilbert transform (blue) do not yield a measurable peak. The amplitude of the resulting complex correlation function (black) does not peak either, demonstrating the absence of a stimulus artifact. (<bold>d</bold>) When a subject listened to a speech signal and then to the same signal with reversed polarity, and when the average over the neural recordings to both stimulus presentations was employed for the analysis, the complex cross-correlation showed the same structure as when it was computed using the neural response to one stimulus only. This shows the absence of a stimulus artifact as well as the absence of the cochlear microphonic in the measured response. (<bold>e</bold>) Putative cortical contributions to the neural response would occur at latencies above 15 ms and likely between 50–500 ms. The complex correlation at those latencies shows, however, no significant peak. To enable comparison, all recordings were obtained from the same subject for whom we report the exemplary recording in <xref ref-type="fig" rid="fig1">Figure 1c</xref>.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27203-fig1-figsupp1-v1"/></fig></fig-group><p>We then recorded the brainstem response to running non-repetitive speech stimuli of several minutes in duration from human volunteers through scalp electrodes. We cross-correlated the obtained recording with the fundamental waveform of the speech signal (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Because the brainstem response may occur at a phase that is different from that of the fundamental waveform, we also correlated the neural signal to the Hilbert transform of the fundamental waveform that has a phase shift of 90˚. The two correlations can be viewed as the real and imaginary part of a complex correlation function that can trace the brainstem response at any phase shift. The amplitude of the complex correlation informs then on the strength of the brainstem response.</p><p>Our statistical analysis showed that the peak amplitude of the complex correlation was significantly different from the noise in fourteen out of sixteen subjects (p&lt;0.05, Materials and methods). The peak occurred at a mean latency of 9.3 ± 0.7 ms, verifying that the measured neural response resulted from the brainstem and not from the cerebral cortex (<xref ref-type="bibr" rid="bib15">Hashimoto et al., 1981</xref>; <xref ref-type="bibr" rid="bib30">Picton et al., 1981</xref>). Moreover, the latency agreed with that found previously regarding the brainstem's response to short repeated speech stimuli (<xref ref-type="bibr" rid="bib34">Skoe and Kraus, 2010</xref>). The average value of the correlation at the peak was 0.015 ± 0.003. We checked that the response did not contain a stimulus artifact or a contribution from the cochlear microphonic, and that the latency of the response was not affected by the processing of the speech signal or of the neural response (Materials and methods; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). We also verified that the response did not contain further peaks at higher latencies, showing that the neural signal did not contain a measurable contribution from the cortex.</p><p>We further considered a highly simplistic model of the auditory brainstem response in which a burst of neural spikes occurred at each cycle of the fundamental waveform, at a fixed phase <italic>φ</italic>, and was then shifted in time by a certain delay <italic>τ</italic> (<xref ref-type="fig" rid="fig2">Figure 2a,b</xref>; Materials and methods). Adding noise that is realistic of neural recordings from scalp electrodes, and computing the complex correlation of the obtained signal with the fundamental waveform, showed a peak in the complex correlation at the time delay <italic>τ</italic> (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). The phase <italic>φ</italic> of the fundamental waveform at which the neural bursts occurred was obtained from the phase of the complex correlation at the time delay <italic>τ.</italic> This demonstrated that the brainstem's response to continuous speech could be reliably extracted through the developed method. The response can be characterized through the latency and amplitude of the correlation's peak.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.27203.004</object-id><label>Figure 2.</label><caption><title>Simplistic model of the auditory brainstem response to continuous speech.</title><p>(<bold>a</bold>) The fundamental waveform (red) as well as its Hilbert transform (blue) oscillate with a varying amplitude and frequency. (<bold>b</bold>) We model a simplistic brainstem response in which bursts of neural spikes occur at each cycle of the fundamental waveform, at a phase of ¼ π rad (black dots). Furthermore, all neural bursts are shifted by a temporal delay of 8 ms. (<bold>c</bold>) When adding realistic noise as emerges from scalp recordings, and then computing the complex correlation with the fundamental waveform as performed for the actual brainstem recording, we find a peak at the modelled delay of 8 ms. The modelled phase of ¼ π rad is obtained as the inverse phase of the complex correlation at that latency.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27203-fig2-v1"/></fig><p>Armed with the ability to quantify the brainstem's response to running non-repetitive speech, we sought to investigate if this neural activity is affected by selective attention. Employing a well-established paradigm of attention to one of two speakers (<xref ref-type="bibr" rid="bib6">Ding and Simon, 2012</xref>), we presented volunteers diotically with two concurrent speech streams of equal intensity, one by a male and another by a female voice. For parts of the speech presentation subjects attended the male voice and ignored the female voice, and <italic>vice versa</italic> for the remaining parts.</p><p>We quantified the brainstem's response to both the male and the female voice by extracting the fundamental waveforms of both speech signals and correlating the neural recording separately to both. We found that the latency of the response was unaffected by attention: the response to the unattended speaker occurred 0.8 ± 0.5 ms later than that to the attended speaker, which was not statistically significant (p=0.2; average over the responses to the male and the female voice as well as all subjects).</p><p>In contrast, all subjects showed a larger response of the auditory brainstem, at the peak latency, to the male voice when attending rather than ignoring it (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). The difference in the responses was statistically significant in nine of the fourteen subjects (p&lt;0.05). The brainstem's response to the attended female speaker similarly exceeded that to the unattended female voice in all but one subject, with eight subjects showing a statistically-significant difference (p&lt;0.05; <xref ref-type="fig" rid="fig3">Figure 3b</xref>). The ratio of the brainstem's response to attended and to ignored speech, averaged over all subjects, was 1.5 ± 0.1 and 1.6 ± 0.2 for the male and for the female speaker, respectively. Both ratios were significantly different from unity (p&lt;0.001, male voice; p&lt;0.01, female voice). The male and the female voice elicited a comparable attentional modulation: the difference between the corresponding ratios was insignificant (p=0.7). The magnitude of the brainstem's response was hence significantly enhanced through attention, and consistently so across subjects and speakers.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.27203.005</object-id><label>Figure 3.</label><caption><title>Modulation of the brainstem response to speech by selective attention.</title><p>(<bold>a</bold>) The brainstem's response to the male speaker is larger for each subject when attending the speaker (dark blue) than when ignoring it (light blue). The average ratio of the brainstem responses to the attended and to the ignored male speaker is significantly larger than 1 (black, mean and standard error of the mean). (<bold>b</bold>) With the exception of subject 13, the neural response to the female voice is also larger when subjects attend to it (dark red) instead of ignoring it (light red). The average ratio of the brainstem responses to the attended and to the ignored female speaker is significantly larger than one as well (black, mean and standard error of the mean).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27203-fig3-v1"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.27203.006</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Correlation between the amplitude of the brainstem response and the fundamental frequency of the speech signal.</title><p>(<bold>a</bold>) The correlation between the amplitude of the brainstem's response to a single speaker and its fundamental frequency is slightly negative. (<bold>b, c</bold>) The amplitude of the brainstem's response to an attended (<bold>b</bold>) as well as an ignored (<bold>c</bold>) speaker decreases slightly with increasing fundamental frequency as well. The range of fundamental frequencies in the competing speaker scenarios (<bold>b, c</bold>) is larger than in the single speaker situation (<bold>a</bold>) since the latter consists of a single female speaker only whereas the former contain both a male and a female speaker, with the female speaker having a fundamental frequency that is on average larger than that of the male speaker.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-27203-fig3-figsupp1-v1"/></fig></fig-group><p>The auditory brainstem response to short speech stimuli has a low-pass nature: the amplitude of the response declines with increasing frequency (<xref ref-type="bibr" rid="bib27">Musacchia et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">Skoe and Kraus, 2010</xref>).To determine if this relation held for the brainstem response to continuous speech that we measured here as well, and how it may be affected by attention, we computed the correlation between the amplitude of the brainstem response to short speech segments and the fundamental frequency of the segments (Materials and methods). We found a small but statistically significant negative correlation between the amplitude of the brainstem response and the fundamental frequency, both for the neural response to a single speaker as well as for the response to the attended and ignored speech signal of two competing speakers, evidencing the low-pass nature of the brainstem response (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The correlations between the amplitude and the frequency did not differ significantly between the different conditions.</p><p>The brainstem response to short clicks in noise can have a delay that becomes longer with increasing noise level, while the amplitude of the brainstem response declines, presumably reflecting varying contributions of auditory-nerve fibers with different spontaneous rates and from different cochlear locations (<xref ref-type="bibr" rid="bib26">Mehraei et al., 2016</xref>). However, for the brainstem response to continuous speech that we measured here, we did not find a statistically-significant correlation between amplitude and latency (Materials and methods).</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our results show that the human auditory brainstem response to continuous speech is larger when attending than when ignoring a speech signal, and consistently so across different subjects and speakers. In particular, the strength of the phase locking of the neural activity to the pitch structure of speech is larger for an attended than for an unattended speech stream. In contrast, we did not observe a difference in the latency of this activity.</p><p>The fundamental waveform of speech that we have obtained from EMD has a temporally varying frequency and amplitude and is therefore not a simple component of Fourier analysis. While it may be obtained from short-time Fourier transform or wavelet analysis, both methods suffer from an inherently limited time-frequency resolution that makes them inferior to the EMD analysis (<xref ref-type="bibr" rid="bib18">Huang and Pan, 2006</xref>).</p><p>Because we have employed a diotic stimulus presentation in which the same acoustical stimulus was presented to each ear, the attentional modulation cannot result from a general modulation of the brainstem's activity to acoustic stimuli between the two hemispheres. Moreover, although the fundamental frequencies of the two competing speakers differ at most time points, their spectra largely overlap. The attentional modulation can therefore not result from a broad-band modulation of the neural activity either. Instead, the attentional effect must result from a modulation of the brainstem's response to the specific pitch structure of a speech stimulus.</p><p>The brainstem response to the pitch of continuous speech that we have measured can reflect a response both to the fundamental frequency of speech as well as to higher harmonics. Indeed, previous studies have found that the brainstem responds at the fundamental frequency of a speech stimulus even when that frequency itself is removed from the acoustic signal (<xref ref-type="bibr" rid="bib10">Galbraith and Doan, 1995</xref>), or when it cancels out due to presentation of stimuli with opposite polarities and averaging of the obtained responses (<xref ref-type="bibr" rid="bib1">Aiken and Picton, 2008</xref>). The attentional modulation of the brainstem response can thus reflect a modulation of the response to the fundamental frequency itself or to higher harmonics. Moreover, attentional modulation of higher harmonics may depend on frequency as shown recently in recordings of otoacoustic emissions from the inner ear (<xref ref-type="bibr" rid="bib25">Maison et al., 2001</xref>).</p><p>The attentional modulation of the brainstem's response to the pitch of a speaker may result from an enhancement of the neural response to an attended speech signal, from the suppression of the response to an ignored speech stimulus, or from both. Further investigation into this issue may compare brainstem responses to speech when attending to the acoustical signal and when attending to a visual stimulus (<xref ref-type="bibr" rid="bib40">Woods et al., 1992</xref>; <xref ref-type="bibr" rid="bib20">Karns and Knight, 2009</xref>; <xref ref-type="bibr" rid="bib31">Saupe et al., 2009</xref>).</p><p>The response at the fundamental frequency of speech can result from multiple sites in the brainstem (<xref ref-type="bibr" rid="bib4">Chandrasekaran and Kraus, 2010</xref>). However, we observed a single peak with a width of a few ms in the correlation of the neural signal to the fundamental waveform of speech. The brainstem response to running speech that we have measured here can therefore only reflect neural sources whose latencies vary by a few ms or less from the peak latency. The neural delay of about 9 ms as well as the similarity of the speech-evoked brainstem response to the frequency-following response suggest that the main neural source may be in the inferior colliculus (<xref ref-type="bibr" rid="bib35">Sohmer et al., 1977</xref>). The attentional effect that we have observed may then result from the multiple feedback loops between the inferior colliculus, the medial geniculate body and the auditory cortex (<xref ref-type="bibr" rid="bib19">Huffman and Henson, 1990</xref>).</p><p>Our study provides a mathematical methodology to analyse the brainstem response to complex, real world stimuli such as speech. Since our method does not require artificial and repeated stimuli, it fosters sustained attention and avoids potential neural adaptation. This method can therefore pave the way to further explore how the brainstem contributes to the processing of complex real-world acoustic environments. It may also be relevant for better understanding and diagnosing the recently discovered cochlear neuropathy or 'hidden hearing loss' (<xref ref-type="bibr" rid="bib21">Kujawa and Liberman, 2009</xref>). Because the latter alters the brainstem's activity (<xref ref-type="bibr" rid="bib32">Schaette and McAlpine, 2011</xref>; <xref ref-type="bibr" rid="bib26">Mehraei et al., 2016</xref>), assessing the auditory brainstem response to speech as well as its modulation by attention may further clarify the origin, prevalence and consequences of such poorly understood supra-threshold hearing loss.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>16 healthy adult volunteers aged 18 to 32, eight of which were female, participated in the study. All subjects were native English speakers and had no history of hearing or neurological impairments. All participants had pure-tone hearing thresholds better than 20 dB hearing level in both ears at octave frequencies between 250 Hz and 8 kHz. Each subject provided written informed consent. All experimental procedures were approved by the Imperial College Research Ethics Committee.</p></sec><sec id="s4-2"><title>Auditory brainstem recordings to running speech</title><p>Samples of continuous speech from a male and a female speaker were obtained from publicly available audiobooks (<ext-link ext-link-type="uri" xlink:href="https://librivox.org">https://librivox.org</ext-link>). All samples had a duration of at least two minutes and ten seconds; some were slightly longer to end upon completion of a sentence. To construct speech samples with two competing speakers, samples from the male and from the female speaker were normalized to the same root-mean-square amplitude and then superimposed.</p><p>Participants were placed in a comfortable chair in an acoustically and electrically insulated room (IAC Acoustics, UK). A personal computer outside the room controlled audio presentation and data acquisition. Speech stimuli were presented at a sampling frequency of 44.1 kHz through a high-performance sound card (Xonar Essence STX, Asus, USA). Stimuli were delivered diotically through insert earphones (ER-3C, Etymotic, USA) at a level of 76 dB(A) SPL (A-weighted frequency response). Sound intensity was calibrated with an ear simulator (Type 4157, Brüel and Kjaer, Denmark). All subjects reported that the stimulus level was comfortable.</p><p>The response from the auditory brainstem was measured through five passive Ag/AgCl electrodes (Multitrode, BrainProducts, Germany). Two electrodes were positioned at the cranial vertex (Cz), two further electrodes were placed on the left and right mastoid processes, and the remaining electrode was positioned on the forehead to measure the ground. The impedance between each electrode and the skin was reduced to below 5 kΩ using abrasive electrolyte-gel (Abralyt HiCl, Easycap, Germany). The electrode on the left mastoid, at the cranial vertex and the ground electrode were connected to a bipolar amplifier with low-level noise and a gain of 50 (EP-PreAmp, BrainProducts, Germany). The remaining two electrodes were connected to a second identical bipolar amplifier. The output from both bipolar amplifiers was fed into an integrated amplifier (actiCHamp, BrainProducts, Germany) where it was low-pass filtered through a hardware anti-aliasing filter with a corner frequency of 4.9 kHz and sampled at 25 kHz. The audio signals were measured by the integrated amplifier as well through an acoustic adapter (Acoustical Stimulator Adapter and StimTrak, BrainProducts, Germany). The electrophysiological data were acquired through PyCorder (BrainProducts, Germany). The simultaneous measurement of the audio signal and the brainstem response from the integrated amplifier was employed to temporally align both signals to a precision of less than 40 μs, the inverse of the sampling rate (25 kHz).</p></sec><sec id="s4-3"><title>Experimental design</title><p>In the first part of the experiment, each volunteer listened to four speech samples of the female speaker only. Comprehension questions were asked at the end of each part in order to verify the subject's attention to the story.</p><p>The second part of the experiment employed eight samples of speech that contained both a male and a female voice. During the presentation of the first four samples, subjects were asked to attend either the male or the female speaker. Volunteers were then presented with the next four speech samples and asked to attend to the speaker that they had ignored earlier. Whether the subject was asked to attend first to the male or to the female voice was determined randomly for every subject. Comprehension questions were asked after each sample.</p></sec><sec id="s4-4"><title>Computation of the fundamental waveform of speech</title><p>The fundamental waveform of each speech sample with a single speaker was computed through a custom-written Matlab program (code available on Github; <xref ref-type="bibr" rid="bib7">Forte, 2017</xref>; a copy is archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/fundamental_waveforms_extraction">https://github.com/elifesciences-publications/fundamental_waveforms_extraction</ext-link>). The fundamental waveform of a speech sample with two speakers followed from the two corresponding samples with a single speaker only.</p><p>First, each speech signal was downsampled to 8820 Hz, low-pass filtered at 1,500 Hz (linear-phase FIR filter, transition band 1500–1650 Hz, stopband attenuation −80 dB, passband ripple 1 dB, order 296) and time-shifted to compensate for the filter delay. Silent parts between words were identified by computing the envelope of the speech signal. Each part where the envelope was less than 10% of the maximal value found in the speech was considered silent, and the speech signal there was set to zero.</p><p>Second, the instantaneous fundamental frequency of the voiced parts of the speech signal was detected through the autocorrelation method, employing rectangular windows of 50 ms duration with a successive overlap of 49 ms. Speech segments that yielded a fundamental frequency outside the range of 60 Hz to 400 Hz, or in which the fundamental frequency varied by more than 10 Hz between two successive windows were considered voiceless. The speech segments that corresponded to voiced speech, as well as their fundamental frequency, were thus obtained. The fundamental frequency of each segment was interpolated through a cubic spline, and varied between 100 and 300 Hz in each segment. Note that this method yields the fundamental frequency but not by itself the fundamental wavemode.</p><p>Third, the voiced speech segments were analysed through the Hilbert-Huang transform. The latter is an adaptive signal processing based on empirical basis functions and can thus be better suited for analysing nonlinear and nonstationary signals such as speech than Fourier analysis (<xref ref-type="bibr" rid="bib18">Huang and Pan, 2006</xref>). The transform consists of two parts. First, empirical mode decomposition extracts intrinsic mode functions (IMFs) that satisfy two properties: (i) the numbers of extrema and zero crossings are either equal or differ by one; (ii) the mean of the upper and lower envelope vanishes. The signal follows as the linear superposition of the IMFs. Second, the Hilbert spectrum of each IMF is determined, which yields, in particular, the mode's instantaneous frequency. This analysis was performed for each short segment of voiced speech, that is, for each part of voiced speech that was preceded and followed by a pause or voiceless speech.</p><p>Fourth, the fundamental frequency of each short speech segment was compared to the instantaneous frequencies of the segment's IMFs at each individual time point. All IMFs with an instantaneous frequency that differed by less than 20% from the segment's fundamental frequency were determined, and the IMF with the largest amplitude was therefrom selected as the fundamental wavemode of that segment and at that time point (<xref ref-type="bibr" rid="bib18">Huang and Pan, 2006</xref>). If no IMF had an instantaneous frequency within 20% of the fundamental frequency, or if a speech segment was unvoiced, that time point was assigned a fundamental waveform of zero. The fundamental waveforms obtained at the different time points were combined through cosine crossfading functions with a window width of 10 ms to obtain the fundamental waveform of the speech signal. The Hilbert transform of that fundamental waveform was computed as well.</p><p>To control for latency changes in the acoustic signal induced by the subsequent processing steps, and in particular by the involved frequency filtering, the cross-correlation between the original speech signal and the fundamental waveform as well as with its Hilbert transform was computed (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a</xref>). The cross-correlations show that the fundamental waveform has no latency change and no phase difference with respect to the original speech stimulus.</p></sec><sec id="s4-5"><title>Analysis of the auditory-brainstem response</title><p>The brainstem responses from the two measurement channels were averaged. A frequency-domain regression technique (CleanLine, EEGLAB) was used to attenuate noise from the power line in the brainstem recording. Moreover, because a voltage amplitude above 20 mV cannot result from the brainstem but represents artefacts such as spurious muscle activity, the signal was set to zero during episodes of such high voltage. The electrophysiological recording was then filtered between 100–300 Hz since the fundamental frequency of the speech was in that range (high-pass filter: linear-phase FIR filter, transition band from 90 to 100 Hz, stopband attenuation −80 dB, passband ripple 1 dB, order 6862; low-pass filter: linear-phase FIR filter, transition band 300–360 Hz, stopband attenuation −80 dB, passband ripple 1 dB, order 1054). In particular, the high-pass filter eliminated neural signals from the cerebral cortex that occur predominantly below 100 Hz. To avoid transient activity at the beginning of each speech sample, the first ten seconds of each brainstem recording in response to a speech sample were discarded. The following two minutes of data were divided into 40 epochs of a duration of 3 s each, and the remaining data were discarded, if any.</p><p>The processing of the neural signal did not induce a latency. This was confirmed by computing the cross-correlation between the processed neural response and the original signal, demonstrating a maximum correlation at zero temporal delay (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>).</p><p>As set out above, the first part of the experiment measured the brainstem response to running speech without background noise. For each subject and each epoch, the cross-correlation of the brainstem response with the corresponding segment of the fundamental waveform as well as with its Hilbert transform were computed. A delay of 1 ms of the acoustic signal produced by the earphones was taken into account. The two cross-correlation functions were interpreted as the real and the imaginary part of a complex correlation function. For each individual subject, the average of the complex cross-correlation over all epochs was then computed, and the latency at which the amplitude peaked was determined.</p><p>The obtained latencies of about 9 ms affirmed that the signal resulted from the auditory brainstem and not from the cerebral cortex (<xref ref-type="bibr" rid="bib15">Hashimoto et al., 1981</xref>). The latency also evidenced that the signal resulted neither from stimulus artifacts nor from the cochlear microphonic, which would occur at or near zero delay (<xref ref-type="bibr" rid="bib34">Skoe and Kraus, 2010</xref>). As an additional control, the brainstem response was recorded when the earphones were near the ear, but not inserted into the ear canal, so the subject could not hear the speech signals. The recording did then not yield a measurable brainstem response (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>). Two presentations of the same speech stimulus, but with opposite polarities, were employed as well, and the neural response to both presentations was averaged before computing the correlation to the fundamental waveform. The correlation was identical to that obtained by a single stimulus presentation, demonstrating the absence of a stimulus artifact and of the cochlear microphonic (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1d</xref>).</p><p>To further verify our analysis we considered a highly simplistic model of the brainstem response. In particular, we constructed a simplified hypothetical brainstem response to speech of a duration of 10 min in which neural spikes occur in bursts (<xref ref-type="fig" rid="fig2">Figure 2a,b</xref>). We described each burst by a Gaussian distribution with a width of 1 ms. Each cycle of the fundamental waveform triggered a burst, which was centered at a fixed phase <italic>φ</italic> of the waveform and shifted by a certain time delay <italic>τ</italic>. We then added an actual neural recording from a trial without sound presentation, representing realistic noise, to the simulated brainstem response, at a signal-to-noise ratio of −20 dB. Processing this simulated brainstem response through the methods described above yielded a complex correlation that peaked at the imposed time delay <italic>τ</italic> (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). The phase of the complex correlation was the inverse of the phase <italic>φ</italic> of the fundamental waveform at which the modelled brainstem response occurred; the inverse of the phase appeared due to the complex conjugate in the definition of the cross-correlation. This confirmed the validity of our methodology as well as that our processing of the neural data did not alter the temporal delay.</p><p>To determine whether the peak in the cross-correlation obtained from a given subject was significant, the values of the complex cross-correlation from the individual epochs, and at the peak latency, were analysed. Because each correlation value is an average of many measurements, it follows from the Central Limit Theorem that the complex correlations from the different epochs exhibit a two-dimensional normal distribution with a mean of zero if the measurements are randomly distributed. A one-sample Hotelling’s T-squared test was therefore used to assess the significance of the complex correlation at the peak latency. Two subjects who did not show a significant correlation (p&gt;0.05) were not included in the further analysis.</p><p>The population mean and standard error of the mean of the latency were computed from the latencies of the individual subjects.</p><p>The brainstem responses to competing speakers were then analysed for each individual subject. For each epoch, the complex cross-correlation between the brainstem response and the fundamental waveform was computed, both for the fundamental waveform of the attended and for that of the unattended speaker. The corresponding complex correlation functions were averaged across epochs, and the amplitudes as well as latencies of the peaks were determined.</p><p>Statistical significance of the difference in latency of the brainstem responses to the attended and the unattended speaker, obtained from the eight samples, was tested by computing population mean as well as standard error of the mean for the differences in latencies obtained from individual subjects. A two-tailed Student's t-test was employed to test if the difference was significantly different from zero.</p><p>To control for differences in the voice of the male and the female speaker, differences in amplitude of the brainstem response to the attended and ignored male speaker were determined separately from differences in the amplitude of the brainstem response to the attended and ignored female speaker. The amplitudes of the complex cross-correlations, at the peak latencies, were computed for all epochs. A two-sample Student’s t-test was then employed to test for a significant difference between the amplitude in response to the attended and the ignored speaker.</p><p>The amplitude of the brainstem response to speech can vary widely between subjects (3), due to variations such as in anatomy and scalp conductivity. The ratios of the amplitudes of the brainstem responses to attended and ignored speech, rather than the differences, were thus computed for each individual. The population mean and standard error of the mean were therefrom obtained. A one-tailed Student's t-test assessed whether the population average of the ratio was significantly larger than unity. A two-tailed two-sample Student's t-test was employed to assess whether the ratios obtained from the responses to the male and to the female speaker were significantly different.</p><p>We assessed the correlation between the amplitude of the brainstem response and the frequency of the fundamental waveform by dividing each speech signal into 160 segments, and by dividing the corresponding neural recording analogously. We then computed the complex correlation of each segment of the neural data with the corresponding segment of the fundamental waveform, and obtained the amplitude at the peak latency. We further computed the average fundamental waveform of each segment. The correlation between amplitude and frequency was tested for statistical significance through a one-tailed Student's t-test. It was found to be significant for the brainstem response to a single speaker as well as for the response to the attended and the ignored speaker in the two-speaker stimuli (p&lt;0.05). However, none of the differences between the obtained correlation coefficients were statistically significant (two-tailed Student's t-test, p&lt;0.05).</p><p>We also computed the correlation between the amplitude and the latency of the brainstem response from short speech segments of 120 s in duration; such long segments were required to obtain a reliable assessment of the latency. We performed this analysis for the auditory brainstem response to a single speaker as well as for those to the attended and ignored speaker when subjects were presented with two competing speakers. However, none of these correlations were statistically significant (two-tailed Student's t-test, p&gt;0.05).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Steve Bell, Karolina Kluk-de Kort, Patrick Naylor, David Simpson and Malcolm Slaney for discussion as well as for comments on the manuscript. This research was supported by EPSRC grant EP/M026728/1 to TR as well as in part by the National Science Foundation under Grant No. NSF PHY-1125915.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Validation, Investigation, Visualization, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Validation, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Investigation, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects provided written informed consent. All experimental procedures were approved by the Imperial College Research Ethics Committee.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.27203.007</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-27203-transrepform-v1.pdf"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aiken</surname> <given-names>SJ</given-names></name><name><surname>Picton</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Envelope and spectral frequency-following responses to vowel sounds</article-title><source>Hearing Research</source><volume>245</volume><fpage>35</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2008.08.004</pub-id><pub-id pub-id-type="pmid">18765275</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bajo</surname> <given-names>VM</given-names></name><name><surname>Nodal</surname> <given-names>FR</given-names></name><name><surname>Moore</surname> <given-names>DR</given-names></name><name><surname>King</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The descending corticocollicular pathway mediates learning-induced auditory plasticity</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>253</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1038/nn.2466</pub-id><pub-id pub-id-type="pmid">20037578</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brix</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>The influence of attention on the auditory brain stem evoked responses. Preliminary report</article-title><source>Acta Oto-Laryngologica</source><volume>98</volume><fpage>89</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.3109/00016488409107538</pub-id><pub-id pub-id-type="pmid">6464728</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname> <given-names>B</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The scalp-recorded brainstem response to speech: neural origins and plasticity</article-title><source>Psychophysiology</source><volume>47</volume><fpage>236</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2009.00928.x</pub-id><pub-id pub-id-type="pmid">19824950</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname> <given-names>I</given-names></name><name><surname>Rajaram</surname> <given-names>S</given-names></name><name><surname>Varghese</surname> <given-names>LA</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Quantifying attentional modulation of auditory-evoked cortical responses from single-trial electroencephalography</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>115</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00115</pub-id><pub-id pub-id-type="pmid">23576968</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>PNAS</source><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id><pub-id pub-id-type="pmid">22753470</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Forte</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Fundamental_waveforms_extraction</data-title><source>GitHub</source><version designator="6cf106436fd7c636fb7c1a72e69241dec12a720b">6cf106436fd7c636fb7c1a72e69241dec12a720b</version><ext-link ext-link-type="uri" xlink:href="https://github.com/antn85/fundamental_waveforms_extraction">https://github.com/antn85/fundamental_waveforms_extraction</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007a</year><article-title>Auditory attention--focusing the searchlight on sound</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>437</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.07.011</pub-id><pub-id pub-id-type="pmid">17714933</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Elhilali</surname> <given-names>M</given-names></name><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007b</year><article-title>Does attention play a role in dynamic receptive field adaptation to changing acoustic salience in A1?</article-title><source>Hearing Research</source><volume>229</volume><fpage>186</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2007.01.009</pub-id><pub-id pub-id-type="pmid">17329048</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galbraith</surname> <given-names>GC</given-names></name><name><surname>Doan</surname> <given-names>BQ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Brainstem frequency-following and behavioral responses during selective attention to pure tone and missing fundamental stimuli</article-title><source>International Journal of Psychophysiology</source><volume>19</volume><fpage>203</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/0167-8760(95)00008-G</pub-id><pub-id pub-id-type="pmid">7558987</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galbraith</surname> <given-names>GC</given-names></name><name><surname>Bhuta</surname> <given-names>SM</given-names></name><name><surname>Choate</surname> <given-names>AK</given-names></name><name><surname>Kitahara</surname> <given-names>JM</given-names></name><name><surname>Mullen</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Brain stem frequency-following response to dichotic vowels during attention</article-title><source>NeuroReport</source><volume>9</volume><fpage>1889</fpage><lpage>1893</lpage><pub-id pub-id-type="doi">10.1097/00001756-199806010-00041</pub-id><pub-id pub-id-type="pmid">9665621</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galbraith</surname> <given-names>GC</given-names></name><name><surname>Olfman</surname> <given-names>DM</given-names></name><name><surname>Huffman</surname> <given-names>TM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Selective attention affects human brain stem frequency-following response</article-title><source>NeuroReport</source><volume>14</volume><fpage>735</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1097/00001756-200304150-00015</pub-id><pub-id pub-id-type="pmid">12692473</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gregory</surname> <given-names>SD</given-names></name><name><surname>Heath</surname> <given-names>JA</given-names></name><name><surname>Rosenberg</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Does selective attention influence the brain-stem auditory evoked potential?</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>73</volume><fpage>557</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(89)90266-6</pub-id><pub-id pub-id-type="pmid">2480891</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackley</surname> <given-names>SA</given-names></name><name><surname>Woldorff</surname> <given-names>M</given-names></name><name><surname>Hillyard</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Cross-modal selective attention effects on retinal, myogenic, brainstem, and cerebral evoked potentials</article-title><source>Psychophysiology</source><volume>27</volume><fpage>195</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1990.tb00370.x</pub-id><pub-id pub-id-type="pmid">2247550</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hashimoto</surname> <given-names>I</given-names></name><name><surname>Ishiyama</surname> <given-names>Y</given-names></name><name><surname>Yoshimoto</surname> <given-names>T</given-names></name><name><surname>Nemoto</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Brain-stem auditory-evoked potentials recorded directly from human brain-stem and thalamus</article-title><source>Brain</source><volume>104</volume><fpage>841</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1093/brain/104.4.841</pub-id><pub-id pub-id-type="pmid">6976818</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillyard</surname> <given-names>SA</given-names></name><name><surname>Hink</surname> <given-names>RF</given-names></name><name><surname>Schwent</surname> <given-names>VL</given-names></name><name><surname>Picton</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Electrical signs of selective attention in the human brain</article-title><source>Science</source><volume>182</volume><fpage>177</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1126/science.182.4108.177</pub-id><pub-id pub-id-type="pmid">4730062</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoormann</surname> <given-names>J</given-names></name><name><surname>Falkenstein</surname> <given-names>M</given-names></name><name><surname>Hohnsbein</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Early attention effects in human auditory-evoked potentials</article-title><source>Psychophysiology</source><volume>37</volume><fpage>29</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.3710029</pub-id><pub-id pub-id-type="pmid">10705765</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>H</given-names></name><name><surname>Pan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Speech pitch determination based on Hilbert-Huang transform</article-title><source>Signal Processing</source><volume>86</volume><fpage>792</fpage><lpage>803</lpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2005.06.011</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huffman</surname> <given-names>RF</given-names></name><name><surname>Henson</surname> <given-names>OW</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>The descending auditory pathway and acousticomotor systems: connections with the inferior colliculus</article-title><source>Brain Research Reviews</source><volume>15</volume><fpage>295</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1016/0165-0173(90)90005-9</pub-id><pub-id pub-id-type="pmid">2289088</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karns</surname> <given-names>CM</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Intermodal auditory, visual, and tactile attention modulates early stages of neural processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>669</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21037</pub-id><pub-id pub-id-type="pmid">18564047</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kujawa</surname> <given-names>SG</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Adding insult to injury: cochlear nerve degeneration after &quot;temporary&quot; noise-induced hearing loss</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>14077</fpage><lpage>14085</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2845-09.2009</pub-id><pub-id pub-id-type="pmid">19906956</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar Neupane</surname> <given-names>A</given-names></name><name><surname>Gururaj</surname> <given-names>K</given-names></name><name><surname>Mehta</surname> <given-names>G</given-names></name><name><surname>Sinha</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Effect of repetition rate on speech evoked auditory brainstem response in younger and middle aged individuals</article-title><source>Audiology Research</source><volume>4</volume><fpage>21</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.4081/audiores.2014.106</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lasky</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Rate and adaptation effects on the auditory evoked brainstem response in human newborns and adults</article-title><source>Hearing Research</source><volume>111</volume><fpage>165</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/S0378-5955(97)00106-8</pub-id><pub-id pub-id-type="pmid">9307322</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehmann</surname> <given-names>A</given-names></name><name><surname>Schönwiesner</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Selective attention modulates human auditory brainstem responses: relative contributions of frequency and spatial cues</article-title><source>PLoS One</source><volume>9</volume><elocation-id>e85442</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0085442</pub-id><pub-id pub-id-type="pmid">24454869</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maison</surname> <given-names>S</given-names></name><name><surname>Micheyl</surname> <given-names>C</given-names></name><name><surname>Collet</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Influence of focused auditory attention on cochlear activity in humans</article-title><source>Psychophysiology</source><volume>38</volume><fpage>35</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1111/1469-8986.3810035</pub-id><pub-id pub-id-type="pmid">11321619</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehraei</surname> <given-names>G</given-names></name><name><surname>Hickox</surname> <given-names>AE</given-names></name><name><surname>Bharadwaj</surname> <given-names>HM</given-names></name><name><surname>Goldberg</surname> <given-names>H</given-names></name><name><surname>Verhulst</surname> <given-names>S</given-names></name><name><surname>Liberman</surname> <given-names>MC</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>auditory brainstem response latency in noise as a marker of cochlear synaptopathy</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>3755</fpage><lpage>3764</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4460-15.2016</pub-id><pub-id pub-id-type="pmid">27030760</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musacchia</surname> <given-names>G</given-names></name><name><surname>Sams</surname> <given-names>M</given-names></name><name><surname>Skoe</surname> <given-names>E</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Musicians have enhanced subcortical auditory and audiovisual processing of speech and music</article-title><source>PNAS</source><volume>104</volume><fpage>15894</fpage><lpage>15898</lpage><pub-id pub-id-type="doi">10.1073/pnas.0701498104</pub-id><pub-id pub-id-type="pmid">17898180</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname> <given-names>R</given-names></name><name><surname>Tervaniemi</surname> <given-names>M</given-names></name><name><surname>Sussman</surname> <given-names>E</given-names></name><name><surname>Paavilainen</surname> <given-names>P</given-names></name><name><surname>Winkler</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>&quot;Primitive intelligence&quot; in the auditory cortex</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>283</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01790-2</pub-id><pub-id pub-id-type="pmid">11311381</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pickels</surname> <given-names>JO</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>An Introduction to the Physiology of Hearing</source><publisher-loc>Bingley</publisher-loc><publisher-name>Emerald Group Publishing</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picton</surname> <given-names>TW</given-names></name><name><surname>Stapells</surname> <given-names>DR</given-names></name><name><surname>Campbell</surname> <given-names>KB</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Auditory evoked potentials from the human cochlea and brainstem</article-title><source>The Journal of Otolaryngology. Supplement</source><volume>9</volume><fpage>1</fpage><lpage>41</lpage><pub-id pub-id-type="pmid">7026799</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saupe</surname> <given-names>K</given-names></name><name><surname>Widmann</surname> <given-names>A</given-names></name><name><surname>Bendixen</surname> <given-names>A</given-names></name><name><surname>Müller</surname> <given-names>MM</given-names></name><name><surname>Schröger</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Effects of intermodal attention on the auditory steady-state response and the event-related potential</article-title><source>Psychophysiology</source><volume>46</volume><fpage>321</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2008.00765.x</pub-id><pub-id pub-id-type="pmid">19207194</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaette</surname> <given-names>R</given-names></name><name><surname>McAlpine</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Tinnitus with a normal audiogram: physiological evidence for hidden hearing loss and computational model</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>13452</fpage><lpage>13457</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2156-11.2011</pub-id><pub-id pub-id-type="pmid">21940438</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Object-based auditory and visual attention</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>182</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.02.003</pub-id><pub-id pub-id-type="pmid">18396091</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skoe</surname> <given-names>E</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Auditory brain stem response to complex sounds: a tutorial</article-title><source>Ear and Hearing</source><volume>31</volume><fpage>302</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3181cdb272</pub-id><pub-id pub-id-type="pmid">20084007</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohmer</surname> <given-names>H</given-names></name><name><surname>Pratt</surname> <given-names>H</given-names></name><name><surname>Kinarti</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Sources of frequency following responses (FFR) in man</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>42</volume><fpage>656</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(77)90282-6</pub-id><pub-id pub-id-type="pmid">67025</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname> <given-names>JH</given-names></name><name><surname>Skoe</surname> <given-names>E</given-names></name><name><surname>Wong</surname> <given-names>PC</given-names></name><name><surname>Kraus</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Plasticity in the adult human auditory brainstem following short-term linguistic training</article-title><source>Journal of Cognitive Neuroscience</source><volume>20</volume><fpage>1892</fpage><lpage>1902</lpage><pub-id pub-id-type="doi">10.1162/jocn.2008.20131</pub-id><pub-id pub-id-type="pmid">18370594</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varghese</surname> <given-names>L</given-names></name><name><surname>Bharadwaj</surname> <given-names>HM</given-names></name><name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Evidence against attentional state modulating scalp-recorded auditory brainstem steady-state responses</article-title><source>Brain Research</source><volume>1626</volume><fpage>146</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2015.06.038</pub-id><pub-id pub-id-type="pmid">26187756</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winer</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Decoding the auditory corticofugal systems</article-title><source>Hearing Research</source><volume>212</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2005.06.014</pub-id><pub-id pub-id-type="pmid">16555378</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Womelsdorf</surname> <given-names>T</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The role of neuronal synchronization in selective attention</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>154</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.02.002</pub-id><pub-id pub-id-type="pmid">17306527</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname> <given-names>DL</given-names></name><name><surname>Alho</surname> <given-names>K</given-names></name><name><surname>Algazi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Intermodal selective attention. I. Effects on event-related potentials to lateralized auditory and visual stimuli</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>82</volume><fpage>341</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(92)90004-2</pub-id><pub-id pub-id-type="pmid">1374703</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.27203.008</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Reviewing Editor</role><aff><institution>Boston University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;The human auditory brainstem response to running speech reveals a subcortical mechanism for selective attention&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by Andrew King (Senior Editor) and three reviewers, one of whom, Barbara G Shinn-Cunningham (Reviewer #1), is a member of our Board of Reviewing Editors. The following individual involved in review of your submission has agreed to reveal their identity: Steve Aiken (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The results of this study are very intriguing, showing differences in auditory steady-state responses (ASSRs) that are specific to which of two speech streams a listener is attending. Although the correlations between the stimulus and response are small, the peak of the cross-correlation function was significant in most of the subjects, as was the effect of attention. The writing is generally clear and concise.</p><p>All three reviewers thought that the conclusion that brainstem-generated responses are modulated by attentional focus, if properly justified, is novel and noteworthy. While many aspects of this paper are intriguing, there are questions that affect the interpretation. These technical issues and concerns must be addressed adequately for the study to be acceptable for publication in <italic>eLife</italic>.</p><p>Essential revisions:</p><p>1) For any response like this, including the more-common FFR (with a steady-state constant-frequency acoustic signal), observations are a mixture of IC responses and other responses (perhaps in thalamus – but also lower responses such as the cochlear microphonic). For the same relative delays and magnitudes of the responses, these different responses will add in different phases, depending on their frequency. Unlike with the FFR, here, the frequency is changing from moment to moment. This will lead to different cancellation / summation at different frequencies that likely result in different peak delays. Only if there is a single truly dominant source in the mixture will the peak delay be at a fixed delay independent of frequency.</p><p>Attention effects in FFRs have been suggested to be due to the involvement of the cortex (the work of Emma Holmes presented at ARO 2017). While the delay of 10 ms relative to the stimulus calls into question a dominant role for the cortex in the present study (see above), it is also possible that the attention effect is mediated through olivocochlear inhibition.</p><p>The stats show that attention is changing the observed responses. The question is just where these responses are coming from. Given that the observed response is a mixture, further analysis is warranted to tease apart whether the effects are due to a single dominant source with a fixed delay that is modulated by attention, or whether higher-level sources, which are modulated by attention, cause different summation / cancellation effects depending on attentional focus.</p><p>2) The latency reported, 10.3ms, is greater than is usually attributed to a brainstem response. The latency of the largest peak of the click-evoked ABR (usually attributed to inferior colliculus) is usually assumed to be ~5ms or somewhat greater for lower-frequency stimuli (Don and Eggermont, 1978).</p><p>Often, the delay in an ASSR is longer (presumably in part because it is a mixture of neural sources as noted above). The authors say that their estimate agrees with that reported by Skoe and Kraus (2010) for speech, but the value reported in that paper is actually 7-8ms. <xref ref-type="fig" rid="fig1">Figure 1</xref> of that paper, where that value is mentioned, illustrates it as the amount by which the response <italic>precedes</italic> the stimulus (!), so that source itself may have some methodological problems.</p><p>Because the latency is the primary fact used to conclude that the induced attentional changes are from brainstem, this issue is very important to consider and discuss.</p><p>3) The latency is calculated as a cross-correlation between electrode signals and a &quot;fundamental waveform&quot; defined as a &quot;nonlinear oscillation&quot; derived from the speech by empirical mode decomposition (EMD). EMD is attractive but not very well defined or theoretically grounded; as far as we can tell, it just extracts an approximation of the fundamental Fourier component. The Hilbert transform is calculated, and both the waveform and its Hilbert transform are cross-correlated with the EEG to obtain a &quot;complex correlation function&quot;, the amplitude of which peaks at a latency of 10.3ms. The rationale for introducing this Hilbert component is not clear, as it would seem more straightforward to correlate simply with the speech waveform (or its &quot;fundamental waveform). The &quot;amplitude of the complex CC&quot; has a wider peak than the raw CC.</p><p>Please explain and justify the analysis more clearly.</p><p>4) An accurate estimate of latency is crucial for saying that the response reflects the brainstem. Temporal alignment between audio and EEG may be affected by acoustic delay in the earphones (not specified, possibly ~1ms for ER 3), as well as the signal processing of the inputs and of the brain measures.</p><p>Audio is down-sampled (interpolation filter unspecified), filtered by a FIR of order 296 (IR temporal extent 33.4 ms), time-shifted to &quot;compensate for delay&quot; of the FIR, processed by the EMD algorithm, and finally by the Hilbert transform. The Hilbert transform is presumably performed by applying an STFT to a window of unspecified duration. It involves a 90-degree phase shift that translates (for the quasi-sinusoidal fundamental wave) to a frequency-dependent time shift of up to 2.5ms at 100Hz.</p><p>On the EEG side, the signal is processed by a frequency-domain method (ClearLine) to attenuate 50Hz and (presumably) harmonics. The possibility that this might affect the fundamental waveform (its time-varying frequency falls in this range) is not discussed. The EEG is filtered by a cascade of FIR filters of order 6862 and 1054 (IR lengths 274ms and 42ms) before correlation with the audio-based signal. There are clearly many stages at which a latency mismatch could arise, and the fact that this is not acknowledged or addressed (for example by calibration) is troubling.</p><p>The peak value of the cross-correlation function shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, 0.05, seems rather high given that the ABR is supposed to have very low SNR. Similar values (0.05 – 0.1) have been reported for cross-correlation with filtered cortical responses that are supposed to have a much better SNR. The shape of the function is also somewhat intriguing because it is approximately symmetrical and extends to negative lag (i.e., there is a noncausal relationship between the input and the neural response). This suggests that it is largely determined by temporal smearing in the processing, for example due to convolution with the various unspecified filter kernels.</p><p>Please carefully outline how the acoustical and neural signal processing affects the estimated latency of the neural responses.</p><p>5) From the description of methods, it would seem that the stimulus is always presented with the same polarity. Electrode signals measured in that case are likely to be dominated by cochlear microphonic or possibly even cross-talk from the earphone drivers or cables. There is no reason to believe that they are from brainstem, except possibly latency (and as discussed above, it is unclear how good an indicator that is of brainstem activity).</p><p>Please include some calibration or analysis to verify that electrical artifact is not a significant factor in your findings.</p><p>6) While the speech presentation levels were relatively high, individual high-frequency harmonics would be relatively low in level given the low-pass characteristic of speech signals. Attentional modulation of cochlear gain has been shown to be frequency-specific (e.g., Maison et al., Psychophysiol 2001) and certainly could be specific to a harmonic complex.</p><p>It would be useful to include this point in the Discussion.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;The human auditory brainstem response to running speech reveals a subcortical mechanism for selective attention&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by Andrew King (Senior Editor) and three reviewers, one of whom, Barbara G Shinn-Cunningham (Reviewer #1), is a member of our Board of Reviewing Editors. The following individual involved in review of your submission has agreed to reveal their identity: Steve Aiken (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission. It is not normally <italic>eLife</italic> policy to allow a paper to go through multiple rounds of reviews, so this will be the last opportunity to revise the manuscript before a final decision is made.</p><p>Summary:</p><p>This paper provides evidence for attentional modulation in neural responses measured in response to running speech. The approach is relatively novel and the findings interesting.</p><p>The revision has gone some way to addressing the concerns raised by the reviewers. However, several questions still need to be answered. The reviewers also provide suggestions for how to strengthen the argument. The controls added here would definitely strengthen the paper.</p><p>Essential revisions:</p><p>1) The paper continues to over-emphasize that the measured responses are from the brainstem. The evidence shows a clear attentional effect; however, the claim that the observed effects are absolutely from the brainstem is still problematic. If cortical contributions cannot be ruled out, it would be presumptuous to conclude that this is an attentional effect in the brainstem.</p><p>The authors should make their case more persuasively. One approach to enhance this argument is to conduct further analysis of their present data. If cortical contributions are indeed responsible for the attentional effects, one would expect to find a positive correlation between peak latency and amplitude in the attended conditions. One might also expect such effects to be stronger for segments with lower fundamental frequencies (e.g., &lt; 200 Hz) given cortical phase-locking limits.</p><p>The authors should test for these possibilities. Of interest would be any relationship between peak cross-correlation latency and peak cross-correlation amplitude for attended streams, and between peak cross-correlation amplitude and segment fundamental frequency (also for attended vs. unattended streams). Such an analysis might lead to a more nuanced understanding of the data or conversely add weight to the conclusions.</p><p>2) The cross-correlation functions between speech and &quot;fundamental wave&quot; and between raw and processed EEG) address earlier concerns about the effect of processing on latency estimates. Still, an end-to-end calibration would be even more convincing. This does not require recording new data, but rather running simulated data through the processing pipeline: (a) formulate a simple speech-to-neural response model based on the conclusions the authors believe follow from their results, (b) add background EEG signal (e.g., from recorded data shifted in time), (c) run the analysis, (d) check whether latencies conform to what is expected based on the model. Given the importance of the timing of the responses to the argument that the effects are subcortical, the extra effort is worthwhile: the EEG processing pipeline involves many convolutional stages that the authors still do not fully characterize (are all filters zero-phase?).</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.27203.009</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) For any response like this, including the more-common FFR (with a steady-state constant-frequency acoustic signal), observations are a mixture of IC responses and other responses (perhaps in thalamus – but also lower responses such as the cochlear microphonic). For the same relative delays and magnitudes of the responses, these different responses will add in different phases, depending on their frequency. Unlike with the FFR, here, the frequency is changing from moment to moment. This will lead to different cancellation / summation at different frequencies that likely result in different peak delays. Only if there is a single truly dominant source in the mixture will the peak delay be at a fixed delay independent of frequency.</p><p>Attention effects in FFRs have been suggested to be due to the involvement of the cortex (the work of Emma Holmes presented at ARO 2017). While the delay of 10 ms relative to the stimulus calls into question a dominant role for the cortex in the present study (see above), it is also possible that the attention effect is mediated through olivocochlear inhibition.</p><p>The stats show that attention is changing the observed responses. The question is just where these responses are coming from. Given that the observed response is a mixture, further analysis is warranted to tease apart whether the effects are due to a single dominant source with a fixed delay that is modulated by attention, or whether higher-level sources, which are modulated by attention, cause different summation / cancellation effects depending on attentional focus.</p></disp-quote><p>We have now added additional controls for a potential stimulus artifact as well as for a contribution from the cochlear microphonic. In particular, we have measured the brainstem response to a speech signal as well as to the same signal with reversed polarity. The averaged neural response shows the same peak in its complex correlation to speech as the response to a single speech presentation. This evidences the absence of a stimulus artifact as well as of a measurable cochlear microphonics.</p><p>The brainstem response at the fundamental frequency of speech can indeed result from multiple sites in the brainstem. However, because we observe a single peak with a width of a few ms in the correlation of the neural response to the fundamental waveform of speech, the brainstem response to speech that we describe here cannot reflect sources whose latencies vary by more than a few ms from the mean latency. The latency of about 9 ms confirms the absence of a cortical contribution and hints at a neural origin in the inferior colliculus. The inferior colliculus is indeed connected to the auditory cortex through multiple segregated feedback loops, involving the olivocochlear efferent system, that likely mediate the attentional modulation that we report here.</p><p>These important issues are now discussed at different places in the revised manuscript, namely in the Results section, in a new paragraph in the Discussion section, and in three new paragraphs in the Materials and methods section. The additional data showing the control for a stimulus artifact and the cochlear microphonic is laid out in a new supplement to <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><disp-quote content-type="editor-comment"><p>2) The latency reported, 10.3ms, is greater than is usually attributed to a brainstem response. The latency of the largest peak of the click-evoked ABR (usually attributed to inferior colliculus) is usually assumed to be ~5ms or somewhat greater for lower-frequency stimuli (Don and Eggermont, 1978).</p><p>Often, the delay in an ASSR is longer (presumably in part because it is a mixture of neural sources as noted above). The authors say that their estimate agrees with that reported by Skoe and Kraus (2010) for speech, but the value reported in that paper is actually 7-8ms. <xref ref-type="fig" rid="fig1">Figure 1</xref> of that paper, where that value is mentioned, illustrates it as the amount by which the response precedes the stimulus (!), so that source itself may have some methodological problems.</p><p>Because the latency is the primary fact used to conclude that the induced attentional changes are from brainstem, this issue is very important to consider and discuss.</p></disp-quote><p><xref ref-type="fig" rid="fig1">Figure 1</xref> of the publication by Skoe and Kraus (2010) reports indeed a delay of 7-8 ms. However, this is only one example of a measured response, and the authors state in the same publication that the measured latencies occur at a delay of 6 – 10 ms (page 15). As pointed out by the reviewers below, the earphones that we have employed introduce a delay of 1 ms that we had not accounted for earlier. We have now corrected our measurements for this delay, and the latencies that we obtain are now on average 9.3 ms, which is consistent with the results summarized in the review by Skoe and Kraus (2010). We have modified the manuscript and Discussion to reflect this important point.</p><p>Regarding the latency reported in <xref ref-type="fig" rid="fig1">Figure 1</xref> of the publication by Skoe and Kraus (2010), the acoustic stimulus has been shifted to a later time by 7 – 8 ms with respect to the neural response to maximize the visual coherence between both signals. The neural response reported in this Figure therefore occurs 7 – 8 ms <italic>after</italic> the acoustic stimulus.</p><disp-quote content-type="editor-comment"><p>3) The latency is calculated as a cross-correlation between electrode signals and a &quot;fundamental waveform&quot; defined as a &quot;nonlinear oscillation&quot; derived from the speech by empirical mode decomposition (EMD). EMD is attractive but not very well defined or theoretically grounded; as far as we can tell, it just extracts an approximation of the fundamental Fourier component. The Hilbert transform is calculated, and both the waveform and its Hilbert transform are cross-correlated with the EEG to obtain a &quot;complex correlation function&quot;, the amplitude of which peaks at a latency of 10.3ms. The rationale for introducing this Hilbert component is not clear, as it would seem more straightforward to correlate simply with the speech waveform (or its &quot;fundamental waveform). The &quot;amplitude of the complex CC&quot; has a wider peak than the raw CC.</p><p>Please explain and justify the analysis more clearly.</p></disp-quote><p>The fundamental frequency of running speech varies with time. Moreover, the amplitude of the voiced parts of speech also varies greatly, between zero for voiceless speech and a maximal value for voiced speech. While these variations may be ignored when investigating responses to short repeating stimuli, where the frequency fluctuations are limited due to the short duration and the amplitude may be approximately constant, the variations cannot be ignored in running speech where the fundamental frequency may vary over an octave and the amplitude varies widely. Fourier analysis decomposes a signal into sinusoidal oscillations at different, but constant, frequency and amplitude. A single component extracted from Fourier analysis can thus not represent the fundamental waveform. Furthermore, classical time-frequency analysis methods such as the Short Time Fourier Transform and wavelet-based methods are not well suited for this task either due to their inherently limited time-frequency resolution (Huang and Pan 2006). EMD, in contrast, decomposes a signal into empirical modes that are nonlinear and non-stationary oscillations with time-varying frequency and amplitude with no inherent limitation to time-frequency resolution. A previous study by Huang and Pan (2006) has shown that one of these modes oscillates at the fundamental frequency. We therefore identify this mode with the fundamental waveform and employ EMD to extract it. As the reviewers state, a precise theoretical grounding of EMD is still lacking, but it is a well-defined constructive procedure that is increasingly used for analyzing nonlinear and non-stationary oscillations. We now comment further on this issue in the Discussion section of our revised manuscript.</p><p>As the reviewers remark, the correlation of the neural response to the fundamental waveform has a narrower peak than the amplitude of the complex correlation. However, this narrower correlation is partly due to phase mismatch, and not to a lower correlation per se. Indeed, the negative correlation at a few ms before and after the peak shows that the brainstem response is then in antiphase, but still correlated to, the fundamental waveform. More generally, the brainstem response can occur at an arbitrary phase with respect to the fundamental waveform. We therefore compute the correlation of the neural signal to the Hilbert transform of the fundamental waveform as well. The Hilbert transform is a version of the fundamental waveform that is phase shifted by 90˚. By interpreting both correlations as the real and imaginary part of a complex correlation, we can track a brainstem response at an arbitrary phase delay. This analysis is reminiscent of Fourier analysis at a particular frequency where the correlation of a signal to a cosine and a sine function are interpreted as real and imaginary part of a complex Fourier coefficient, the phase of which represents the phase delay of the signal with respect to the cosine function. We now explain this important point in the Results section.</p><disp-quote content-type="editor-comment"><p>4) An accurate estimate of latency is crucial for saying that the response reflects the brainstem. Temporal alignment between audio and EEG may be affected by acoustic delay in the earphones (not specified, possibly ~1ms for ER 3), as well as the signal processing of the inputs and of the brain measures.</p><p>Audio is down-sampled (interpolation filter unspecified), filtered by a FIR of order 296 (IR temporal extent 33.4 ms), time-shifted to &quot;compensate for delay&quot; of the FIR, processed by the EMD algorithm, and finally by the Hilbert transform. The Hilbert transform is presumably performed by applying an STFT to a window of unspecified duration. It involves a 90-degree phase shift that translates (for the quasi-sinusoidal fundamental wave) to a frequency-dependent time shift of up to 2.5ms at 100Hz.</p><p>On the EEG side, the signal is processed by a frequency-domain method (ClearLine) to attenuate 50Hz and (presumably) harmonics. The possibility that this might affect the fundamental waveform (its time-varying frequency falls in this range) is not discussed. The EEG is filtered by a cascade of FIR filters of order 6862 and 1054 (IR lengths 274ms and 42ms) before correlation with the audio-based signal. There are clearly many stages at which a latency mismatch could arise, and the fact that this is not acknowledged or addressed (for example by calibration) is troubling.</p><p>The peak value of the cross-correlation function shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, 0.05, seems rather high given that the ABR is supposed to have very low SNR. Similar values (0.05 – 0.1) have been reported for cross-correlation with filtered cortical responses that are supposed to have a much better SNR. The shape of the function is also somewhat intriguing because it is approximately symmetrical and extends to negative lag (i.e., there is a noncausal relationship between the input and the neural response). This suggests that it is largely determined by temporal smearing in the processing, for example due to convolution with the various unspecified filter kernels.</p><p>Please carefully outline how the acoustical and neural signal processing affects the estimated latency of the neural responses.</p></disp-quote><p>We are grateful to the reviewers for pointing out the acoustic delay in the earphones. Manufacture specifications state that the earphones indeed introduce a delay of 1 ms. As explained above, we have now adjusted for this delay, and the average delay of the brainstem response to speech that we have measured is now 9.3 ms. The figures and the text of our revised manuscript have been modified accordingly.</p><p>We have taken care that the various steps involved in the processing of the acoustic and the neural signals do not affect the latencies. In particular, the down-sampling of the audio signal was carried out using the Matlab resample function, which resamples an input sequence by applying an antialiasing FIR lowpass filter and compensating for the delay introduced by the filter. The Hilbert transform did not involve a STFT, and the other frequency filters were time-compensated to not produce a latency shift either.</p><p>Due to the importance of potential latency shifts introduced by signal processing, we have investigated this issue further through computing cross-correlations of the processed and the unprocessed signals. The obtained data are presented in the novel supplement to <xref ref-type="fig" rid="fig1">Figure 1</xref>, and are discussed in the Results section as well as in two new paragraphs in the Materials and methods section. Our analysis shows that there is no latency shift between the computed fundamental waveform and the original speech signal, and neither is there a latency shift between the processed and the original neural recording.</p><p>The brainstem response is indeed much weaker than cortical responses. We therefore require long recordings of 10 minutes in duration to achieve a good SNR. Nonetheless, the average of the correlation across the different subjects is only 0.015 ± 0.003 as we now report in the manuscript. Moreover, we employed passive electrodes in combination with a specialized bipolar preamplifier with a very high common-mode rejection; this helps to achieve a high SNR. Measurements of cortical responses often employ active electrodes that have a lower SNR than passive electrodes as well as a main amplifier with a lower common-mode rejection than our specialized bipolar preamplifier, which decreases the SNR of the so-obtained cortical responses further.</p><p>As the reviewers remark, the peak of the correlation is approximately symmetric and extends to negative delays. The shape of the peak reflects indeed the autocorrelation of the speech signal (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) which is symmetric. The peak of the complex correlation of the neural response to the stimulus is wider than that of the autocorrelation since it is not only affected by the self-similarity of the fundamental waveform at short times, but also by that of the neural response as well as by noise. The causality between the acoustic signal and the neural response is established by the positive delay of the peak of the correlation. The extension of the lower-delay part of this peak to negative times does, in contrast, not reflect a violation of causality. Indeed, the peak in the autocorrelation of the fundamental waveform extends to negative times as well but it is clearly the lag at the peak (0 ms) that determines the casual relationship.</p><disp-quote content-type="editor-comment"><p>5) From the description of methods, it would seem that the stimulus is always presented with the same polarity. Electrode signals measured in that case are likely to be dominated by cochlear microphonic or possibly even cross-talk from the earphone drivers or cables. There is no reason to believe that they are from brainstem, except possibly latency (and as discussed above, it is unclear how good an indicator that is of brainstem activity).</p><p>Please include some calibration or analysis to verify that electrical artifact is not a significant factor in your findings.</p></disp-quote><p>We agree that special care needs to be taken to ensure that the measured response is free from stimulus artifacts.</p><p>The influence of stimulus artifacts or of the cochlear microphonics is unlikely since both would manifest at zero latency where we do not observe a measurable response. However, we have now carried out two additional tests as suggested by the reviewers. The results are reported in a new supplement to <xref ref-type="fig" rid="fig1">Figure 1</xref> and are discussed in the Results section as well as in a new paragraph in the Materials and methods section.</p><p>First, we have recorded brainstem responses when the earphones were not inside the ear canal of a subject but close to the ear, so that the subject could not hear the stimulus. We do then not obtain a measurable brainstem response to speech, evidencing the absence of stimulus artifacts. Second, we have played the same story to a subject twice, the second time with inverted polarity. We have then averaged the neural response to both stimulus presentations and performed the correlation analysis on the average. This procedure removes putative stimulus artifacts as well as the cochlear microphonic. We obtain the same correlation as when we analyze the response to a single stimulus, demonstrating that the response reflects neither a stimulus artifact nor the cochlear microphonic.</p><disp-quote content-type="editor-comment"><p>6) While the speech presentation levels were relatively high, individual high-frequency harmonics would be relatively low in level given the low-pass characteristic of speech signals. Attentional modulation of cochlear gain has been shown to be frequency-specific (e.g., Maison et al., Psychophysiol 2001) and certainly could be specific to a harmonic complex.</p><p>It would be useful to include this point in the Discussion.</p></disp-quote><p>The response at the fundamental frequency that we have measured can indeed result from higher harmonics as we had already discussed in our manuscript. As the reviewer remarks, attentional modulation may depend on the frequency of the harmonics. We have now included this valuable point in the Discussion section of our revised manuscript.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The paper continues to over-emphasize that the measured responses are from the brainstem. The evidence shows a clear attentional effect; however, the claim that the observed effects are absolutely from the brainstem is still problematic. If cortical contributions cannot be ruled out, it would be presumptuous to conclude that this is an attentional effect in the brainstem.</p><p>The authors should make their case more persuasively. One approach to enhance this argument is to conduct further analysis of their present data. If cortical contributions are indeed responsible for the attentional effects, one would expect to find a positive correlation between peak latency and amplitude in the attended conditions. One might also expect such effects to be stronger for segments with lower fundamental frequencies (e.g., &lt; 200 Hz) given cortical phase-locking limits.</p><p>The authors should test for these possibilities. Of interest would be any relationship between peak cross-correlation latency and peak cross-correlation amplitude for attended streams, and between peak cross-correlation amplitude and segment fundamental frequency (also for attended vs. unattended streams). Such an analysis might lead to a more nuanced understanding of the data or conversely add weight to the conclusions.</p></disp-quote><p>Cortical contributions can be ruled out due to their longer latency. Hashimoto et al. (1981), for instance, employed recordings in neurosurgical patients from different parts of the brainstem as well as the cortex during sound stimulation. They found that the cortex did not contribute to any scalp-recorded potential with a delay of 10 ms or less. Picton et al. (1981) found that any auditory-evoked scalp-recorded potential with a latency of 15 ms or less originates from the cochlea or the brainstem. The neural response to the fundamental frequency of speech that we measure here has a latency of 9.3 ms which shows that this response does not contain a cortical contribution.</p><p>Our obtained latency is now further validated through a toy model of the brainstem response that we have included in our revised manuscript as set out below. Moreover, we have now included the new panel (E) to <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> that shows the correlation between the neural signal and the complex fundamental waveform up to a latency of 700 ms, evidencing that there is no peak except for the one at 9 ms, and therefore no measurable contribution from the cerebral cortex. We now also point out that we assess the brainstem response through the amplitude of the complex cross-correlation at the peak amplitude. Even if there was a small cortical contribution to the neural response it would occur at a higher latency and would thus not affect our results regarding attention.</p><p>We now make this case more persuasively in our revised manuscript and cite the two important references described above. We now write:</p><p>&quot;The peak occurred at a mean latency of 9.3 ± 0.7 ms, verifying that the measured neural response resulted from the brainstem and not from the cerebral cortex (Hashimoto et al. 1981; Picton et al. 1981).&quot;</p><p>We have thereby added the new references to Hashimoto et al. (1981) as well as to Picton et al. (1981).</p><p>We have also followed the valuable suggestion to investigate correlations between amplitude and latency of the brainstem response. We find, however, no statistically significant correlation. In particular, we have analysed the correlation between amplitude and latency of short segments of the recordings and of the corresponding speech signal. We have performed this analysis for several conditions, namely for the brainstem response to a single speaker as well as for the neural response to both the attended and the ignored speaker of the two-speaker stimulus. For each speech stimulus we have also determined the parts with a fundamental frequency that is lower than the mean frequency, as well as the parts with a fundamental frequency above the mean, and considered each condition separately. However, in neither condition did we find a statistically significant correlation between the amplitude and the latency.</p><p>We would like to point out that this does not necessarily mean that there is no correlation between amplitude and latency, but only that we were not able to find such a correlation from our data. Indeed, a recent study by Mehraei et al. (J. Neurosci. 2016), finds a correlation between amplitude and latency of wave V of the brainstem response to clicks in noise when the noise level is varied. This correlation presumably arises from the varying contribution of auditory-nerve fibers with different thresholds and from different parts of the cochlea. Since speech contains varying contributions other than the fundamental waveform, it is therefore conceivable that a similar correlation exists in the brainstem response to continuous speech that we describe, but that the effect is too small to emerge from our data. On the contrary, this implies that even if such an effect was found, it alone would not allow to differentiate between brainstem and cortical contributions to the observed response.</p><p>We have also followed the important suggestion to investigate a correlation between the amplitude of the brainstem response and the fundamental frequency of the speech signal. We have done this through considering short speech segments and the corresponding neural data. This analysis has been performed for the brainstem response to a single speaker as well as for that to the attended and the ignored speaker of the two-speaker stimulus. We find small negative and statistically significant correlations between amplitude and fundamental frequency in all conditions. This concurs with previous results that have shown a low-pass nature of the brainstem response (e.g. Musacchia, et al. (2007)). We find, however, no statistically significant difference between the correlation coefficients in the different conditions. In particular, the correlation obtained from the attended speaker is not significantly larger than that obtained from the ignored speaker.</p><p>We now describe these findings in detail in our revised manuscript, and are convinced that they will trigger further studies into the important issues of the precise neural mechanisms that govern the brainstem response to continuous speech.</p><p>In particular, we have added the new <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> in which we show the correlation between the amplitude of the brainstem response and the fundamental frequency of the speech signal.</p><p>In the Results section, we write:</p><p>&quot;The auditory brainstem response to short speech stimuli has a low-pass nature: the amplitude of the response declines with increasing frequency (Musacchia et al. 2007; Skoe &amp; Kraus 2010). […] However, for the brainstem response to continuous speech that we measured here, we did not find a statistically-significant correlation between amplitude and latency (Materials and methods).”</p><p>In the Materials and methods section, we write:</p><p>&quot;We assessed the correlation between the amplitude and the frequency of the fundamental waveform by dividing each speech signal into 160 segments (3 s duration each), and by dividing the corresponding neural recording analogously. […] However, none of these correlations were statistically significant (two-tailed Student's t-test, p &gt; 0.05).”</p><disp-quote content-type="editor-comment"><p>2) The cross-correlation functions between speech and &quot;fundamental wave&quot; and between raw and processed EEG) address earlier concerns about the effect of processing on latency estimates. Still, an end-to-end calibration would be even more convincing. This does not require recording new data, but rather running simulated data through the processing pipeline: (a) formulate a simple speech-to-neural response model based on the conclusions the authors believe follow from their results, (b) add background EEG signal (e.g., from recorded data shifted in time), (c) run the analysis, (d) check whether latencies conform to what is expected based on the model. Given the importance of the timing of the responses to the argument that the effects are subcortical, the extra effort is worthwhile: the EEG processing pipeline involves many convolutional stages that the authors still do not fully characterize (are all filters zero-phase?).</p></disp-quote><p>We agree that a simulated brainstem response, processed through our pipeline and analysed with our methodology, will strengthen our argument in two ways. First, it will show that our methodology is able to accurately extract both phase shift <italic>and</italic> time delay of the brainstem signal. Second, it will provide a verification that our processing, such as through the involved filtering, does not introduce an additional time delay.</p><p>We have therefore followed this very valuable suggestion and considered a toy model for the brainstem response. In a highly simplistic fashion, the brainstem response is thereby modelled as a series of bursts of spikes. Each cycle of the fundamental waveform triggers a burst that occurs at a fixed phase of the fundamental waveform. All bursts are then shifted by a certain fixed time delay. We further added noise that is realistic for scalp recordings, and then analysed the resulting modelled signal through our processing methodology. We then process the simulated response through our signal-processing pipeline. To clarify, all the filters that we used to process the data are linear phase FIR filter, and we systematically compensated for their delay.</p><p>We find a complex correlation that performs as expected: the peak of the amplitude occurs at the time delay of the modelled response, and the phase yields the phase of the fundamental waveform at which the bursts occur. This verifies that our methodology is indeed able to compute both phase shift <italic>and</italic> time delay of the brainstem response. It also verifies that the processing that we employ for the neural recording does not yield an additional time delay.</p><p>We now describe this simplistic model and the important findings that follow from it in our revised manuscript. In particular, in the Results section, we now write:</p><p>&quot;We further considered a highly simplistic model of the auditory brainstem response in which a burst of neural spikes occurred at each cycle of the fundamental waveform, at a fixed phase φ, and was then shifted in time by a certain delay τ (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>; Materials and methods). […] This demonstrated that the brainstem's response to continuous speech could be reliably extracted through the developed method.&quot;</p><p>In the Materials and methods section, we elaborate further:</p><p>&quot;To further verify our analysis we considered a highly simplistic model of the brainstem response. […] This confirmed the validity of our methodology as well as that our processing of the neural data did not alter the temporal delay.”</p><p>Our simplistic brainstem response, together with the resulting complex correlation to the fundamental waveform, is illustrated in the new <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><p>We now also clarify in our revised manuscript that we employed linear phase FIR filter, and that we systematically compensated for their delay (subsection “Computation of the fundamental waveform of speech”, second paragraph and subsection “Analysis of the auditory-brainstem response”, first paragraph).</p></body></sub-article></article>