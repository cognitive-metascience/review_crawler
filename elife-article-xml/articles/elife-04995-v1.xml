<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">04995</article-id><article-id pub-id-type="doi">10.7554/eLife.04995</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Auditory selective attention is enhanced by a task-irrelevant temporally coherent visual stimulus in human listeners</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-19568"><name><surname>Maddox</surname><given-names>Ross K</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2668-0238</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-20184"><name><surname>Atilgan</surname><given-names>Huriye</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8582-4815</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-20185"><name><surname>Bizley</surname><given-names>Jennifer K</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6605-2362</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-20186"><name><surname>Lee</surname><given-names>Adrian KC</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7611-0500</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Institute for Learning and Brain Sciences</institution>, <institution>University of Washington</institution>, <addr-line><named-content content-type="city">Seattle</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Ear Institute</institution>, <institution>University College London</institution>, <addr-line><named-content content-type="city">London</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Speech and Hearing Sciences</institution>, <institution>University of Washington</institution>, <addr-line><named-content content-type="city">Seattle</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Culham</surname><given-names>Jody C</given-names></name><role>Reviewing editor</role><aff><institution>University of Western Ontario</institution>, <country>Canada</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>akclee@uw.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>05</day><month>02</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e04995</elocation-id><history><date date-type="received"><day>02</day><month>10</month><year>2014</year></date><date date-type="accepted"><day>27</day><month>12</month><year>2014</year></date></history><permissions><copyright-statement>© 2014, Maddox et al</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Maddox et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-04995-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.04995.001</object-id><p>In noisy settings, listening is aided by correlated dynamic visual cues gleaned from a talker's face—an improvement often attributed to visually reinforced linguistic information. In this study, we aimed to test the effect of audio–visual temporal coherence alone on selective listening, free of linguistic confounds. We presented listeners with competing auditory streams whose amplitude varied independently and a visual stimulus with varying radius, while manipulating the cross-modal temporal relationships. Performance improved when the auditory target's timecourse matched that of the visual stimulus. The fact that the coherence was between task-irrelevant stimulus features suggests that the observed improvement stemmed from the integration of auditory and visual streams into cross-modal objects, enabling listeners to better attend the target. These findings suggest that in everyday conditions, where listeners can often see the source of a sound, temporal cues provided by vision can help listeners to select one sound source from a mixture.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.001">http://dx.doi.org/10.7554/eLife.04995.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.04995.002</object-id><title>eLife digest</title><p>In the noisy din of a cocktail party, there are many sources of sound that compete for our attention. Even so, we can easily block out the noise and focus on a conversation, especially when we are talking to someone in front of us.</p><p>This is possible in part because our sensory system combines inputs from our senses. Scientists have proposed that our perception is stronger when we can hear and see something at the same time, as opposed to just being able to hear it. For example, if we tried to talk to someone on a phone during a cocktail party, the background noise would probably drown out the conversation. However, when we can see the person we are talking to, it is easier to hold a conversation.</p><p>Maddox et al. have now explored this phenomenon in experiments that involved human subjects listening to an audio stream that was masked by background sound. While listening, the subjects also watched completely irrelevant videos that moved in sync with either the audio stream or with the background sound. The subjects then had to perform a task that involved pushing a button when they heard random changes (such as subtle changes in tone or pitch) in the audio stream.</p><p>The experiment showed that the subjects performed well when they saw a video that was in sync with the audio stream. However, their performance dropped when the video was in sync with the background sound. This suggests that when we hold a conversation during a noisy cocktail party, seeing the other person's face move as they talk creates a combined audio–visual impression of that person, helping us separate what they are saying from all the noise in the background. However, if we turn to look at other guests, we become distracted and the conversation may become lost.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.002">http://dx.doi.org/10.7554/eLife.04995.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>auditory-visual integration</kwd><kwd>auditory scene analysis</kwd><kwd>multisensory</kwd><kwd>selective attention</kwd><kwd>temporal coherence</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01DC013260</award-id><principal-award-recipient><name><surname>Lee</surname><given-names>Adrian KC</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>WT098418MA</award-id><principal-award-recipient><name><surname>Bizley</surname><given-names>Jennifer K</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000703</institution-id><institution>Action on Hearing Loss</institution></institution-wrap></funding-source><award-id>596:UEI:JB</award-id><principal-award-recipient><name><surname>Atilgan</surname><given-names>Huriye</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100002046</institution-id><institution>Hearing Health Foundation</institution></institution-wrap></funding-source><award-id>Emerging Research Grant</award-id><principal-award-recipient><name><surname>Maddox</surname><given-names>Ross K</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>International Exchanges Scheme</award-id><principal-award-recipient><name><surname>Bizley</surname><given-names>Jennifer K</given-names></name><name><surname>Lee</surname><given-names>Adrian KC</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.0</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Faced with multiple sources of sound, humans can better perceive all of a target sound's features when one of those features changes in time with a visual stimulus.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A key challenge faced by the auditory system is to appropriately segregate and group sound elements into their component sources. This process of auditory scene analysis underlies our ability to listen to one sound while ignoring others and is crucial for everyday listening. While psychophysical studies have focused on the perceptual ‘rules’ that govern the likelihood of elements being grouped into single objects in audition (<xref ref-type="bibr" rid="bib11">Bregman, 1990</xref>; <xref ref-type="bibr" rid="bib6">Bizley and Cohen, 2013</xref>) or in vision (<xref ref-type="bibr" rid="bib27">Marr, 1982</xref>; <xref ref-type="bibr" rid="bib23">Lee and Yuille, 2006</xref>), perception is a seamlessly multisensory process whereby sensory information is integrated both within and across different sensory modalities. It is of great interest to understand how the formation of cross-modal objects might influence perception, and in particular, whether visual information may provide additional cues that listeners can use to facilitate auditory scene segregation through the generation of auditory-visual objects. To date, there has yet to be a study utilizing ongoing, continuous stimuli that demonstrates a perceptual benefit of an uninformative visual stimulus when selectively attending to one auditory stimulus in a mixture.</p><p>Many previous studies have focused on audio–visual speech processing. When listening in a noisy setting, watching a talker's face drastically improves speech intelligibility (<xref ref-type="bibr" rid="bib5">Binnie, 1973</xref>; <xref ref-type="bibr" rid="bib3">Bernstein and Grant, 2009</xref>) and this benefit is particularly apparent under difficult listening conditions, such as when speech is masked by spatially coincident competing speech (<xref ref-type="bibr" rid="bib22">Helfer and Freyman, 2005</xref>). Much work has focused on the role that visual information plays in contributing linguistic information—for example, the unvoiced consonants /p/, /t/, and /k/ can be disambiguated based upon mouth movements, a fact which underlies the McGurk effect whereby conflicting acoustic and visual information create an intermediary percept, changing which syllable a listener hears (<xref ref-type="bibr" rid="bib29">McGurk and MacDonald, 1976</xref>). Such benefits are specific to speech signals, however, leaving questions about the general principles that govern multisensory integration and its perceptual benefits.</p><p>In the case of selective listening, visual timing cues may provide important listening benefits. When performing simple tone-in-noise detection, thresholds improve when the timing of the potential tone is unambiguous (<xref ref-type="bibr" rid="bib48">Watson and Nichols, 1976</xref>). Visual speech cues, which precede the auditory signal (<xref ref-type="bibr" rid="bib12">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib43">Stevenson et al., 2012</xref>), provide helpful information about when to listen (<xref ref-type="bibr" rid="bib19">Grant and Seitz, 2000</xref>; <xref ref-type="bibr" rid="bib20">Grant, 2001</xref>; <xref ref-type="bibr" rid="bib4">Bernstein et al., 2004</xref>) and may help target speech to be separated from other competing speech (<xref ref-type="bibr" rid="bib44">Summerfield, 1992</xref>; <xref ref-type="bibr" rid="bib15">Devergie et al., 2011</xref>). In addition to information about when to listen, timing may be important for another reason: temporal coherence may facilitate binding of auditory and visual stimuli into single cross-modal objects. However, previous studies of audio–visual binding—the integration of auditory and visual stimuli into a single percept—have focused on judgments of coherence or simultaneity (<xref ref-type="bibr" rid="bib35">Recanzone, 2003</xref>; <xref ref-type="bibr" rid="bib41">Spence and Squire, 2003</xref>; <xref ref-type="bibr" rid="bib17">Fujisaki and Nishida, 2005</xref>; <xref ref-type="bibr" rid="bib14">Denison et al., 2013</xref>), rather than on the perceptual enhancements provided by that binding.</p><p>Past studies employing transient stimuli have shown a number of ways in which task-irrelevant stimuli in one modality can affect perception of another modality. A sound can affect the perceived number of visual stimuli (<xref ref-type="bibr" rid="bib38">Shams et al., 2000</xref>, <xref ref-type="bibr" rid="bib39">2002</xref>; <xref ref-type="bibr" rid="bib8">Bizley et al., 2012</xref>), the color (<xref ref-type="bibr" rid="bib31">Mishra et al., 2013</xref>), and the direction of visual motion (<xref ref-type="bibr" rid="bib16">Freeman and Driver, 2008</xref>). A task-irrelevant sound can also increase detection of a visual stimulus (<xref ref-type="bibr" rid="bib28">McDonald et al., 2000</xref>) by affecting the bias and sensitivity, as well as alter the perceived visual intensity (<xref ref-type="bibr" rid="bib42">Stein et al., 1996</xref>; <xref ref-type="bibr" rid="bib33">Odgaard et al., 2003</xref>). Similarly, an irrelevant visual stimulus can affect the detection of an auditory stimulus (<xref ref-type="bibr" rid="bib24">Lovelace et al., 2003</xref>) as well as the perceived loudness (<xref ref-type="bibr" rid="bib34">Odgaard et al., 2004</xref>). These effects could all conceivably help when segregating streams, but none of these studies demonstrated such benefits.</p><p>In this study, we developed a novel paradigm that was designed to test whether cross-modal temporal coherence was sufficient to promote the segregation of two competing sound sources. To achieve this, we created audio–visual stimuli with dynamic, continuous noise envelopes. We chose to manipulate temporal coherence in an ethologically relevant modulation frequency range (<xref ref-type="bibr" rid="bib12">Chandrasekaran et al., 2009</xref>), amplitude modulating our stimuli with a noisy envelope low-pass filtered at 7 Hz. Listeners were asked to perform an auditory selective attention task that required them to report brief perturbations in the pitch or the timbre of one of two competing streams. Additionally, a concurrent visual stream was presented where the radius of the visual stimulus could vary coherently with the amplitude changes either of the target or the masker, or be independent of both. We hypothesized that modulating the visual stimulus coherently with one of the auditory streams would cause these stimuli to automatically bind together, with a consequent improvement in performance when the visual stimulus was coherent with the target auditory stream. Importantly, any perceptual benefit could only result from the temporal coherence between the auditory and visual stimulus, as the visual stimulus itself provided no additional information about the timing of the target auditory perturbations.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>All methods were approved by the Institutional Review Board of the University of Washington and the Ethics Committee of the University College London (ref: 5139). We sought to measure a behavioral benefit that could be ascribed directly to the temporal coherence between auditory and visual streams: specifically, whether coherence between a visual stimulus and a task-irrelevant auditory feature could improve performance in an auditory selective attention task. Importantly, the visual stimulus was uninformative of the auditory task, such that any behavioral benefit could be unambiguously determined to be the result of selective enhancement of an auditory stream due to binding with the visual stimulus. This cross-feature enhancement thus served simultaneously as our assay of audio–visual binding (eschewing subjective judgments) and an investigation of binding's impact on auditory scene analysis.</p><p>We designed a task that required selectively attending to one of two competing auditory streams and manipulated the coherence of the visual stimulus relative to that of the target and masker auditory streams. The temporal dynamics of each stream were defined by low-pass noise envelopes with a 7 Hz cutoff frequency, roughly approximating the modulation frequency range of speech amplitude envelopes and visual mouth movements (<xref ref-type="bibr" rid="bib12">Chandrasekaran et al., 2009</xref>). In each trial of 14 s duration there were two auditory streams (one target, one masker; <xref ref-type="fig" rid="fig1">Figure 1A</xref>) that were amplitude-modulated by independent tokens of the noise envelope and one visual stream: a gray disc surrounded by a white ring whose radius was also modulated by a noise envelope (envelope in <xref ref-type="fig" rid="fig1">Figure 1B</xref>; images of disc in <xref ref-type="fig" rid="fig1">Figure 1C</xref>). The visual radius envelope could match the amplitude envelope of the target auditory stream (example stimuli in <xref ref-type="other" rid="media1">Video 1</xref>) or masker auditory stream (<xref ref-type="other" rid="media2">Video 2</xref>), or be independent from both (<xref ref-type="other" rid="media3">Video 3</xref>). This audio–visual coherence (specifically which, if either, auditory stream's amplitude envelope was matched by the visual radius) defined the three experimental conditions, henceforth referred to as match-target, match-masker, and match-neither, respectively (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The goal of the subject was to respond by pressing a button to brief perturbation events in the target stream and ignore events in the masker stream. Importantly, the task-relevant auditory feature was not the same feature that was coherent with the visual stimulus. A response to an event in the target stream would be deemed a ‘hit’ and a response to a masker event would be deemed a ‘false alarm’, allowing us to calculate <italic>d′</italic> sensitivity for each of the three conditions. Subjects were also required to respond to brief color change flashes in the visual stimulus (<xref ref-type="fig" rid="fig1">Figure 1F</xref>)—this task was not difficult (overall hit rate was 87.7%), but ensured attentiveness to the visual stream. We reason that an improvement in behavioral performance in the match-target condition over the match-masker condition could only result from a benefit in separating the target and masker stream and thus be indicative of true, ongoing cross-modal binding, beyond a simple subjective report.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.04995.003</object-id><label>Figure 1.</label><caption><title>Construction of the auditory and visual stimuli.</title><p>(<bold>A</bold>) Amplitude envelopes shown for 2 s of the target (black) and masker (red) auditory streams. Trials were 14 s long, over which the target and masker envelopes were independent. (<bold>B</bold>) Visual radius envelopes for the three audio–visual coherence conditions: match-target (black), match-masker (red), and match-neither (blue). (<bold>C</bold>) Example frames of the visual disc at three radius values, according to the match-target envelope in <bold>B</bold>. (<bold>D</bold>) Carrier frequency modulation events for the pitch task. Deflection was one period of a sinusoid, reaching ±1.5 semitones over 100 ms. (<bold>E</bold>) Changes in vowel formants F1 and F2 for the timbre events. There were two streams, one with vowel /u/ and the other with vowel /a/. Timbre events lasted 200 ms and morphed formants F1 and F2 slightly toward /ε/ and /i/, respectively, and then back to /u/ and /a/. The closed circle endpoints show the steady-state vowel and the open circle point shows the average reversal point across subjects. Note that the change in formats during the morph event was small compared to the distance between vowels in the F1–F2 space. (<bold>F</bold>) The visual stimulus during a flash (100 ms duration).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.003">http://dx.doi.org/10.7554/eLife.04995.003</ext-link></p></caption><graphic xlink:href="elife-04995-fig1-v1.tif"/></fig><media content-type="glencoe play-in-place height-250 width-310" id="media1" mime-subtype="mp4" mimetype="video" xlink:href="elife-04995-media1.mp4"><object-id pub-id-type="doi">10.7554/eLife.04995.004</object-id><label>Video 1.</label><caption><title>Two example trials from the match-target condition.</title><p>The video shows two trials from the pitch task in which the target auditory stream is coherent with the visual stimulus. The target auditory stream starts 1 s before the masker stream (lower pitch in the first trial, higher pitch in the second). The task is to respond by pressing a button to brief pitch perturbations in the target auditory stream but not the masker auditory stream, as well as to cyan flashes in the ring of the visual stimulus.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.004">http://dx.doi.org/10.7554/eLife.04995.004</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media2" mime-subtype="mp4" mimetype="video" xlink:href="elife-04995-media2.mp4"><object-id pub-id-type="doi">10.7554/eLife.04995.005</object-id><label>Video 2.</label><caption><title>Two example trials from the match-masker condition.</title><p>As in <xref ref-type="other" rid="media1">Video 1</xref>, except the visual stream is coherent with the masker auditory stream.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.005">http://dx.doi.org/10.7554/eLife.04995.005</ext-link></p></caption></media><media content-type="glencoe play-in-place height-250 width-310" id="media3" mime-subtype="mp4" mimetype="video" xlink:href="elife-04995-media3.mp4"><object-id pub-id-type="doi">10.7554/eLife.04995.006</object-id><label>Video 3.</label><caption><title>Two example trials from the match-neither condition.</title><p>As in <xref ref-type="other" rid="media1">Video 1</xref>, except the visual stream is coherent neither auditory stream.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.006">http://dx.doi.org/10.7554/eLife.04995.006</ext-link></p></caption></media></p><p>If true cross-modal objects were being formed, then the auditory perceptual feature to which the listener must attend should not matter. To assess this generality, two types of auditory events were used, in two different groups of subjects. For half of the subjects (N = 16), each auditory stream was an amplitude-modulated tone (fundamental frequency, F0, of 440 or 565 Hz, counterbalanced), and the events were 100 ms fluctuations in the carrier frequency of ±1.5 semitones (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). For the other half of subjects (N = 16), each auditory stream was a band-pass filtered click train with a distinct pitch and timbre (synthetic vowels /u/ and /a/; F0 of 175 and 195 Hz), and the events were small changes in the timbre of the stream generated by slightly modulating the first and second formant frequencies (F1 and F2, <xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p><p>In addition to <italic>d′</italic>, we calculated response bias (calculated as ln <italic>β</italic>, where <italic>β</italic> is the likelihood ratio of a present target vs an absent target at the subject's decision criterion [<xref ref-type="bibr" rid="bib25">Macmillan and Creelman, 2005</xref>]), hit rates, false alarm rates, and visual hit rates. These values are all plotted in the left panels of <xref ref-type="fig" rid="fig2">Figure 2</xref>. Since there was broad variation in subjects' overall abilities, as well as in the difficulty between pitch and timbre tasks, the right panels of <xref ref-type="fig" rid="fig2">Figure 2</xref> show the across-subject means in each condition relative to each subject's overall mean. This is the visual parallel of the within-subjects statistical tests described below.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.04995.007</object-id><label>Figure 2.</label><caption><title>Behavioral results.</title><p>Each behavioral measure shown in two panels: (<bold>A</bold>) <italic>d′</italic> sensitivity, (<bold>B</bold>) bias, (<bold>C</bold>) hit rate, (<bold>D</bold>) false alarm rate, (<bold>E</bold>) visual hit rate. Left: mean ± SEM for each condition across all subjects (solid squares) as well as for pitch and timbre events separately (empty triangles and circles, respectively). Right: normalized mean ± SEM across all subjects demonstrating the within-subjects effects. Measurements with significant effects of coherence (viz., sensitivity and hit rate) are denoted with bold type and an asterisk on their vertical axis label. <italic>Post hoc</italic> differences between conditions that are significant at p &lt; 0.017 are shown with brackets and asterisks. See ‘Results’ for outcomes of all statistical tests.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.007">http://dx.doi.org/10.7554/eLife.04995.007</ext-link></p><p><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.04995.008</object-id><label>Figure 2—source data 1.</label><caption><title>Behavioral results for individual subjects.</title><p>Raw performance data for each subject for each of the panels in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The data are in plaintext CSV format and can be opened with any text or spreadsheet editor. See ‘Materials and methods’ for specific descriptions of how each category was calculated.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.008">http://dx.doi.org/10.7554/eLife.04995.008</ext-link></p></caption><media mime-subtype="csv" mimetype="text" xlink:href="elife-04995-fig2-data1-v1.csv"/></supplementary-material></p></caption><graphic xlink:href="elife-04995-fig2-v1.tif"/></fig></p><p>Subjects were more sensitive to target events when the visual stimulus was coherent with the target auditory stream than when it was coherent with the masker. We ran an ANOVA for <italic>d′</italic>, bias, hit rate, false alarm rate, and visual hit rate with a between-subjects factor of auditory event type (pitch or timbre) and a within-subjects factor of audio–visual coherence condition (match-target, match-masker, match-neither) on the data uploaded as <xref ref-type="supplementary-material" rid="SD1-data">Figure 2—source data 1</xref>. For <italic>d′</italic>, we found a significant between-groups effect of event type [F(1, 30) = 9.36, p = 0.0046, α = 0.05] and a significant within-subjects effect of coherence [F(2, 60) = 4.28, p = 0.018]. There was no interaction between these two factors (p = 0.60), indicating the generality of the effect of cross-modal coherence on the selective attention task to different features and experimental setups, as well as different task difficulties (as performance was significantly better with pitch events vs timbre events). <italic>Post hoc</italic> comparisons demonstrated that subjects performed better when the visual stimulus was coherent with the target auditory stream vs the auditory masker (match-target &gt; match-masker; p = 0.0049, Bonferroni-corrected α = 0.017). Similar results were obtained for hit rate: there was a significant effect of event type [F(1, 30) = 10.1, p = 0.0034] and also of visual coherence [F(2, 60) = 1.286, p = 0.0497]. <italic>Post hoc</italic> tests also showed a significant difference between hit rate in the match-target and match-masker conditions (p = 0.011). These results suggest that a visual stimulus can enhance a coherent auditory stimulus, even when that visual stimulus provides no useful visual information. There was no significant difference between the match-neither condition and either of the other two conditions. Thus, while there is a clear benefit when the visual stimulus is coherent with an attentional target vs a distractor, it is not yet clear if the changes in performance represent a helpful enhancement of the target or a deleterious enhancement of the masker vs some neutral condition, such as the match-neither case or an auditory-only case.</p><p>For bias, there was no significant effect of event type or coherence condition (ANOVA as above, p = 0.284 and 0.985, respectively), and subjects were generally conservative in their responses (ln <italic>β</italic> &gt; 0, intercept significant at F[1, 30] = 43.6, p &lt; 0.0005). The lack of a variation in bias solidifies the notion that the observed pattern of responses results from changes in auditory detectability. Neither factor had a significant effect on false alarm rate (p = 0.29 and 0.18) or visual hit rate (p = 0.10 and 0.091). For this reason, <italic>post hoc</italic> paired comparisons were not made.</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we used temporal coherence between auditory and visual features to influence the way subjects parsed an audio–visual scene. Many studies have examined how the timing of single auditory and visual events (<xref ref-type="bibr" rid="bib17">Fujisaki and Nishida, 2005</xref>; <xref ref-type="bibr" rid="bib49">Zampini et al., 2005</xref>) or simple periodic modulations of stimulus features (<xref ref-type="bibr" rid="bib35">Recanzone, 2003</xref>; <xref ref-type="bibr" rid="bib41">Spence and Squire, 2003</xref>; <xref ref-type="bibr" rid="bib17">Fujisaki and Nishida, 2005</xref>) influence the likelihood of an integrated multisensory percept. However, the temporal dynamics of one event or a repeating sequence are quite unlike many natural sounds, including speech. A recent study addressed this by creating randomly timed sequences of discrete auditory-visual events, and showed that coherence discrimination was better for the unpredictable sequences than for predictable ones (<xref ref-type="bibr" rid="bib14">Denison et al., 2013</xref>). Thus, the noisy dynamics of stimulus features used here not only allowed our stimuli to more closely emulate speech, but likely also strengthened the bound percept.</p><p>The integration of visual and auditory information to create a cross-modal percept relies on the sensory signals being bound together. The well-known McGurk illusion (<xref ref-type="bibr" rid="bib29">McGurk and MacDonald, 1976</xref>) can be reduced by contextually ‘unbinding’ the auditory and visual stimuli by preceding a test trial with non-matching audio and video (<xref ref-type="bibr" rid="bib32">Nahorna et al., 2012</xref>). Illusory syllables are also more likely when audio is paired with a dynamic face as opposed to a static image of a face that provides the same information regarding place of articulation (<xref ref-type="bibr" rid="bib36">Rosenblum and Saldaña, 1996</xref>). Additionally, individual differences in purely temporal processing (specifically, how long a discrete auditory event can lag a visual event and still be perceived as simultaneous) are predictive of audio–visual fusion in multiple cross-modal illusions (<xref ref-type="bibr" rid="bib43">Stevenson et al., 2012</xref>).</p><p>To our knowledge, the present study is the only one to base a behavioral task on one auditory feature and use an orthogonal feature to create binding with the visual stimulus, thus demonstrating an enhancement of an auditory feature about which the visual stimulus was uninformative. How, then, did a coherent visual stimulus facilitate enhanced behavioral performance?</p><p>Coherence-driven binding of auditory and visual features to create a cross-modal object may underlie the present results. In the unimodal case, temporal coherence between auditory features (i.e., pitch, timbre, location) is a principal contributor to the formation of auditory objects (<xref ref-type="bibr" rid="bib37">Shamma et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Teki et al., 2013</xref>); however, the definition of an object can be difficult to pin down. <xref ref-type="bibr" rid="bib21">Griffiths and Warren (2004)</xref> suggest criteria for what constitutes an auditory object. They first suggest that the incoming information corresponds to something in the sensory world; this often means a physical sound source, but they leave room in their definition for artificial sounds that could conceivably come from a real-world source. Second, object-related information should be separable from other incoming sensory information—this was a major component of the present task. Third, an object demonstrates perceptual invariance. While this was not a focus of the present study, listeners' perceptions of the attended features were invariant in the face of fluctuating amplitude. The fourth and most interesting (in the context of the present discussion) criterion is that an object is generalizable between senses. The authors point out that it is ‘less clear’ whether analyzing sensory objects is affected by cross-modal correspondence. The notion of cross-modal objecthood is easy to accept but difficult to demonstrate conclusively. The sound-induced flash illusion (<xref ref-type="bibr" rid="bib38">Shams et al., 2000</xref>, <xref ref-type="bibr" rid="bib39">2002</xref>), where the number of quick flashes perceived is strongly biased by the number of coincident beeps, is affected by audio–visual spatial congruence when observers are required to selectively attend to two spatially separated stimulus streams (<xref ref-type="bibr" rid="bib8">Bizley et al., 2012</xref>). This suggests that cross-modal sensory interaction is strongest when evidence supports information from both modalities as originating from the same object.</p><p>We prefer a functional definition of ‘objecthood’ because it provides testable predictions about the perception of a purported object's features. There is evidence from unimodal studies that both visual and auditory attention operate on perceptual objects, such that attention to one feature of an object enhances the perception of all its features (<xref ref-type="bibr" rid="bib1">Alain and Arnott, 2000</xref>; <xref ref-type="bibr" rid="bib9">Blaser et al., 2000</xref>; <xref ref-type="bibr" rid="bib40">Shinn-Cunningham, 2008</xref>; <xref ref-type="bibr" rid="bib37">Shamma et al., 2011</xref>; <xref ref-type="bibr" rid="bib26">Maddox and Shinn-Cunningham, 2012</xref>). Here, we show that processing of auditory pitch and timbre are enhanced when auditory amplitude is coherent with visual size. In the case of the match-target condition, this is beneficial, with attention to the visual stimulus improving listening performance. Performance suffers in the match-masker case, where the incorrect auditory stream's features are enhanced through the same binding process.</p><p><xref ref-type="fig" rid="fig3">Figure 3</xref> shows a conceptual model of the processing underlying these results, using the pitch task as an example. Auditory target and masker (on the left) and visual streams (on the right) are depicted as bound sets of component features, with those connections shown as line segments. When there is cross-modal temporal coherence between a feature of an auditory and visual stream, those features are bound. This results in a cross-modal object (just as two auditory or visual streams with the same envelope very likely would have bound together as well). In the match-target condition (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), the to-be-attended features (highlighted with a yellow ellipse) become part of one object and are enhanced (shown by a thick bounding box). The features of the to-be-ignored stream are suppressed (shown as a broken bounding box). However, in the match-masker condition (center), attention to the target auditory and visual features is now across objects. To make matters worse, processing of the to-be-ignored auditory features is enhanced, increasing the false-alarm likelihood. In the match-neither case, attention must still be split between unbound auditory and visual streams, but the masking stream is not enhanced, leading to performance between the other two conditions. How this model might be biologically implemented remains to be elucidated, but one possibility is that cross-modal coherence between stimulus modalities enables neural activity patterns in visual cortex to reinforce the activity elicited by the coherent auditory stream in auditory cortex (<xref ref-type="bibr" rid="bib18">Ghazanfar and Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bib7">Bizley and King, 2012</xref>).<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.04995.009</object-id><label>Figure 3.</label><caption><title>Conceptual model of coherence-based cross-modal object formation in the pitch task.</title><p>Sensory streams are shown as a box containing connected sets of features. Auditory streams are on the left half of the gray sensory boundary and visual on the right. Cross-modal coherence, where present, is shown as a line connecting the coherent auditory and visual features: specifically, the auditory amplitude and the visual size. This results in cross-modal binding of the coherent auditory and visual streams, enhancing each streams' features, which is beneficial in the match-target condition (<bold>A</bold>), problematic in the match-masker condition (<bold>B</bold>), and not present in the match-neither condition (<bold>C</bold>). Attended features are indicated with a yellow ellipse. Enhancement/suppression resulting from object formation is reflected in the strength of the box drawn around each stream's features (i.e., thick lines indicate enhancement, broken lines show suppression).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.04995.009">http://dx.doi.org/10.7554/eLife.04995.009</ext-link></p></caption><graphic xlink:href="elife-04995-fig3-v1.tif"/></fig></p><p>If the formation of cross-modal objects underlies the present results, then the enhancement should be bidirectional. While the present experiment was designed specifically to test the effect of a visual stimulus on auditory performance, the model also makes predictions for enhancement of a task-irrelevant visual cue. Since there was only one visual stimulus (i.e., no competing distractor visual stimulus), its features should be enhanced when it is coherent with any auditory stimulus compared to when it is not, but further experiments specifically designed to test this bi-directionality are needed.</p><p>Within one modality, a stream or object can be thought of as occupying a point or region of a multidimensional feature space: it can have a specific pitch, location, onset or offset time, etc that define it (<xref ref-type="bibr" rid="bib37">Shamma et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Teki et al., 2011</xref>, <xref ref-type="bibr" rid="bib47">2013</xref>; <xref ref-type="bibr" rid="bib30">Micheyl et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Christiansen et al., 2014</xref>). Two streams may bind if they are close together in one or more of these dimensions, such as tones of two frequencies that start and stop at the same time (<xref ref-type="bibr" rid="bib37">Shamma et al., 2011</xref>). Similar principles should govern the formation of cross-modal objects, with the caveat that only some stimulus features span sensory boundaries. These dimensions could be physical, such as time and space for audio-visual stimuli (<xref ref-type="bibr" rid="bib2">Alais and Burr, 2004</xref>; <xref ref-type="bibr" rid="bib45">Talsma et al., 2010</xref>; <xref ref-type="bibr" rid="bib8">Bizley et al., 2012</xref>), or learned statistical correspondences, such as the linguistic connection between an image of someone with protruded lips and an auditory /u/ vowel. Within this object, all features are enhanced and disagreements or ambiguities are resolved, either by letting the more reliable modality dominate (typically vision for space, audition for timing [<xref ref-type="bibr" rid="bib45">Talsma et al., 2010</xref>]), or by creating an intermediary percept (e.g., the McGurk effect). Such integration is necessary for a perceptual object to be a reasonable model of a physical one, which cannot, for example, occupy more than one location at one time.</p><p>How does one establish that cross-modal binding has occurred? Here, we suggest that the demonstration of feature enhancement within a putative object is the marker of binding. We showed that attending to the color of a visual stream enhanced the perception of the pitch or timbre of an auditory stream, despite the fact that those two (i.e., color and pitch or color and timbre) features on their own were unrelated. This influence instead occurred through a chain of bound features within a cross-modal object, with a shared temporal trajectory linking visual size and auditory amplitude and is, to our knowledge, the first example of a continuous visual signal benefitting listening abilities through enhanced auditory scene segregation.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>Subjects gave written informed consent and were paid for their participation. Subjects had normal hearing (audiologic thresholds ≤20 dB hearing loss at octave frequencies from 250 to 8000 Hz) and normal or corrected visual acuity. A total of 32 subjects participated (17 male, age 18–33 years). A separate group of 16 subjects each participated in the pitch event and timbre event tasks. The pitch task was run at the University of Washington in the lab of AKCL. The timbre task was performed at the University College London in the lab of JKB.</p><sec id="s4-1"><title>General stimulus construction and presentation</title><p>Envelopes for visual envelope and auditory amplitude were created using the same frequency domain synthesis. For each trial, an envelope was created by first setting all amplitudes of frequency bins above 0 Hz and below 7 Hz to unity and others to zero. At an audio sampling rate of 24,414 Hz, all non-zero bins were given a random phase from a uniform distribution between 0 and 2π, the corresponding frequency bins across Nyquist frequency were set to the complex conjugates to maintain Hermitian symmetry, and the inverse Fourier transform was computed yielding a time domain envelope. A second and third envelope were created using the same method, and orthogonalized using a Gram-Schmidt procedure. Each envelope was then normalized so that it spanned the interval [0, 1] and then sine-transformed [<bold>y</bold> = sin<sup>2</sup>(π<bold>x</bold>/2)] so that the extremes were slightly accentuated. Visual envelopes were created by subsampling the auditory envelope at the monitor frame-rate of 60 Hz, starting with the first auditory sample, so that auditory amplitude corresponded with the disc radius at the beginning of each frame.</p><p>Stimuli were presented in an unlit sound-attenuating room over earphones. Auditory stimuli were created in MATLAB and presented using an RP2 signal processor (Tucker–Davis Technologies, Alachua, FL, USA). Each began and ended with a 10 ms cosine ramp. All stimuli were presented diotically. Visual stimuli were synthesized in MATLAB (The Mathworks, Natick, MA, USA) and presented using the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib10">Brainard, 1997</xref>). Gray discs subtended between 1° and 2.5° at 50 cm viewing distance. The white ring extended 0.125° beyond the gray disc.</p><p>Trials lasted 14 s. They began with only the target auditory stimulus and the visual stimulus, indicating the to-be-attended auditory stream to the subject. The auditory masker began 1 s later. As with the rest of the trial, the visual stimulus was only coherent with the auditory target during the first second if it was a match-target trial. All streams ended simultaneously. Events did not occur in the first 2 s (1 s after the masker began) or the last 1 s of each trial, or within 1.2 s of any other events in either modality. A response made within 1 s following an event was attributed to that event. Responses not attributed to an event were recorded but were infrequent and are not analyzed here. To ensure audibility and equivalent target to masker ratios without providing confounding information to the subject, an event in either auditory stream or the visual stream could only begin when <italic>both</italic> auditory envelopes were above 75% maximum. There were between 1 and 3 inclusive (mean 2) events in both the target and masker in each trial. There were also between 0 and 2 inclusive (mean 1) visual flashes per trial, in which the outer ring changed from white to cyan (0% red, 100% blue, 100% green) and back. Each subject completed 32 trials of each stimulus condition (96 total), leading to 64 potential hits and 64 potential false alarms for each condition (i.e., 128 responses considered for each <italic>d′</italic> calculation) as well as 32 visual flashes per condition. When computing <italic>d′</italic>, auditory hit and false alarm rates were calculated by adding 0.5 to the numerator and 1 to the denominator so that <italic>d′</italic> had finite limits.</p></sec><sec id="s4-2"><title>Pitch task</title><p>The pitch task was conducted at the University of Washington. Subjects were seated 50 cm from the screen. Auditory stimuli were presented over earphones (ER-2, Etymotic Research, Elk Grove Village, IL, USA). Each auditory stream was an amplitude-modulated sinusoid with a frequency of either 440 or 565 Hz and was presented at an average of 66 or 63 dB sound pressure level (SPL), respectively in 42 dB SPL white noise to mask any residual sound from outside the sound-treated booth. The higher frequency stream was more salient in piloting and so was attenuated 3 dB so that both streams were of equivalent perceived loudness. Auditory events were 100 ms sinusoidal carrier frequency deflections with a peak amplitude of 1.5 semitones (where <italic>n</italic> semitones is a ratio of 2<sup><italic>n</italic>/12</sup>; <xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p></sec><sec id="s4-3"><title>Timbre task</title><p>The timbre task was conducted at University College London. Subjects were seated 60 cm from the screen with their heads held stationary by a chinrest. Auditory stimuli were presented over headphones (HD 555, Sennheiser, Wedemark, Germany). Each auditory stream was generated as a periodic impulse train and then filtered with synthetic vowels simulated as four-pole filters (formants F1–F4). The /u/ stream (F0 = 175 Hz) had formant peaks F1–F4 at 460, 1105, 2857, 4205 Hz and moved slightly towards /ε/ during timbre events, with formant peaks at 730, 2058, 2857, 4205 Hz. The /a/ stream (F0 = 195 Hz) had formant peaks F1–F4 at 936, 1551, 2975, 4263 Hz and moved slightly towards /i/ during timbre events, with formant peaks at 437, 2761, 2975, 4263 Hz. During timbre events the formants moved linearly toward the deviant for 100 ms and then linearly back for 100 ms. Streams were calibrated to be 65 dB SPL (RMS normalized) using an artificial ear (Brüel &amp; Kjær, Nærum, Denmark) and presented against a low level of background noise (54 dB SPL). Before testing, subjects completed a threshold task to determine the size of timbre shift that resulted in 70% detection. The average perturbation (as percentage of distance from steady-state vowel to deviant vowel in the F1–F2 plane) was 12.25%. Four subjects out of twenty tested did not perform well enough to be included (<italic>d′</italic> &lt; 0.7), leading to the final N = 16 analyzed.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Katherine Pratt for assistance with data collection and Dean Pospisil for helpful discussions.</p></ack><sec sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>RKM, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article, Contributed unpublished essential data or reagents</p></fn><fn fn-type="con" id="con2"><p>HA, Conception and design, Acquisition of data, Analysis and interpretation of data</p></fn><fn fn-type="con" id="con3"><p>JKB, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con4"><p>AKCL, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Subjects gave written informed consent and were paid for their participation. All methods were approved by the Institutional Review Board of the University of Washington and the Ethics Committee of the University College London (ref: 5139).</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alain</surname><given-names>C</given-names></name><name><surname>Arnott</surname><given-names>SR</given-names></name></person-group><year>2000</year><article-title>Selectively attending to auditory objects</article-title><source>Frontiers in Bioscience</source><volume>5</volume><fpage>D202</fpage><lpage>D212</lpage><pub-id pub-id-type="doi">10.2741/Alain</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Burr</surname><given-names>D</given-names></name></person-group><year>2004</year><article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title><source>Current Biology</source><volume>14</volume><fpage>257</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>JG</given-names></name><name><surname>Grant</surname><given-names>KW</given-names></name></person-group><year>2009</year><article-title>Auditory and auditory-visual intelligibility of speech in fluctuating maskers for normal-hearing and hearing-impaired listeners</article-title><source>The Journal of the Acoustical Society of America</source><volume>125</volume><fpage>3358</fpage><lpage>3372</lpage><pub-id pub-id-type="doi">10.1121/1.3110132</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name><name><surname>Takayanagi</surname><given-names>S</given-names></name></person-group><year>2004</year><article-title>Auditory speech detection in noise enhanced by lipreading</article-title><source>Speech Communication</source><volume>44</volume><fpage>5</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.specom.2004.10.011</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binnie</surname><given-names>CA</given-names></name></person-group><year>1973</year><article-title>Bi-sensory articulation functions for normal hearing and sensorineural hearing loss patients</article-title><source>Journal of the Academy of Rehabilitative Audiology</source><volume>6</volume><fpage>43</fpage><lpage>53</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year>2013</year><article-title>The what, where and how of auditory-object perception</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>693</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1038/nrn3565</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year>2012</year><article-title>What can multisensory processing tell us about the functional organization of auditory cortex?</article-title><person-group person-group-type="editor"><name><surname>Murray</surname><given-names>MM</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><source>The neural bases of multisensory processes</source><source>Frontiers in Neuroscience</source><publisher-loc>Boca Raton, FL</publisher-loc><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Lee</surname><given-names>AKC</given-names></name></person-group><year>2012</year><article-title>Nothing is irrelevant in a noisy world: sensory illusions reveal obligatory within-and across-modality integration</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>13402</fpage><lpage>13410</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2495-12.2012</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blaser</surname><given-names>E</given-names></name><name><surname>Pylyshyn</surname><given-names>ZW</given-names></name><name><surname>Holcombe</surname><given-names>AO</given-names></name></person-group><year>2000</year><article-title>Tracking an object through feature space</article-title><source>Nature</source><volume>408</volume><fpage>196</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1038/35041567</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year>1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bregman</surname><given-names>AS</given-names></name></person-group><year>1990</year><source>Auditory scene analysis: the perceptual organization of sound</source><publisher-loc>Cambridge, Mass</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Trubanova</surname><given-names>A</given-names></name><name><surname>Stillittano</surname><given-names>S</given-names></name><name><surname>Caplier</surname><given-names>A</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><year>2009</year><article-title>The natural statistics of audiovisual speech</article-title><source>PLOS Computational Biology</source><volume>5</volume><fpage>e1000436</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christiansen</surname><given-names>SK</given-names></name><name><surname>Jepsen</surname><given-names>ML</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name></person-group><year>2014</year><article-title>Effects of tonotopicity, adaptation, modulation tuning, and temporal coherence in ‘primitive’ auditory stream segregation</article-title><source>The Journal of the Acoustical Society of America</source><volume>135</volume><fpage>323</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1121/1.4845675</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denison</surname><given-names>RN</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name><name><surname>Ruff</surname><given-names>CC</given-names></name></person-group><year>2013</year><article-title>Temporal structure and complexity affect audio-visual correspondence detection</article-title><source>Frontiers in Psychology</source><volume>3</volume><fpage>619</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00619</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devergie</surname><given-names>A</given-names></name><name><surname>Grimault</surname><given-names>N</given-names></name><name><surname>Gaudrain</surname><given-names>E</given-names></name><name><surname>Healy</surname><given-names>EW</given-names></name><name><surname>Berthommier</surname><given-names>F</given-names></name></person-group><year>2011</year><article-title>The effect of lip-reading on primary stream segregation</article-title><source>The Journal of the Acoustical Society of America</source><volume>130</volume><fpage>283</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1121/1.3592223</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>E</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name></person-group><year>2008</year><article-title>Direction of visual apparent motion driven solely by timing of a static sound</article-title><source>Current Biology</source><volume>18</volume><fpage>1262</fpage><lpage>1266</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.07.066</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisaki</surname><given-names>W</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name></person-group><year>2005</year><article-title>Temporal frequency characteristics of synchrony–asynchrony discrimination of audio-visual signals</article-title><source>Experimental Brain Research</source><volume>166</volume><fpage>455</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2385-8</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year>2006</year><article-title>Is neocortex essentially multisensory?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>278</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>KW</given-names></name></person-group><year>2001</year><article-title>The effect of speechreading on masked detection thresholds for filtered speech</article-title><source>The Journal of the Acoustical Society of America</source><volume>109</volume><fpage>2272</fpage><lpage>2275</lpage><pub-id pub-id-type="doi">10.1121/1.1362687</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Seitz</surname><given-names>PF</given-names></name></person-group><year>2000</year><article-title>The use of visible speech cues for improving auditory detection of spoken sentences</article-title><source>The Journal of the Acoustical Society of America</source><volume>108</volume><fpage>1197</fpage><lpage>1208</lpage><pub-id pub-id-type="doi">10.1121/1.1288668</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Warren</surname><given-names>JD</given-names></name></person-group><year>2004</year><article-title>What is an auditory object?</article-title><source>Nature Reviews Neuroscience</source><volume>5</volume><fpage>887</fpage><lpage>892</lpage><pub-id pub-id-type="doi">10.1038/nrn1538</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helfer</surname><given-names>KS</given-names></name><name><surname>Freyman</surname><given-names>RL</given-names></name></person-group><year>2005</year><article-title>The role of visual speech cues in reducing energetic and informational masking</article-title><source>The Journal of the Acoustical Society of America</source><volume>117</volume><fpage>842</fpage><lpage>849</lpage><pub-id pub-id-type="doi">10.1121/1.1836832</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>TS</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group><year>2006</year><article-title>Efficient coding of visual scenes by grouping and segmentation</article-title><person-group person-group-type="editor"><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Ishii</surname><given-names>S</given-names></name><name><surname>Rao</surname><given-names>RPN</given-names></name></person-group><source>Bayesian brain: probabilistic approaches to neural coding</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>141</fpage><lpage>185</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lovelace</surname><given-names>CT</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year>2003</year><article-title>An irrelevant light enhances auditory detection in humans: a psychophysical analysis of multisensory integration in stimulus detection</article-title><source>Brain Research Cognitive Brain Research</source><volume>17</volume><fpage>447</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(03)00160-5</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macmillan</surname><given-names>NA</given-names></name><name><surname>Creelman</surname><given-names>CD</given-names></name></person-group><year>2005</year><source>Detection theory: a user’s guide</source><publisher-loc>Mahwah, NJ</publisher-loc><publisher-name>Lawrence Erlbaum Associates</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname><given-names>RK</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><year>2012</year><article-title>Influence of task-relevant and task-irrelevant feature continuity on selective auditory attention</article-title><source>Journal of the Association for Research in Otolaryngology</source><volume>13</volume><fpage>119</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1007/s10162-011-0299-7</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year>1982</year><source>Vision: a computational investigation into the human representation and processing of visual information</source><publisher-loc>San Francisco</publisher-loc><publisher-name>W.H. Freeman</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDonald</surname><given-names>JJ</given-names></name><name><surname>Teder-Sälejärvi</surname><given-names>WA</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year>2000</year><article-title>Involuntary orienting to sound improves visual perception</article-title><source>Nature</source><volume>407</volume><fpage>906</fpage><lpage>908</lpage><pub-id pub-id-type="doi">10.1038/35038085</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGurk</surname><given-names>H</given-names></name><name><surname>MacDonald</surname><given-names>J</given-names></name></person-group><year>1976</year><article-title>Hearing lips and seeing voices</article-title><source>Nature</source><volume>264</volume><fpage>746</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1038/264746a0</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Micheyl</surname><given-names>C</given-names></name><name><surname>Hanson</surname><given-names>C</given-names></name><name><surname>Demany</surname><given-names>L</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><year>2013</year><article-title>Auditory stream segregation for alternating and synchronous tones</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>39</volume><fpage>1568</fpage><lpage>1580</lpage><pub-id pub-id-type="doi">10.1037/a0032241</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishra</surname><given-names>J</given-names></name><name><surname>Martinez</surname><given-names>A</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year>2013</year><article-title>Audition influences color processing in the sound-induced visual flash illusion</article-title><source>Vision Research</source><volume>93</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2013.10.013</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nahorna</surname><given-names>O</given-names></name><name><surname>Berthommier</surname><given-names>F</given-names></name><name><surname>Schwartz</surname><given-names>JL</given-names></name></person-group><year>2012</year><article-title>Binding and unbinding the auditory and visual streams in the McGurk effect</article-title><source>The Journal of the Acoustical Society of America</source><volume>132</volume><fpage>1061</fpage><lpage>1077</lpage><pub-id pub-id-type="doi">10.1121/1.4728187</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odgaard</surname><given-names>EC</given-names></name><name><surname>Arieh</surname><given-names>Y</given-names></name><name><surname>Marks</surname><given-names>LE</given-names></name></person-group><year>2003</year><article-title>Cross-modal enhancement of perceived brightness: sensory interaction versus response bias</article-title><source>Perception and Psychophysics</source><volume>65</volume><fpage>123</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.3758/BF03194789</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odgaard</surname><given-names>EC</given-names></name><name><surname>Arieh</surname><given-names>Y</given-names></name><name><surname>Marks</surname><given-names>LE</given-names></name></person-group><year>2004</year><article-title>Brighter noise: sensory enhancement of perceived loudness by concurrent visual stimulation</article-title><source>Cognitive, Affective &amp; Behavioral Neuroscience</source><volume>4</volume><fpage>127</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.3758/CABN.4.2.127</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanzone</surname><given-names>GH</given-names></name></person-group><year>2003</year><article-title>Auditory influences on visual temporal rate perception</article-title><source>Journal of Neurophysiology</source><volume>89</volume><fpage>1078</fpage><lpage>1093</lpage><pub-id pub-id-type="doi">10.1152/jn.00706.2002</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblum</surname><given-names>LD</given-names></name><name><surname>Saldaña</surname><given-names>HM</given-names></name></person-group><year>1996</year><article-title>An audiovisual test of kinematic primitives for visual speech perception</article-title><source>Journal of Experimental Psychology Human Perception and Performance</source><volume>22</volume><fpage>318</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.22.2.318</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Micheyl</surname><given-names>C</given-names></name></person-group><year>2011</year><article-title>Temporal coherence and attention in auditory scene analysis</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>114</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.11.002</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name></person-group><year>2000</year><article-title>Illusions: what you see is what you hear</article-title><source>Nature</source><volume>408</volume><fpage>788</fpage><pub-id pub-id-type="doi">10.1038/35048669</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name></person-group><year>2002</year><article-title>Visual illusion induced by sound</article-title><source>Brain Research Cognitive Brain Research</source><volume>14</volume><fpage>147</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00069-1</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><year>2008</year><article-title>Object-based auditory and visual attention</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>182</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.02.003</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C</given-names></name><name><surname>Squire</surname><given-names>S</given-names></name></person-group><year>2003</year><article-title>Multisensory integration: maintaining the perception of synchrony</article-title><source>Current Biology</source><volume>13</volume><fpage>R519</fpage><lpage>R521</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(03)00445-7</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>London</surname><given-names>N</given-names></name><name><surname>Wilkinson</surname><given-names>LK</given-names></name><name><surname>Price</surname><given-names>DD</given-names></name></person-group><year>1996</year><article-title>Enhancement of perceived visual intensity by auditory stimuli: a psychophysical analysis</article-title><source>Journal of Cognitive Neuroscience</source><volume>8</volume><fpage>497</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1162/jocn.1996.8.6.497</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson</surname><given-names>RA</given-names></name><name><surname>Zemtsov</surname><given-names>RK</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year>2012</year><article-title>Individual differences in the multisensory temporal binding window predict susceptibility to audiovisual illusions</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>38</volume><fpage>1517</fpage><lpage>1529</lpage><pub-id pub-id-type="doi">10.1037/a0027339</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>Q</given-names></name></person-group><year>1992</year><article-title>Lipreading and audio-visual speech perception</article-title><source>Philosophical Transactions of the Royal Society of London Series B, Biological Sciences</source><volume>335</volume><fpage>71</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0009</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Senkowski</surname><given-names>D</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><year>2010</year><article-title>The multifaceted interplay between attention and multisensory integration</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.06.008</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teki</surname><given-names>S</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><year>2013</year><article-title>Segregation of complex acoustic scenes based on temporal coherence</article-title><source>eLife</source><volume>2</volume><fpage>e00699</fpage><pub-id pub-id-type="doi">10.7554/eLife.00699</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teki</surname><given-names>S</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>von Kriegstein</surname><given-names>K</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><year>2011</year><article-title>Brain bases for auditory stimulus-driven figure–ground segregation</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>164</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3788-10.2011</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>CS</given-names></name><name><surname>Nichols</surname><given-names>TL</given-names></name></person-group><year>1976</year><article-title>Detectability of auditory signals presented without defined observation intervals</article-title><source>The Journal of the Acoustical Society of America</source><volume>59</volume><fpage>655</fpage><lpage>668</lpage><pub-id pub-id-type="doi">10.1121/1.380915</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zampini</surname><given-names>M</given-names></name><name><surname>Guest</surname><given-names>S</given-names></name><name><surname>Shore</surname><given-names>DI</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year>2005</year><article-title>Audio-visual simultaneity judgments</article-title><source>Perception &amp; Psychophysics</source><volume>67</volume><fpage>531</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.3758/BF03193329</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.04995.010</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Culham</surname><given-names>Jody C</given-names></name><role>Reviewing editor</role><aff><institution>University of Western Ontario</institution>, <country>Canada</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Auditory selective attention is enhanced by a task-irrelevant temporally coherent visual stimulus in human listeners” for consideration at <italic>eLife</italic>. Your article has been favorably evaluated by Eve Marder (Senior editor), a Reviewing editor, and 3 reviewers.</p><p>The following individuals responsible for the peer review of your submission have agreed to reveal their identity: Jody Culham (Reviewing editor) and Simon Carlile (peer reviewer). Two other reviewers remain anonymous.</p><p>The Reviewing editor and the reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>All three reviewers had favorable responses to the manuscript, with remarks such as: “This is an excellent paper which provides original findings that are highly significant”. However, the reviewers brought up points that must be addressed in a revision before the manuscript can be considered suitable for publication in <italic>eLife</italic>.</p><p>The reviewing editor has grouped the recommendations into required changes and recommended considerations that have been summarized from individual reviewers’ comments.</p><p>1) Two of the reviewers commented that the aim of the paper should be provided earlier in the Introduction.</p><p>One reviewer suggests including a statement such as “There has yet to be a study utilizing ongoing, continuous stimuli that demonstrates a perceptual benefit of an uninformative visual stimulus when selectively attending to one auditory stimulus in a mixture”, followed by an overview of previous studies that differed from this goal.</p><p>Another reviewer states: “The readability of the manuscript could be improved by a clearer description of the aim of the study, the background literature and the hypotheses addressed. For example, the last sentences of the second paragraph of the Introduction are not clear. What is meant with e.g. “higher-level influences” and “universal feature of multisensory integration”? Also, first the authors write about the influence of visual stimuli on auditory perception, then about the influence of auditory stimuli on visual perception, and then again about the opposite relationship. Please provide better guidance to the reader when discussing these different relationships. In both the Introduction and Discussion sections, the authors repeatedly discuss topics that could be discussed just once. Also, the Discussion section describes studies that provide relevant background, which should be provided in the Introduction section. The conceptual model is very confusing and does not add to the textual description of the results.”</p><p>2) There was not much enthusiasm for the first “subjective” experiment. One reviewer thinks it should be deleted. Another reviewer wonders what it really measures. The reviewing editor thought that it got the manuscript off to a weak start and notes that the authors themselves realize this, in the Results section: “The design of this first experiment was admittedly simplistic”.</p><p>The authors should “play their strongest card first”—i.e., jump right to the objective experiment. The simplest approach would be to delete the first experiment. However, if the authors do think it is important, they should figure out how to present it in a manner as to not detract from the stronger portions of the paper. If it is included, the following concern from one reviewer should be addressed:</p><p>“In the subjective study, subjects were asked to detect binding. This instruction may have resulted in ‘binding experiences’ relatively often, as the subjects were not naïve listeners/viewers. Please discuss this issue.”</p><p>Really, though, it doesn't appear that much would be lost by just removing the experiment.</p><p>3) The discussion should address the fact that <italic>d'</italic> for the matching target condition did not differ from that in the independent condition. This indicates that binding is only beneficial compared to a condition in which the perception of the masker is facilitated.</p><p>4) Alternatives to binding by “objectness” and the framework for considering auditory objects should be discussed.</p><p>In the words of one reviewer:</p><p>“The main ‘quantitative’ experiment provides strong evidence that the temporal co-modulation of the auditory and visual stimuli does produce a detection advantage for the orthogonal (frequency based) auditory deviations. The overall effect is consistent with the previous literature (although the observation is novel in its own right) and it is very likely that this is mediated by some attentional element in the processing. Indeed, it is plausible that this is mediated through an increase in the ‘objectness’ of the multi-sensory stimulus used here, however, this is not necessarily so. <xref ref-type="bibr" rid="bib21">Griffiths and Warren (2004)</xref> argue that objects will tend to represent actual sources, have boundaries (they can be segregated) and are relatively invariant. These stimuli are artificial so do not reflect the first characteristic and given the data reported here, may reflect the second characteristic. If these features are indeed bound to create a perceptual object then possibly they will reflect invariance. These data may suggest this is the case but the argument is not made explicitly in the manuscript. A more structured frame work for what constitutes an object in general and an auditory object in particular is required and the data discussed in that light.”</p><p>Recommended considerations:</p><p>1) One reviewer and the Reviewing editor questioned the value added by Figure 4 and recommend removing it.</p><p>2) In the analysis, several ANOVAs were conducted. Perhaps a MANOVA is more appropriate, given the dependence of the various dependent measures.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.04995.011</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>The reviewing editor has grouped the recommendations into required changes and recommended considerations that have been summarized from individual reviewers’ comments</italic>.</p><p>We appreciate the reviewers’ constructive criticism. We have made all of the required revisions, and the majority of the requested revisions (the model figure was retained, but made less stylized). We believe these changes make the manuscript clearer, more concise and direct, focusing on the most novel and important results and discussion.</p><p><italic>1) Two of the reviewers commented that the aim of the paper should be provided earlier in the Introduction</italic>.</p><p><italic>One reviewer suggests including a statement such as “There has yet to be a study utilizing ongoing, continuous stimuli that demonstrates a perceptual benefit of an uninformative visual stimulus when selectively attending to one auditory stimulus in a mixture”, followed by an overview of previous studies that differed from this goal</italic>.</p><p>We agree that making the purpose of the study clear sooner in the Introduction will strengthen it. We have taken the quoted sentence and moved it to the end of the first paragraph, motivating the remainder of the Introduction.</p><p><italic>Another reviewer states: “The readability of the manuscript could be improved by a clearer description of the aim of the study, the background literature and the hypotheses addressed. For example, the last sentences of the second paragraph of the Introduction are not clear. What is meant with e.g. “higher-level influences” and universal feature of multisensory integration”? Also, first the authors write about the influence of visual stimuli on auditory perception, then about the influence of auditory stimuli on visual perception, and then again about the opposite relationship. Please provide better guidance to the reader when discussing these different relationships. In both the Introduction and Discussion sections, the authors repeatedly discuss topics that could be discussed just once. Also, the Discussion section describes studies that provide relevant background which should be provided in the Introduction section. The conceptual model is very confusing and does not add to the textual description of the results</italic>.<italic>”</italic></p><p>Based on these suggestions and in those of the previous reviewer, we have made a number of changes to the Introduction and Discussion sections that we hope address these concerns, moving some important concepts to the Introduction (“Past studies employing transient stimuli…” were moved from the Discussion to the Introduction) and drastically reducing the redundancy with which some topics were discussed (“Many studies have examined how…” represent two combined paragraphs that were previously in the Introduction and Discussion).</p><p><italic>2) There was not much enthusiasm for the first “subjective” experiment. One reviewer thinks it should be deleted. Another reviewer wonders what it really measures. The reviewing editor thought that it got the manuscript off to a weak start and notes that the authors themselves realize this, in the Results section: “The design of this first experiment was admittedly simplistic”</italic>.</p><p><italic>The authors should “play their strongest card first”—i.e., jump right to the objective experiment. The simplest approach would be to delete the first experiment. However, if the authors do think it is important, they should figure out how to present it in a manner as to not detract from the stronger portions of the paper. If it is included, the following concern from one reviewer should be addressed</italic>:</p><p><italic>“In the subjective study, subjects were asked to detect binding. This instruction may have resulted in ‘binding experiences’ relatively often, as the subjects were not naïve listeners/viewers. Please discuss this issue</italic>.<italic>”</italic></p><p><italic>Really, though, it doesn't appear that much would be lost by just removing the experiment</italic>.</p><p>We agree with the reviewers that perhaps including the experiment, while it does provide some information, makes the manuscript less effective overall. We have thus removed the subjective experiment and the corresponding figure 1.</p><p><italic>3) The discussion should address the fact that</italic> d' <italic>for the matching target condition did not differ from that in the independent condition. This indicates that binding is only beneficial compared to a condition in which the perception of the masker is facilitated.</italic></p><p>This point is now addressed in the Results section where we address the basic trends found in the results (“There was no significant difference between…”).</p><p><italic>4) Alternatives to binding by “objectness” and the framework for considering auditory objects should be discussed</italic>.</p><p><italic>In the words of one reviewer</italic>:</p><p><italic>“The main ‘quantitative’ experiment provides strong evidence that the temporal co-modulation of the auditory and visual stimuli does produce a detection advantage for the orthogonal (frequency based) auditory deviations. The overall effect is consistent with the previous literature (although the observation is novel in its own right) and it is very likely that this is mediated by some attentional element in the processing. Indeed, it is plausible that this is mediated through an increase in the ‘objectness’ of the multi-sensory stimulus used here, however, this is not necessarily so.</italic> <xref ref-type="bibr" rid="bib21"><italic>Griffiths and Warren (2004)</italic></xref> <italic>argue that objects will tend to represent actual sources, have boundaries (they can be segregated) and are relatively invariant. These stimuli are artificial so do not reflect the first characteristic and given the data reported here, may reflect the second characteristic. If these features are indeed bound to create a perceptual object then possibly they will reflect invariance. These data may suggest this is the case but the argument is not made explicitly in the manuscript. A more structured frame work for what constitutes an object in general and an auditory object in particular is required and the data discussed in that light</italic>.<italic>”</italic></p><p>We now discuss the Griffiths and Warren paper in depth in the Discussion section (“Griffiths and Warren [37] suggest…”), which provides a set of criteria for what could define an auditory object. We then use this as a launching point for a discussion on the perceptual <italic>benefits</italic> of auditory-visual object formation, and how measuring such benefits actually provide an objective test of whether an object was formed or not.</p><p><italic>Recommended considerations</italic>:</p><p><italic>1) One reviewer and the Reviewing editor questioned the value added by Figure 4 and recommend removing it</italic>.</p><p>We included the original Figure 4 to provide a visual aid for what we feel is a complicated topic. Having shown the figure to a few people locally, some seemed not to like the rather stylized look of it. We have hence opted to retain it (now as <xref ref-type="fig" rid="fig3">Figure 3</xref>), but have reworked it with a more modest aesthetic.</p><p><italic>2) In the analysis, several ANOVAs were conducted. Perhaps a MANOVA is more appropriate, given the dependence of the various dependent measures</italic>.</p><p>While <italic>d′</italic> and bias are each dependent on hit rate and false alarm rate, there is no dependence between <italic>d′</italic> and bias, or between false alarm rate and hit rate, and none of these should be dependent with visual hit rate. While the MANOVA might provide a bit more statistical power, we feel that the separate ANOVAs run here are easier to interpret while being more specific.</p></body></sub-article></article>