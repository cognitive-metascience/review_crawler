<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="discussion" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">45261</article-id><article-id pub-id-type="doi">10.7554/eLife.45261</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature Article</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Point of View</subject></subj-group></article-categories><title-group><article-title>Four erroneous beliefs thwarting more trustworthy research</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-129674"><name><surname>Yarborough</surname><given-names>Mark</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8188-4968</contrib-id><email>mayarborough@ucdavis.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Mark Yarborough</bold> is in the Bioethics Program, University of California, Davis, Sacramento, CA, United States</p></bio></contrib><contrib contrib-type="author" id="author-135647"><name><surname>Nadon</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Robert Nadon</bold> is in the Department of Human Genetics, McGill University, Montreal, Canada</p></bio></contrib><contrib contrib-type="author" id="author-135648"><name><surname>Karlin</surname><given-names>David G</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>David G Karlin</bold> is an independent researcher based in Marseille, France</p></bio></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Bioethics Program</institution><institution>University of California, Davis</institution><addr-line><named-content content-type="city">Sacramento</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Human Genetics</institution><institution>McGill University</institution><addr-line><named-content content-type="city">Montreal</named-content></addr-line><country>Canada</country></aff><aff id="aff3"><label>3</label><institution>Independent researcher</institution><addr-line><named-content content-type="city">Marseille</named-content></addr-line><country>France</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Senior Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="editor"><name><surname>Pewsey</surname><given-names>Emma</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>29</day><month>07</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e45261</elocation-id><history><date date-type="received" iso-8601-date="2019-01-17"><day>17</day><month>01</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-07-25"><day>25</day><month>07</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Yarborough et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Yarborough et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-45261-v2.pdf"/><abstract><p>A range of problems currently undermines public trust in biomedical research. We discuss four erroneous beliefs that may prevent the biomedical research community from recognizing the need to focus on deserving this trust, and thus which act as powerful barriers to necessary improvements in the research process.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>trustworthy research</kwd><kwd>quality improvement</kwd><kwd>research reforms</kwd><kwd>reproducibility</kwd><kwd>point of view</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><funding-statement>The authors declare that there was no funding for this work</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Strategies to improve public trust in biomedical research are being hindered by a scientific mindset that stifles interest in reform.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In 2014, in an essay titled ‘Why scientists should be held to a higher standard of honesty than the average person,’ a former editor of the British Medical Journal argued that science depends wholly on trust (<xref ref-type="bibr" rid="bib71">Smith, 2014</xref>). While many in the biomedical research community may quibble over the word ‘wholly’ here, few would dispute his overall point: the public’s confidence is essential to the future of research. According to a noted scholar on the subject, the best way to enjoy trust is to deserve it (<xref ref-type="bibr" rid="bib32">Hardin, 2002</xref>). One would hope that the research community is a deserving case, given the existence of safeguards such as professional norms, regulatory compliance and peer review. Unfortunately, there is an ever-growing body of evidence that calls into question the effectiveness of these measures.</p><p>This evidence includes, but is by no means limited to, findings about underpowered studies (<xref ref-type="bibr" rid="bib40">Ioannidis, 2005</xref>), routine overestimations of efficacy (<xref ref-type="bibr" rid="bib68">Sena et al., 2010</xref>; <xref ref-type="bibr" rid="bib76">Tsilidis et al., 2013</xref>), the failure to take prior research into account (<xref ref-type="bibr" rid="bib66">Robinson and Goodman, 2011</xref>; <xref ref-type="bibr" rid="bib49">Lund et al., 2016</xref>), a propensity to confuse hypothesis-generating studies with hypothesis-confirming ones (<xref ref-type="bibr" rid="bib47">Kimmelman et al., 2014</xref>), a worrisome waste of resources (<xref ref-type="bibr" rid="bib14">Chalmers and Glasziou, 2009</xref>), and the low uptake of critical reforms meant to improve research (<xref ref-type="bibr" rid="bib20">Enserink, 2017</xref>; <xref ref-type="bibr" rid="bib63">Peers et al., 2014</xref>). A recent popular book, <italic>Rigor Mortis</italic>, synthesizes such evidence into a compelling narrative that casts the reputation of research in a negative light (<xref ref-type="bibr" rid="bib33">Harris, 2017</xref>).</p><p>While all of this evidence is cause for concern, we are most concerned by the reluctance of the research community to implement the reforms that could improve research quality. One can imagine a continuum of research practices that impact how scientific understanding advances. At one end one encounters the unforgivable, such as data fabrication or falsification. At the other end one finds the perfect, such as published research reports so thorough that findings can be easily reproduced from them.</p><p>The concerns of interest to us in what follows have little to do with the misconduct found on the unforgivable end of the continuum. Instead, they fall all along it and pertain to unsound research practices (such as non-robust reporting of methods, flawed study designs, incomplete reporting of data handling, and deficient statistical analyses) that nevertheless impede the advance of science. These are the practices that reform measures could counter if researchers were less reluctant to adopt them. In an effort to account for this reluctance, we review four erroneous beliefs that we think contribute to it.</p><p>We acknowledge that we lack extensive data confirming the prevalence and distribution of these beliefs. Thus, readers can form their own opinions about whether the beliefs are as widespread as we fear they are. We have come upon our concerns as a result of our careers related to biomedical research, which will be the focus of our remarks below, though we think the issues are relevant to life sciences research more broadly. One of us (MY) has extensively studied how to promote trustworthiness in biomedical research, and another (RN) has a long and successful career devoted to understanding the role of sound methodologies in producing it. The final author (DGK) is a preclinical researcher who was among those who pioneered early efforts to learn how researchers and research institutions can meaningfully connect the research community with the publics it seeks to serve. We think this collective pedigree lends credence to our analysis and to the strategy for moving forward that we recommend in the conclusion.</p></sec><sec id="s2"><title>Recognizing the barriers to a greater focus on deserving trust</title><sec id="s2-1"><title>It’s about the science, not the scientists</title><p>Erroneous belief one is that questioning the trustworthiness of research simultaneously questions the integrity of researchers. As a result, many individuals react counterproductively to calls to improve trustworthiness. They are akin to pilots who confuse discussions about improving the flightworthiness of airplanes with criticism of their aviation skills. Though understandable, such concerns miss the point (<xref ref-type="bibr" rid="bib82">Yarborough, 2014a</xref>). The multitude of methods, materials, highly sophisticated procedures and complex analyses intrinsic to biomedical research all create ways for it to err, making it exceptionally difficult to detect problems (<xref ref-type="bibr" rid="bib36">Hines et al., 2014</xref>). These are the critical matters that all researchers must learn to direct their attention to. Yet they cannot do so if constructive criticism about how to improve science is taken personally.</p></sec><sec id="s2-2"><title>We need to focus on the health of the orchard, not just the bad apples in it</title><p>Erroneous belief two is that the bulk of problems in research is due to bad actors. There is no doubt that misconduct is a substantial problem (<xref ref-type="bibr" rid="bib24">Fang et al., 2012</xref>). This should not blind us, however, to how common study design and data analysis errors are in biomedical research (<xref ref-type="bibr" rid="bib4">Altman, 1994</xref>). Indeed, these errors are likely to increase due to trends in current scientific practice, particularly the growing size and interdisciplinarity of investigative teams (<xref ref-type="bibr" rid="bib80">Wuchty et al., 2007</xref>; <xref ref-type="bibr" rid="bib34">He and Zhang, 2009</xref>; <xref ref-type="bibr" rid="bib25">Gazni et al., 2012</xref>). Because they require divisions of labor and expertise, such collaborations create fertile ground for producing unreliable research. Affected publications draw much less scrutiny than those of authors who engage in misconduct (<xref ref-type="bibr" rid="bib73">Steen et al., 2013</xref>), and thus problems in them are likely to be discovered much later, if at all. For example, consider that the number of retracted publications is much less than 1% of published articles (<xref ref-type="bibr" rid="bib29">Grieneisen and Zhang, 2012</xref>), yet publication bias has been found to affect entire classes of research (<xref ref-type="bibr" rid="bib76">Tsilidis et al., 2013</xref>; <xref ref-type="bibr" rid="bib52">Macleod et al., 2015</xref>).</p><p>The prevalence of erroneous research results and the enduring problems they cause require proactive efforts to detect and prevent them. What we find instead is a disproportionate emphasis on detecting and punishing ‘bad apples.’ The more we concentrate on this, the more difficult it becomes to identify strategies that allow us to focus on what should be seen as more pressing issues.</p></sec><sec id="s2-3"><title>Our beliefs about self-correcting science need self-correcting</title><p>Erroneous belief three is that science self-corrects. Assumptions that published studies are systematically replicated/replicable, or are later identified if they are not, build resistance against reforms. In theory, reproducibility injects quality assurance into the very heart of research. When one adds other traditional safeguards such as professional research norms and peer review, the reliability of research seems well guarded.</p><p>However, a growing body of research to check whether scientific results can be reproduced confirms the shortcomings of these safeguards (<xref ref-type="bibr" rid="bib37">Hudson, 2003</xref>; <xref ref-type="bibr" rid="bib3">Allchin, 2015</xref>; <xref ref-type="bibr" rid="bib7">Banobi et al., 2011</xref>; <xref ref-type="bibr" rid="bib85">Zimmer, 2011</xref>; <xref ref-type="bibr" rid="bib77">Twaij et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Drew, 2019</xref>). We mention just two examples of this research here. The Reproducibility Project: Cancer Biology has been underway for almost five years and originally sought to reproduce 50 critical cancer biology studies (<xref ref-type="bibr" rid="bib16">Couzin-Frankel, 2013</xref>). The project was scaled back to 18 studies, due largely to costs, but also because important details about research methods were unreported in some of the studies the effort sought to reproduce. As for results, of the first 13 completed replication studies, only five produced results similar to the original studies while the other eight produced either mixed or negative results (<xref ref-type="bibr" rid="bib46">Kaiser, 2018</xref>).</p><p>An effort to replicate the findings of 100 experimental studies in psychology journals produced a similarly low rate of replication. Only 36% of the original findings were replicated according to the conventional statistical significance standard of p&lt;0.05 for an effect in the same direction (<xref ref-type="bibr" rid="bib62">Open Science Collaboration, 2015</xref>).</p><disp-quote><p>When errors get corrected, it is more often due to happenstance than any kind of methodical effort</p></disp-quote><p>Such findings serve as a vivid wake-up call that alerts us to how easily and how often erroneous research results make their way into print, often in leading journals. Once there, they may linger for years or even decades prior to being discovered (if they are ever discovered) (<xref ref-type="bibr" rid="bib45">Judson, 2004</xref>; <xref ref-type="bibr" rid="bib8">Bar-Ilan and Halevi, 2017</xref>), and may continue to be cited post-discovery (<xref ref-type="bibr" rid="bib72">Steen, 2011</xref>). And when errors get corrected, it is more often due to happenstance than any kind of methodical effort (<xref ref-type="bibr" rid="bib3">Allchin, 2015</xref>). All this is sobering when we consider that erroneous findings can result in potentially dangerous clinical trials (<xref ref-type="bibr" rid="bib72">Steen, 2011</xref>).</p><p>Further shaking our confidence in the ability of science to self-correct is how few opportunities there actually are to confirm results. Efforts such as the Reproducibility Project: Cancer Biology notwithstanding, most research sponsors and publishers value, and thus fund and publish, innovative studies rather than research that tries to confirm past findings. And even if sponsors did place higher value on confirmatory studies, the growing complexity of science can make confirmation difficult, or even impossible (<xref ref-type="bibr" rid="bib43">Jasny et al., 2011</xref>). Besides information about study methods and materials possibly not being available, studies may also use novel and/or highly sensitive/volatile study materials (<xref ref-type="bibr" rid="bib36">Hines et al., 2014</xref>), impinge on intellectual property rights (<xref ref-type="bibr" rid="bib79">Williams, 2010</xref>; <xref ref-type="bibr" rid="bib28">Godfrey and German, 2008</xref>), or deal with proprietary data sets (<xref ref-type="bibr" rid="bib64">Peng, 2011</xref>). Thus, even if there was a time in science when there were chances ‘to get it right’ or when consensus could emerge, that is no longer the case (<xref ref-type="bibr" rid="bib83">Yarborough, 2014b</xref>).</p></sec><sec id="s2-4"><title>Following the rules does not guarantee we are getting it right</title><p>Erroneous belief four is that compliance with regulations is capable of solving the problems that gave rise to the regulations themselves. Governments, research sponsors and publishers have gone to great lengths to implement reforms that one hopes contribute to deserved trust. But this is true only to a point; one can follow all the rules, extensive though they may be, and still not get it right (<xref ref-type="bibr" rid="bib81">Yarborough et al., 2009</xref>). We offer efforts to combat research misconduct in the United States as evidence.</p><p>The United States Congress, following a series of research scandals, issued a mandate for corrective action to combat falsification, fabrication and plagiarism. This eventually led to a program that endures to this day (<xref ref-type="bibr" rid="bib61">Office of Research Integrity, 2015</xref>), requiring federally funded institutions to investigate allegations of research misconduct. The much larger body of poor-quality science is left completely unaddressed by these government rules. Research shows that about 2% of researchers report engaging in misconduct while fifteen times as many (30%) report having engaged in practices that contribute to irreproducible research (<xref ref-type="bibr" rid="bib21">Fanelli, 2009</xref>); other studies report even higher percentages (<xref ref-type="bibr" rid="bib44">John et al., 2012</xref>; <xref ref-type="bibr" rid="bib1">Agnoli et al., 2017</xref>). Yet, due to the need to follow the rules, resources go overwhelmingly to investigating misconduct. Thus, while such rules bestow quite modest protections to research, they require significant time, energy and money (<xref ref-type="bibr" rid="bib55">Michalek et al., 2010</xref>), and simultaneously provide a false sense of security that problems are being resolved – when in fact they are not (<xref ref-type="bibr" rid="bib83">Yarborough, 2014b</xref>).</p></sec></sec><sec id="s3"><title>Suggestions to help build cultures and climates that assure deserved trust</title><p>If we can find a way to shed these erroneous beliefs, we could become more proactive in showing how we deserve the public’s trust. We would not need to start de novo. There are already some proven solutions, as well as promising new recommendations and reforms, that can make inroads on many of the problems identified above. We highlight just a few of them below. Broad implementation of such initiatives could pay valuable dividends. For instance, rather than expend extraordinary resources on investigations of misconduct after it has caused damage (<xref ref-type="bibr" rid="bib55">Michalek et al., 2010</xref>), we might instead fund empirical studies of both existing and proposed reforms. In consequence, we could determine which reforms are most capable of strengthening the overall health of biomedical research (<xref ref-type="bibr" rid="bib41">Ioannidis, 2014</xref>).</p><p>We recognize that the solutions that we highlight below do not do justice to them as a class, but we do believe they constitute a reasonably representative group. Nor do we mean to suggest that they are without controversy. The main point of our essay, however, is not to provide a thorough review of current and proposed reforms and their individual merits. To do so would focus readers’ attention on what changes need to be made in research; our purpose is to explore erroneous beliefs that may prevent sufficient focus on why changes are needed in the first place.</p><disp-quote><p>If authors felt safe bringing honest errors to the attention of others, it would encourage much-needed openness about the mistakes that inevitably occur within fields as complex as biomedical research.</p></disp-quote><sec id="s3-1"><title>Publishing reforms: underway but they could be more ambitious</title><p>It is encouraging to see that many journals have begun to implement important reform measures. Among the most encouraging is that some now perform rigorous statistical review of appropriate studies, or make such reviews available to peer reviewers or associate editors who request them. Some journals have also modified their instructions to authors in order to improve the reporting of research results. The improved instructions bring transparency to research and aid reproducibility efforts. Recent studies of these modified instructions show that they improve published preclinical study reports, suggesting that even modest journal reforms can work to good effect (<xref ref-type="bibr" rid="bib75">The NPQIP Collaborative group, 2019</xref>; <xref ref-type="bibr" rid="bib56">Minnerup et al., 2016</xref>). It should be noted, though, that the benefits of such reforms might be small. A recent study showed that a checklist designed to improve compliance with the ARRIVE guidelines had a quite limited effect (<xref ref-type="bibr" rid="bib30">Hair et al., 2018</xref>), showing that having helpful tools is no guarantee that they will be used. Thus, it remains unclear what the ultimate impact of such reform measures might be.</p><p>With this evidence in mind, it would be nice if journals were even more ambitious and took on some more novel recommendations. One example is to consider expanding the taxonomy for correcting and retracting publications so that authors can avoid the current stigma around correcting the scientific record (<xref ref-type="bibr" rid="bib23">Fanelli et al., 2018</xref>). This would make it possible to take up a 2016 recommendation to reward authors for self-corrections and retractions (<xref ref-type="bibr" rid="bib22">Fanelli, 2016</xref>). If authors felt safe bringing honest errors to the attention of others, it would encourage much-needed openness about the mistakes that inevitably occur within fields as complex as biomedical research.</p></sec><sec id="s3-2"><title>Researcher practices: plentiful recommendations with too few takers</title><p>Publisher reforms can only accomplish so much. Most of the improvements that are required to demonstrate how the research community deserves the public’s trust need to arise from how research is conducted. A wealth of thoughtful recommendations are already in place, but too many are awaiting widespread adoption. Among the most notable are a set of recommendations for increasing value and reducing waste in biomedical research that appeared as part of a series of articles in <italic>The Lancet</italic> in 2014.</p><p>Those recommendations center around several needs: to carefully set research priorities; improve research design, conduct and analysis; improve research regulation and management; reduce incomplete or unusable reports of studies; and make research results more accessible (<xref ref-type="bibr" rid="bib50">Macleod et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Chalmers et al., 2014</xref>; <xref ref-type="bibr" rid="bib42">Ioannidis et al., 2014</xref>; <xref ref-type="bibr" rid="bib67">Salman et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Glasziou et al., 2014</xref>; <xref ref-type="bibr" rid="bib15">Chan et al., 2014</xref>). The series has not gone without notice, with more than 46,000 downloads of articles in the series within the first year of publication (<xref ref-type="bibr" rid="bib57">Moher et al., 2016</xref>) and over 900 citations (as of early 2019) in PubMed Central registered articles. Early evidence suggested that the series placed the issues that it addressed on the radar screens of research sponsors, regulators and journals. Disappointingly, academic institutions initially did not seem to pay them much notice (<xref ref-type="bibr" rid="bib57">Moher et al., 2016</xref>). This reinforces our concern that we need to identify what it is about the mindset of so many in the research community that is currently stifling interest in reform. So long as this lack of interest persists, there is little hope that what we consider the highest impact changes will occur anytime soon. We have two such changes in mind that researchers themselves need to take more of the lead on.</p><sec id="s3-2-1"><title>We need to improve research design and its reporting</title><p>Researchers need to pay more attention to research methodology, given its central role in establishing the reliability of published research results. Some journals now encourage this behavior by, for instance, requiring that authors complete checklists to indicate whether or not they have used study design procedures such as blinding, randomization and statistical power analysis. Depending on the journal and type of study, modest to substantial gains in reporting prevalence of study design details are achieved when researchers can complete these requirements (<xref ref-type="bibr" rid="bib75">The NPQIP Collaborative group, 2019</xref>; <xref ref-type="bibr" rid="bib30">Hair et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Han et al., 2017</xref>). Such improved reporting allows for better assessment of the published literature. Better still would be researchers routinely using universally accepted basic procedures. For example, it is widely acknowledged that for animal studies, randomly allocating animals to groups and blinding experimenters to group allocations is required for sound statistical inference (<xref ref-type="bibr" rid="bib51">Macleod, 2014</xref>).</p></sec><sec id="s3-2-2"><title>We need to increase data sharing</title><p>Routine sharing of data should be the new default for researchers, unless there are compelling reasons not to share. Data sharing can, among other things, promote reproducibility, improve the accuracy of results, accelerate research, and promote better risk-benefit analysis in clinical trials (<xref ref-type="bibr" rid="bib39">Institute of Medicine, 2013</xref>). Despite the growing consensus about the value that data sharing brings to research, we must acknowledge that when and how data sharing should occur remains controversial. As recently noted, “[s]ome argue that the researchers who invested time, dollars, and effort in producing data should have exclusive rights to analyze the data and publish their findings. Others point out that data sharing is difficult to enforce in any case, leading to an imbalance in who benefits from the practice – a problem that some researchers say has yet to be satisfactorily resolved” (<xref ref-type="bibr" rid="bib12">Callier, 2019</xref>). Given such issues, it comes as no surprise that compliance with journal data sharing policies can be lackluster (<xref ref-type="bibr" rid="bib74">Stodden et al., 2018</xref>).</p><p>Taking these difficulties into consideration, realistic suggestions to encourage data sharing include: 1) that all journals implement a clear data sharing policy (<xref ref-type="bibr" rid="bib60">Nosek et al., 2015</xref>) that allows reasonable flexibility to take into account cases when data cannot be shared because of ethical or identity protection concerns, or that allow ‘embargo’ periods during which data are not shared (<xref ref-type="bibr" rid="bib6">Banks et al., 2019</xref>); 2) that journals systematically require data sharing during the review process, to help reviewers to evaluate the results (this would have the additional benefit of meaning that no additional effort is required afterward to make the data public); 3) that training courses in Responsible Conduct of Research (RCR) include methods to de-identify study participants and aggregate their results (a major prerequisite to data sharing [<xref ref-type="bibr" rid="bib6">Banks et al., 2019</xref>]); and 4) the creation of awards for researchers who promote data sharing (<xref ref-type="bibr" rid="bib12">Callier, 2019</xref>).</p><p>Finally, we need to know whether improved methodology and increased data sharing are really leading to reproducible research. Unfortunately, we could not locate studies that have addressed this question, making this an important line of future research.</p></sec></sec><sec id="s3-3"><title>Institution level practices: promising and proven remedies looking for suitors</title><p>When it comes to institutional practices that could strengthen the trustworthiness of research, surely the holy grail would be to better align researcher incentives with good science (<xref ref-type="bibr" rid="bib78">Ware and Munafò, 2015</xref>). This would be a heavy lift since it would involve changes to how institutions collectively approach recruitment, tenure and promotion. Rather than relying upon current surrogates such as bibliometrics for assessing faculty productivity and success (<xref ref-type="bibr" rid="bib54">McKiernan, 2019</xref>), they would need to use more direct measures of good science. A workshop involving research quality and other experts was convened in Washington DC in 2017 to explore what such measures might be and how they might be used. It identified six key principles that institutions could embrace to effect such a transition (<xref ref-type="bibr" rid="bib58">Moher et al., 2018</xref>), but their effectiveness remains untested as they have yet to be implemented. It is worth noting, however, that at least one institution – the University Medical Center Utrecht – has tried to reengineer how it assesses its research programs and faculty in order to better align incentives with good science. In the words of the champions of that change initiative, they are learning how to better “shape the structures that shape science…[to] make sure that [those structures] do not warp it” (<xref ref-type="bibr" rid="bib10">Benedictus et al., 2016</xref>).</p><p>There are smaller scale reforms that institutions could also embrace to help ensure high quality standards in research. For example, there are many innovative practices that institutions could currently use to prevent problems, but are not. Perhaps the most obvious one is a research data audit. Akin to a finance audit, a research data audit is meant to check that published data are “quantifiable and verifiable&quot; by examining “the degree of correspondence of the published data with the original source data” (<xref ref-type="bibr" rid="bib69">Shamoo, 2013</xref>). First proposed at scientific conferences in the 1970 s, (<xref ref-type="bibr" rid="bib69">Shamoo, 2013</xref>) and later in print in <italic>Nature</italic> in 1987 (<xref ref-type="bibr" rid="bib17">Dawson, 1987</xref>), such audits “would typically require the examination of data in laboratory notebooks and other work sheets, upon which research publications are based” (<xref ref-type="bibr" rid="bib27">Glick, 1989</xref>). Advocates argue that data audits should be routine in as many settings as possible. This would provide a double benefit; it would help to deter fraud on the one hand and promote quality assurance on the other (<xref ref-type="bibr" rid="bib69">Shamoo, 2013</xref>).</p><p>The FDA and the United States Office of Research Integrity currently conduct such audits ‘for cause’ when misconduct or other misbehaviors are suspected. The FDA also uses them for certain new drugs deemed to be potentially ‘high risk.’ Although most current audits typically review the proper use of specified research procedures, there is no reason that they could not also be used to encourage the proper generation and use of actual data (<xref ref-type="bibr" rid="bib69">Shamoo, 2013</xref>).</p><p>Critical incident reporting (CRI) is another promising prevention practice. It can be used to uncover problems, that, if left unchecked, might prove detrimental to a group’s research or reports about their research. Open software exists for implementing such a system. Accessed anonymously online, the system prompts users to report in their own terms what happened that is of concern to them. Experts can then promptly analyze incidents to see what systems changes might prevent future recurrences. The first adopters of such a system report that it “has led to the emergence of a mature error culture, and has made the laboratory a safer and more communicative environment” (<xref ref-type="bibr" rid="bib18">Dirnagl et al., 2016</xref>).</p><p>The same opportunity pertains to two other successful problem reduction methods: root cause analysis (RCA) and failure modes and effects analysis (FMEA) (<xref ref-type="bibr" rid="bib82">Yarborough, 2014a</xref>). RCA examines past near misses and problems in order to identify their main contributors. FMEA anticipates ways that future concerns might occur and prioritizes the severity of negative consequences if they do occur (for example, in aviation one might compare increased fuel consumption by a plane versus the catastrophic failure of a wing). The most critically needed preventive measures can then be targeted to avoid severe problems occurring in the first place.</p><p>RCA and FMEA have both been used to good effect across a wide spectrum of industries and endeavors, including the pharmaceutical industry and clinical medicine. Their track record clearly shows that they can be used to reduce medication, surgical and anesthesia errors, and ensure quality in the drug manufacturing process. Both these methods lend themselves most easily to manufacturing and engineering settings, but their successes suggest they also warrant testing for use in research. In particular, they may improve the human factors that can lead to avoidable problems, especially in team-based science settings where geographic dispersion and distributed expertise are the norm (<xref ref-type="bibr" rid="bib82">Yarborough, 2014a</xref>; <xref ref-type="bibr" rid="bib18">Dirnagl et al., 2016</xref>).</p><p>It seems clear that data audits, CRI, RCA, and FMEA each have tremendous potential for improving research: potential that, like the above publishing reforms and researcher practices, has gone largely untapped to this point. We worry that the four erroneous beliefs that we have highlighted are blunting curiosity about the health of biomedical research, and are thereby preventing the adoption of a more proactive stance toward quality concerns. Hence, a critical next challenge is learning how to erode the appeal of these beliefs.</p><p>One strategy that we think is particularly worth considering is education. A wider appreciation of evidence that demonstrates the range and extent of quality concerns in research, combined with evidence about how few of them stem from research misconduct, should diminish belief that a few bad apples are our biggest problems. A placeholder for this education is already in place. RCR education is now firmly ensconced in many graduate and postgraduate life sciences courses and could naturally incorporate modules that tackle the erroneous beliefs head on.</p><p>We should note, however, that this strategy is far from perfect, given longstanding concerns about the effectiveness of RCR curricula (<xref ref-type="bibr" rid="bib5">Antes et al., 2010</xref>; <xref ref-type="bibr" rid="bib65">Presidential Commission for the Study of Bioethical Issues, 2011</xref>) and the fact that sponsors who mandate RCR instruction, like the National Institutes of Health (NIH) and the National Science Foundation (NSF) in the United States, often stipulate content that needs to be covered by it. The latter challenge need not be insuperable, though, since both NIH and NSF also encourage innovation and customization of RCR learning activities. Using RCR education as a vehicle for fostering improved quality in research may also help to make such instruction appear more relevant to the careers of learners.</p><p>As an example, RCR sessions could examine the scientific record on self-correction. The aforementioned cancer and psychology replication projects would surely warrant consideration, but we think that an equally relevant and highly illustrative case study showing how this might be done is a recently published study (<xref ref-type="bibr" rid="bib11">Border et al., 2019</xref>) about the lasting detrimental impact of a 1996 study about the SLC6A<sub>4</sub> gene on depression research (<xref ref-type="bibr" rid="bib48">Lesch et al., 1996</xref>). This publication spurred at least an additional 450 published ones, consumed millions of dollars, and controversy about it continues to this day (<xref ref-type="bibr" rid="bib84">Yong, 2019</xref>). Such case studies can drive home multiple lessons because they simultaneously show how science cannot be relied upon to self-correct in a timely or efficient way and that regulations often fail to touch upon matters critical to the health of research.</p><disp-quote><p>There are plenty of thoughtfully tailored recommendations that have not yet resulted in the improvements to research they are surely capable of producing</p></disp-quote></sec></sec><sec id="s4"><title>Conclusion</title><p>Readers may be tempted to dismiss the foregoing analysis of erroneous beliefs as mere personal observations. They may prefer instead either hard data about how research measures up against metrics that contribute to deserving trust. Or they may wish for yet another round of study design and data analysis recommendations capable of solving the broad range of ills currently diminishing the quality of research. The recommendations would plot the path to progress while the data would make our pace of progress apparent to all.</p><p>As we have tried to make clear, there are plenty of thoughtfully tailored recommendations that have not yet resulted in the improvements to research they are surely capable of producing – simply because there has been too little uptake of them. Nor, for that matter, is there any shortage of calls to arms and manifestos, including those from some of the most eminent scholars and leaders in biomedical research (<xref ref-type="bibr" rid="bib2">Alberts et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Munafò et al., 2017</xref>). Since these have had such little effect so far, especially at the institutional level, it is not clear why we would expect yet more recommendations to enjoy a better reception. Besides, many questionable research practices are hidden from view. For example, inconvenient data points, or even entire experiments, are at times ignored (<xref ref-type="bibr" rid="bib53">Martinson et al., 2005</xref>); data are added to experiments until desired p-values are obtained (<xref ref-type="bibr" rid="bib70">Simmons et al., 2011</xref>); and unreliable methods are used when randomizing animals in studies (<xref ref-type="bibr" rid="bib38">Institute for Laboratory Animal Research Roundtable on Science and Welfare in Laboratory Animal Use, 2015</xref>). Because these behaviors are hidden, traditional metrics are unlikely to capture their extent or their influence on the trustworthiness of research.</p><p>These behaviors notwithstanding, ‘open science’ practices would be one way to increase confidence in research results that could also provide metrics of trustworthiness. For example, some questionable research practices, such as p-hacking (<xref ref-type="bibr" rid="bib35">Head et al., 2015</xref>), could be detected more easily by requiring that data and analysis code be publicly available in all but the most exceptional circumstances. Indeed, one group has called for traditional institutional performance metrics such as impact factor and number of publications to be replaced with open science metrics (<xref ref-type="bibr" rid="bib9">Barnett and Moher, 2019</xref>). Although measurable open science would not eliminate questionable research practices, it would move biomedical research toward increased accountability.</p><p>Open science practices are still no panacea, however, for all the quality concerns we have highlighted here. What is most needed at this juncture is a collective focus on deserving trust. Such a focus could make researchers and the leaders of research institutions more receptive to reform efforts. The four erroneous beliefs we have discussed surely hinder that collective focus, and thus deter the research community from adopting reforms that can secure the public’s trust – which is vital to biomedical research.</p></sec></body><back><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing—original draft, Writing—review and editing</p></fn></fn-group><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agnoli</surname> <given-names>F</given-names></name><name><surname>Wicherts</surname> <given-names>JM</given-names></name><name><surname>Veldkamp</surname> <given-names>CL</given-names></name><name><surname>Albiero</surname> <given-names>P</given-names></name><name><surname>Cubelli</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Questionable research practices among Italian research psychologists</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0172792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0172792</pub-id><pub-id pub-id-type="pmid">28296929</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alberts</surname> <given-names>B</given-names></name><name><surname>Kirschner</surname> <given-names>MW</given-names></name><name><surname>Tilghman</surname> <given-names>S</given-names></name><name><surname>Varmus</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rescuing US biomedical research from its systemic flaws</article-title><source>PNAS</source><volume>111</volume><fpage>5773</fpage><lpage>5777</lpage><pub-id pub-id-type="doi">10.1073/pnas.1404402111</pub-id><pub-id pub-id-type="pmid">24733905</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allchin</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Correcting the “self-correcting” mythos of science</article-title><source>Filosofia E História Da Biologia</source><volume>10</volume><fpage>19</fpage><lpage>35</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altman</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>The scandal of poor medical research</article-title><source>BMJ</source><volume>308</volume><fpage>283</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1136/bmj.308.6924.283</pub-id><pub-id pub-id-type="pmid">8124111</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antes</surname> <given-names>AL</given-names></name><name><surname>Wang</surname> <given-names>X</given-names></name><name><surname>Mumford</surname> <given-names>MD</given-names></name><name><surname>Brown</surname> <given-names>RP</given-names></name><name><surname>Connelly</surname> <given-names>S</given-names></name><name><surname>Devenport</surname> <given-names>LD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Evaluating the effects that existing instruction on responsible conduct of research has on ethical decision making</article-title><source>Academic Medicine</source><volume>85</volume><fpage>519</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1097/ACM.0b013e3181cd1cc5</pub-id><pub-id pub-id-type="pmid">20182131</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banks</surname> <given-names>GC</given-names></name><name><surname>Field</surname> <given-names>JG</given-names></name><name><surname>Oswald</surname> <given-names>FL</given-names></name><name><surname>O’Boyle</surname> <given-names>EH</given-names></name><name><surname>Landis</surname> <given-names>RS</given-names></name><name><surname>Rupp</surname> <given-names>DE</given-names></name><name><surname>Rogelberg</surname> <given-names>SG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Answers to 18 questions about open science practices</article-title><source>Journal of Business and Psychology</source><volume>34</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1007/s10869-018-9547-8</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banobi</surname> <given-names>JA</given-names></name><name><surname>Branch</surname> <given-names>TA</given-names></name><name><surname>Hilborn</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Do rebuttals affect future science?</article-title><source>Ecosphere</source><volume>2</volume><fpage>art37</fpage><pub-id pub-id-type="doi">10.1890/ES10-00142.1</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Ilan</surname> <given-names>J</given-names></name><name><surname>Halevi</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Post retraction citations in context: A case study</article-title><source>Scientometrics</source><volume>113</volume><fpage>547</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1007/s11192-017-2242-0</pub-id><pub-id pub-id-type="pmid">29056790</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname> <given-names>AG</given-names></name><name><surname>Moher</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Turning the tables: A university league-table based on quality not quantity [version 1; peer review: 1 approved]</article-title><source>F1000Research</source></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benedictus</surname> <given-names>R</given-names></name><name><surname>Miedema</surname> <given-names>F</given-names></name><name><surname>Ferguson</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fewer numbers, better science</article-title><source>Nature</source><volume>538</volume><fpage>453</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1038/538453a</pub-id><pub-id pub-id-type="pmid">27786219</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Border</surname> <given-names>R</given-names></name><name><surname>Johnson</surname> <given-names>EC</given-names></name><name><surname>Evans</surname> <given-names>LM</given-names></name><name><surname>Smolen</surname> <given-names>A</given-names></name><name><surname>Berley</surname> <given-names>N</given-names></name><name><surname>Sullivan</surname> <given-names>PF</given-names></name><name><surname>Keller</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>No support for historical candidate gene or candidate gene-by-interaction hypotheses for major depression across multiple large samples</article-title><source>American Journal of Psychiatry</source><volume>176</volume><fpage>376</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.2018.18070881</pub-id><pub-id pub-id-type="pmid">30845820</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Callier</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The open data explosion</article-title><source>The Scientist</source><ext-link ext-link-type="uri" xlink:href="https://www.the-scientist.com/careers/the-open-data-explosion-65248">https://www.the-scientist.com/careers/the-open-data-explosion-65248</ext-link><date-in-citation iso-8601-date="2019-07-18">July 18, 2019</date-in-citation></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalmers</surname> <given-names>I</given-names></name><name><surname>Bracken</surname> <given-names>MB</given-names></name><name><surname>Djulbegovic</surname> <given-names>B</given-names></name><name><surname>Garattini</surname> <given-names>S</given-names></name><name><surname>Grant</surname> <given-names>J</given-names></name><name><surname>Gülmezoglu</surname> <given-names>AM</given-names></name><name><surname>Howells</surname> <given-names>DW</given-names></name><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name><name><surname>Oliver</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How to increase value and reduce waste when research priorities are set</article-title><source>The Lancet</source><volume>383</volume><fpage>156</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62229-1</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalmers</surname> <given-names>I</given-names></name><name><surname>Glasziou</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Avoidable waste in the production and reporting of research evidence</article-title><source>The Lancet</source><volume>374</volume><fpage>86</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(09)60329-9</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname> <given-names>A-W</given-names></name><name><surname>Song</surname> <given-names>F</given-names></name><name><surname>Vickers</surname> <given-names>A</given-names></name><name><surname>Jefferson</surname> <given-names>T</given-names></name><name><surname>Dickersin</surname> <given-names>K</given-names></name><name><surname>Gøtzsche</surname> <given-names>PC</given-names></name><name><surname>Krumholz</surname> <given-names>HM</given-names></name><name><surname>Ghersi</surname> <given-names>D</given-names></name><name><surname>van der Worp</surname> <given-names>HB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Increasing value and reducing waste: Addressing inaccessible research</article-title><source>The Lancet</source><volume>383</volume><fpage>257</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62296-5</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Couzin-Frankel</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Complete. Repeat? Initiative gets $1.3 million to try to replicate cancer studies</article-title><source>Science</source><ext-link ext-link-type="uri" xlink:href="https://www.sciencemag.org/news/2013/10/complete-repeat-initiative-gets-13-million-try-replicate-cancer-studies">https://www.sciencemag.org/news/2013/10/complete-repeat-initiative-gets-13-million-try-replicate-cancer-studies</ext-link><date-in-citation iso-8601-date="2019-07-18">July 18, 2019</date-in-citation></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dawson</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Ensuring scientific integrity</article-title><source>Nature</source><volume>327</volume><elocation-id>550</elocation-id><pub-id pub-id-type="doi">10.1038/327550a0</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dirnagl</surname> <given-names>U</given-names></name><name><surname>Przesdzing</surname> <given-names>I</given-names></name><name><surname>Kurreck</surname> <given-names>C</given-names></name><name><surname>Major</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A laboratory critical incident and error reporting system for experimental biomedicine</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e2000705</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2000705</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Drew</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>APS replication initiative under way</article-title><source>Observer. Vol 26: Association for Psychological Science 2013</source><ext-link ext-link-type="uri" xlink:href="https://www.psychologicalscience.org/observer/aps-replication-initiative-underway">https://www.psychologicalscience.org/observer/aps-replication-initiative-underway</ext-link><date-in-citation iso-8601-date="2019-07-18">July 18, 2019</date-in-citation></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enserink</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sloppy reporting on animal studies proves hard to change</article-title><source>Science</source><volume>357</volume><fpage>1337</fpage><lpage>1338</lpage><pub-id pub-id-type="doi">10.1126/science.357.6358.1337</pub-id><pub-id pub-id-type="pmid">28963232</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e5738</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0005738</pub-id><pub-id pub-id-type="pmid">19478950</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Set up a 'self-retraction' system for honest errors</article-title><source>Nature</source><volume>531</volume><elocation-id>415</elocation-id><pub-id pub-id-type="doi">10.1038/531415a</pub-id><pub-id pub-id-type="pmid">27008933</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname> <given-names>D</given-names></name><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name><name><surname>Goodman</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Improving the integrity of published science: An expanded taxonomy of retractions and corrections</article-title><source>European Journal of Clinical Investigation</source><volume>48</volume><elocation-id>e12898</elocation-id><pub-id pub-id-type="doi">10.1111/eci.12898</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname> <given-names>FC</given-names></name><name><surname>Steen</surname> <given-names>RG</given-names></name><name><surname>Casadevall</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Misconduct accounts for the majority of retracted scientific publications</article-title><source>PNAS</source><volume>109</volume><fpage>17028</fpage><lpage>17033</lpage><pub-id pub-id-type="doi">10.1073/pnas.1212247109</pub-id><pub-id pub-id-type="pmid">23027971</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gazni</surname> <given-names>A</given-names></name><name><surname>Sugimoto</surname> <given-names>CR</given-names></name><name><surname>Didegah</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mapping world scientific collaboration: Authors, institutions, and countries</article-title><source>Journal of the American Society for Information Science and Technology</source><volume>63</volume><fpage>323</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1002/asi.21688</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasziou</surname> <given-names>P</given-names></name><name><surname>Altman</surname> <given-names>DG</given-names></name><name><surname>Bossuyt</surname> <given-names>P</given-names></name><name><surname>Boutron</surname> <given-names>I</given-names></name><name><surname>Clarke</surname> <given-names>M</given-names></name><name><surname>Julious</surname> <given-names>S</given-names></name><name><surname>Michie</surname> <given-names>S</given-names></name><name><surname>Moher</surname> <given-names>D</given-names></name><name><surname>Wager</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reducing waste from incomplete or unusable reports of biomedical research</article-title><source>The Lancet</source><volume>383</volume><fpage>267</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62228-X</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glick</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1989">1989</year><chapter-title>On the cost effectiveness of data auditing</chapter-title><person-group person-group-type="editor"><name><surname>Shamoo</surname> <given-names>A. E</given-names></name></person-group><source>Principles of Research Data Audit</source><publisher-name>Taylor &amp; Francis</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Godfrey</surname> <given-names>MW</given-names></name><name><surname>German</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The past, present and future of software evolution</article-title><conf-name>2008 Frontiers of Software Maintenance</conf-name><pub-id pub-id-type="doi">10.1109/fosm.2008.4659256</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieneisen</surname> <given-names>ML</given-names></name><name><surname>Zhang</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A comprehensive survey of retracted articles from the scholarly literature</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e44118</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0044118</pub-id><pub-id pub-id-type="pmid">23115617</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hair</surname> <given-names>K</given-names></name><name><surname>Macleod</surname> <given-names>M</given-names></name><name><surname>Sena</surname> <given-names>E</given-names></name><collab>IICARus Collaboration</collab></person-group><year iso-8601-date="2018">2018</year><article-title>A randomised controlled trial of an intervention to improve compliance with the ARRIVE guidelines (IICARus)</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/370874</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname> <given-names>S</given-names></name><name><surname>Olonisakin</surname> <given-names>TF</given-names></name><name><surname>Pribis</surname> <given-names>JP</given-names></name><name><surname>Zupetic</surname> <given-names>J</given-names></name><name><surname>Yoon</surname> <given-names>JH</given-names></name><name><surname>Holleran</surname> <given-names>KM</given-names></name><name><surname>Jeong</surname> <given-names>K</given-names></name><name><surname>Shaikh</surname> <given-names>N</given-names></name><name><surname>Rubio</surname> <given-names>DM</given-names></name><name><surname>Lee</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A checklist is associated with increased quality of reporting preclinical biomedical research: A systematic review</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0183591</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0183591</pub-id><pub-id pub-id-type="pmid">28902887</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hardin</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Trust and Trustworthiness</source><publisher-loc>New York</publisher-loc><publisher-name>Russell Sage Foundation</publisher-name></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>RF</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Rigor Mortis: How Sloppy Science Creates Worthless Cures, Crushes Hope, and Wastes Billions</source><publisher-loc>New York</publisher-loc><publisher-name>Basic Books</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname> <given-names>X</given-names></name><name><surname>Zhang</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>On the growth of scientific knowledge: Yeast biology as a case study</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000320</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000320</pub-id><pub-id pub-id-type="pmid">19300476</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Head</surname> <given-names>ML</given-names></name><name><surname>Holman</surname> <given-names>L</given-names></name><name><surname>Lanfear</surname> <given-names>R</given-names></name><name><surname>Kahn</surname> <given-names>AT</given-names></name><name><surname>Jennions</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The extent and consequences of p-hacking in science</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002106</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002106</pub-id><pub-id pub-id-type="pmid">25768323</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname> <given-names>WC</given-names></name><name><surname>Su</surname> <given-names>Y</given-names></name><name><surname>Kuhn</surname> <given-names>I</given-names></name><name><surname>Polyak</surname> <given-names>K</given-names></name><name><surname>Bissell</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sorting out the FACS: A devil in the details</article-title><source>Cell Reports</source><volume>6</volume><fpage>779</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2014.02.021</pub-id><pub-id pub-id-type="pmid">24630040</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hudson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Applying the lessons of high risk industries to health care</article-title><source>Quality and Safety in Health Care</source><volume>12</volume><fpage>7i</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1136/qhc.12.suppl_1.i7</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Institute for Laboratory Animal Research Roundtable on Science and Welfare in Laboratory Animal Use</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Reproducibility issues in research with animals and animal models workshop in brief October 2015</article-title><ext-link ext-link-type="uri" xlink:href="https://www.nap.edu/read/21835/#slide1">https://www.nap.edu/read/21835/#slide1</ext-link><date-in-citation iso-8601-date="2019-07-18">July 18, 2019</date-in-citation></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><collab>Institute of Medicine</collab></person-group><year iso-8601-date="2013">2013</year><source>Sharing Clinical Research Data:Workshop Summary</source><publisher-loc>Washington, DC</publisher-loc><publisher-name>Institute of Medicine</publisher-name></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Why most published research findings are false</article-title><source>PLOS Medicine</source><volume>2</volume><elocation-id>e124</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.0020124</pub-id><pub-id pub-id-type="pmid">16060722</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How to make more published research true</article-title><source>PLOS Medicine</source><volume>11</volume><elocation-id>e1001747</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.1001747</pub-id><pub-id pub-id-type="pmid">25334033</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name><name><surname>Greenland</surname> <given-names>S</given-names></name><name><surname>Hlatky</surname> <given-names>MA</given-names></name><name><surname>Khoury</surname> <given-names>MJ</given-names></name><name><surname>Macleod</surname> <given-names>MR</given-names></name><name><surname>Moher</surname> <given-names>D</given-names></name><name><surname>Schulz</surname> <given-names>KF</given-names></name><name><surname>Tibshirani</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Increasing value and reducing waste in research design, conduct, and analysis</article-title><source>The Lancet</source><volume>383</volume><fpage>166</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62227-8</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jasny</surname> <given-names>BR</given-names></name><name><surname>Chin</surname> <given-names>G</given-names></name><name><surname>Chong</surname> <given-names>L</given-names></name><name><surname>Vignieri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Again, and again, and again ..</article-title><source>Science</source><volume>334</volume><elocation-id>1225</elocation-id><pub-id pub-id-type="doi">10.1126/science.334.6060.1225</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>John</surname> <given-names>LK</given-names></name><name><surname>Loewenstein</surname> <given-names>G</given-names></name><name><surname>Prelec</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Measuring the prevalence of questionable research practices with incentives for truth telling</article-title><source>Psychological Science</source><volume>23</volume><fpage>524</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1177/0956797611430953</pub-id><pub-id pub-id-type="pmid">22508865</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Judson</surname> <given-names>HF</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>The Great Betrayal: Fraud in Science</source><publisher-loc>Orlando</publisher-loc><publisher-name>Harcourt, Inc</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Plan to replicate 50 high-impact cancer papers shrinks to just 18</article-title><source>Science</source><ext-link ext-link-type="uri" xlink:href="http://www.sciencemag.org/news/2018/07/plan-replicate-50-high-impact-cancer-papers-shrinks-just-18">http://www.sciencemag.org/news/2018/07/plan-replicate-50-high-impact-cancer-papers-shrinks-just-18</ext-link><date-in-citation iso-8601-date="2018-08-06">August 6, 2018</date-in-citation></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimmelman</surname> <given-names>J</given-names></name><name><surname>Mogil</surname> <given-names>JS</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Distinguishing between exploratory and confirmatory preclinical research will improve translation</article-title><source>PLOS Biology</source><volume>12</volume><elocation-id>e1001863</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001863</pub-id><pub-id pub-id-type="pmid">24844265</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lesch</surname> <given-names>KP</given-names></name><name><surname>Bengel</surname> <given-names>D</given-names></name><name><surname>Heils</surname> <given-names>A</given-names></name><name><surname>Sabol</surname> <given-names>SZ</given-names></name><name><surname>Greenberg</surname> <given-names>BD</given-names></name><name><surname>Petri</surname> <given-names>S</given-names></name><name><surname>Benjamin</surname> <given-names>J</given-names></name><name><surname>Müller</surname> <given-names>CR</given-names></name><name><surname>Hamer</surname> <given-names>DH</given-names></name><name><surname>Murphy</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Association of anxiety-related traits with a polymorphism in the serotonin transporter gene regulatory region</article-title><source>Science</source><volume>274</volume><fpage>1527</fpage><lpage>1531</lpage><pub-id pub-id-type="doi">10.1126/science.274.5292.1527</pub-id><pub-id pub-id-type="pmid">8929413</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lund</surname> <given-names>H</given-names></name><name><surname>Brunnhuber</surname> <given-names>K</given-names></name><name><surname>Juhl</surname> <given-names>C</given-names></name><name><surname>Robinson</surname> <given-names>K</given-names></name><name><surname>Leenaars</surname> <given-names>M</given-names></name><name><surname>Dorch</surname> <given-names>BF</given-names></name><name><surname>Jamtvedt</surname> <given-names>G</given-names></name><name><surname>Nortvedt</surname> <given-names>MW</given-names></name><name><surname>Christensen</surname> <given-names>R</given-names></name><name><surname>Chalmers</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Towards evidence based research</article-title><source>BMJ</source><volume>355</volume><elocation-id>i5440</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.i5440</pub-id><pub-id pub-id-type="pmid">27797786</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname> <given-names>MR</given-names></name><name><surname>Michie</surname> <given-names>S</given-names></name><name><surname>Roberts</surname> <given-names>I</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name><name><surname>Chalmers</surname> <given-names>I</given-names></name><name><surname>Ioannidis</surname> <given-names>JP</given-names></name><name><surname>Al-Shahi Salman</surname> <given-names>R</given-names></name><name><surname>Chan</surname> <given-names>AW</given-names></name><name><surname>Glasziou</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Biomedical research: Increasing value, reducing waste</article-title><source>The Lancet</source><volume>383</volume><fpage>101</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62329-6</pub-id><pub-id pub-id-type="pmid">24411643</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Preclinical research: Design animal studies better</article-title><source>Nature</source><volume>510</volume><elocation-id>35</elocation-id><pub-id pub-id-type="doi">10.1038/510035a</pub-id><pub-id pub-id-type="pmid">24899295</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname> <given-names>MR</given-names></name><name><surname>Lawson McLean</surname> <given-names>A</given-names></name><name><surname>Kyriakopoulou</surname> <given-names>A</given-names></name><name><surname>Serghiou</surname> <given-names>S</given-names></name><name><surname>de Wilde</surname> <given-names>A</given-names></name><name><surname>Sherratt</surname> <given-names>N</given-names></name><name><surname>Hirst</surname> <given-names>T</given-names></name><name><surname>Hemblade</surname> <given-names>R</given-names></name><name><surname>Bahor</surname> <given-names>Z</given-names></name><name><surname>Nunes-Fonseca</surname> <given-names>C</given-names></name><name><surname>Potluru</surname> <given-names>A</given-names></name><name><surname>Thomson</surname> <given-names>A</given-names></name><name><surname>Baginskitae</surname> <given-names>J</given-names></name><name><surname>Egan</surname> <given-names>K</given-names></name><name><surname>Vesterinen</surname> <given-names>H</given-names></name><name><surname>Currie</surname> <given-names>GL</given-names></name><name><surname>Churilov</surname> <given-names>L</given-names></name><name><surname>Howells</surname> <given-names>DW</given-names></name><name><surname>Sena</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Risk of bias in reports of in vivo research: A focus for improvement</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002273</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002273</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinson</surname> <given-names>BC</given-names></name><name><surname>Anderson</surname> <given-names>MS</given-names></name><name><surname>de Vries</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Scientists behaving badly</article-title><source>Nature</source><volume>435</volume><fpage>737</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1038/435737a</pub-id><pub-id pub-id-type="pmid">15944677</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKiernan</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Use of the journal impact factor in academic review, promotion, and tenure evaluations</article-title><source>PeerJ Preprints</source><volume>7</volume><elocation-id>e27638</elocation-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michalek</surname> <given-names>AM</given-names></name><name><surname>Hutson</surname> <given-names>AD</given-names></name><name><surname>Wicher</surname> <given-names>CP</given-names></name><name><surname>Trump</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The costs and underappreciated consequences of research misconduct: a case study</article-title><source>PLOS Medicine</source><volume>7</volume><elocation-id>e1000318</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.1000318</pub-id><pub-id pub-id-type="pmid">20808955</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minnerup</surname> <given-names>J</given-names></name><name><surname>Zentsch</surname> <given-names>V</given-names></name><name><surname>Schmidt</surname> <given-names>A</given-names></name><name><surname>Fisher</surname> <given-names>M</given-names></name><name><surname>Schäbitz</surname> <given-names>WR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Methodological quality of experimental stroke studies published in the stroke journal: Time trends and effect of the basic science checklist</article-title><source>Stroke</source><volume>47</volume><fpage>267</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1161/STROKEAHA.115.011695</pub-id><pub-id pub-id-type="pmid">26658439</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moher</surname> <given-names>D</given-names></name><name><surname>Glasziou</surname> <given-names>P</given-names></name><name><surname>Chalmers</surname> <given-names>I</given-names></name><name><surname>Nasser</surname> <given-names>M</given-names></name><name><surname>Bossuyt</surname> <given-names>PMM</given-names></name><name><surname>Korevaar</surname> <given-names>DA</given-names></name><name><surname>Graham</surname> <given-names>ID</given-names></name><name><surname>Ravaud</surname> <given-names>P</given-names></name><name><surname>Boutron</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Increasing value and reducing waste in biomedical research: who's listening?</article-title><source>The Lancet</source><volume>387</volume><fpage>1573</fpage><lpage>1586</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(15)00307-4</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moher</surname> <given-names>D</given-names></name><name><surname>Naudet</surname> <given-names>F</given-names></name><name><surname>Cristea</surname> <given-names>IA</given-names></name><name><surname>Miedema</surname> <given-names>F</given-names></name><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name><name><surname>Goodman</surname> <given-names>SN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Assessing scientists for hiring, promotion, and tenure</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004089</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004089</pub-id><pub-id pub-id-type="pmid">29596415</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munafò</surname> <given-names>MR</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Bishop</surname> <given-names>DVM</given-names></name><name><surname>Button</surname> <given-names>KS</given-names></name><name><surname>Chambers</surname> <given-names>CD</given-names></name><name><surname>Percie du Sert</surname> <given-names>N</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name><name><surname>Ware</surname> <given-names>JJ</given-names></name><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A manifesto for reproducible science</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0021</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-016-0021</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Alter</surname> <given-names>G</given-names></name><name><surname>Banks</surname> <given-names>GC</given-names></name><name><surname>Borsboom</surname> <given-names>D</given-names></name><name><surname>Bowman</surname> <given-names>SD</given-names></name><name><surname>Breckler</surname> <given-names>SJ</given-names></name><name><surname>Buck</surname> <given-names>S</given-names></name><name><surname>Chambers</surname> <given-names>CD</given-names></name><name><surname>Chin</surname> <given-names>G</given-names></name><name><surname>Christensen</surname> <given-names>G</given-names></name><name><surname>Contestabile</surname> <given-names>M</given-names></name><name><surname>Dafoe</surname> <given-names>A</given-names></name><name><surname>Eich</surname> <given-names>E</given-names></name><name><surname>Freese</surname> <given-names>J</given-names></name><name><surname>Glennerster</surname> <given-names>R</given-names></name><name><surname>Goroff</surname> <given-names>D</given-names></name><name><surname>Green</surname> <given-names>DP</given-names></name><name><surname>Hesse</surname> <given-names>B</given-names></name><name><surname>Humphreys</surname> <given-names>M</given-names></name><name><surname>Ishiyama</surname> <given-names>J</given-names></name><name><surname>Karlan</surname> <given-names>D</given-names></name><name><surname>Kraut</surname> <given-names>A</given-names></name><name><surname>Lupia</surname> <given-names>A</given-names></name><name><surname>Mabry</surname> <given-names>P</given-names></name><name><surname>Madon</surname> <given-names>T</given-names></name><name><surname>Malhotra</surname> <given-names>N</given-names></name><name><surname>Mayo-Wilson</surname> <given-names>E</given-names></name><name><surname>McNutt</surname> <given-names>M</given-names></name><name><surname>Miguel</surname> <given-names>E</given-names></name><name><surname>Paluck</surname> <given-names>EL</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Soderberg</surname> <given-names>C</given-names></name><name><surname>Spellman</surname> <given-names>BA</given-names></name><name><surname>Turitto</surname> <given-names>J</given-names></name><name><surname>VandenBos</surname> <given-names>G</given-names></name><name><surname>Vazire</surname> <given-names>S</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Wilson</surname> <given-names>R</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Promoting an open research culture</article-title><source>Science</source><volume>348</volume><fpage>1422</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1126/science.aab2374</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Office of Research Integrity</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Historical background</article-title><ext-link ext-link-type="uri" xlink:href="https://ori.hhs.gov/historical-background">https://ori.hhs.gov/historical-background</ext-link><date-in-citation iso-8601-date="2015-07-08">July 8, 2015</date-in-citation></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id><pub-id pub-id-type="pmid">26315443</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peers</surname> <given-names>IS</given-names></name><name><surname>South</surname> <given-names>MC</given-names></name><name><surname>Ceuppens</surname> <given-names>PR</given-names></name><name><surname>Bright</surname> <given-names>JD</given-names></name><name><surname>Pilling</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Can you trust your animal study data?</article-title><source>Nature Reviews Drug Discovery</source><volume>13</volume><elocation-id>560</elocation-id><pub-id pub-id-type="doi">10.1038/nrd4090-c1</pub-id><pub-id pub-id-type="pmid">24903777</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reproducible research in computational science</article-title><source>Science</source><volume>334</volume><fpage>1226</fpage><lpage>1227</lpage><pub-id pub-id-type="doi">10.1126/science.1213847</pub-id><pub-id pub-id-type="pmid">22144613</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Presidential Commission for the Study of Bioethical Issues</collab></person-group><year iso-8601-date="2011">2011</year><article-title>&quot;Ethically Impossible&quot; STD Research in Guatemala from 1946 to1948</article-title><ext-link ext-link-type="uri" xlink:href="https://bioethicsarchive.georgetown.edu/pcsbi/sites/default/files/Ethically%20Impossible%20(with%20linked%20historical%20documents)%202.7.13.pdf">https://bioethicsarchive.georgetown.edu/pcsbi/sites/default/files/Ethically%20Impossible%20(with%20linked%20historical%20documents)%202.7.13.pdf</ext-link><date-in-citation iso-8601-date="2019-07-18">July 18, 2019</date-in-citation></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname> <given-names>KA</given-names></name><name><surname>Goodman</surname> <given-names>SN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A systematic examination of the citation of prior research in reports of randomized, controlled trials</article-title><source>Annals of Internal Medicine</source><volume>154</volume><fpage>50</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.7326/0003-4819-154-1-201101040-00007</pub-id><pub-id pub-id-type="pmid">21200038</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salman</surname> <given-names>RA-S</given-names></name><name><surname>Beller</surname> <given-names>E</given-names></name><name><surname>Kagan</surname> <given-names>J</given-names></name><name><surname>Hemminki</surname> <given-names>E</given-names></name><name><surname>Phillips</surname> <given-names>RS</given-names></name><name><surname>Savulescu</surname> <given-names>J</given-names></name><name><surname>Macleod</surname> <given-names>M</given-names></name><name><surname>Wisely</surname> <given-names>J</given-names></name><name><surname>Chalmers</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Increasing value and reducing waste in biomedical research regulation and management</article-title><source>The Lancet</source><volume>383</volume><fpage>176</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62297-7</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sena</surname> <given-names>ES</given-names></name><name><surname>van der Worp</surname> <given-names>HB</given-names></name><name><surname>Bath</surname> <given-names>PM</given-names></name><name><surname>Howells</surname> <given-names>DW</given-names></name><name><surname>Macleod</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Publication bias in reports of animal stroke studies leads to major overstatement of efficacy</article-title><source>PLOS Biology</source><volume>8</volume><elocation-id>e1000344</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000344</pub-id><pub-id pub-id-type="pmid">20361022</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamoo</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Data audit as a way to prevent/contain misconduct</article-title><source>Accountability in Research</source><volume>20</volume><fpage>369</fpage><lpage>379</lpage><pub-id pub-id-type="doi">10.1080/08989621.2013.822259</pub-id><pub-id pub-id-type="pmid">24028483</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname> <given-names>JP</given-names></name><name><surname>Nelson</surname> <given-names>LD</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title><source>Psychological Science</source><volume>22</volume><fpage>1359</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1177/0956797611417632</pub-id><pub-id pub-id-type="pmid">22006061</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Why scientists should be held to a higher standard of honesty than the average person</article-title><source>The BMJ</source><ext-link ext-link-type="uri" xlink:href="https://blogs.bmj.com/bmj/2014/09/02/richard-smith-why-scientists-should-be-held-to-a-higher-standard-of-honesty-than-the-average-person/">https://blogs.bmj.com/bmj/2014/09/02/richard-smith-why-scientists-should-be-held-to-a-higher-standard-of-honesty-than-the-average-person/</ext-link><date-in-citation iso-8601-date="2019-07-25">July 25, 2019</date-in-citation></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steen</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Retractions in the medical literature: How many patients are put at risk by flawed research?</article-title><source>Journal of Medical Ethics</source><volume>37</volume><fpage>688</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1136/jme.2011.043133</pub-id><pub-id pub-id-type="pmid">21586404</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steen</surname> <given-names>RG</given-names></name><name><surname>Casadevall</surname> <given-names>A</given-names></name><name><surname>Fang</surname> <given-names>FC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Why has the number of scientific retractions increased?</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e68397</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0068397</pub-id><pub-id pub-id-type="pmid">23861902</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stodden</surname> <given-names>V</given-names></name><name><surname>Seiler</surname> <given-names>J</given-names></name><name><surname>Ma</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An empirical analysis of journal policy effectiveness for computational reproducibility</article-title><source>PNAS</source><volume>115</volume><fpage>2584</fpage><lpage>2589</lpage><pub-id pub-id-type="doi">10.1073/pnas.1708290115</pub-id><pub-id pub-id-type="pmid">29531050</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>The NPQIP Collaborative group</collab></person-group><year iso-8601-date="2019">2019</year><article-title>Did a change in Nature journals’ editorial policy for life sciences research improve reporting?</article-title><source>BMJ Open Science</source><volume>3</volume><elocation-id>e000035</elocation-id><pub-id pub-id-type="doi">10.1136/bmjos-2017-000035</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsilidis</surname> <given-names>KK</given-names></name><name><surname>Panagiotou</surname> <given-names>OA</given-names></name><name><surname>Sena</surname> <given-names>ES</given-names></name><name><surname>Aretouli</surname> <given-names>E</given-names></name><name><surname>Evangelou</surname> <given-names>E</given-names></name><name><surname>Howells</surname> <given-names>DW</given-names></name><name><surname>Al-Shahi Salman</surname> <given-names>R</given-names></name><name><surname>Macleod</surname> <given-names>MR</given-names></name><name><surname>Ioannidis</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Evaluation of excess significance bias in animal studies of neurological diseases</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001609</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001609</pub-id><pub-id pub-id-type="pmid">23874156</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twaij</surname> <given-names>H</given-names></name><name><surname>Oussedik</surname> <given-names>S</given-names></name><name><surname>Hoffmeyer</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Peer review</article-title><source>The Bone &amp; Joint Journal</source><volume>96-B</volume><fpage>436</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1302/0301-620X.96B4.33041</pub-id><pub-id pub-id-type="pmid">24692607</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ware</surname> <given-names>JJ</given-names></name><name><surname>Munafò</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Significance chasing in research practice: Causes, consequences and possible solutions</article-title><source>Addiction</source><volume>110</volume><fpage>4</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1111/add.12673</pub-id><pub-id pub-id-type="pmid">25040652</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname> <given-names>HL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Intellectual property rights and innovation: Evidence from the human genome</article-title><source>Journal of Political Economy</source><volume>121</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1086/669706</pub-id><pub-id pub-id-type="pmid">24639594</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wuchty</surname> <given-names>S</given-names></name><name><surname>Jones</surname> <given-names>BF</given-names></name><name><surname>Uzzi</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The increasing dominance of teams in production of knowledge</article-title><source>Science</source><volume>316</volume><fpage>1036</fpage><lpage>1039</lpage><pub-id pub-id-type="doi">10.1126/science.1136099</pub-id><pub-id pub-id-type="pmid">17431139</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarborough</surname> <given-names>M</given-names></name><name><surname>Fryer-Edwards</surname> <given-names>K</given-names></name><name><surname>Geller</surname> <given-names>G</given-names></name><name><surname>Sharp</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Transforming the culture of biomedical research from compliance to trustworthiness: Insights from nonmedical sectors</article-title><source>Academic Medicine</source><volume>84</volume><fpage>472</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1097/ACM.0b013e31819a8aa6</pub-id><pub-id pub-id-type="pmid">19318781</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarborough</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Taking steps to increase the trustworthiness of scientific research</article-title><source>The FASEB Journal</source><volume>28</volume><fpage>3841</fpage><lpage>3846</lpage><pub-id pub-id-type="doi">10.1096/fj.13-246603</pub-id><pub-id pub-id-type="pmid">24928193</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarborough</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Openness in science is key to keeping public trust</article-title><source>Nature</source><volume>515</volume><elocation-id>313</elocation-id><pub-id pub-id-type="doi">10.1038/515313a</pub-id><pub-id pub-id-type="pmid">25409791</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Yong</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A waste of 1,000 research papers</article-title><source>The Atlantic</source><ext-link ext-link-type="uri" xlink:href="https://www.theatlantic.com/science/archive/2019/05/waste-1000-studies/589684/">https://www.theatlantic.com/science/archive/2019/05/waste-1000-studies/589684/</ext-link><date-in-citation iso-8601-date="2019-07-18">July 18, 2019</date-in-citation></element-citation></ref><ref id="bib85"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Zimmer</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>It's science, but not necessarily right</article-title><source>International Herald Tribune</source><ext-link ext-link-type="uri" xlink:href="https://carlzimmer.com/its-science-but-not-necessarily-right-293/">https://carlzimmer.com/its-science-but-not-necessarily-right-293/</ext-link><date-in-citation iso-8601-date="2019-08-07">August 7, 2019</date-in-citation></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.45261.002</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Pewsey</surname><given-names>Emma</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>MacLeod</surname><given-names>Malcolm R</given-names></name><role>Reviewer</role><aff><institution>The University of Edinburgh</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Michel</surname><given-names>Martin</given-names> </name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Amaral</surname><given-names>Olavo B</given-names></name><role>Reviewer</role><aff><institution>Federal University of Rio de Janeiro</institution><country>Brazil</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Four erroneous beliefs thwarting more trustworthy research&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by Emma Pewsey (Associate Features Editor) and Peter Rodgers (Features Editor). The following individuals involved in review of your submission have agreed to reveal their identity: Malcolm R MacLeod (Reviewer #1); Martin Michel (Reviewer #2); Olavo B Amaral (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and I have drafted this decision to help you prepare a revised submission.</p><p>Please note that the reviewers have raised a number of discussion points. You do not need to address these in the manuscript itself, but you may wish to respond to them in your author response, which will appear on the <italic>eLife</italic> website along with your article and the decision letter.</p><p>Summary:</p><p>The article nicely summarises most of the major problems surrounding trust in biomedical research. While it is not the first article on this topic, the authors have written it up in a very refreshing and enjoyable way.</p><p>Essential revisions:</p><p>1) Throughout the text it appears to be more accurate to refer to &quot;irreproducible&quot; instead of &quot;erroneous&quot; research results. A result that is not reproduced might be a fully accurate description of a real experiment, but one that either describes a chance finding, a finding that is dependent on very specific experimental conditions, or it could indeed be erroneous (i.e. fabricated, falsified, or due to an erroneous analysis or method). If you feel it is important to use the term &quot;error&quot;, please clearly define what it means.</p><p>One suggestion from the reviewers to help you reframe the discussion to avoid a focus on &quot;errors&quot; is to think of research practices on a continuum, from the unforgivable to the perfect. At each point, the value of research can be reduced by making &quot;errors&quot; (be these deliberate or inadvertent, or even just consequences of the limitations of the methodologies used) or enhanced by adopting better practice (e.g. open data). For instance, the value of a non-randomised animal study is enhanced if this is clearly stated (because you can interpret the findings accordingly), and the value of any evidentiary claim is enhanced if it was asserted as the primary outcome measure in an a priori study protocol. Strategies that incorporate these different approaches might produce marginal improvements across the continuum.</p><p>2) Although the recommendations that follow the discussion of the &quot;four erroneous beliefs&quot; are all sensible, most of them do not address the beliefs directly. Please discuss in greater depth how the beliefs thwart attempts at reform, the approaches that could be taken to change them, and any further barriers that prevent this from happening. Ideally this discussion will include some empirical evidence and concrete examples.</p><p>Other discussion points:</p><p>1) In reviewer #2's experience of teaching on better reproducibility by more vigorous study design, data analysis and reporting, the PhD students in the class immediately see the point. However, they often come back and ask for advice when their supervisor dismisses concerns about reproducibility with &quot;we've always done it that way&quot; and &quot;everybody does it that way&quot;. Have you also found that young researchers are more open to change than established ones?</p><p>2) When discussing beliefs 1 and 2 (&quot;it's about the science, not the scientists&quot; and &quot;we need to focus on the health of the orchard, not just the bad apples&quot;), it could be worth discussing the fact that much of the training in research ethics over the last decades has revolved around misconduct. This is likely to reinforce these two beliefs – and, according to the authors' view, might preclude scientists from seeing the problem as their own (as misconduct and ethics breaches are rarely something that one will admit to). Perhaps framing research ethics training around the science (e.g. irreproducible research and its consequences), rather than the scientists, might help in overcoming these beliefs.</p><p>3) The authors mention at various points that there is &quot;reluctance within the research community to implement the reforms&quot;, that &quot;there has been too little uptake of them&quot; and that &quot;calls to arms have had little effect so far&quot;. The authors could discuss this issue in more depth and provide evidence to show that this is the case. What constitutes &quot;little effect&quot;? Some change does appear to be happening, albeit slowly. Although data and code sharing are still subpar, they are likely to be on the rise (e.g. Campbell et al., 2019 at doi: 10.1016/j.tree.2018.11.010), as are open access (Piwowar et al., 2018 at doi: 10.7717/peerj.4375) and preprint deposition (Kaiser, 2017 at doi: 10.1126/science.357.6358.1344).</p><p>4) The authors speak of &quot;reluctance within the research community to implement the reforms&quot;. But is the community really reluctant, or are many people unaware of the problem or unfamiliar with the possible solutions and necessary reforms? Similarly, the authors state that &quot;these beliefs are blunting curiosity about the health of biomedical research&quot;. The rising interest in the subject over the last few years, suggests that inertia in taking necessary changes and a feeling of non-responsibility and/or powerlessness towards them (&quot;it's the system, not me&quot;) might be more important than lack of acknowledgement. How do these different factors contribute and interact to prevent necessary changes?</p><p>5) On Belief 4, the authors argue that the fact that investment in tackling misconduct has failed to prevent irreproducible research is evidence that &quot;following the rules doesn't guarantee we are getting it right&quot;. But couldn't it be the case that this effort has focused on the major rules (e.g. do not falsify, fabricate or plagiarize) but has ignored the (many more) minor ones that are just as vital for published research findings to be reproducible (e.g. use adequate power, differentiate exploratory and confirmatory research, avoid p-hacking and HARKing)? The failure to prevent bad science could be because there are too few rules, rather than because following the rules doesn't work.</p><p>6) On the issue of science not being systematically self-correcting, it might be worth mentioning the high prevalence of failed replication attempts that are not published, most commonly due to the authors of replications not attempting to publish them – see Baker et al., 2016 (doi: 10.1038/533452a) for a survey-based indication of that.</p><p>7) &quot;… rather than research that tries to confirm past findings.&quot; One possibility to increase confirmation is to place higher value on confirmatory studies of past findings, but the other would be to raise the threshold for publication in the first place in some instances (e.g. requiring preregistration or independent confirmation, see Mogil and Macleod, 2017 – doi: 10.1038/542409a). As there are arguments for both sides, it could be worth touching on this point.</p><p>8) Subsection “Publishing Reforms: underway but they could be more ambitious”: You could discuss whether, with such a large number of journals (in which peer review varies widely in quality), and preprints making important headway in biology, we should expect the main source of quality control to come from journals. Note that arguments have been made in the opposite direction (e.g. removing barriers to publication to diminish its reward value and make 'publish or perish' senseless – e.g. Nosek and Bar-Anan, 2012 http://dx.doi.org/10.1080/1047840X.2012.692215 and others).</p><p>9) It sounds somewhat incongruent to state that there has been little change in research practices, while at the same time arguing that many of these practices cannot be measured. A counterargument to the statements that metrics are unable to counter irreproducibility is that to change incentives in order to foster trust, assessing what kind of research is more &quot;deserving&quot; of trust is important – thus, good metrics are perhaps precisely what is needed to build up trust. Sharing of data and analysis code, for example, can help to assess whether p values have been hacked (as they allow for reanalysis of the data using other methods). Thus, using appropriate sharing of data as a metric is likely to improve some of these issues.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.45261.003</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Throughout the text it appears to be more accurate to refer to &quot;irreproducible&quot; instead of &quot;erroneous&quot; research results. A result that is not reproduced might be a fully accurate description of a real experiment, but one that either describes a chance finding, a finding that is dependent on very specific experimental conditions, or it could indeed be erroneous (i.e. fabricated, falsified, or due to an erroneous analysis or method). If you feel it is important to use the term &quot;error&quot;, please clearly define what it means.</p><p>One suggestion from the reviewers to help you reframe the discussion to avoid a focus on &quot;errors&quot; is to think of research practices on a continuum, from the unforgivable to the perfect. At each point, the value of research can be reduced by making &quot;errors&quot; (be these deliberate or inadvertent, or even just consequences of the limitations of the methodologies used) or enhanced by adopting better practice (e.g. open data). For instance, the value of a non-randomised animal study is enhanced if this is clearly stated (because you can interpret the findings accordingly), and the value of any evidentiary claim is enhanced if it was asserted as the primary outcome measure in an a priori study protocol. Strategies that incorporate these different approaches might produce marginal improvements across the continuum.</p></disp-quote><p>To address these comments, we have dropped most of the uses of the term errors and mistakes and, unless the context for their use is clear, use instead such terms as “problems” or “unreliable research.” We thank the reviewer for helping us to see how vague the terms “errors” and “mistakes” can be. We gave a lot of thought to alternative phrasing, including using “irreproducible” as suggested but in the end decided against using that particular term. We explain our thinking here. First, our original reason for using terms like error and mistake was because we wanted to be clear to readers that our main focus was not research misconduct. As we explain later in the text, research misconduct is already consuming enough of the oxygen in the room that could be used instead to combat more prevalent problems. Second, while “irreproducible research” certainly covers much, perhaps even most, of the terrain that is in fact our focus, we worried that phrase may be off-putting for some readers and a bit narrow for our purposes. Despite the widely-cited surveys documenting that most researchers believe there is a “reproducibility crisis,” many in the research community nevertheless reject that narrative and these are the very readers we are most hoping to engage in our essay. Third, we thought that, at least technically speaking, framing everything in terms of irreproducible research might perhaps also be a bit vague, given the distinct ways that results might fail to reproduce. (Here we have the Goodman/Fanelli/Ioannidis STM discussion in mind.) We think that terms such as “unreliable research”, “problems”, and “erroneous research results” avoid these issues while accurately conveying our intent so those are the new terms we settled on. We made similar changes near the end of the manuscript. Finally, we found the suggestion to reference a continuum of research practices especially helpful, so we incorporated it into the manuscript near the outset.</p><disp-quote content-type="editor-comment"><p>2) Although the recommendations that follow the discussion of the &quot;four erroneous beliefs&quot; are all sensible, most of them do not address the beliefs directly. Please discuss in greater depth how the beliefs thwart attempts at reform, the approaches that could be taken to change them, and any further barriers that prevent this from happening. Ideally this discussion will include some empirical evidence and concrete examples.</p></disp-quote><p>This is a somewhat complex request to address. The recommendations following our discussion of the erroneous beliefs were added at the request of the editors, which they wanted to see prior to deciding to send the manuscript out for review. So, we feel like they should remain in the manuscript. The reviewers are correct in pointing out, however, that they do not directly mitigate the erroneous beliefs themselves. Thus, we have added a discussion of educational activities to show how one could take on the erroneous beliefs more directly, as well as an example as to how this might be done. We are not sure how to integrate into the manuscript, though, more in-depth discussion about exactly how the four beliefs are thwarting attempts at reform. The impetus for the manuscript was curiosity about why so many reforms that target a range of concerns have yet to be acted upon by too many in the research community, a question that we acknowledge mainly affords speculation at this point. We attribute this inattention to a failure to fully appreciate the need for these reforms, despite the wealth of published evidence establishing how much they are needed. What might account for this inattention is surely a complex range of factors that it would be hard to study and definitively quantify, but it seems reasonable to assume that a certain mindset, characterized in part by the erroneous beliefs, is part of that range. So, we wanted to try to tease out some of the components about that mindset in order to spur discussion about them. We continue to believe that an ensuing discussion about them should prove valuable even in the absence of targeted empirical study about how widespread they are and how they may be specifically thwarting individual recommendations. We think that highlighting the general issues covered by the four beliefs can better spur reflection and conversation than would an effort to try to tie specific beliefs to the uptake rate of specific reforms in the absence of studies designed to test those ties. We trust that this more general approach is acceptable for articles in the “Features” section of the journal.</p><disp-quote content-type="editor-comment"><p>Other discussion points:</p></disp-quote><p>We have given a lot of thought about how/whether to address in the manuscript the discussion points below and decided to only make one minor addition and would like to briefly explain our thinking here. While all of the suggestions have a lot of merit, we worried that discussing them in the manuscript would focus readers’ attention on <italic>what</italic> changes need to be made in research whereas the broad focus of our manuscript is <italic>why</italic> changes need to be made. We have inserted new text in the manuscript to this effect. We would very much like to keep that the focus and we worry that too much discussion about actual proposed changes will prompt readers to dwell on whether they agree with those particular changes rather than the need for change itself. Reforms preceded by widespread recognition of the need to change will arguably fare better than ones undertaken when there is no consensus about the need for them in the first place. That is why we prefer not to include more discussion than we already have about representative reforms to tackle various problems. We know that the reviewers are very familiar with the extensive landscape of current reforms underway and the many candidates for additional ones. We also hope that the reviewers and editors will agree that global considerations about the health of the research enterprise and the current systems that shape it have value. We wanted to preserve this more global perspective and were a bit worried that too much discussion of particular reforms will draw attention away from the broader considerations. Our edits are an attempt to strike this balance. We trust that this will be acceptable to the reviewers and editors.</p><disp-quote content-type="editor-comment"><p>1) In reviewer #2's experience of teaching on better reproducibility by more vigorous study design, data analysis and reporting, the PhD students in the class immediately see the point. However, they often come back and ask for advice when their supervisor dismisses concerns about reproducibility with &quot;we've always done it that way&quot; and &quot;everybody does it that way&quot;. Have you also found that young researchers are more open to change than established ones?</p><p>2) When discussing beliefs 1 and 2 (&quot;it's about the science, not the scientists&quot; and &quot;we need to focus on the health of the orchard, not just the bad apples&quot;), it could be worth discussing the fact that much of the training in research ethics over the last decades has revolved around misconduct. This is likely to reinforce these two beliefs – and, according to the authors' view, might preclude scientists from seeing the problem as their own (as misconduct and ethics breaches are rarely something that one will admit to). Perhaps framing research ethics training around the science (e.g. irreproducible research and its consequences), rather than the scientists, might help in overcoming these beliefs.</p></disp-quote><p>The discussion we added includes some of these same points. Also, with respect to Other discussion point 1 above, in the experiences of the lead author, he finds much more student interest in his research ethics class in research quality and reproducibility issues than was the case as recently as 3-4 years ago. However, given the length of our manuscript and the anecdotal nature of the observation, we chose not to include this in the body of the paper.</p><disp-quote content-type="editor-comment"><p>3) The authors mention at various points that there is &quot;reluctance within the research community to implement the reforms&quot;, that &quot;there has been too little uptake of them&quot; and that &quot;calls to arms have had little effect so far&quot;. The authors could discuss this issue in more depth and provide evidence to show that this is the case. What constitutes &quot;little effect&quot;? Some change does appear to be happening, albeit slowly. Although data and code sharing are still subpar, they are likely to be on the rise (e.g. Campbell et al., 2019 at doi:10.1016/j.tree.2018.11.010), as are open access (Piwowar et al., 2018 at doi:10.7717/peerj.4375) and preprint deposition (Kaiser, 2017 at 10.1126/science.357.6358.1344).</p></disp-quote><p>We have also chosen not to add any extensive additional text, though we did add “especially at the institutional level” to moderate our claim somewhat. We trust that the reviewers and editors will be ok with this decision. One complicating factor here is that much of the evidence would be found in the blogosphere and popular press, where comments make it clear that there is still a lot of resistance in many quarters about both the extent and severity of problems. See, for example, https://www.theatlantic.com/science/archive/2018/11/psychologys-replication-crisis-real/576223/?utm_source=feed and the skeptics addressed there. We can also point to numerous conversations of our own and of colleagues who regularly get pushback when the topic is problems in research. Also, we think that we have made reference in the manuscript to several examples of reforms in place and the extent of their impact to date.</p><disp-quote content-type="editor-comment"><p>4) The authors speak of &quot;reluctance within the research community to implement the reforms&quot;. But is the community really reluctant, or are many people unaware of the problem or unfamiliar with the possible solutions and necessary reforms? Similarly, the authors state that &quot;these beliefs are blunting curiosity about the health of biomedical research&quot;. The rising interest in the subject over the last few years, suggests that inertia in taking necessary changes and a feeling of non-responsibility and/or powerlessness towards them (&quot;it's the system, not me&quot;) might be more important than lack of acknowledgement. How do these different factors contribute and interact to prevent necessary changes?</p></disp-quote><p>Again, we have chosen not to address this suggestion specifically. The reviewer is no doubt correct to point out that some important changes are afoot. It is also worth noting that reluctance may not be the preferred term to use here since, as the reviewer suggests, it might be a feeling of powerlessness, lack of awareness, or something else. But we remain comfortable with the term “reluctance.” Our collective sense is that the impetus for reform is still confined to a significant degree within the meta-research community, despite the years, at times even decades, e.g., continued use of mislabeled cell lines and citations of studies known to involve them, of publicizing and discussing the problems. This gap is what we are curious to try to understand. Hence this essay speculating that the 4 erroneous beliefs we identify are likely contributors to it. Perhaps one finds it easy to deflect concern about irreproducible or otherwise unreliable research if one is confident that science self-corrects. If it does, then there is less need to worry about methodologic quality or reproducibility. Or, why worry about bias if there are now required disclosures about financial interest? We think such erroneous beliefs are at least as contributory to the current disappointing pace of reform as is lack of awareness or feelings of powerlessness and as such warrant the consideration of readers. And please note that our discussion about RCR does pertain to the awareness issue.</p><disp-quote content-type="editor-comment"><p>5) On Belief 4, the authors argue that the fact that investment in tackling misconduct has failed to prevent irreproducible research is evidence that &quot;following the rules doesn't guarantee we are getting it right&quot;. But couldn't it be the case that this effort has focused on the major rules (e.g. do not falsify, fabricate or plagiarize) but has ignored the (many more) minor ones that are just as vital for published research findings to be reproducible (e.g. use adequate power, differentiate exploratory and confirmatory research, avoid p-hacking and HARKing)? The failure to prevent bad science could be because there are too few rules, rather than because following the rules doesn't work.</p></disp-quote><p>Again, we have chosen not to address this suggestion specifically. While we share much of this reviewer’s diagnosis, we also believe that researchers have to internalize a deep commitment to proper research methodology and we are skeptical that assuring the use of proper methods is best accomplished by having more rules for researchers to follow. We think that a more productive approach is having more virtuous researchers in the Aristotelian sense (habitually doing things in the right way with the proper motivations and the right reasons) and we are suggesting that unhelpful beliefs like the ones we have highlighted are a possible major culprit hindering a stronger allegiance to proper research methodology that deserves greater focus.</p><disp-quote content-type="editor-comment"><p>6) On the issue of science not being systematically self-correcting, it might be worth mentioning the high prevalence of failed replication attempts that are not published, most commonly due to the authors of replications not attempting to publish them – see Baker et al., 2016 (doi: 10.1038/533452a) for a survey-based indication of that.</p></disp-quote><p>We think that devoting space within the article to this suggestion would take us a bit off target in that there are so many causes that hinder self-correction, including failure to publish results, which is already discussed and documented in the literature, some of which we have cited, while our focus is the broad issue of science not self-correcting.</p><disp-quote content-type="editor-comment"><p>7) &quot;… rather than research that tries to confirm past findings.&quot; One possibility to increase confirmation is to place higher value on confirmatory studies of past findings, but the other would be to raise the threshold for publication in the first place in some instances (e.g. requiring preregistration or independent confirmation, see Mogil and Macleod, 2017; doi:10.1038/542409a). As there are arguments for both sides, it could be worth touching on this point.</p></disp-quote><p>We completely agree that this would be another possibility. Our response here is similar to the immediate one above. We are a bit reluctant to extend our discussion because we think the current discussion already supports the major points we wanted to make. In addition, changing the nature of journals and what they publish, as the reviewer suggests, is a long and heterogenous process and shaking faith that science self-corrects, which is part of our aim here, might prove to be a useful accelerant to that process. Thus, rather than focus on what changes are most needed, we have chosen instead to try to spur greater reflection that could help show why changes are needed in the first place.</p><disp-quote content-type="editor-comment"><p>8) Subsection “Publishing Reforms: underway but they could be more ambitious”: You could discuss whether, with such a large number of journals (in which peer review varies widely in quality), and preprints making important headway in biology, we should expect the main source of quality control to come from journals. Note that arguments have been made in the opposite direction (e.g. removing barriers to publication to diminish its reward value and make 'publish or perish' senseless – e.g. Nosek and Bar-Anan, 2012 http://dx.doi.org/10.1080/1047840X.2012.692215 and others).</p></disp-quote><p>This suggestion has a lot to recommend it as well but we only have so much space and we had to have some parameters for our discussion of publisher reforms so we chose to limit those to the kinds of incremental reforms we highlighted, as opposed to a radical rethink of scientific publishing. We trust this demarcation will be acceptable to the reviewers and editors.</p><disp-quote content-type="editor-comment"><p>9) It sounds somewhat incongruent to state that there has been little change in research practices, while at the same time arguing that many of these practices cannot be measured. A counterargument to the statements that metrics are unable to counter irreproducibility is that to change incentives in order to foster trust, assessing what kind of research is more &quot;deserving&quot; of trust is important – thus, good metrics are perhaps precisely what is needed to build up trust. Sharing of data and analysis code, for example, can help to assess whether p values have been hacked (as they allow for reanalysis of the data using other methods). Thus, using appropriate sharing of data as a metric is likely to improve some of these issues.</p></disp-quote><p>We like this idea and have modified the text accordingly.</p></body></sub-article></article>