<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">13824</article-id><article-id pub-id-type="doi">10.7554/eLife.13824</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Neural oscillations as a signature of efficient coding in the presence of synaptic delays</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-47830"><name><surname>Chalk</surname><given-names>Matthew</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7782-4436</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-6744"><name><surname>Gutkin</surname><given-names>Boris</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-4559"><name><surname>Denève</surname><given-names>Sophie</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Institute of Science and Technology Austria</institution>, <addr-line><named-content content-type="city">Klosterneuburg</named-content></addr-line>, <country>Austria</country></aff><aff id="aff2"><label>2</label><institution>École Normale Supérieure</institution>, <addr-line><named-content content-type="city">Paris</named-content></addr-line>, <country>France</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Center for Cognition and Decision Making</institution>, <institution>National Research University Higher School of Economics</institution>, <addr-line><named-content content-type="city">Moscow</named-content></addr-line>, <country>Russia</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing editor</role><aff id="aff4"><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>matthewjchalk@gmail.com</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>07</day><month>07</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e13824</elocation-id><history><date date-type="received"><day>17</day><month>12</month><year>2015</year></date><date date-type="accepted"><day>05</day><month>07</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Chalk et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Chalk et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-13824-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.13824.001</object-id><p>Cortical networks exhibit 'global oscillations', in which neural spike times are entrained to an underlying oscillatory rhythm, but where individual neurons fire irregularly, on only a fraction of cycles. While the network dynamics underlying global oscillations have been well characterised, their function is debated. Here, we show that such global oscillations are a direct consequence of optimal efficient coding in spiking networks with synaptic delays and noise. To avoid firing unnecessary spikes, neurons need to share information about the network state. Ideally, membrane potentials should be strongly correlated and reflect a 'prediction error' while the spikes themselves are uncorrelated and occur rarely. We show that the most efficient representation is when: (i) spike times are entrained to a global Gamma rhythm (implying a consistent representation of the error); but (ii) few neurons fire on each cycle (implying high efficiency), while (iii) excitation and inhibition are tightly balanced. This suggests that cortical networks exhibiting such dynamics are tuned to achieve a maximally efficient population code.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.001">http://dx.doi.org/10.7554/eLife.13824.001</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>neural oscillations</kwd><kwd>neural coding</kwd><kwd>computational neuroscience</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-10-LABX-0087</award-id><principal-award-recipient><name><surname>Chalk</surname><given-names>Matthew</given-names></name><name><surname>Gutkin</surname><given-names>Boris</given-names></name><name><surname>Denève</surname><given-names>Sophie</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Predispike</award-id><principal-award-recipient><name><surname>Chalk</surname><given-names>Matthew</given-names></name><name><surname>Denève</surname><given-names>Sophie</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Denève</surname><given-names>Sophie</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural oscillations are a necessary consequence of efficient coding of sensory signals by a spiking neural network, limited by synaptic delays and noise.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Oscillations are a prominent feature of cortical activity. In sensory areas, one typically observes 'global oscillations' in the gamma-band range (30–80 Hz), alongside single neuron responses that are irregular and sparse (<xref ref-type="bibr" rid="bib14">Buzsáki and Wang, 2012</xref>; <xref ref-type="bibr" rid="bib41">Yu and Ferster, 2010</xref>). The magnitude and frequency of gamma-band oscillations are modulated by changes to the sensory environment (e.g. visual stimulus contrast (<xref ref-type="bibr" rid="bib24">Henrie and Shapley, 2005</xref>) and behavioural state (e.g. attention [<xref ref-type="bibr" rid="bib20">Fries et al., 2001</xref>]) of the animal. This has led a number of authors to propose that neural oscillations play a fundamental role in cortical computation (<xref ref-type="bibr" rid="bib22">Fries, 2009</xref>; <xref ref-type="bibr" rid="bib17">Engel et al., 2001</xref>). Others argue that oscillations emerge as a consequence of interactions between populations of inhibitory and excitatory neurons, and do not perform a direct functional role in themselves (<xref ref-type="bibr" rid="bib30">Ray and Maunsell, 2010</xref>).</p><p>A prevalent theory of sensory processing, the 'efficient coding hypothesis', posits that the role of early sensory processing is to communicate information about the environment using a minimal number of spikes (<xref ref-type="bibr" rid="bib5">Barlow, 1961</xref>). This implies that the responses of individual neurons should be as asynchronous as possible, so that they do not communicate redundant information (<xref ref-type="bibr" rid="bib36">Simoncelli and Olshausen, 2001</xref>). Thus, oscillations are generally seen as a bad thing for efficient rate coding, as they tend to synchronise neural responses, and thus, introduce redundancy.</p><p>Here we propose that global oscillations are a necessary consequence of efficient rate coding in recurrent neural networks with synaptic delays.</p><p>In general, to avoid communicating redundant information, neurons driven by common inputs should actively decorrelate their spike trains. To illustrate this, consider a simple set-up in which neurons encode a common sensory variable through their firing rates, with a constant value added to the sensory reconstruction each time a neuron fires a spike (<xref ref-type="fig" rid="fig1">Figure 1a</xref>).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.002</object-id><label>Figure 1.</label><caption><title>Relationship between synchrony and coding accuracy.</title><p>(<bold>a</bold>) Each panel illustrates the response of 10 neurons. An encoded sensory variable is denoted by a horizontal blue line. Each spike fired by the network increases the sensory reconstruction by a fixed amount, before it decays. Greatest accuracy is achieved when the population fires at regular intervals, but no two neurons fire together (left). Coding accuracy is reduced when multiple neurons fire together (middle and right panels). (<bold>b</bold>) Same as panel a, but where neurons show independent poisson variability. (<bold>c</bold>) Reconstruction error (root-mean squared error divided by the mean) for a regular spiking network shown in panel a, versus the number of synchronous spikes on each cycle. A horizontal line denotes the performance when neurons fire with independent Poisson variability. (<bold>d</bold>) Cartoon illustrating tradeoff between trade-off faced by neural networks.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.002">http://dx.doi.org/10.7554/eLife.13824.002</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig1-v2"/></fig></p><p>With a constant input, the reconstruction error is minimized when the population fires spikes at regular intervals, while no two neurons fire spikes at the same time (as in <xref ref-type="fig" rid="fig1">Figure 1a</xref>, left panel, in red). To achieve this ideal, however, requires incredibly fast inhibitory connections, so that each time a neuron fires a spike it suppresses all other neurons from firing (<xref ref-type="bibr" rid="bib9">Boerlin et al., 2013</xref>). In reality, inhibitory connections are subject to unavoidable delays (e.g. synaptic and transmission delays), and thus, cannot always prevent neurons from firing together. Worse, in the presence of delays, inhibitory signals, intended to prevent neurons from firing together, can actually have the reverse effect of <italic>synchronising</italic> the network, so that many neurons fire together on each cycle (as in <xref ref-type="fig" rid="fig1">Figure 1a</xref>, middle and right panels). This is analogous to the so-called ‘hipster effect’ where a group of individuals strive to look different from each other, but due to delayed reaction times, end up making similar decisions and all looking alike (<xref ref-type="bibr" rid="bib38">Touboul and Effect, 2014</xref>).</p><p>Spiking synchronicity generally has a negative effect on coding performance. For example, <xref ref-type="fig" rid="fig1">Figure 1c</xref> shows how, in the regular spiking network described above, coding error increases with the number of synchronous spikes per cycle (while firing rate is held constant). It is thus tempting to conclude that neural networks should do everything possible to avoid synchronous firing. However, one also observes that a completely asynchronous network, in which neurons fire with independent Poisson variability (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), performs far worse than the regular spiking network, even when multiple neurons fire together on each cycle (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, horizontal dashed line).</p><p>Thus, to perform efficiently, neural networks face a trade-off (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). On the one hand, recurrent connections should coordinate the activity of different neurons in the network, so as to achieve an efficient and accurate population code. On the other hand, in the presence of synaptic delays, it is important that these recurrent signals do not overly synchronize the network, as this will reduce coding performance.</p><p>Here, we show that, in a network of leaky integrate-and-fire neurons (LIF) optimized for efficient coding, this trade-off is best met by adding noise to the network (e.g. via random fluctuations in membrane potential, or synaptic failure) to ensure that: (i) neural spike trains are entrained to a global oscillatory rhythm (for a consistent representation of global information), but (ii) only a small fraction of cells fire on each oscillation cycle. In this regime, individual neurons fire irregularly (<xref ref-type="bibr" rid="bib35">Shadlen and Newsome, 1998</xref>) and exhibit weak pairwise correlations (<xref ref-type="bibr" rid="bib33">Schneidman et al., 2006</xref>), despite the presence of rhythmic population activity. Moreover, excitation and inhibition are tightly balanced on each oscillation cycle, with inhibition lagging excitation by a few milliseconds (<xref ref-type="bibr" rid="bib4">Atallah and Scanziani, 2009</xref>; <xref ref-type="bibr" rid="bib39">Wehr and Zador, 2003</xref>). Thus, ‘global oscillations’ come about as a direct consequence of efficient rate coding, in a recurrent network with synaptic delays.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Efficient coding in an idealized recurrent network</title><p>It is instructive to first consider the behaviour of an idealized network, with instantaneous synapses. For this, we consider a model proposed by Boerlin et al., in which a network of integrate-and-fire neurons is optimized to efficiently encode a time varying input. As the model has already been described elsewhere (<xref ref-type="bibr" rid="bib9">Boerlin et al., 2013</xref>) we restrict ourselves to outlining the basic principles, with a mathematical description reserved for the <bold>Materials and methods</bold>.</p><p>Underlying the model is the idea that downstream neurons should be able to reconstruct the input to the network by performing a linear summation of its output spike trains. To do this efficiently (i.e. with as few spikes as possible), the spiking output of the network is fed-back and subtracted from its original input (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). In consequence, the total input to each neuron is equal to a ‘prediction error’; the difference between the original input and the network reconstruction. This prediction error is also reflected in neural membrane potentials. When a neuron’s membrane potential becomes larger than a constant threshold, then it fires a spike; recurrent feedback then reduces the prediction error encoded by other neurons, preventing them from firing further redundant spikes.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.003</object-id><label>Figure 2.</label><caption><title>Efficient coding in a recurrent neural network.</title><p>(<bold>a</bold>) Schematic of network. Inhibitory recurrent connections are represented by an open circle. Excitatory feed-forward connections are represented by closed circles. (<bold>b</bold>) Stimulus (blue) and neural reconstruction (black) on a single trial. The spikes and membrane potential for each cell are shown in separate rows. Vertical dashed lines illustrate how each spike alters the neural reconstruction. (<bold>c</bold>) Same as (<bold>b</bold>), but with a constant input (also note the change of temporal scale). (<bold>d</bold>–<bold>e</bold>) Distribution of inter-spike intervals in population (<bold>d</bold>) and single-cell (<bold>e</bold>) spike trains.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.003">http://dx.doi.org/10.7554/eLife.13824.003</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig2-v2"/></fig></p><p>To illustrate the principles underlying the model, we consider a network of three identical neurons. <xref ref-type="fig" rid="fig2">Figure 2b</xref> shows how spikes fired by the network are ‘read-out’, to obtain a continuous reconstruction. Each time a neuron fires a spike, it increases the reconstruction by a fixed amount, decreasing the difference between the input and the neural reconstruction. Immediately after, feedback connections alter the membrane potentials of other neurons, to ensure that they maintain a consistent representation of the error, and do not fire further spikes (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, lower panel). As a result, membrane potentials are highly correlated, while spikes are asynchronous.</p><p><xref ref-type="fig" rid="fig2">Figure 2c</xref> shows the behaviour of the network in response to a constant input. To optimally encode a constant input, the network generates a regular train of spikes (as in the left panel of <xref ref-type="fig" rid="fig1">Figure 1a</xref>), resulting in a narrow distribution of population inter-spike intervals (ISIs) (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). Neural membrane potentials, which encode a common prediction error, fluctuate in synchrony, with a frequency dictated by the population firing rate (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, lower panel). However, as only one neuron fires per cycle, the spike trains of <italic>individual neurons</italic> are irregular and sparse, resulting in a near-exponential distribution of single-cell ISIs (<xref ref-type="fig" rid="fig2">Figure 2e</xref>).</p></sec><sec id="s2-2"><title>Efficient coding with synaptic delays</title><p>In real neural networks, recurrent inhibition is not instantaneous, but subject to synaptic and transmission delays. Far from being a biological detail, even very short synaptic delays can profoundly change the behaviour of the idealized efficient coding network, and pose fundamental limits on its performance.</p><p>To render the idealized model biologically feasible, we extended it in two ways. First, to comply with Dale’s law, we introduced a population of inhibitory neurons, which mediates recurrent inhibition (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). In our implementation, excitatory and inhibitory populations both encode two separate reconstructions of the target variable, an ‘excitatory reconstruction’ and an ‘inhibitory reconstruction’. Inhibitory neurons, which receive input from excitatory neurons, fire spikes in order to predict and cancel the inputs to the excitatory population. Consequently, the inhibitory reconstruction closely tracks fluctuations in the excitatory reconstruction (see <bold>Materials and methods</bold>).<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.004</object-id><label>Figure 3.</label><caption><title>Coding performance of an excitatory/inhibitory network with synaptic delays.</title><p>(<bold>a</bold>) Schematic of network connectivity. Excitatory neurons and inhibitory neurons are shown in black and red, respectively. Connections between different neural populations are schematised by lines terminating with solid (excitatory connections) and open (inhibitory connections) circles. (<bold>b</bold>) The postsynaptic current (PSC) waveform used in our simulations. (<bold>c</bold>) Schematic, illustrating how delays affect the decoding performance of the network. (<bold>d</bold>) Mean-squared reconstruction error (normalized by variance of encoded signal) versus mean firing rate for an ‘ideal’ efficient coding network with instantaneous synapses (blue), an efficient coding network with finite synaptic delays (black cross) independent Poisson units whose firing rate varies as a fixed function of the feed-forward input (red). (<bold>f</bold>) Stimulus and neural reconstruction for the recurrent network, with synaptic delays. (<bold>inset</bold>) Excitatory (black) and inhibitory (red) neural reconstructions, during a short segment from this trial.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.004">http://dx.doi.org/10.7554/eLife.13824.004</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig3-v2"/></fig></p><p>Second, and more importantly, we replaced the instantaneous synapses in the idealized model with continuous synaptic dynamics, as shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref> (see <bold>Materials and methods</bold>). As stated previously, adding synaptic delays substantially alters the performance of the network. Without delays, recurrent inhibition prevents all but one cell from firing per oscillation cycle, resulting in an optimally efficient code (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, left panel). With delays however, inhibition cannot always act fast enough to prevent neurons firing together. As a result, neural activity quickly becomes synchronised, and the sensory reconstruction is destroyed by large population-wide oscillations (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, middle panel). To improve performance, one can increase the membrane potential noise and spike threshold, so as to reduce the chance of neurons firing together (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, right panel). Too much noise, however, and the firing of different neurons becomes uncoordinated, and network performance is diminished (see later).</p><p>We compared the performance of the efficient coding network (with excitatory/inhibitory populations and synaptic delays) to: (i) an ‘ideal’ model with no delays and (ii) a ‘rate model’, consisting of a population of independent Poisson units whose firing rate varies as a function of the feed-forward input (see <bold>Materials and methods</bold>).</p><p>In the Poisson network, random fluctuations in firing rate degrade the reconstruction, resulting in large coding error. Increasing the population firing rate increases the signal-to-noise ratio, and thus, also decreases the reconstruction error (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, red). In contrast, in the ‘ideal’ efficient coding network, noise fluctuations are automatically corrected for by the recurrent connections (see Materials and methods). Thus, in this network, the only source of inaccuracy comes from the discreteness of the code (where each spike adds a fixed quantity to the readout), leading to a small error (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, blue). Finally, while synaptic delays increase the coding error (relative to the ideal efficient coding network), the error remains significantly smaller than for a Poisson network with the matching rate (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, black cross).</p><p><xref ref-type="fig" rid="fig3">Figure 3e</xref> illustrates the ability of the efficient coding network to track a time varying input signal. Zooming in to a 1 s period within the trial (lower inset), one observes rhythmic fluctuations in the excitatory and inhibitory neural reconstructions. These fluctuations are essentially the same phenomena as observed for the ideal network, where the neural reconstruction fluctuated periodically around the target signal following the arrival of each new spike (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). However, with synaptic delays, several neurons fire together before the arrival of recurrent inhibition. As a result, oscillations are slower and larger in magnitude than for the idealised network, where only one neuron fires on each cycle.</p></sec><sec id="s2-3"><title>Oscillations and efficient coding</title><p>We sought to quantify the effect of oscillations on coding performance. To do this, we varied parameters of the model, so as to alter the degree of network synchrony, while keeping firing rates the same. In the main text we illustrate the effect of adding white noise to the membrane potentials (while simultaneously varying the spike threshold, to maintain constant firing rate; see <bold>Materials and methods</bold>).</p><p>Increasing the magnitude of the membrane potential noise desynchronized the network activity, resulting in a reduction in pairwise voltage correlations (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). With increased noise, single neural spike trains also became more irregular, reflected by an increase in the spiking CV (<xref ref-type="fig" rid="fig4">Figure 4b</xref>).<fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.005</object-id><label>Figure 4.</label><caption><title>Effect of varying noise amplitude on network dynamics and coding performance.</title><p>(<bold>a</bold>) Pairwise voltage correlations, versus injected noise amplitude. (<bold>b</bold>) Coefficient of variation in inter-spike intervals, versus injected noise amplitude. (<bold>c</bold>) Root-mean-square (rms) reconstruction error (solid line), plotted alongside rms difference between excitatory and inhibitory reconstructions (dashed line), versus injected noise amplitude. Horizontal dashed line shows the rms reconstruction error for a population of Poisson units with matched firing rates. (<bold>e</bold>–<bold>f</bold>) (upper panels) Spiking response of inhibitory (red) and excitatory (black) neurons with low (<bold>d</bold>), medium (<bold>e</bold>), and high (<bold>f</bold>) noise amplitude (indicated by open circles in panels <bold>a</bold>–<bold>c</bold>). (lower panels). Inhibitory (red) and excitatory (black) neural reconstructions, alongside target stimulus (blue dashed line). (<bold>g</bold>) Distribution of single-cell inter-spike intervals in each noise condition. The prediction for a population of Poisson units is shown in black. Note the log-scale. (<bold>h</bold>) Power spectrum of population firing rate, in each noise condition. The power spectrum for a population of Poisson units is shown in black.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.005">http://dx.doi.org/10.7554/eLife.13824.005</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig4-v2"/></fig></p><p>Coding performance, however, varied non-monotonically with the noise amplitude. The reconstruction error followed a u-shaped curve, being minimised for a certain intermediate level of noise (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, solid curve). For this intermediate noise level, coding performance was significantly better than a network of independent Poisson units with a matching firing rate (horizontal dashed line). Interestingly, deviations between excitation and inhibition followed a similar u-shape curve, being minimised for the same intermediate noise level (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, thick dashed curve). Thus, optimal coding was achieved when the balance between excitatory and inhibition was the tightest. Further, at the optimal level of noise, the spiking CV value was near unity (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), implying irregular (near-poisson) single cell responses.</p><p>To further understand the effect of varying noise amplitude, we plotted the network responses and neural reconstruction in three regimes: with low, intermediate, and high noise (indicated by green, blue and red circles respectively in <xref ref-type="fig" rid="fig4">Figure 4a–c</xref>).</p><p>With low noise, neural membrane potentials were highly correlated, leading many neurons to fire together on each oscillation cycle (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, upper panel). As a result, the neural reconstruction exhibited large periodic fluctuations about the encoded input, leading to poor coding performance (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, lower panel).</p><p>At the other extreme, when the injected noise was very high, the spike trains of different neurons were uncorrelated (<xref ref-type="fig" rid="fig4">Figure 4f</xref>, upper panel). As, in this regime, effectively no information was shared between neurons, inhibitory and excitatory reconstructions were decoupled, and coding performance was similar to a population of independent Poisson units (<xref ref-type="fig" rid="fig4">Figure 4f</xref>, lower panel).</p><p>In the intermediate noise regime, for which performance was optimal, spikes were aligned to rhythmic fluctuations in the prediction error, but few neurons fired on each cycle (<xref ref-type="fig" rid="fig4">Figure 4e</xref>, upper panel). These dynamics were reflected by a near-exponential distribution of interspike-intervals (<xref ref-type="fig" rid="fig4">Figure 4e</xref>), coupled with a narrow peak in the population firing rate spectrum (<xref ref-type="fig" rid="fig4">Figure 4e</xref>). In this regime, rhythmic fluctuations in the neural reconstruction were small in magnitude, and there was a tight coupling between inhibitory and excitatory reconstructions (<xref ref-type="fig" rid="fig4">Figure 4e</xref>, lower panel).</p></sec><sec id="s2-4"><title>Manipulating synaptic failures and neural noise</title><p>To assess the generality of the results shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we manipulated the degree of synchrony in the network in different ways. First, we varied the synaptic reliability, by varying the probability that a presynaptic spike led to a change in the postsynaptic membrane potential (<xref ref-type="fig" rid="fig5">Figure 5a–b</xref>). Note that the average recurrent input received by each neuron varies proportionally with the synaptic failure probability. Thus, to correct for this, and retain balance in the network, the magnitude of recurrent connection was scaled inversely with the synaptic failure probability (see Materials and methods).</p><p>Next, we selected a subset of cells to fire with random, Poisson distributed, firing rates (and matching rate) (<xref ref-type="fig" rid="fig5">Figure 5c–d</xref>). For both manipulations, the added membrane potential noise was held constant (equal to the ‘low-noise’ condition indicated by green open circle in <xref ref-type="fig" rid="fig4">Figure 4a–c</xref>).</p><p>We observed qualitatively similar changes in coding performance and network synchrony, regardless of how we manipulate noise in the network. In both cases, the coding error was smaller at an intermediate level of noise, for which there was tightest balance between inhibition and excitation (<xref ref-type="fig" rid="fig5">Figure 5a and c</xref>).<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.006</object-id><label>Figure 5.</label><caption><title>Effect of varying network synchrony in different ways.</title><p>(<bold>a</bold>–<bold>b</bold>) Effect of varying the probability of synaptic failure. Panel <bold>a</bold> shows the root-mean-square (rms) reconstruction error (solid line), plotted alongside the rms difference between excitatory and inhibitory reconstructions (dashed line). Panel <bold>b</bold> shows the firing rate spectrum with varying probability of synaptic failure (indicated by open circles in panel <bold>a</bold>). (<bold>c</bold>–<bold>d</bold>) Same as panels <bold>a</bold>–<bold>b</bold>, but where we varied the fraction of cells that fired randomly (i.e. with Poisson spike trains).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.006">http://dx.doi.org/10.7554/eLife.13824.006</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig5-v2"/></fig></p></sec><sec id="s2-5"><title>Oscillatory neural dynamics</title><p>We investigated the behaviour of the network in the optimal noise regime, shown in <xref ref-type="fig" rid="fig4">Figure 4e</xref>. <xref ref-type="fig" rid="fig6">Figure 6a</xref> shows the network reconstruction and population firing rates in response to a low (blue), medium (red) and high (green) amplitude stimulus, with the optimal noise amplitude (i.e. 17 mV <inline-formula><mml:math id="inf1"><mml:msup><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). The population firing rate was characterised by a transient peak following stimulus onset, followed by decay to a constant value. A spectrogram of the population firing rate (<xref ref-type="fig" rid="fig6">Figure 6a</xref>, lower panel) reveals the presence of 30–50 Hz oscillations during the period of sustained activity. <xref ref-type="fig" rid="fig6">Figure 6b</xref> plots the spiking response and population firing rates during a 600 ms period of sustained activity. Here, one clearly sees correlated rhythmic fluctuations in excitatory (black) and inhibitory (red) activity. The strength of these oscillations increases with stimulus amplitude (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). Nevertheless, for all input amplitudes, individual neurons fired irregularly, with a near-exponential distribution of inter-spike intervals (<xref ref-type="fig" rid="fig6">Figure 6d</xref>).<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.007</object-id><label>Figure 6.</label><caption><title>Spiking response to a constant input.</title><p>(<bold>a</bold>) (<bold>top</bold>) Neural reconstruction (solid lines) of a presented constant stimulus (dashed lines), before and after stimulus onset. Low, medium and high amplitude stimuli are shown in blue, red and green, respectively. (<bold>middle</bold>) Population firing rate, for each stimulus amplitude. (<bold>bottom</bold>) Spectrogram of population firing rate. (<bold>b</bold>) The upper panel shows a raster-gram of excitatory (black) and inhibitory (red) responses, during a 0.6 s period of sustained activity. The lower panel shows the instantaneous firing rates of the excitatory and inhibitory populations. (<bold>c</bold>) Power spectrum of excitatory population firing rate during the sustained activity period, for each stimulus amplitude. (<bold>d</bold>) Average distribution of single-cell inter-spike intervals, for each stimulus amplitude.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.007">http://dx.doi.org/10.7554/eLife.13824.007</ext-link></p></caption><graphic mime-subtype="x-tiff" mimetype="image" xlink:href="elife-13824-fig6-v2"/></fig></p><p>We next considered the dynamics of neural membrane potentials. Previously, Yu &amp; Ferster (<xref ref-type="bibr" rid="bib41">Yu and Ferster, 2010</xref>) reported that, in area V1, visual stimulation increases gamma-band correlations between pairs of neural membrane potentials (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). Qualitatively similar results were obtained with our model (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). Increasing the amplitude of the feed-forward input led to increased correlations between neural membrane potentials (<xref ref-type="fig" rid="fig7">Figure 7c</xref>), with strongest coherence observed in the gamma-band range (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). This is because more neurons fire spikes on each cycle, leading to stronger oscillations.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.008</object-id><label>Figure 7.</label><caption><title>Voltage responses in response to a constant input.</title><p>(<bold>a</bold>) Data from Yu &amp; Ferster (Neuron, 2010), showing voltage traces of two V1 cells, in the absence (top), and presence (bottom) of a visual stimulus. (<bold>b</bold>) Voltage traces of two cells in the model in absence (top) and presence (bottom) of a feed-forward input. (<bold>c</bold>) Average pairwise cross-correlation between membrane potentials, in spontaneous and evoked condition. (<bold>d</bold>) Average coherence spectrum between pairs of voltage traces, in spontaneous and evoked condition.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.008">http://dx.doi.org/10.7554/eLife.13824.008</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig7-v2"/></fig></p><p>Several studies have reported a tight balance between inhibition and excitation (<xref ref-type="bibr" rid="bib4">Atallah and Scanziani, 2009</xref>; <xref ref-type="bibr" rid="bib39">Wehr and Zador, 2003</xref>; <xref ref-type="bibr" rid="bib15">Cafaro and Rieke, 2010</xref>). Recently, Atallah et al. (<xref ref-type="bibr" rid="bib4">Atallah and Scanziani, 2009</xref>) reported that inhibitory and excitatory currents are precisely balanced on individual cycles of an ongoing gamma oscillation (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). In our model, efficient coding is achieved by maintaining such a tight balance between inhibitory and excitatory reconstructions. Thus, inhibitory and excitatory currents closely track each other (<xref ref-type="fig" rid="fig8">Figure 8b</xref>), with a high correlation between the amplitude of inhibitory and excitatory currents on each cycle (<xref ref-type="fig" rid="fig8">Figure 8c</xref>). In common with Atallah et al.’s data, inhibition lags behind excitation by a few milliseconds (<xref ref-type="fig" rid="fig8">Figure 8d</xref>). Fluctuations in the amplitude of inhibitory and excitatory currents instantaneously modulate the oscillation frequency, with a significant correlation observed between the peak amplitude on a given oscillation cycle and the period of the following cycle (<xref ref-type="fig" rid="fig8">Figure 8e</xref>).<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.009</object-id><label>Figure 8.</label><caption><title>Balanced fluctuations in excitatory and inhibitory currents.</title><p>(<bold>a</bold>) Data from Atallah &amp; Scanziani (Neuron, 2009), showing inhibitory (blue) and excitatory (red) postsynaptic currents. (<bold>b</bold>) Inhibitory (blue) and excitatory currents (red) in the model, in response to a constant feed-forward input. Black dots indicate detected peaks. (<bold>c</bold>) Amplitude of inhibitory current versus magnitude of excitatory currents on each cycle. (<bold>d</bold>) Distribution of time lags between excitatory and inhibitory peaks. (<bold>e</bold>) Period between excitatory peaks, versus the peak amplitude on the previous oscillation cycle.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.009">http://dx.doi.org/10.7554/eLife.13824.009</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig8-v2"/></fig></p></sec><sec id="s2-6"><title>Gamma oscillations and behavioural performance</title><p>In general, the optimal network parameters depend on the properties of the feed-forward sensory input. For example, the higher the input amplitude, the more noise is required to achieve the optimal level of network synchrony (<xref ref-type="fig" rid="fig9">Figure 9a</xref>). While the network achieves reasonable coding accuracy for a large range of different inputs, adaptive tuning of the dynamics (for example, changing the noise level) can be beneficial for a more limited input range. This would affect the level of population synchrony and thus introduce a correlation between performance and the strength of Gamma oscillations.<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.010</object-id><label>Figure 9.</label><caption><title>Effect of varying the input amplitude.</title><p>(<bold>a</bold>) Root-mean-square (rms) reconstruction error versus injected noise amplitude with three different input amplitudes. The optimal noise level in each condition is denoted by vertical dashed lines. (<bold>b</bold>) Power spectrum of population firing rate, in response to a low amplitude input. The blue curve corresponds to when the network is optimised for the low amplitude input, the green curve for when it is optimised for higher amplitude input. (<bold>c</bold>) Fractional performance in discriminating between two 100 ms input segments, equally spaced around the low amplitude input. Performance is best in the condition with strongest oscillations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.010">http://dx.doi.org/10.7554/eLife.13824.010</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig9-v2"/></fig></p><p>For example, if the task is to detect weak (low amplitude) inputs, performance would be higher if top down modulations (such as attention) reduced the level of noise, and thus increased the degree of population synchrony (<xref ref-type="fig" rid="fig9">Figure 9b</xref>) without significantly changing firing rates. We can thus expect higher detection performance to correlate with a stronger level of Gamma oscillations (<xref ref-type="fig" rid="fig9">Figure 9c</xref>). This could account for certain attention-dependent increases in gamma-power and its correlation with behavioural performance (see <bold>Discussion</bold>).</p><p>Note that a similar correlation between behavioural performance and Gamma power could arise from purely bottom up effects. In the presence of input noise causing trial-by-trial changes in input strength, trials with stronger input amplitude would result in more detection but also exhibit more Gamma oscillations. In that case, however, the increase in Gamma power would be associated with a commensurate increase in population firing rate.</p></sec><sec id="s2-7"><title>Sensitivity to network paramaters</title><p>We investigated the degree to which our results depended on the network size, and the ratio of inhibitory to excitatory neurons. With readout weights held constant, neural firing rates in our model are inversely proportional to the network size (such that the population firing rate stays constant; <xref ref-type="fig" rid="fig10">Figure 10a</xref>). The optimal coding performance was practically unaltered by increasing or decreasing the population size by a factor of two, although there was a small trend for the optimal noise magnitude to increase with population size (<xref ref-type="fig" rid="fig10">Figure 10b</xref>). The oscillatory dynamics were unaltered by varying the network size (<xref ref-type="fig" rid="fig10">Figure 10c</xref>). Varying the number of inhibitory neurons (while keeping the number of excitatory neurons constant), had a similar effect on inhibitory firing rates (<xref ref-type="fig" rid="fig10">Figure 10d</xref>), with little effect on coding performance or oscillatory dynamics (<xref ref-type="fig" rid="fig10">Figure 10e–f</xref>).<fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.011</object-id><label>Figure 10.</label><caption><title>Effect of varying population size.</title><p>(<bold>a</bold>) With recurrent and feed-forward weights held constant, neural firing rates are inversely proportional to population size. (<bold>b</bold>) Root-mean-squared (rms) reconstruction error versus added membrane noise, for three population sizes. A horizontal line denotes the performance of a Poisson spiking network with matching population firing rate. (<bold>c</bold>) Power spectrum of population of population firing rate, for each network size (in each case, the noise was set to its optimal value). (<bold>d</bold>–<bold>e</bold>) Same as panels <bold>a</bold>–<bold>c</bold>, but where we only varied the size of the inhibitory population.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.011">http://dx.doi.org/10.7554/eLife.13824.011</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig10-v2"/></fig></p><p>We next investigated the effect of varying the time constants of the network. We first rescaled all the time-constants for the synaptic waveform. For plotting purposes, the effective ‘synaptic delay’ was defined as the time for the cumulative input due to a single synaptic event to attain half its maximum value. Increasing the synaptic delay meant that more neurons fired on each oscillation cycle before receiving recurrent inhibition, and thus resulted in decreased coding performance (<xref ref-type="fig" rid="fig11">Figure 11 b</xref>), and larger, lower frequency oscillations (<xref ref-type="fig" rid="fig11">Figure 11c</xref>).<fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.012</object-id><label>Figure 11.</label><caption><title>Effect of varying the network time constants.</title><p>(<bold>a</bold>) Neural firing rates versus the synaptic delay. (<bold>b</bold>) Root-mean-squared (rms) reconstruction error versus the synaptic delay. A horizontal line denotes the performance of a Poisson spiking network with matching population firing rate. (<bold>c</bold>) Power spectrum of population firing rate, for three different settings of the synaptic delay. (<bold>d</bold>) Peak oscillation frequency, versus synaptic delay. (<bold>e</bold>–<bold>h</bold>) Same as panels <bold>a</bold>–<bold>d</bold>, but where we varied the read-out time constant (which determines the effective membrane time constant).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.012">http://dx.doi.org/10.7554/eLife.13824.012</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig11-v2"/></fig></p><p>Because there are only two time constants in the model network, when feed-forward and recurrent weights are rescaled appropriately, decreasing the read-out time, <inline-formula><mml:math id="inf2"><mml:mi>τ</mml:mi></mml:math></inline-formula>, is equivalent to increasing the synaptic delay. However, when recurrent weights are held constant, decreasing <inline-formula><mml:math id="inf3"><mml:mi>τ</mml:mi></mml:math></inline-formula> increases firing rates (which are inversely proportional to <inline-formula><mml:math id="inf4"><mml:mi>τ</mml:mi></mml:math></inline-formula>) in contrast to varying the synaptic delay, which has no effect on firing rates (compare <xref ref-type="fig" rid="fig11">Figure 11 a and e</xref>). Decreasing <inline-formula><mml:math id="inf5"><mml:mi>τ</mml:mi></mml:math></inline-formula> also increases the oscillation magnitude with a resulting decrease in coding quality (<xref ref-type="fig" rid="fig11">Figure 11f–g</xref>). However, unlike varying the synaptic delay, <inline-formula><mml:math id="inf6"><mml:mi>τ</mml:mi></mml:math></inline-formula> has a relatively weak effect on the oscillation frequency (<xref ref-type="fig" rid="fig11">Figure 11h</xref>). Intuitively, this is because varying <inline-formula><mml:math id="inf7"><mml:mi>τ</mml:mi></mml:math></inline-formula> causes two changes in network dynamics, which have opposite effects on the oscillation frequency. On the one hand, decreasing <inline-formula><mml:math id="inf8"><mml:mi>τ</mml:mi></mml:math></inline-formula> results in a faster integration time, speeding up the network dynamics (and thus, increasing the oscillation frequency). On the other hand, decreasing <inline-formula><mml:math id="inf9"><mml:mi>τ</mml:mi></mml:math></inline-formula> increases the oscillation magnitude, leading to stronger inhibition on each oscillation cycle, and thus decreasing the oscillation frequency.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We present a novel hypothesis for the role of neural oscillations, as a consequence of efficient coding in recurrent neural networks with noise and synaptic delays. In order to efficiently code incoming information, neural networks must trade-off two competing demands. On the one hand, to ensure that each spike conveys new information, neurons should actively desynchronise their spike trains. On the other hand, to do this optimally, neural membrane potentials should encode shared global information about what has already been coded by the network, which will tend to synchronise neural activity.</p><p>In a network of LIF neurons with dynamics and connectivity tuned for efficient coding, we found that this trade-off is best met when neural spike trains are entrained to a global oscillatory rhythm (implying a consistent representation of the prediction error), but where few neurons fire spikes on each cycle (implying high efficiency). This also corresponds to the regime in which inhibition and excitation are most tightly balanced. Our results provide a functional explanation for why cortical networks operate in a regime in which: (i) global oscillations in population firing rates occur alongside individual neurons with low, irregular, firing rates (<xref ref-type="bibr" rid="bib11">Brunel and Hakim, 1999</xref>) (ii) there is a tight balance between excitation and inhibition (<xref ref-type="bibr" rid="bib4">Atallah and Scanziani, 2009</xref>; <xref ref-type="bibr" rid="bib39">Wehr and Zador, 2003</xref>; <xref ref-type="bibr" rid="bib15">Cafaro and Rieke, 2010</xref>).</p><p>For simplicity, we considered a homogeneous network with one-dimensional feed-forward input. However, the results presented here can be generalised to networks with heterogenous connection, as well as networks that encode high-dimensional dynamical variables.</p><sec id="s3-1"><title>Relation to balanced network models</title><p>Previously, Brunel &amp; colleagues derived the conditions under which a recurrent network of integrate-and-fire neurons with sparse irregular firing rates exhibits fast global oscillations (<xref ref-type="bibr" rid="bib11">Brunel and Hakim, 1999</xref>; <xref ref-type="bibr" rid="bib31">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Brunel and Wang, 2003</xref>; <xref ref-type="bibr" rid="bib28">Mazzoni et al., 2008</xref>). This behaviour is qualitatively similar to the network dynamics observed in our model. However, these previous models differ in several ways from the model presented here. For example, they assume sparse connections (and/or weak connectivity), in which the probability of connections (and/or connection strengths) scales inversely with the number of neurons. In contrast, the connections in our network are non-sparse and finite. Thus, our network achieves a tighter balance between inhibitory and excitatory currents, and smaller fluctuations in membrane potentials (they scale as <inline-formula><mml:math id="inf10"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> in the absence of delays, rather than <inline-formula><mml:math id="inf11"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>).</p><p>Previous models of balanced neural networks typically consider medium to large neural populations, for which finite size effects are limited. In contrast, the recurrent network proposed here is particularly relevant to networks with a relatively small number of neurons (i.e. hundreds, rather than thousands, of neurons per input dimension). This is because in cases where a very large number of neurons encode a low-dimensional input, fluctuations in firing rate due to Poisson noise can be averaged out, and thus are not a limiting factor for coding accuracy. Note that the population size at which our proposed recurrent coding scheme ceases to be advantageous depends on several factors, including the input dimensionality and average neural firing rates.</p><p>The most important distinction between our work and previous mean-field models lies in the way the network is constructed. In our work, the network connectivity and dynamics are derived from functional principles, in order to minimise a specific loss function (i.e. the squared difference between the neural reconstruction and input signal). This ‘top-down’ modelling approach allows us to directly ask questions about the network dynamics that subserve optimal efficient coding. For example, balanced inhibition and excitation are not imposed in our model, but rather, required for efficient coding. Further, while previous models showed mechanistically how fast oscillations can emerge in a network with slow irregular firing rates (<xref ref-type="bibr" rid="bib11">Brunel and Hakim, 1999</xref>), our work goes further, showing that these dynamics are in fact required for optimal efficient coding.</p><p>Finally, it is important to realise that, while efficient coding in a recurrent network leads to global oscillations, the reverse is not true: just because a network oscillates, does not mean that it is performing efficiently. To demonstrate this point, we repeated our simulations in a network with heterogeneous read-out weights (<xref ref-type="fig" rid="fig12">Figure 12a–b</xref>). Both the coding performance and spiking dynamics of this network were indistinguishable from the homogeneous network described in the main text. In contrast, when we randomised the recurrent connection strengths (<xref ref-type="fig" rid="fig12">Figure 12c–d</xref>; see Materials and methods), the coding performance of the network was greatly reduced (<xref ref-type="fig" rid="fig12">Figure 12e</xref>), despite the fact that the network dynamics and firing rate power spectrum were virtually unchanged (<xref ref-type="fig" rid="fig12">Figure 12f</xref>).<fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.13824.013</object-id><label>Figure 12.</label><caption><title>Spiking response (<bold>a</bold>), and neural reconstruction (<bold>b</bold>) of a network with heterogenous read-out weights (and connection strengths).</title><p>(<bold>c</bold>–<bold>d</bold>) Similar to panels <bold>a</bold>–<bold>b</bold>, except with randomized recurrent connections. (<bold>e</bold>) Root-mean-squared reconstruction error for each model variant, including a population of Poisson units with matching firing rates. (<bold>f</bold>) Power spectrum of population firing rate for each model variant.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13824.013">http://dx.doi.org/10.7554/eLife.13824.013</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-13824-fig12-v2"/></fig></p><p>Indeed, the coherence between excitatory and inhibitory current oscillations (i.e. the level of balance) is a much more reliable signature of efficient coding than global population synchrony. While population synchrony can occur in globally balanced network as well, only networks with an intracellular detailed balance between excitatory and inhibitory currents achieve high coding performance.</p></sec><sec id="s3-2"><title>Relation to previous efficient coding models</title><p>Previous work on efficient coding has mostly concentrated on using information theory to ask what information ‘should’ be represented by sensory systems (<xref ref-type="bibr" rid="bib36">Simoncelli and Olshausen, 2001</xref>). Recently, however, researchers have begun to ask, mechanistically, how neural networks should be setup in order to operate as efficiently as possible (<xref ref-type="bibr" rid="bib9">Boerlin et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Boerlin and Denève, 2011</xref>; <xref ref-type="bibr" rid="bib16">Deneve, 2008</xref>). This approach can provide certain insights not obtainable from information theory alone.</p><p>For example, according to information theory, in the low-noise limit, the most efficient spiking representation will be one in which the spike trains of different neurons are statistically independent, and thus, there are no oscillations. In practice however, neural networks must operate in the face of biological constraints, such as synaptic delays. Considering these constraints changes our conclusions. Specifically, contrary to what one might expect from a purely information theoretical analysis, we find that oscillations emerge (even in a regime with low input-noise) as a consequence of neurons performing as efficiently as possible given synaptic delays, and should not be removed at all cost.</p></sec><sec id="s3-3"><title>Relation to previous predictive spiking models</title><p>Previously, Deneve, Machens and colleagues described how a population of spiking neurons can efficiently encode a dynamic input variable (<xref ref-type="bibr" rid="bib9">Boerlin et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Boerlin and Denève, 2011</xref>; <xref ref-type="bibr" rid="bib16">Deneve, 2008</xref>; <xref ref-type="bibr" rid="bib10">Bourdoukan et al., 2013</xref>). In this work, we showed that a recurrent population of integrate-and-fire neurons with dynamics and connectivity tuned for efficient coding maintains a balance between excitation and inhibition, exhibits Poisson-like spike statistics (<xref ref-type="bibr" rid="bib9">Boerlin et al., 2013</xref>), and is highly robust against perturbations such as neuronal cell death. However, we did not previously demonstrate a relation between efficient coding and neural oscillations. The main reason for this is that we always considered an idealised network, with instantaneous synapses. In this idealised network, only one cell fires at a time. As a result, oscillations are generally extremely weak and fast (with frequency equal to the population firing rate), and thus, completely washed out in a large network with added noise and/or heterogenous read-out weights. In contrast, in a network with finite synaptic delays, more than one neuron may fire per oscillation cycle, before the arrival of inhibition. As a result, oscillations are generally much stronger, even with significant added noise and/or heterogenous read-out weights.</p></sec><sec id="s3-4"><title>Attentional modulation</title><p>Directing attention to a particular stimulus feature/location has been shown to increase the gamma-band synchronisation of visual neurons that respond to the attended feature/location (<xref ref-type="bibr" rid="bib20">Fries et al., 2001</xref>). <xref ref-type="fig" rid="fig9">Figure 9</xref> illustrates how such an effect could come about. Here, we show that attentional modulations that increase the strength of gamma-band oscillations will serve to increase perceptual discrimination of low contrast stimuli. Such attentional modulation could be achieved in a number of different ways, for example, by decreasing noise fluctuations, or modulating the effective gain of feed-forward or recurrent connections (<xref ref-type="bibr" rid="bib32">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib29">Mitchell et al., 2007</xref>).</p><p>In general, the way in which attention should modulate the network dynamics will depend on the stimulus statistics and task-setup. Future work, that considered higher dimensional sensory inputs (as well as competing ‘distractor’ stimuli), could allow us to investigate this question further.</p></sec><sec id="s3-5"><title>The benefits of noise</title><p>An interesting aspect of our work is that it suggests various sources of noise (e.g. such as synaptic failure), often thought of as a ‘problem’ for neural coding, may in fact help neural networks achieve higher coding performance than would otherwise be possible.</p><p>With low noise, neural membrane potentials in our model are highly correlated (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), and inhibition is not able to prevent multiple neurons firing together. To avoid this, we needed to add noise to neural membrane potentials (while simultaneously increasing spiking thresholds). With the right level of noise, fewer neurons fired on each oscillation cycle, resulting in increased coding performance. Too much noise, however, led to inconsistent information being encoded by different neurons, decreasing coding performance (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). This phenomena, where noise fluctuations increase the signal processing performance of a system, is often referred to as ‘stochastic resonance’ (<xref ref-type="bibr" rid="bib7">Benzi et al., 1981</xref>; <xref ref-type="bibr" rid="bib19">Faisal et al., 2008</xref>), and has been observed in multiple sensory systems, including cat visual neurons (<xref ref-type="bibr" rid="bib27">Longtin et al., 1991</xref>; <xref ref-type="bibr" rid="bib25">Levin and Miller, 1996</xref>). Previously however, stochastic resonance has usually been seen as a method to amplify sub-threshold sensory signals that would not normally drive neurons to spike. Here, in contrast, noise desynchronises neurons that receive similar recurrent inputs, increasing the coding efficiency of the population.</p></sec><sec id="s3-6"><title>Biological limitations</title><p>While it is interesting that, starting from a pure top-down coding rule, one can arrive at a network of recurrently coupled effective integrate-and-fire (IF) neurons, we emphasize that this derived network is still far from being ‘biologically realistic. In the current paper, we address a fundamental limitation of the idealized model that emerges from the derivation, where neural feedback is both noiseless and instantaneous. We show how efficient neural coding can be performed in a network where feedback is noisy and delayed, with resulting oscillatory dynamics that resemble what is observed experimentally.</p><p>Nonetheless, significant challenges remain in order to draw a closer connection between the top-down neural model and biology. For example, neurons in our model have voltage-driven synapses, while real synapses are better approximated by conductance based models. In another study, we showed how to adapt the framework to more realistic Hodgkin Huxley neurons that included synapses with finite rise time, but no delays (<xref ref-type="bibr" rid="bib34">Schwemmer et al., 2015</xref>). Further, the slow time constant (100 ms), required in our simulations to achieve high coding performance, contrasts with the relatively fast (∼10–20 ms) membrane time constants observed experimentally. It will be important in the future to investigate how extensions to the model (such as higher-dimensional inputs and/or alternative implementations with sparse recurrent connectivity) can help recover coding performance given fast, biologically realistic, membrane time constants.</p></sec><sec id="s3-7"><title>Alternative functional roles for oscillations</title><p>Neural oscillations have been hypothesised to fulfill a number of different functional roles, including feature binding (<xref ref-type="bibr" rid="bib37">Singer, 1999</xref>), gating communication between different neural assemblies (<xref ref-type="bibr" rid="bib21">Fries, 2005</xref>; <xref ref-type="bibr" rid="bib40">Womelsdorf et al., 2007</xref>; <xref ref-type="bibr" rid="bib1">Akam and Kullmann, 2010</xref>), encoding feed-forward and feed-back prediction errors (<xref ref-type="bibr" rid="bib3">Arnal et al., 2011</xref>; <xref ref-type="bibr" rid="bib2">Arnal and Giraud, 2012</xref>; <xref ref-type="bibr" rid="bib6">Bastos et al., 2012</xref>) and facilitating ‘phase codes’ in which information is communicated via the timing of spikes relative to the ongoing oscillation cycle (<xref ref-type="bibr" rid="bib13">Buzsáki and Chrobak, 1995</xref>).</p><p>Many of these theories propose new ways in which oscillations encode incoming sensory information. In contrast, in our work network oscillations do not directly code for anything, but rather, are predicted as a consequence of efficient rate coding, an idea whose origins go back more than 50 years (<xref ref-type="bibr" rid="bib5">Barlow, 1961</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Efficient spiking network</title><p>We consider a dynamical variable that evolves in time according to:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM5">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM6">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM7">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM8">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a time-varying external input or command variable, and <inline-formula><mml:math id="inf13"><mml:mi>τ</mml:mi></mml:math></inline-formula> is a fixed time constant. Our goal is to build a network of <inline-formula><mml:math id="inf14"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons that take <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM9">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as input, and reproduce the trajectory of <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM10">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Specifically, we want to be able to read an estimate <inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM11">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM12">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> of the dynamical variable from the network’s spike trains <inline-formula><mml:math id="inf18"><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM13">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM17"><mml:msub><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM14">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow id="XM18"><mml:msub><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM15">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi id="XM19" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow id="XM20"><mml:msub><mml:mi>o</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM16">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These output spike trains are given by <inline-formula><mml:math id="inf19"><mml:mrow><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM21">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM22"><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf20"><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> is the time of the <inline-formula><mml:math id="inf21"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> spike in neuron <inline-formula><mml:math id="inf22"><mml:mi>i</mml:mi></mml:math></inline-formula>.</p><p>We first assume that the estimate, <inline-formula><mml:math id="inf23"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM23">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, can be read out by a weighted leaky integration of spike trains:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM24">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM25">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM26">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is a constant read-out weight associated with the <inline-formula><mml:math id="inf25"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> neuron. For simplicity, we set the readout time-constant, <inline-formula><mml:math id="inf26"><mml:mi>τ</mml:mi></mml:math></inline-formula>, equal to the timescale of the input, <inline-formula><mml:math id="inf27"><mml:mi>x</mml:mi></mml:math></inline-formula>.</p><p>We next assume that the network minimises the distance between <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM27">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf29"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM28">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by optimising over spike times <inline-formula><mml:math id="inf30"><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula>. The network minimises the loss function,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM29">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM32"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM30">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM31">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM33">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM34">t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The first term in the loss function is the squared distance between the <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM35">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM36">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The second term and third term represent L1 and L2 penalties on the firing rate, respectively. <inline-formula><mml:math id="inf33"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf34"><mml:mi>β</mml:mi></mml:math></inline-formula> are constants that determine the size of the penalty. The time varying firing rate of the <inline-formula><mml:math id="inf35"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> neuron is defined to by the differential equation:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM37">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>A neuron fires a spike at time <inline-formula><mml:math id="inf36"><mml:mi>t</mml:mi></mml:math></inline-formula> if it can reduce the instantaneous error <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM38">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (i.e. when <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>neuron </mml:mtext><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mtext> </mml:mtext><mml:mtext> spikes</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mtext> </mml:mtext><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext>neuron </mml:mtext><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mtext> </mml:mtext><mml:mtext> doesn't</mml:mtext><mml:mtext> </mml:mtext><mml:mtext> spike</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>). This results in a spiking rule:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where,<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM50">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a time-varying variable, whereas <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is a constant, we identify the former with the <inline-formula><mml:math id="inf41"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> neuron’s membrane potential <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM51">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the latter with its firing threshold <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>To obtain the network dynamics, we take the derivative of each neuron’s membrane potential to obtain:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM52">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM53">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM54">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM55">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM56">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM57">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf44"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM58">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to a white ‘background noise’, with unit variance (added for biological realism). Thus, the resultant dynamics are equivalent to a recurrent network of leaky integrate-and-fire (LIF) neurons, with leak, <inline-formula><mml:math id="inf45"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM59">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, feed-forward input, <inline-formula><mml:math id="inf46"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM60">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, recurrent input, <inline-formula><mml:math id="inf47"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM61">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and self-inhibition (or reset), <inline-formula><mml:math id="inf48"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM62">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Note that in the case considered here, where the decoding timescale is equal to the membraine time constant, the ‘leak term’, <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, emerges directly from the derivation.</p></sec><sec id="s4-2"><title>Balanced network of inhibitory and excitatory neurons</title><p>To construct a network that respects the Dale’s law, we introduce a population of inhibitory neurons, that tracks the estimate encoded by the excitatory neurons, and provides recurrent feedback. For simplicity, we consider a network in which all read-out weights are positive. In our framework, this results in a particularly simple network architecture, in which a single population of excitatory neurons is recurrently connected to a population of inhibitory neurons (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). For further discussion of different network architectures, see (<xref ref-type="bibr" rid="bib9">Boerlin et al., 2013</xref>).</p><p>We first introduce a population of inhibitory neurons, that receive input from excitatory cells. The objective of the inhibitory population is to minimise the squared distance between excitatory and inhibitory neural reconstructions (<inline-formula><mml:math id="inf50"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf51"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula>, respectively), by optimising over spike times <inline-formula><mml:math id="inf52"><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula>. Thus, an inhibitory neuron spikes when it can reduce the loss function:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM63">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM66"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM64">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM65">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM67">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>I</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM68">t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Following the same prescription as before, we obtain the following dynamics for the inhibitory neurons:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, inhibitory neurons receive input from excitatory neurons (second term), and recurrent inhibition from other inhibitory neurons (third term).</p><p>Now, as the inhibitory reconstruction tracks the excitatory reconstruction, we can make the simplifying assumption of replacing <inline-formula><mml:math id="inf53"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> with <inline-formula><mml:math id="inf54"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula>, in our earlier expression for the excitatory membrane potential (<xref ref-type="disp-formula" rid="equ6">equation 6</xref>), giving:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>E</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM77">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>E</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM80"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM78">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM79">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>E</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Taking the derivative of this expression as before, we obtain the following dynamics for the excitatory neurons:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo></mml:mstyle></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Thus, excitatory neurons receive excitatory feed-forward input (second term) and recurrent inhibitory input (third term).</p></sec><sec id="s4-3"><title>Synaptic dynamics</title><p>To account for transmission delays and continuous synaptic dynamics we assume that each spike generates a continuous current input to other neurons, with dynamics described by the synaptic waveform, <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM89"><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The shape of this waveform is given by:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext> if </mml:mtext></mml:mrow><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext> if </mml:mtext></mml:mrow><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mo>&gt;</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>τ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> is the synaptic rise time, <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>τ</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> is the decay time and <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the transmission delay. The normalisation constant, <inline-formula><mml:math id="inf59"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, ensures that <inline-formula><mml:math id="inf60"><mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM98">t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. This profile is plotted in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, with <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf62"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To incorporate continuous synaptic currents into the model, we alter <xref ref-type="disp-formula" rid="equ1 equ1">equations 10 and 12</xref>, by replacing each of the recurrent spiking inputs (<inline-formula><mml:math id="inf64"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM99">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) by the convolution of the spiking input and current waveform (<inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋆</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></sec><sec id="s4-4"><title>Simulation parameters</title><p>For the simulations shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, we considered a toy network of 3 neurons with equal read-out weights, <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The L1 spike cost was <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and the L2 spike cost was set to <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></inline-formula>. The read-out time constant was set to <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The magnitude of injected membrane potential noise was set to <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula>. In each case, network dynamics were computed from <xref ref-type="disp-formula" rid="equ8">equation 8</xref>.</p><p>For the simulations shown in <xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig12">12</xref>, we considered a larger network of 50 excitatory neurons, and 50 inhibitory neurons. All neurons had equal read-out weights, equal to <inline-formula><mml:math id="inf71"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>mV</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The L1 spike cost was set to 0. The L2 spike cost was set to <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>8.5</mml:mn></mml:mrow></mml:math></inline-formula> mV. If we assume a spike threshold of −55 mV, this corresponds to a resting potential of −60 mV (<inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM103"><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), a reset of −65 mV (<inline-formula><mml:math id="inf74"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>), and post-synaptic potentials of 1.45 mV (<inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>; se<italic>e (<xref ref-type="bibr" rid="bib9">Boerlin et al., 2013</xref>) </italic>for details of scaling to biological parameters).</p><p>For <xref ref-type="fig" rid="fig3">Figures 3</xref>, <xref ref-type="fig" rid="fig6">6</xref>–<xref ref-type="fig" rid="fig8">8</xref>, <xref ref-type="fig" rid="fig10">10</xref> and <xref ref-type="fig" rid="fig12">12</xref> the magnitude of injected membrane potential noise was set to its ‘optimal value’ <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>17</mml:mn></mml:mrow></mml:math></inline-formula> mV <inline-formula><mml:math id="inf77"><mml:msup><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. For <xref ref-type="fig" rid="fig5">Figure 5</xref> the membrane potential noise was set lower, at <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> mV <inline-formula><mml:math id="inf79"><mml:msup><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p><p>Network dynamics were computed from <xref ref-type="disp-formula" rid="equ1 equ1">equations 10 and 12</xref> (with the exception that recurrent inputs were convolved with the synaptic current waveform, <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM104">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, described in <xref ref-type="disp-formula" rid="equ1">equation 13</xref>).</p></sec><sec id="s4-5"><title>Algorithm</title><p>Simulations were run using a Euler method, with discrete time steps of 0.5 ms. For the ‘ideal’ network (i.e. with instantaneous synapses) only one neuron (with highest membrane potential) was allowed to fire a spike within each bin. Changing the temporal discretisation did not qualitatively alter our results.</p></sec><sec id="s4-6"><title>Stimulus details</title><p>In <xref ref-type="fig" rid="fig2">Figure 2b</xref>, the encoded variable, <inline-formula><mml:math id="inf81"><mml:mi>x</mml:mi></mml:math></inline-formula>, was obtained by low-pass filtering white noise with a first-order Butterworth filter, with cut-off frequency of 4 Hz (the low-pass Butterworth filter, with transfer function proportional to <inline-formula><mml:math id="inf82"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM106"><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM105"><mml:mi>ω</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf83"><mml:mi>n</mml:mi></mml:math></inline-formula> is the filter order, and <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>ω</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the cut-off frequency, was implemented using Matlab’s ‘butter’ command).</p><p>After filtering, <inline-formula><mml:math id="inf85"><mml:mi>x</mml:mi></mml:math></inline-formula>, is rescaled to have a mean of 3, and standard deviation of 1. In <xref ref-type="fig" rid="fig2">Figure 2c</xref> the encoded variable is constant, <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig3">Figure 3</xref>, the encoded variable is obtained by low-pass filtering a white-noise input in the same way as for <xref ref-type="fig" rid="fig2">Figure 2b</xref>, this time with a cut-off frequency of 2 Hz. After filtering, <inline-formula><mml:math id="inf87"><mml:mi>x</mml:mi></mml:math></inline-formula>, is rescaled to have non-zero mean of 50, and standard deviation of 10. In <xref ref-type="fig" rid="fig4">Figures 4</xref>–<xref ref-type="fig" rid="fig5">5</xref>, <xref ref-type="fig" rid="fig7">7</xref>–<xref ref-type="fig" rid="fig8">8</xref> and <xref ref-type="fig" rid="fig10">10</xref>–<xref ref-type="fig" rid="fig12">12</xref> the encoded variable was held constant at <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig9">9</xref>, the ’low’, ’medium’ and ’high’ amplitude inputs are <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> 35, 50, and 65, respectively.</p></sec><sec id="s4-7"><title>Poisson model</title><p>In <xref ref-type="fig" rid="fig3">Figure 3d–e</xref>, we compare the efficient coding model to a rate model, in which neural firing rates vary as a function of the feed-forward input, <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM107">t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Firing rates for the Poisson model were equal to the mean firing rate in the recurrent model (when excitatory readout weights are all equal, firing rates are proportional to the feed-forward input, <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM108">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). Spiking responses were obtained by drawing from a Poisson distribution.</p></sec><sec id="s4-8"><title>Varying the noise</title><p>For the simulations shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we varied the injected membrane potential noise. In general, varying the noise amplitude changes neural firing rates, leading to systematic estimation biases. To compensate for this, we adjusted the L2 spike cost for the inhibitory and excitatory neurons, so as to maintain zero estimation bias (or equivalently, to keep firing rates constant). For each noise level, we ran an initial simulation, modifying excitatory and inhibitory costs, <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>β</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>β</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula>, in real time (via a stochastic gradient descent algorithm), until both the excitatory and inhibitory estimation biases converged to zero.</p><p>To generate <xref ref-type="fig" rid="fig5">Figure 5a–b</xref> we varied the synaptic reliability, by altering the probability that a presynaptic spike produces a change in the postsynaptic membrane potential. To keep the total synaptic input to each neuron constant, we divided the recurrent connection strengths by the probability of synaptic failure.</p><p>To generate <xref ref-type="fig" rid="fig5">Figure 5c–d</xref> we chose a fraction of inhibitory and excitatory neurons. The selected neurons fired spikes random Poisson spike trains. Their firing rates remained unchanged.</p></sec><sec id="s4-9"><title>Population firing rates</title><p>To plot the population firing rate (<xref ref-type="fig" rid="fig6">Figure 6a and b</xref>), we low-pass filtered neural spike trains using a first order Butterworth filter (with cut-off frequency of 5.5 Hz, and 66 Hz, for panels b and d, respectively), before averaging over neurons.</p></sec><sec id="s4-10"><title>Spectral analysis</title><p>The spectrogram of the population firing rate, shown in <xref ref-type="fig" rid="fig6">Figure 6a</xref> (lower panel), was computed using a short-time Fourier-transform, with a Hamming time window of 60 ms (Matlab’s ‘spectrogram function’). Finally, the instantaneous power spectrum was low-pass filtered with a first-order Butterworth filter, with cut-off frequency 3 Hz. The power spectrum of the population firing rate and neural membrane potentials (<xref ref-type="fig" rid="fig4">Figures 4h</xref>, <xref ref-type="fig" rid="fig5">5b and d</xref>, <xref ref-type="fig" rid="fig6">6c</xref>) was computed using the multi-taper method (using Matlab’s ‘pmtm’ function), with bandwidth chosen empirically to achieve a <bold>s</bold>pectrum that varied smoothly with frequency.</p></sec><sec id="s4-11"><title>Excitatory and inhibitory currents</title><p>To plot the currents shown in <xref ref-type="fig" rid="fig8">Figure 8b</xref>, we divided the total excitatory and inhibitory input to each cell by a presumed membrane resistance of <inline-formula><mml:math id="inf94"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mtext>M</mml:mtext><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (changing this value rescales the y-axis). We then defined peaks in excitatory and inhibitory currents as local maxima in the currents, separated by a drop an 80% drop in the current magnitude. Further, we only included peaks in inhibitory and excitatory currents that occurred within 15 ms of each other.</p></sec><sec id="s4-12"><title>Discrimination threshold</title><p>For the simulation shown in <xref ref-type="fig" rid="fig9">Figure 9</xref>, we considered the performance of the network in discriminating between two 0.1 s long stimulus segments, with equally spaced around the ‘low’ amplitude input (<inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>48</mml:mn></mml:mrow></mml:math></inline-formula>). From signal detection theory, a subject’s probability of selecting between two stimuli is given by: <inline-formula><mml:math id="inf96"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub id="XM109"><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM110"><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>erfc</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM117"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub id="XM115"><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM116"><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>erfc</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi id="XM118">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the cumulative error function, and <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub id="XM119"><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM120"><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the normalized distance (or d-prime) between the distribution of estimates: <inline-formula><mml:math id="inf99"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub id="XM125"><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM126"><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub id="XM121"><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub id="XM122"><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub id="XM123"><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub id="XM124"><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></inline-formula>. These quantities can be directly computed from the network output.</p></sec><sec id="s4-13"><title>‘Random’ versus ‘precisely’ balanced network</title><p>The network used to generate <xref ref-type="fig" rid="fig12">Figure 12a–b</xref> was the same as before, with the exception that the readout weights varied (and thus, the connection strengths) for each neuron. Readout weights were sampled from a gamma distribution, with mean <inline-formula><mml:math id="inf100"><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>mV</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and standard deviation <inline-formula><mml:math id="inf101"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>mV</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. For the optimal ‘precisely balanced’ network (<xref ref-type="fig" rid="fig12">Figure 12a–b</xref>), the reciprocal connection strengths between each inhibitory and excitatory neuron are equal in magnitude. For the ‘randomly’ balanced network (<xref ref-type="fig" rid="fig12">Figure 12c–d</xref>), we disrupted this symmetry by randomly permuting the input to each excitatory/inhibitory neuron (while ensuring that the summed input to each neuron remained unchanged). Recurrent inhibitory connections were left unchanged.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>Support from the Basic Research Program of the National Research University Higher School of Economics is gratefully acknowledged by BSG.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>MC, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>BG, Conception and design, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>SD, Conception and design, Drafting or revising the article</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Kullmann</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Oscillations and filtering networks support flexible routing of information</article-title><source>Neuron</source><volume>67</volume><fpage>308</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.06.019</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and sensory predictions</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.05.003</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transitions in neural oscillations reflect prediction errors generated in audiovisual speech</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>797</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1038/nn.2810</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Instantaneous modulation of gamma oscillation frequency by balancing excitation with inhibition</article-title><source>Neuron</source><volume>62</volume><fpage>566</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.04.027</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1961">1961</year><source>In Sensory Communication</source><publisher-name>MIT Press</publisher-name><fpage>217</fpage><lpage>234</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Usrey</surname><given-names>WM</given-names></name><name><surname>Adams</surname><given-names>RA</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benzi</surname><given-names>R</given-names></name><name><surname>Sutera</surname><given-names>A</given-names></name><name><surname>Vulpiani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>The mechanism of stochastic resonance</article-title><source>Journal of Physics A: Mathematical and General</source><volume>14</volume><fpage>L453</fpage><lpage>L457</lpage><pub-id pub-id-type="doi">10.1088/0305-4470/14/11/006</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boerlin</surname><given-names>M</given-names></name><name><surname>Denève</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spike-based population coding and working memory</article-title><source>PLoS Computational Biology</source><volume>7</volume><elocation-id>e1001080</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001080</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boerlin</surname><given-names>M</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Denève</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1003258</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003258</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourdoukan</surname><given-names>R</given-names></name><name><surname>Barrett</surname><given-names>D</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Learning optimal spike-based representations</article-title><source>Advances in Neural Information Processing Systems</source><volume>14</volume><fpage>2979</fpage><lpage>3010</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Hakim</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Fast global oscillations in networks of integrate-and-fire neurons with low firing rates</article-title><source>Neural Computation</source><volume>11</volume><fpage>1621</fpage><lpage>1671</lpage><pub-id pub-id-type="doi">10.1162/089976699300016179</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>What determines the frequency of fast network oscillations with irregular neural discharges? I. Synaptic dynamics and excitation-inhibition balance</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>415</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1152/jn.01095.2002</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Chrobak</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Temporal structure in spatially organized neuronal ensembles: a role for interneuronal networks</article-title><source>Current Opinion in Neurobiology</source><volume>5</volume><fpage>504</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(95)80012-3</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms of gamma oscillations</article-title><source>Annual Review of Neuroscience</source><volume>35</volume><fpage>203</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150444</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cafaro</surname><given-names>J</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Noise correlations improve response fidelity and stimulus encoding</article-title><source>Nature</source><volume>468</volume><fpage>964</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1038/nature09570</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Bayesian spiking neurons I: inference</article-title><source>Neural Computation</source><volume>20</volume><fpage>91</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.20.1.91</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dynamic predictions: oscillations and synchrony in top-down processing</article-title><source>Nature Reviews Neuroscience</source><volume>2</volume><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1038/35094565</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Lewen</surname><given-names>GD</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter Van Steveninck</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficiency and ambiguity in an adaptive neural code</article-title><source>Nature</source><volume>412</volume><fpage>787</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1038/35090500</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname><given-names>AA</given-names></name><name><surname>Selen</surname><given-names>LP</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Noise in the nervous system</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nrn2258</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Rorie</surname><given-names>AE</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modulation of oscillatory neuronal synchronization by selective visual attention</article-title><source>Science</source><volume>291</volume><fpage>1560</fpage><lpage>1563</lpage><pub-id pub-id-type="doi">10.1126/science.1055465</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A mechanism for cognitive dynamics: neuronal communication through neuronal coherence</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>474</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.08.011</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neuronal gamma-band synchronization as a fundamental process in cortical computation</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>209</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135603</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilboa</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>R</given-names></name><name><surname>Brenner</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>History-dependent multiple-time-scale dynamics in a single-neuron model</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>6479</fpage><lpage>6489</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0763-05.2005</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henrie</surname><given-names>JA</given-names></name><name><surname>Shapley</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>LFP power spectra in V1 cortex: the graded effect of stimulus contrast</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>479</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1152/jn.00919.2004</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname><given-names>JE</given-names></name><name><surname>Miller</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Broadband neural encoding in the cricket cercal sensory system enhanced by stochastic resonance</article-title><source>Nature</source><volume>380</volume><fpage>165</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1038/380165a0</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>London</surname><given-names>M</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dendritic computation</article-title><source>Annual Review of Neuroscience</source><volume>28</volume><fpage>503</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.28.061604.135703</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Longtin</surname><given-names>A</given-names></name><name><surname>Bulsara</surname><given-names>A</given-names></name><name><surname>Moss</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Time-interval sequences in bistable systems and the noise-induced transmission of information by sensory neurons</article-title><source>Physical Review Letters</source><volume>67</volume><fpage>656</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.67.656</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzoni</surname><given-names>A</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Encoding of naturalistic stimuli by local field potential spectra in networks of excitatory and inhibitory neurons</article-title><source>PLoS Computational Biology</source><volume>4</volume><elocation-id>e1000239</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000239</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>JF</given-names></name><name><surname>Sundberg</surname><given-names>KA</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Differential attention-dependent response modulation across cell classes in macaque visual area V4</article-title><source>Neuron</source><volume>55</volume><fpage>131</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.018</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Differences in gamma frequencies across visual cortex restrict their possible use in computation</article-title><source>Neuron</source><volume>67</volume><fpage>885</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.08.004</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>de la Rocha</surname><given-names>J</given-names></name><name><surname>Bartho</surname><given-names>P</given-names></name><name><surname>Hollender</surname><given-names>L</given-names></name><name><surname>Parga</surname><given-names>N</given-names></name><name><surname>Reyes</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The asynchronous state in cortical circuits</article-title><source>Science</source><volume>327</volume><fpage>587</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1126/science.1179850</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title><source>Nature</source><volume>440</volume><fpage>1007</fpage><pub-id pub-id-type="doi">10.1038/nature04701</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwemmer</surname><given-names>MA</given-names></name><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Denéve</surname><given-names>S</given-names></name><name><surname>Shea-Brown</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Constructing precisely computing networks with biophysical spiking neurons</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>10112</fpage><lpage>10134</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4951-14.2015</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The variable discharge of cortical neurons: implications for connectivity, computation, and information coding</article-title><source>Journal of Neuroscience</source><volume>18</volume><fpage>3870</fpage><lpage>3896</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neuronal synchrony: a versatile code for the definition of relations?</article-title><source>Neuron</source><volume>24</volume><fpage>49</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)80821-1</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Touboul</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The hipster effect: when anticonformists all look the same</article-title><source>arxiv</source><uri xlink:href="http://arxiv.org/abs/1410.8001">http://arxiv.org/abs/1410.8001</uri></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehr</surname><given-names>M</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex</article-title><source>Nature</source><volume>426</volume><fpage>442</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1038/nature02116</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Womelsdorf</surname><given-names>T</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Modulation of neuronal interactions through neuronal synchronization</article-title><source>Science</source><volume>316</volume><fpage>1609</fpage><lpage>1612</lpage><pub-id pub-id-type="doi">10.1126/science.1139597</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Ferster</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Membrane potential synchrony in primary visual cortex during sensory stimulation</article-title><source>Neuron</source><volume>68</volume><fpage>1187</fpage><lpage>1201</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.11.027</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.13824.014</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing editor</role><aff id="aff5"><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your work entitled &quot;Neural oscillations as a signature of efficient coding in the presence of synaptic delays&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by Timothy Behrens as the Senior editor and three reviewers, one of whom is a member of our Board of Reviewing Editors.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors have previously shown that a an optimal error correcting code requires a balanced excitatory (E) and inhibitory (I) network, where a spike occurs only to reduce the error between a network estimate and the true stimulus. In that previous work, feedback was instantaneous. Here the authors show that if feedback is delayed (as it must be in realistic networks), oscillations develop. However, excessive synchronous network oscillations degrades coding. The central result of the manuscript is that noise mitigates some of the deleterious aspects of network-wide oscillatory synchrony on neural coding.</p><p>Essential revisions:</p><p>1) Some features of the result will naturally depend on the readout time constant τ. The only place I see its value mentioned is in connection with <xref ref-type="fig" rid="fig2">Figure 2</xref> (subsection &quot;Simulation parameters&quot;, first paragraph). There it says that τ was 100 ms. Was this value also used in the later simulations? It seems rather a large value to want to associate with a membrane time constant, so I think it would be useful if the authors said something about how this filtering might be implemented biologically. And what would the results look like if τ had a value more like a typical membrane time constant?</p><p>2) How sensitive are the results to the parameters of the network? In particular, how do they scale with network size, and with the ratio of excitatory to inhibitory neurons? In particular, when the network is scaled up and the ratio of excitatory to inhibitory neurons is set to a more realistic value, like 4, what happens to the following:</p><p>A) Does the optimal noise (<xref ref-type="fig" rid="fig4">Figure 4c</xref>) stay at 15 mV? And does the ratio of the optimal RMS error to the Poisson RMS error stay the same?</p><p>B) Does the optimal failure probability stay at about 0.5? And does the ratio of the optimal RMS error to the Poisson RMS error (which should be shown in that figure) stay the same?</p><p>C) Do the oscillation frequencies stay in the 30-50 Hz range?</p><p>D) Do the oscillation frequencies depend most strongly on the delay, or on other network parameters?</p><p>3) In the text referring to <xref ref-type="fig" rid="fig3">Figure 3(e)</xref>, it would be good to explain why the rate in the performance-matched case is so high.</p><p>4) The fact that failures improves network performance may be one of the most interesting results in the paper, as it implies that failures are a feature, not a bug. We suggest that the paper would have more impact on the community if you emphasized this point, although we will leave that up to you.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.13824.015</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>1) Some features of the result will naturally depend on the readout time constant τ. The only place I see its value mentioned is in connection with <xref ref-type="fig" rid="fig2">Figure 2</xref> (subsection &quot;Simulation parameters&quot;, first paragraph). There it says that τ was 100 ms. Was this value also used in the later simulations? It seems rather a large value to want to associate with a membrane time constant, so I think it would be useful if the authors said something about how this filtering might be implemented biologically. And what would the results look like if τ had a value more like a typical membrane time constant?</italic></p><p>There are two time constants in our model, the readout time, τ, and the synaptic delay. Thus (with other parameters rescaled accordingly), reducing the readout time constant is equivalent to increasing the delay. We conducted additional simulations (<xref ref-type="fig" rid="fig11">Figure 11</xref>) showing that both decreasing τ or increasing the synaptic delay have similar effects on coding performance, which is reduced. These simulations are described in more detail later, in our response to the second comment raised by the reviewers.</p><p>To retain coding performance with a smaller read-out time constant requires introducing changes to the ideal network, to prevent multiple neurons firing synchronously, and dampen the resulting excessive oscillations. In the main text we illustrated how encoding performance can be recovered by adding noise (e.g. synaptic failure, additive membrane potential noise or Poisson spiking neurons). We note that coding performance can also be improved by altering other aspects of the network, such as recurrent connectivity. For example, while beyond the scope of the current work, we found that a locally connected network consisting of several overlapping inhibitory sub-populations, each of which encodes its own ‘version’ of the input, can lead to weaker oscillations, improving the performance of the ‘all-to-all’ network presented in our work.</p><p>The reviewers raise the fair point that the decoding time constant of 100 ms in the paper is considerably longer than typical membrane time constants. Indeed, the value of 100 ms was used primarily to ensure consistency with our previous theoretical work (Boerlin et al. 2013, PLoS Comp), rather than for explicit biological realism. It is worth noting however, that in contrast to the simple effective integrate-and-fire (IF) model that results from deriving our framework, real neurons exhibit dynamics on multiple timescales, including slow adaptation time scales (Fairhall et al., 2001, Nature), slow inactivation dynamics of intrinsic conductances (Gilboa et al., 2015, J Neurosci), slow integration due to voltage-dependent potassium currents (Storm et al., 1988, Nature) and slower dendritic integration time-scales due to NMDA-synaptic currents (London et al., 2005, Ann. Rev. Neurosci). It is possible that these slower dynamics could account for the slow readout time, that is required by the network in order to achieve a high degree of coding precision.</p><p>Finally, while it is interesting to show how, starting from a pure ‘top-down’ coding rule, one can arrive at a network of recurrently coupled effective integrate-and-fire (IF) neurons, we emphasize that this derived network is still far from being 'biologically realistic’. In the current paper, we addressed a major inconsistency between our previous work, where synapses were noiseless and instantaneous, and biology, where synapses are noisy and delayed. Significantly, we believe that the principles that emerge extend beyond the model network, to many other recurrent systems where interacting sub-units perform a global optimization. Nonetheless, we concede that significant challenges remain in order to draw a closer connection between our top-down neural model and biology: not least the introduction of conductance based synapses and/or understanding how the multiple cellular mechanisms may lead to slow decoding time-scales in spite of fast membrane time constants.</p><p>We have added a section to the Discussion (“Biological limitations”), with the above arguments.</p><p><italic>2) How sensitive are the results to the parameters of the network? In particular, how do they scale with network size, and with the ratio of excitatory to inhibitory neurons? In particular, when the network is scaled up and the ratio of excitatory to inhibitory neurons is set to a more realistic value, like 4, what happens to the following:</italic> </p><p><italic>A) Does the optimal noise (<xref ref-type="fig" rid="fig4">Figure 4c</xref>) stay at 15 mV? And does the ratio of the optimal RMS error to the Poisson RMS error stay the same?</italic> </p><p><italic>B) Does the optimal failure probability stay at about 0.5? And does the ratio of the optimal RMS error to the Poisson RMS error (which should be shown in that figure) stay the same?</italic> </p><p><italic>C) Do the oscillation frequencies stay in the 30-50 Hz range?</italic> </p><p><italic>D) Do the oscillation frequencies depend most strongly on the delay, or on other network parameters?</italic></p><p>As suggested by the reviewers, we investigated the behavior of the model network with varying: (i) population size, (ii) inhibitory population size only, (iii) synaptic delay, and (iv) the decoding timescale. These results are presented in two new figures (<xref ref-type="fig" rid="fig10">Figure 10</xref>–<xref ref-type="fig" rid="fig11">11</xref>), described in a new section in the Results (‘Sensitivity to network parameters’).</p><p>With all other parameters held constant, increasing the population size results in a lower firing rate for each neuron (such that the summed firing rate of all neurons is constant; <xref ref-type="fig" rid="fig10">Figure 10a</xref>). When only the inhibitory population size was altered, then the inhibitory firing rate varied while the excitatory firing rate is constant (<xref ref-type="fig" rid="fig10">Figure 10d</xref>).</p><p>The coding performance and oscillatory dynamics, on the other hand, remain relatively unchanged when we vary the population size or inhibitory/excitatory ratio. For example, neither the ‘optimal’ noise level nor the oscillation frequency were greatly changed by increasing/decreasing the population size or excitatory/inhibitory ratio by a factor of two (<xref ref-type="fig" rid="fig10">Figure 10b–c and e–f</xref>).</p><p>Note that, although in our simulations, varying the ratio of excitatory to inhibitory neurons leads to unequal firing rate for the two populations (<xref ref-type="fig" rid="fig10">Figure 10d</xref>), this does not have to be the case: one could rescale the inhibitory/excitatory readout weights so that both populations have equal rates. Nonetheless, whatever the manipulation, EI currents should equal the total IE currents, so that balance in the network is preserved.</p><p>Finally, we emphasize that our efficient coding model is particularly applicable for small to medium size neural ensembles (with correspondingly low population firing rates), where noise fluctuations resulting from Poisson spiking would otherwise lead to decreased coding performance (see <xref ref-type="fig" rid="fig3">Figure 3d</xref>). Thus, we did not find it relevant to scale our model to represent very large networks (i.e. with 1000s of neurons).</p><p>In contrast to variations in the population size, varying the synaptic delay had a significant effect on both network dynamics and coding performance. Increasing the synaptic delay resulted in larger and lower frequency oscillations, with a concomitant decrease in coding accuracy (<xref ref-type="fig" rid="fig11">Figure 11A–d</xref>).</p><p>There are only two time constants in the network: the delay, and the decoding time constant, τ. Therefore, with other parameters (e.g. feed-forward/recurrent weights) scaled appropriately, decreasing τ is equivalent to increasing the synaptic delay. On the other hand, when all other parameters are held constant, decreasing τ serves to increase firing rates (which are inversely proportional to τ), unlike varying the synaptic delay, which has no effect (compare <xref ref-type="fig" rid="fig11">Figures 11a and e</xref>).</p><p>In common with increasing the length of the delay, decreasing τ also increases the magnitude of network oscillations, while decreasing the coding quality (<xref ref-type="fig" rid="fig11">Figure 11f–g</xref>). However, unlike the delay, varying τ had a relatively weak effect on the oscillation frequency (<xref ref-type="fig" rid="fig11">Figure 11g–h</xref>). Intuitively, this is because varying τ causes two different changes in network dynamics that push in different directions. On the one hand, decreasing τ results in faster integration time, speeding up the network dynamics (and thus, tending to increase oscillation frequency). On the other hand, decreasing τ increases the oscillation magnitude, leading to stronger inhibition on each oscillation cycle and tending to slow down the oscillations.</p><p><italic>3) In the text referring to <xref ref-type="fig" rid="fig3">Figure 3(e)</xref>, it would be good to explain why the rate in the performance-matched case is so high.</italic></p><p>We added a new figure panel (<xref ref-type="fig" rid="fig3">Figure 3d</xref>) to show the relation between firing rate and coding performance in each of the model networks. Instead of showing bar plots of performance at a given fixed rate (or conversely, rate required to achieve a given level of performance), we plot the full error/rate curves for the ‘ideal recurrent’ and Poisson models. The non-ideal recurrent network is shown as a black cross on this plot.</p><p>In the Poisson network, random fluctuations in firing rate cause the reconstruction to deviate from its true value, and decrease coding performance. These noise fluctuations become less important as the population firing rate increases, with a corresponding decrease in the reconstruction error (that scales as ~1/√F), where F is the population firing rate).</p><p>In contrast, in the ideal efficient coding network, noise fluctuations are automatically ‘corrected for’ by the recurrent connection. Thus, the only source of inaccuracy comes from the discreteness of the code (where each spike adds a fixed quantity to the readout), leading to a much smaller reconstruction error (that scales as ‘1/F’).</p><p>With the addition of synaptic delays, it is no longer possible to achieve the performance of the ideal network. Nonetheless, by desynchronizing the network with an appropriate level of noise, this problem can be minimized, leading to a reconstruction error significantly smaller than for the Poisson network.</p><p>We have added text to the Results (“Efficient coding with synaptic delays”) to clarify these concepts.</p><p><italic>4) The fact that failures improves network performance may be one of the most interesting results in the paper, as it implies that failures are a feature, not a bug. We suggest that the paper would have more impact on the community if you emphasized this point, although we will leave that up to you.</italic> </p><p>We thank the reviewer for this suggestion. We also think that this is an interesting point to make. For simplicity, we chose to continue to use additive noise on the membrane potential for the majority of the simulations (we could redo all of them with the failures without changing the results qualitatively). However, we have added text to the Abstract, Introduction (see final paragraph) and Discussion (‘The benefits of noise’) to emphasize how our work suggests that synaptic failures (and noise in general) may be a feature, not a bug.</p></body></sub-article></article>