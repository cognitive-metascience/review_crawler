<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">21492</article-id><article-id pub-id-type="doi">10.7554/eLife.21492</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Reward-based training of recurrent neural networks for cognitive and value-based tasks</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-68664"><name><surname>Song</surname><given-names>H Francis</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-68680"><name><surname>Yang</surname><given-names>Guangyu R</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-6345"><name><surname>Wang</surname><given-names>Xiao-Jing</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3124-8474</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Center for Neural Science</institution>, <institution>New York University</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">NYU-ECNU Institute of Brain and Cognitive Science</institution>, <institution>NYU Shanghai</institution>, <addr-line><named-content content-type="city">Shanghai</named-content></addr-line>, <country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Behrens</surname><given-names>Timothy EJ</given-names></name><role>Reviewing editor</role><aff id="aff3"><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>xjwang@nyu.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>13</day><month>01</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e21492</elocation-id><history><date date-type="received"><day>13</day><month>09</month><year>2016</year></date><date date-type="accepted"><day>12</day><month>01</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Song et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Song et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-21492-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.21492.001</object-id><p>Trained neural network models, which exhibit features of neural activity recorded from behaving animals, may provide insights into the circuit mechanisms of cognitive functions through systematic analysis of network activity and connectivity. However, in contrast to the graded error signals commonly used to train networks through supervised learning, animals learn from reward feedback on definite actions through reinforcement learning. Reward maximization is particularly relevant when optimal behavior depends on an animal’s internal judgment of confidence or subjective preferences. Here, we implement reward-based training of recurrent neural networks in which a value network guides learning by using the activity of the decision network to predict future reward. We show that such models capture behavioral and electrophysiological findings from well-known experimental paradigms. Our work provides a unified framework for investigating diverse cognitive and value-based computations, and predicts a role for value representation that is essential for learning, but not executing, a task.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.001">http://dx.doi.org/10.7554/eLife.21492.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.21492.002</object-id><title>eLife digest</title><p>A major goal in neuroscience is to understand the relationship between an animal’s behavior and how this is encoded in the brain. Therefore, a typical experiment involves training an animal to perform a task and recording the activity of its neurons – brain cells – while the animal carries out the task. To complement these experimental results, researchers “train” artificial neural networks – simplified mathematical models of the brain that consist of simple neuron-like units – to simulate the same tasks on a computer. Unlike real brains, artificial neural networks provide complete access to the “neural circuits” responsible for a behavior, offering a way to study and manipulate the behavior in the circuit.</p><p>One open issue about this approach has been the way in which the artificial networks are trained. In a process known as reinforcement learning, animals learn from rewards (such as juice) that they receive when they choose actions that lead to the successful completion of a task. By contrast, the artificial networks are explicitly told the correct action. In addition to differing from how animals learn, this limits the types of behavior that can be studied using artificial neural networks.</p><p>Recent advances in the field of machine learning that combine reinforcement learning with artificial neural networks have now allowed Song et al. to train artificial networks to perform tasks in a way that mimics the way that animals learn. The networks consisted of two parts: a “decision network” that uses sensory information to select actions that lead to the greatest reward, and a “value network” that predicts how rewarding an action will be. Song et al. found that the resulting artificial “brain activity” closely resembled the activity found in the brains of animals, confirming that this method of training artificial neural networks may be a useful tool for neuroscientists who study the relationship between brains and behavior.</p><p>The training method explored by Song et al. represents only one step forward in developing artificial neural networks that resemble the real brain. In particular, neural networks modify connections between units in a vastly different way to the methods used by biological brains to alter the connections between neurons. Future work will be needed to bridge this gap.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.002">http://dx.doi.org/10.7554/eLife.21492.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>recurrent neural network</kwd><kwd>reinforcement learning</kwd><kwd>prefrontal cortex</kwd><kwd>decision making</kwd><kwd>orbitofrontal cortex</kwd><kwd>working memory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-13-1-0297</award-id><principal-award-recipient><name><surname>Song</surname><given-names>H Francis</given-names></name><name><surname>Yang</surname><given-names>Guangyu R</given-names></name><name><surname>Wang</surname><given-names>Xiao-Jing</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006785</institution-id><institution>Google</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Song</surname><given-names>H Francis</given-names></name><name><surname>Yang</surname><given-names>Guangyu R</given-names></name><name><surname>Wang</surname><given-names>Xiao-Jing</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A two-part neural network models reward-based training and provides a unified framework in which to study diverse computations that can be compared to electrophysiological recordings from behaving animals.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A major challenge in uncovering the neural mechanisms underlying complex behavior is our incomplete access to relevant circuits in the brain. Recent work has shown that model neural networks optimized for a wide range of tasks, including visual object recognition (<xref ref-type="bibr" rid="bib10">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib101">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib31">Hong et al., 2016</xref>), perceptual decision-making and working memory (<xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Carnevale et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Miconi, 2016</xref>), timing and sequence generation (<xref ref-type="bibr" rid="bib41">Laje and Buonomano, 2013</xref>; <xref ref-type="bibr" rid="bib61">Rajan et al., 2015</xref>), and motor reach (<xref ref-type="bibr" rid="bib28">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib84">Sussillo et al., 2015</xref>), can reproduce important features of neural activity recorded in numerous cortical areas of behaving animals. The analysis of such circuits, whose activity and connectivity are fully known, has therefore re-emerged as a promising tool for understanding neural computation (<xref ref-type="bibr" rid="bib103">Zipser and Andersen, 1988</xref>; <xref ref-type="bibr" rid="bib83">Sussillo, 2014</xref>; <xref ref-type="bibr" rid="bib24">Gao and Ganguli, 2015</xref>). Constraining network training with tasks for which detailed neural recordings are available may also provide insights into the principles that govern learning in biological circuits (<xref ref-type="bibr" rid="bib84">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Brea and Gerstner, 2016</xref>).</p><p>Previous applications of this approach to 'cognitive-type' behavior such as perceptual decision-making and working memory have focused on supervised learning from graded error signals. Animals, however, learn to perform specific tasks from reward feedback provided by the experimentalist in response to definite actions, i.e., through reinforcement learning (<xref ref-type="bibr" rid="bib85">Sutton and Barto, 1998</xref>). Unlike in supervised learning where the network is given the correct response on each trial in the form of a continuous target output to be followed, reinforcement learning provides evaluative feedback to the network on whether each selected action was 'good' or 'bad.' This form of feedback allows for a graded notion of behavioral correctness that is distinct from the graded difference between the network’s output and the target output in supervised learning. For the purposes of using model networks to generate hypotheses about neural mechanisms, this is particularly relevant in tasks where the optimal behavior depends on an animal’s internal state or subjective preferences. In a perceptual decision-making task with postdecision wagering, for example, on a random half of the trials the animal can opt for a sure choice that results in a small (compared to the correct choice) but certain reward (<xref ref-type="bibr" rid="bib39">Kiani and Shadlen, 2009</xref>). The optimal decision regarding whether or not to select the sure choice depends not only on the task condition, such as the proportion of coherently moving dots, but also on the animal’s own confidence in its decision <italic>during the trial</italic>. Learning to make this judgment cannot be reduced to reproducing a predetermined target output without providing the full probabilistic solution to the network. It can be learned in a natural, ethologically relevant way, however, by choosing the actions that result in greatest overall reward; through training, the network learns from the reward contingencies alone to condition its output on its internal estimate of the probability that its answer is correct.</p><p>Meanwhile, supervised learning is often not appropriate for value-based, or economic, decision-making where the 'correct' judgment depends explicitly on rewards associated with different actions, even for identical sensory inputs (<xref ref-type="bibr" rid="bib57">Padoa-Schioppa and Assad, 2006</xref>). Although such tasks can be transformed into a perceptual decision-making task by providing the associated rewards as inputs, this sheds little light on how value-based decision-making is learned by the animal because it conflates external with 'internal,' learned inputs. More fundamentally, reward plays a central role in all types of animal learning (<xref ref-type="bibr" rid="bib80">Sugrue et al., 2005</xref>). Explicitly incorporating reward into network training is therefore a necessary step toward elucidating the biological substrates of learning, in particular reward-dependent synaptic plasticity (<xref ref-type="bibr" rid="bib75">Seung, 2003</xref>; <xref ref-type="bibr" rid="bib76">Soltani et al., 2006</xref>; <xref ref-type="bibr" rid="bib33">Izhikevich, 2007</xref>; <xref ref-type="bibr" rid="bib92">Urbanczik and Senn, 2009</xref>; <xref ref-type="bibr" rid="bib23">Frémaux et al., 2010</xref>; <xref ref-type="bibr" rid="bib77">Soltani and Wang, 2010</xref>; <xref ref-type="bibr" rid="bib30">Hoerzer et al., 2014</xref>; <xref ref-type="bibr" rid="bib9">Brosch et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Friedrich and Lengyel, 2016</xref>) and the role of different brain structures in learning (<xref ref-type="bibr" rid="bib21">Frank and Claus, 2006</xref>).</p><p>In this work, we build on advances in recurrent policy gradient reinforcement learning, specifically the application of the REINFORCE algorithm (<xref ref-type="bibr" rid="bib98">Williams, 1992</xref>; <xref ref-type="bibr" rid="bib1">Baird and Moore, 1999</xref>; <xref ref-type="bibr" rid="bib86">Sutton et al., 2000</xref>; <xref ref-type="bibr" rid="bib4">Baxter and Bartlett, 2001</xref>; <xref ref-type="bibr" rid="bib60">Peters and Schaal, 2008</xref>) to recurrent neural networks (RNNs) (<xref ref-type="bibr" rid="bib97">Wierstra et al., 2009</xref>), to demonstrate reward-based training of RNNs for several well-known experimental paradigms in systems neuroscience. The networks consist of two modules in an 'actor-critic' architecture (<xref ref-type="bibr" rid="bib3">Barto et al., 1983</xref>; <xref ref-type="bibr" rid="bib27">Grondman et al., 2012</xref>), in which a decision network uses inputs provided by the environment to select actions that maximize reward, while a value network uses the selected actions and activity of the decision network to predict future reward and guide learning. We first present networks trained for tasks that have been studied previously using various forms of supervised learning (<xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>); they are characterized by 'simple' input-output mappings in which the correct response for each trial depends only on the task condition, and include perceptual decision-making, context-dependent integration, multisensory integration, and parametric working memory tasks. We then show results for tasks in which the optimal behavior depends on the animal’s internal judgment of confidence or subjective preferences, specifically a perceptual decision-making task with postdecision wagering (<xref ref-type="bibr" rid="bib39">Kiani and Shadlen, 2009</xref>) and a value-based economic choice task (<xref ref-type="bibr" rid="bib57">Padoa-Schioppa and Assad, 2006</xref>). Interestingly, unlike for the other tasks where we focus on comparing the activity of units in the decision network to neural recordings in the dorsolateral prefrontal and posterior parietal cortex of animals performing the same tasks, for the economic choice task we show that the activity of the value network exhibits a striking resemblance to neural recordings from the orbitofrontal cortex (OFC), which has long been implicated in the representation of reward-related signals (<xref ref-type="bibr" rid="bib93">Wallis, 2007</xref>).</p><p>An interesting feature of our REINFORCE-based model is that a reward baseline—in this case, the output of a recurrently connected value network (<xref ref-type="bibr" rid="bib97">Wierstra et al., 2009</xref>)—is essential for learning, but not for executing the task, because the latter depends only on the decision network. Importantly, learning can sometimes still occur without the value network but is much more unreliable. It is sometimes observed in experiments that reward-modulated structures in the brain such as the basal ganglia or OFC are necessary for learning or adapting to a changing environment, but not for executing a previously learned skill (<xref ref-type="bibr" rid="bib91">Turner and Desmurget, 2010</xref>; <xref ref-type="bibr" rid="bib72">Schoenbaum et al., 2011</xref>; <xref ref-type="bibr" rid="bib79">Stalnaker et al., 2015</xref>). This suggests that one possible role for such circuits may be representing an accurate baseline to guide learning. Moreover, since confidence is closely related to expected reward in many cognitive tasks, the explicit computation of expected reward by the value network provides a concrete, learning-based rationale for confidence estimation as a ubiquitous component of decision-making (<xref ref-type="bibr" rid="bib37">Kepecs et al., 2008</xref>; <xref ref-type="bibr" rid="bib96">Wei and Wang, 2015</xref>), even when it is not strictly required for performing the task.</p><p>Conceptually, the formulation of behavioral tasks in the language of reinforcement learning presented here is closely related to the solution of partially observable Markov decision processes (POMDPs) (<xref ref-type="bibr" rid="bib36">Kaelbling et al., 1998</xref>) using either model-based belief states (<xref ref-type="bibr" rid="bib63">Rao, 2010</xref>) or model-free working memory (<xref ref-type="bibr" rid="bib90">Todd et al., 2008</xref>). Indeed, as in <xref ref-type="bibr" rid="bib15">Dayan and Daw, (2008)</xref> one of the goals of this work is to unify related computations into a common language that is applicable to a wide range of tasks in systems neuroscience. Such policies can also be compared more directly to behaviorally 'optimal' solutions when they are known, for instance to the signal detection theory account of perceptual decision-making (<xref ref-type="bibr" rid="bib25">Gold and Shadlen, 2007</xref>). Thus, in addition to expanding the range of tasks and neural mechanisms that can be studied with trained RNNs, our work provides a convenient framework for the study of cognitive and value-based computations in the brain, which have often been viewed from distinct perspectives but in fact arise from the same reinforcement learning paradigm.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Policy gradient reinforcement learning for behavioral tasks</title><p>For concreteness, we illustrate the following in the context of a simplified perceptual decision-making task based on the random dots motion (RDM) discrimination task as described in <xref ref-type="bibr" rid="bib38">Kiani et al. (2008)</xref> (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In its simplest form, in an RDM task the monkey must maintain fixation until a 'go' cue instructs the monkey to indicate its decision regarding the direction of coherently moving dots on the screen. Thus the three possible actions available to the monkey at any given time are fixate, choose left, or choose right. The true direction of motion, which can be considered a <italic>state</italic> of the environment, is not known to the monkey with certainty, i.e., is <italic>partially observable</italic>. The monkey must therefore use the noisy sensory evidence to infer the direction in order to select the correct response at the end of the trial. Breaking fixation early results in a negative reward in the form of a timeout, while giving the correct response after the fixation cue is extinguished results in a positive reward in the form of juice. Typically, there is neither a timeout nor juice for an incorrect response during the decision period, corresponding to a 'neutral' reward of zero. The goal of this section is to give a general description of such tasks and how an RNN can learn a behavioral <italic>policy</italic> for choosing actions at each time to maximize its cumulative reward.<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.21492.003</object-id><label>Figure 1.</label><caption><title>Recurrent neural networks for reinforcement learning.</title><p>(<bold>A</bold>) Task structure for a simple perceptual decision-making task with variable stimulus duration. The agent must maintain fixation (<inline-formula><mml:math id="inf1"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>F</mml:mtext></mml:mrow></mml:math></inline-formula>) until the go cue, which indicates the start of a decision period during which choosing the correct response (<inline-formula><mml:math id="inf2"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>R</mml:mtext></mml:mrow></mml:math></inline-formula>) results in a positive reward. The agent receives zero reward for responding incorrectly, while breaking fixation early results in an aborted trial and negative reward. (<bold>B</bold>) At each time <inline-formula><mml:math id="inf4"><mml:mi>t</mml:mi></mml:math></inline-formula> the agent selects action <inline-formula><mml:math id="inf5"><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> according to the output of the decision network <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula>, which implements a policy that can depend on all past and current inputs <inline-formula><mml:math id="inf7"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> provided by the environment. In response, the environment transitions to a new state and provides reward <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to the agent. The value network <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> uses the selected action and the activity of the decision network <inline-formula><mml:math id="inf10"><mml:msubsup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mi>t</mml:mi><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> to predict future rewards. All the weights shown are plastic, i.e., trained by gradient descent. (<bold>C</bold>) Performance of the network trained for the task in (<bold>A</bold>), showing the percent correct by stimulus duration, for different coherences (the difference in strength of evidence for L and R). (<bold>D</bold>) Neural activity of an example decision network unit, sorted by coherence and aligned to the time of stimulus onset. Solid lines are for positive coherence, dashed for negative coherence. (<bold>E</bold>) Output of the value network (expected return) aligned to stimulus onset. Expected return is computed by performing an 'absolute value'-like operation on the accumulated evidence.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.003">http://dx.doi.org/10.7554/eLife.21492.003</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.004</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Learning curves for the simple perceptual decision-making task.</title><p>(<bold>A</bold>) Average reward per trial. Black indicates the network realization shown in the main text, gray additional realizations, i.e., trained with different random number generator seeds. (<bold>B</bold>) Percent correct, for trials on which the network made a decision (<inline-formula><mml:math id="inf11"><mml:mo>≥</mml:mo></mml:math></inline-formula>99% required for termination). Red: target performance (when training was terminated).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.004">http://dx.doi.org/10.7554/eLife.21492.004</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig1-figsupp1-v2"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.005</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Reaction-time version of the perceptual decision-making task, in which the go cue coincides with the onset of stimulus, allowing the agent to choose when to respond.</title><p>(<bold>A</bold>) Task structure for the reaction-time version of the simple perceptual decision-making task, in which the agent can choose to respond any time after the onset of stimulus. (<bold>B</bold>) Reaction time as a function of coherence for correct (solid circles) and error (open circles) trials. (<bold>C</bold>) Neural activity of an example decision network unit, sorted by the coherence (the difference in strength of evidence for L and R) and aligned to the time of stimulus onset. Each trial ends when the network breaks fixation.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.005">http://dx.doi.org/10.7554/eLife.21492.005</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig1-figsupp2-v2"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.006</object-id><label>Figure 1—figure supplement 3.</label><caption><title>Learning curves for the reaction-time version of the simple perceptual decision-making task.</title><p>(<bold>A</bold>) Average reward per trial. Black indicates the network realization shown in the main text, gray additional realizations, i.e., trained with different random number generator seeds. (<bold>B</bold>) Percent correct, for trials on which the network made a decision (<inline-formula><mml:math id="inf12"><mml:mo>≥</mml:mo></mml:math></inline-formula>99% required for termination). Red: target performance (when training was terminated).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.006">http://dx.doi.org/10.7554/eLife.21492.006</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig1-figsupp3-v2"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.007</object-id><label>Figure 1—figure supplement 4.</label><caption><title>Learning curves for the simple perceptual decision-making task with a linear readout of the decision network as the baseline.</title><p>(<bold>A</bold>) Average reward per trial. Black indicates the network realization shown in the main text, gray additional realizations, i.e., trained with different random number generator seeds. (<bold>B</bold>) Percent correct, for trials on which the network made a decision (<inline-formula><mml:math id="inf13"><mml:mo>≥</mml:mo></mml:math></inline-formula>99% required for termination). Red: target performance (when training was terminated).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.007">http://dx.doi.org/10.7554/eLife.21492.007</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig1-figsupp4-v2"/></fig></fig-group></p><p>Consider a typical interaction between an experimentalist and animal, which we more generally call the environment <inline-formula><mml:math id="inf14"><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi></mml:math></inline-formula> and agent <inline-formula><mml:math id="inf15"><mml:mi class="ltx_font_mathcaligraphic">𝒜</mml:mi></mml:math></inline-formula>, respectively (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). At each time <inline-formula><mml:math id="inf16"><mml:mi>t</mml:mi></mml:math></inline-formula> the agent chooses to perform actions <inline-formula><mml:math id="inf17"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> after observing inputs <inline-formula><mml:math id="inf18"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> provided by the environment, and the probability of choosing actions <inline-formula><mml:math id="inf19"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is given by the agent’s policy <inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with parameters <inline-formula><mml:math id="inf21"><mml:mi>θ</mml:mi></mml:math></inline-formula>. Here the policy is implemented as the output of an RNN, so that <inline-formula><mml:math id="inf22"><mml:mi>θ</mml:mi></mml:math></inline-formula> comprises the connection weights, biases, and initial state of the decision network. The policy at time <inline-formula><mml:math id="inf23"><mml:mi>t</mml:mi></mml:math></inline-formula> can depend on all past and current inputs <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM4"><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM5"><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi id="XM6" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub id="XM7"><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, allowing the agent to integrate sensory evidence or use working memory to perform the task. The exception is at <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, when the agent has yet to interact with the environment and selects its actions 'spontaneously' according to <inline-formula><mml:math id="inf26"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM8"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We note that, if the inputs give exact information about the environmental state <inline-formula><mml:math id="inf27"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, i.e., if <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then the environment can be described by a Markov decision process. In general, however, the inputs only provide partial information about the environmental states, requiring the network to accumulate evidence over time to determine the state of the world. In this work we only consider cases where the agent chooses one out of <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> possible actions at each time, so that <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each <inline-formula><mml:math id="inf31"><mml:mi>t</mml:mi></mml:math></inline-formula> is a discrete, normalized probability distribution over the possible actions <inline-formula><mml:math id="inf32"><mml:mrow><mml:msub id="XM9"><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi id="XM10" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub id="XM11"><mml:mi>a</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>. More generally, <inline-formula><mml:math id="inf33"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> can implement several distinct actions or even continuous actions by representing, for example, the means of Gaussian distributions (<xref ref-type="bibr" rid="bib60">Peters and Schaal, 2008</xref>; <xref ref-type="bibr" rid="bib97">Wierstra et al., 2009</xref>). After each set of actions by the agent at time <inline-formula><mml:math id="inf34"><mml:mi>t</mml:mi></mml:math></inline-formula> the environment provides a reward (or special observable) <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> at time <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, which the agent attempts to maximize in the sense described below.</p><p>In the case of the example RDM task above (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), the environment provides (and the agent receives) as inputs a fixation cue and noisy evidence for two choices L(eft) and R(ight) during a variable-length stimulus presentation period. The strength of evidence, or the difference between the evidence for L and R, is called the coherence, and in the actual RDM experiment corresponds to the percentage of dots moving coherently in one direction on the screen. The agent chooses to perform one of <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> actions at each time: fixate (<inline-formula><mml:math id="inf38"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>F</mml:mtext></mml:mrow></mml:math></inline-formula>), choose L (<inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>L</mml:mtext></mml:mrow></mml:math></inline-formula>), or choose R (<inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>R</mml:mtext></mml:mrow></mml:math></inline-formula>). Here, the agent must choose F as long as the fixation cue is on, and then, when the fixation cue is turned off to indicate that the agent should make a decision, correctly choose L or R depending on the sensory evidence. Indeed, for all tasks in this work we required that the network 'make a decision' (i.e., break fixation to indicate a choice at the appropriate time) on at least 99% of the trials, whether the response was correct or not. A trial ends when the agent chooses L or R regardless of the task epoch: breaking fixation early before the go cue results in an aborted trial and a negative reward <inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, while a correct decision is rewarded with <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Making the wrong decision results in no reward, <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. For the zero-coherence condition the agent is rewarded randomly on half the trials regardless of its choice. Otherwise the reward is always <inline-formula><mml:math id="inf44"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Formally, a trial proceeds as follows. At time <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the environment is in state <inline-formula><mml:math id="inf46"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM12"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The state <inline-formula><mml:math id="inf48"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> can be considered the starting time (i.e., <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) and 'task condition,' which in the RDM example consists of the direction of motion of the dots (i.e., whether the correct response is L or R) and the coherence of the dots (the difference between evidence for L and R). The time component of the state, which is updated at each step, allows the environment to present different inputs to the agent depending on the task epoch. The true state <inline-formula><mml:math id="inf50"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (such as the direction of the dots) is only partially observable to the agent, so that the agent must instead infer the state through inputs <inline-formula><mml:math id="inf51"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> provided by the environment during the course of the trial. As noted previously, the agent initially chooses actions <inline-formula><mml:math id="inf52"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM13"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The networks in this work almost always begin by choosing F, or fixation.</p><p>At time <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the environment, depending on its previous state <inline-formula><mml:math id="inf55"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and the agent’s action <inline-formula><mml:math id="inf56"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, transitions to state <inline-formula><mml:math id="inf57"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and generates reward <inline-formula><mml:math id="inf59"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>. In the perceptual decision-making example, only the time advances since the trial condition remains constant throughout. From this state the environment generates observable <inline-formula><mml:math id="inf60"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> with a distribution given by <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> were in the stimulus presentation period, for example, <inline-formula><mml:math id="inf63"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> would provide noisy evidence for L or R, as well as the fixation cue. In response, the agent, depending on the inputs <inline-formula><mml:math id="inf64"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> it receives from the environment, chooses actions <inline-formula><mml:math id="inf65"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The environment, depending on its previous states <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM20"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM21"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the agent’s previous actions <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM22"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM23"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, then transitions to state <inline-formula><mml:math id="inf69"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and generates reward <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. These steps are repeated until the end of the trial at time <inline-formula><mml:math id="inf72"><mml:mi>T</mml:mi></mml:math></inline-formula>. Trials can terminate at different times (e.g., for breaking fixation early or because of variable stimulus durations), so that <inline-formula><mml:math id="inf73"><mml:mi>T</mml:mi></mml:math></inline-formula> in the following represents the maximum length of a trial. In order to emphasize that rewards follow actions, we adopt the convention in which the agent performs actions at <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM30">0</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM31" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM32">T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and receives rewards at <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM33">1</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM34" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow id="XM35"><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The goal of the agent is to maximize the sum of expected future rewards at time <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, or expected return<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM36">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>𝔼</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="260%" minsize="260%">[</mml:mo><mml:mrow id="XM37"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo maxsize="260%" minsize="260%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the expectation <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>𝔼</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:math></inline-formula> is taken over all possible trial histories <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM38"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo rspace="7.5pt">,</mml:mo><mml:msub id="XM39"><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo rspace="7.5pt">,</mml:mo><mml:msub id="XM40"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> consisting of the states of the environment, the inputs given to the agent, and the actions of the agent. In practice, the expectation value in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is estimated by performing <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>N</mml:mi><mml:mtext>trials</mml:mtext></mml:msub></mml:math></inline-formula> trials for each policy update, i.e., with a Monte Carlo approximation. The expected return depends on the policy and hence parameters <inline-formula><mml:math id="inf80"><mml:mi>θ</mml:mi></mml:math></inline-formula>, and we use Adam stochastic gradient descent (SGD) (<xref ref-type="bibr" rid="bib40">Kingma and Ba, 2015</xref>) with gradient clipping (<xref ref-type="bibr" rid="bib26">Graves, 2013</xref>; <xref ref-type="bibr" rid="bib59">Pascanu et al., 2013b</xref>) to find the parameters that maximize this reward (Materials and methods).</p><p>More specifically, after every <inline-formula><mml:math id="inf81"><mml:msub><mml:mi>N</mml:mi><mml:mtext>trials</mml:mtext></mml:msub></mml:math></inline-formula> trials the decision network uses gradient descent to update its parameters in a direction that minimizes an objective function <inline-formula><mml:math id="inf82"><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mi>π</mml:mi></mml:msup></mml:math></inline-formula> of the form<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>trials</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>trials</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>with respect to the connection weights, biases, and initial state of the decision network, which we collectively denote as <inline-formula><mml:math id="inf83"><mml:mi>θ</mml:mi></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf84"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>n</mml:mi><mml:mi>π</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM45">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can contain any regularization terms for the decision network, for instance an entropy term to control the degree of exploration (<xref ref-type="bibr" rid="bib100">Xu et al., 2015</xref>). The key gradient <inline-formula><mml:math id="inf85"><mml:mrow><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM46">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is given for each trial <inline-formula><mml:math id="inf86"><mml:mi>n</mml:mi></mml:math></inline-formula> by the REINFORCE algorithm (<xref ref-type="bibr" rid="bib98">Williams, 1992</xref>; <xref ref-type="bibr" rid="bib1">Baird and Moore, 1999</xref>; <xref ref-type="bibr" rid="bib86">Sutton et al., 2000</xref>; <xref ref-type="bibr" rid="bib4">Baxter and Bartlett, 2001</xref>; <xref ref-type="bibr" rid="bib60">Peters and Schaal, 2008</xref>; <xref ref-type="bibr" rid="bib97">Wierstra et al., 2009</xref>) as<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf87"><mml:msubsup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> are the firing rates of the decision network units up to time <inline-formula><mml:math id="inf88"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> denotes the value function as described below, and the gradient <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, known as the <italic>eligibility</italic>, [and likewise <inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>n</mml:mi><mml:mi>π</mml:mi></mml:msubsup></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM48">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>] is computed by backpropagation through time (BPTT) (<xref ref-type="bibr" rid="bib70">Rumelhart et al., 1986</xref>) for the selected actions <inline-formula><mml:math id="inf92"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>. The sum over rewards in large brackets only runs over <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi id="XM49">t</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM50" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM51">T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which reflects the fact that present actions do not affect past rewards. In this form the terms in the gradient have the intuitive property that they are nonzero only if the actual return deviates from what was predicted by the baseline. It is worth noting that this form of the value function (with access to the selected action) can, in principle, lead to suboptimal policies if the value network’s predictions become perfect before the optimal decision policy is learned; we did not find this to be the case in our simulations.</p><p>The reward baseline is an important feature in the success of almost all REINFORCE-based algorithms, and is here represented by a second RNN <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> with parameters <inline-formula><mml:math id="inf95"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> in addition to the decision network <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula> (to be precise, the value function is the readout of the value network). This baseline network, which we call the <italic>value network</italic>, uses the selected actions <inline-formula><mml:math id="inf97"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and activity of the decision network <inline-formula><mml:math id="inf98"><mml:msubsup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> to predict the expected return at each time <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM52">1</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM53" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM54">T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; the value network also predicts the expected return at <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> based on its own initial states, with the understanding that <inline-formula><mml:math id="inf101"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∅</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf102"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>π</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∅</mml:mi></mml:mrow></mml:math></inline-formula> are empty sets. The value network is trained by minimizing a second objective function<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-4_8"><mml:mtext>(4)</mml:mtext></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>trials</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>trials</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-5_8"><mml:mtext>(5)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>every <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>N</mml:mi><mml:mtext>trials</mml:mtext></mml:msub></mml:math></inline-formula> trials, where <inline-formula><mml:math id="inf104"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM71">ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes any regularization terms for the value network. The necessary gradient <inline-formula><mml:math id="inf105"><mml:mrow><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM72">ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> [and likewise <inline-formula><mml:math id="inf106"><mml:mrow><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:msubsup></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM73">ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>] is again computed by BPTT.</p></sec><sec id="s2-2"><title>Decision and value recurrent neural networks</title><p>The policy probability distribution over actions <inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and scalar baseline <inline-formula><mml:math id="inf108"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM74"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup id="XM75"><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>π</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are each represented by an RNN of <inline-formula><mml:math id="inf109"><mml:mi>N</mml:mi></mml:math></inline-formula> firing-rate units <inline-formula><mml:math id="inf110"><mml:msup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mi>π</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf111"><mml:msup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:math></inline-formula>, respectively, where we interpret each unit as the mean firing rate of a group of neurons. In the case where the agent chooses a single action at each time <inline-formula><mml:math id="inf112"><mml:mi>t</mml:mi></mml:math></inline-formula>, the activity of the decision network determines <inline-formula><mml:math id="inf113"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> through a linear readout followed by softmax normalization:<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-6_5"><mml:mtext>(6)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-7_5"><mml:mtext>(7)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>for <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM80">1</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM81" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub id="XM82"><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf115"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>out</mml:mtext><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> is an <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix of connection weights from the units of the decision network to the <inline-formula><mml:math id="inf117"><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> linear readouts <inline-formula><mml:math id="inf118"><mml:msub><mml:mi mathvariant="bold">𝐳</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf119"><mml:msubsup><mml:mi mathvariant="bold">𝐛</mml:mi><mml:mtext>out</mml:mtext><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> are <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula> biases. Action selection is implemented by randomly sampling from the probability distribution in <xref ref-type="disp-formula" rid="equ5">Equation 7</xref>, and constitutes an important difference from previous approaches to training RNNs for cognitive tasks (<xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib11">Carnevale et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Miconi, 2016</xref>), namely, here the final output of the network (during training) is a specific action, not a graded decision variable. We consider this sampling as an abstract representation of the downstream action selection mechanisms present in the brain, including the role of noise in implicitly realizing stochastic choices with deterministic outputs (<xref ref-type="bibr" rid="bib94">Wang, 2002</xref>, <xref ref-type="bibr" rid="bib95">2008</xref>). Meanwhile, the activity of the value network predicts future returns through a linear readout<disp-formula id="equ6"><label>(8)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM83"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup id="XM84"><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>π</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>out</mml:mtext><mml:mi>v</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mtext>out</mml:mtext><mml:mi>v</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf121"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>out</mml:mtext><mml:mi>v</mml:mi></mml:msubsup></mml:math></inline-formula> is an <inline-formula><mml:math id="inf122"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrix of connection weights from the units of the value network to the single linear readout <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf124"><mml:msubsup><mml:mi>b</mml:mi><mml:mtext>out</mml:mtext><mml:mi>v</mml:mi></mml:msubsup></mml:math></inline-formula> is a bias term.</p><p>In order to take advantage of recent developments in training RNNs [in particular, addressing the problem of vanishing gradients (<xref ref-type="bibr" rid="bib6">Bengio et al., 1994</xref>)] while retaining intepretability, we use a modified form of Gated Recurrent Units (GRUs) (<xref ref-type="bibr" rid="bib12">Cho et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Chung et al., 2014</xref>) with a threshold-linear '<inline-formula><mml:math id="inf125"><mml:mi>f</mml:mi></mml:math></inline-formula>-<inline-formula><mml:math id="inf126"><mml:mi>I</mml:mi></mml:math></inline-formula>' curve <inline-formula><mml:math id="inf127"><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi id="XM85">x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo id="XM86">max</mml:mo><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn id="XM87">0</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM88">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to obtain positive, non-saturating firing rates. Since firing rates in cortex rarely operate in the saturating regime, previous work (<xref ref-type="bibr" rid="bib84">Sussillo et al., 2015</xref>) used an additional regularization term to prevent saturation in common nonlinearities such as the hyperbolic tangent; the threshold-linear activation function obviates such a need. These units are thus leaky, threshold-linear units with dynamic time constants and gated recurrent inputs. The equations that describe their dynamics can be derived by a naïve discretization of the following continuous-time equations for the <inline-formula><mml:math id="inf128"><mml:mi>N</mml:mi></mml:math></inline-formula> currents <inline-formula><mml:math id="inf129"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> and corresponding rectified-linear firing rates <inline-formula><mml:math id="inf130"><mml:mi mathvariant="bold">𝐫</mml:mi></mml:math></inline-formula>:<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-9_151"><mml:mtext>(9)</mml:mtext></mml:mtd><mml:mtd><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-10_140"><mml:mtext>(10)</mml:mtext></mml:mtd><mml:mtd><mml:mi mathvariant="bold-italic">γ</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-11_53"><mml:mtext>(11)</mml:mtext></mml:mtd><mml:mtd><mml:mfrac><mml:mi>τ</mml:mi><mml:mi mathvariant="bold-italic">λ</mml:mi></mml:mfrac><mml:mo>⊙</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">x</mml:mi><mml:mo mathvariant="bold">.</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mo>⊙</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mi>τ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt><mml:mi mathvariant="bold-italic">ξ</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-12_40"><mml:mtext>(12)</mml:mtext></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the derivative of <inline-formula><mml:math id="inf132"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> with respect to time, <inline-formula><mml:math id="inf133"><mml:mo>⊙</mml:mo></mml:math></inline-formula> denotes elementwise multiplication, <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the logistic sigmoid, <inline-formula><mml:math id="inf135"><mml:msup><mml:mi mathvariant="bold">𝐛</mml:mi><mml:mi>λ</mml:mi></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf136"><mml:msup><mml:mi mathvariant="bold">𝐛</mml:mi><mml:mi>γ</mml:mi></mml:msup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf137"><mml:mi mathvariant="bold">𝐛</mml:mi></mml:math></inline-formula> are biases, <inline-formula><mml:math id="inf138"><mml:mi mathvariant="bold-italic">𝝃</mml:mi></mml:math></inline-formula> are <inline-formula><mml:math id="inf139"><mml:mi>N</mml:mi></mml:math></inline-formula> independent Gaussian white noise processes with zero mean and unit variance, and <inline-formula><mml:math id="inf140"><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>rec</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> controls the size of this noise. The multiplicative gates <inline-formula><mml:math id="inf141"><mml:mi mathvariant="bold-italic">𝝀</mml:mi></mml:math></inline-formula> dynamically modulate the overall time constant <inline-formula><mml:math id="inf142"><mml:mi>τ</mml:mi></mml:math></inline-formula> for network units, while the <inline-formula><mml:math id="inf143"><mml:mi mathvariant="bold-italic">𝜸</mml:mi></mml:math></inline-formula> control the recurrent inputs. The <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> matrices <inline-formula><mml:math id="inf145"><mml:msub><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf146"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext><mml:mi>λ</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf147"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext><mml:mi>γ</mml:mi></mml:msubsup></mml:math></inline-formula> are the recurrent weight matrices, while the <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> matrices <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf150"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>λ</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf151"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>γ</mml:mi></mml:msubsup></mml:math></inline-formula> are connection weights from the <inline-formula><mml:math id="inf152"><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:math></inline-formula> inputs <inline-formula><mml:math id="inf153"><mml:mi mathvariant="bold">𝐮</mml:mi></mml:math></inline-formula> to the <inline-formula><mml:math id="inf154"><mml:mi>N</mml:mi></mml:math></inline-formula> units of the network. We note that in the case where <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi mathvariant="bold-italic">𝝀</mml:mi><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi mathvariant="bold-italic">𝜸</mml:mi><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> the equations reduce to 'simple' leaky threshold-linear units without the modulation of the time constants or gating of inputs. We constrain the recurrent connection weights (<xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>) so that the overall connection probability is <inline-formula><mml:math id="inf157"><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>; specifically, the number of incoming connections for each unit, or in-degree <inline-formula><mml:math id="inf158"><mml:mi>K</mml:mi></mml:math></inline-formula>, was set to <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="table" rid="tbl1">Table 1</xref> for a list of all parameters).<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.21492.008</object-id><label>Table 1.</label><caption><p>Parameters for reward-based recurrent neural network training. Unless noted otherwise in the text, networks were trained and run with the parameters listed here.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.008">http://dx.doi.org/10.7554/eLife.21492.008</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>Symbol</th><th>Default value</th></tr></thead><tbody><tr><td>Learning rate</td><td><inline-formula><mml:math id="inf160"><mml:mi>η</mml:mi></mml:math></inline-formula></td><td>0.004</td></tr><tr><td>Maximum gradient norm</td><td><inline-formula><mml:math id="inf161"><mml:mi mathvariant="normal">Γ</mml:mi></mml:math></inline-formula></td><td>1</td></tr><tr><td>Size of decision/value network</td><td><inline-formula><mml:math id="inf162"><mml:mi>N</mml:mi></mml:math></inline-formula></td><td>100</td></tr><tr><td>Connection probability (decision network)</td><td><inline-formula><mml:math id="inf163"><mml:msubsup><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula></td><td>0.1</td></tr><tr><td>Connection probability (value network)</td><td><inline-formula><mml:math id="inf164"><mml:msubsup><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:msubsup></mml:math></inline-formula></td><td>1</td></tr><tr><td>Time step</td><td><inline-formula><mml:math id="inf165"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula></td><td>10 ms</td></tr><tr><td>Unit time constant</td><td><inline-formula><mml:math id="inf166"><mml:mi>τ</mml:mi></mml:math></inline-formula></td><td>100 ms</td></tr><tr><td>Recurrent noise</td><td><inline-formula><mml:math id="inf167"><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>rec</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula></td><td>0.01</td></tr><tr><td>Initial spectral radius for recurrent weights</td><td><inline-formula><mml:math id="inf168"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula></td><td>2</td></tr><tr><td>Number of trials per gradient update</td><td><inline-formula><mml:math id="inf169"><mml:msub><mml:mi>N</mml:mi><mml:mtext>trials</mml:mtext></mml:msub></mml:math></inline-formula></td><td># of task conditions</td></tr></tbody></table></table-wrap></p><p>The result of discretizing <xref ref-type="disp-formula" rid="equ7">Equations 9–12</xref>, as well as details on initializing the network parameters, are given in Materials and methods. We successfully trained networks with time steps <inline-formula><mml:math id="inf170"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mtext>1 ms</mml:mtext></mml:mrow></mml:math></inline-formula>, but for computational convenience all of the networks in this work were trained and run with <inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mtext>10 ms</mml:mtext></mml:mrow></mml:math></inline-formula>. We note that, for typical tasks in systems neuroscience lasting on the order of several seconds, this already implies trials lasting hundreds of time steps. Unless noted otherwise in the text, all networks were trained using the parameters listed in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p><p>While the inputs to the decision network <inline-formula><mml:math id="inf172"><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula> are determined by the environment, the value network always receives as inputs the activity of the decision network <inline-formula><mml:math id="inf173"><mml:msup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mi>π</mml:mi></mml:msup></mml:math></inline-formula>, together with information about which actions were actually selected at each time step (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The value network serves two purposes: first, the output of the value network is used as the baseline in the REINFORCE gradient, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, to reduce the variance of the gradient estimate (<xref ref-type="bibr" rid="bib98">Williams, 1992</xref>; <xref ref-type="bibr" rid="bib1">Baird and Moore, 1999</xref>; <xref ref-type="bibr" rid="bib4">Baxter and Bartlett, 2001</xref>; <xref ref-type="bibr" rid="bib60">Peters and Schaal, 2008</xref>); second, since policy gradient reinforcement learning does not explicitly use a value function but value information is nevertheless implicitly contained in the policy, the value network serves as an explicit and potentially nonlinear readout of this information. In situations where expected reward is closely related to confidence, this may explain, for example, certain disassociations between perceptual decisions and reports of the associated confidence (<xref ref-type="bibr" rid="bib42">Lak et al., 2014</xref>).</p><p>A reward baseline, which allows the decision network to update its parameters based on a relative quantity akin to prediction error (<xref ref-type="bibr" rid="bib73">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib5">Bayer and Glimcher, 2005</xref>) rather than absolute reward magnitude, is essential to many learning schemes, especially those based on REINFORCE. Indeed, it has been suggested that in general such a baseline should be not only task-specific but stimulus (task-condition)-specific (<xref ref-type="bibr" rid="bib23">Frémaux et al., 2010</xref>; <xref ref-type="bibr" rid="bib18">Engel et al., 2015</xref>; <xref ref-type="bibr" rid="bib52">Miconi, 2016</xref>), and that this information may be represented in OFC (<xref ref-type="bibr" rid="bib93">Wallis, 2007</xref>) or basal ganglia (<xref ref-type="bibr" rid="bib16">Doya, 2000</xref>). Previous schemes, however, did not propose how this baseline <italic>critic</italic> may be instantiated, instead implementing it algorithmically. Here we use a simple neural implementation of the baseline that automatically depends on the stimulus and thus does not require the learning system to have access to the true trial type, which in general is not known with certainty to the agent.</p></sec><sec id="s2-3"><title>Tasks with simple input-output mappings</title><p>The training procedure described in the previous section can be used for a variety of tasks, and results in networks that qualitatively reproduce both behavioral and electrophysiological findings from experiments with behaving animals. For the example perceptual decision-making task above, the trained network learns to integrate the sensory evidence to make the correct decision about which of two noisy inputs is larger (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). This and additional networks trained for the same task were able to reach the target performance in <inline-formula><mml:math id="inf174"><mml:mo>∼</mml:mo></mml:math></inline-formula>7000 trials starting from completely random connection weights, and moreover the networks learned the 'core' task after <inline-formula><mml:math id="inf175"><mml:mo>∼</mml:mo></mml:math></inline-formula>2000 trials (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). As with monkeys performing the task, longer stimulus durations allow the network to improve its performance by continuing to integrate the incoming sensory evidence (<xref ref-type="bibr" rid="bib94">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib38">Kiani et al., 2008</xref>). Indeed, the output of the value network shows that the expected reward (in this case equivalent to confidence) is modulated by stimulus difficulty (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Prior to the onset of the stimulus, the expected reward is the same for all trial conditions and approximates the overall reward rate; incoming sensory evidence then allows the network to distinguish its chances of success.</p><p>Sorting the activity of individual units in the network by the signed coherence (the strength of the evidence, with negative values indicating evidence for L and positive for R) also reveals coherence-dependent ramping activity (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) as observed in neural recordings from numerous perceptual decision-making experiments, e.g., <xref ref-type="bibr" rid="bib68">Roitman and Shadlen (2002)</xref>. This pattern of activity illustrates why a nonlinear readout by the value network is useful: expected return is computed by performing an 'absolute value'-like operation on the accumulated evidence (plus shifts), as illustrated by the overlap of the expected return for positive and negative-coherence trials (<xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p><p>The reaction time as a function of coherence in the reaction-time version of the same task, in which the go cue coincides with the time of stimulus onset, is also shown in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> and may be compared, e.g., to <xref ref-type="bibr" rid="bib94">Wang (2002)</xref>; <xref ref-type="bibr" rid="bib51">Mazurek et al. (2003)</xref>; <xref ref-type="bibr" rid="bib99">Wong and Wang (2006)</xref>. We note that in many neural models [e.g., <xref ref-type="bibr" rid="bib94">Wang (2002)</xref>; <xref ref-type="bibr" rid="bib99">Wong and Wang (2006)</xref>] a 'decision' is made when the output reaches a fixed threshold. Indeed, when networks are trained using supervised learning (<xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>), the decision threshold is imposed retroactively and has no meaning during training; since the outputs are continuous, the speed-accuracy tradeoff is also learned in the space of continuous error signals. Here, the time at which the network commits to a decision is unambiguously given by the time at which the selected action is L or R. Thus the appropriate speed-accuracy tradeoff is learned in the space of concrete actions, illustrating the desirability of using reward-based training of RNNs when modeling reaction-time tasks. Learning curves for this and additional networks trained for the same reaction-time task are shown in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>.</p><p>In addition to the example task from the previous section, we trained networks for three well-known behavioral paradigms in which the correct, or optimal, behavior is (pre-)determined on each trial by the task condition alone. Similar tasks have previously been addressed with several different forms of supervised learning, including FORCE (<xref ref-type="bibr" rid="bib81">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib11">Carnevale et al., 2015</xref>), Hessian-free (<xref ref-type="bibr" rid="bib49">Martens and Sutskever, 2011</xref>; <xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>), and stochastic gradient descent (<xref ref-type="bibr" rid="bib59">Pascanu et al., 2013b</xref>; <xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>), so that the results shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> are presented as confirmation that the same tasks can also be learned using reward feedback on definite actions alone. For all three tasks the pre-stimulus fixation period was 750 ms; the networks had to maintain fixation until the start of a 500 ms 'decision' period, which was indicated by the extinction of the fixation cue. At this time the network was required to choose one of two alternatives to indicate its decision and receive a reward of +1 for a correct response and 0 for an incorrect response; otherwise, the networks received a reward of −1.<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.21492.009</object-id><label>Figure 2.</label><caption><title>Performance and neural activity of RNNs trained for 'simple' cognitive tasks in which the correct response depends only on the task condition.</title><p>Left column shows behavioral performance, right column shows mixed selectivity for task parameters of example units in the decision network. (<bold>A</bold>) Context-dependent integration task (<xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>). Left: Psychometric curves show the percentage of R choices as a function of the signed 'motion' and 'color' coherences in the motion (black) and color (blue) contexts. Right: Normalized firing rates of examples units sorted by different combinations of task parameters exhibit mixed selectivity. Firing rates were normalized by mean and standard deviation computed over the responses of all units, times, and trials. Solid and dashed lines indicate choice 1 (same as preferred direction of unit) and choice 2 (non-preferred), respectively. For motion and choice and color and choice, dark to light corresponds to high to low motion and color coherence, respectively. (<bold>B</bold>) Multisensory integration task (<xref ref-type="bibr" rid="bib64">Raposo et al., 2012</xref>, <xref ref-type="bibr" rid="bib65">2014</xref>). Left: Psychometric curves show the percentage of high choices as a function of the event rate, for visual only (blue), auditory only (green), and multisensory (orange) trials. Improved performance on multisensory trials shows that the network learns to combine the two sources of information in accordance with <xref ref-type="disp-formula" rid="equ8">Equation 13</xref>. Right: Sorted activity on visual only and auditory only trials for units selective for choice (high vs. low, left), modality [visual (vis) vs. auditory (aud), middle], and both (right). Error trials were excluded. (<bold>C</bold>) Parametric working memory task (<xref ref-type="bibr" rid="bib69">Romo et al., 1999</xref>). Left: Percentage of correct responses for different combinations of <inline-formula><mml:math id="inf176"><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf177"><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. The conditions are colored here and in the right panels according to the first stimulus (base frequency) <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>; due to the overlap in the values of <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, the 10 task conditions are represented by seven distinct colors. Right: Activity of example decision network units sorted by <inline-formula><mml:math id="inf180"><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>. The first two units are positively tuned to <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> during the delay period, while the third unit is negatively tuned.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.009">http://dx.doi.org/10.7554/eLife.21492.009</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig2-v2"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.010</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Learning curves for the context-dependent integration task.</title><p>(<bold>A</bold>) Average reward per trial. Black is for the network realization in the main text, gray for additional realizations, i.e., trained with different random number generator seeds. (<bold>B</bold>) Percent correct, for trials on which the network made a decision (<inline-formula><mml:math id="inf182"><mml:mo>≥</mml:mo></mml:math></inline-formula>99% required for termination). Red: target performance (when training was terminated).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.010">http://dx.doi.org/10.7554/eLife.21492.010</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig2-figsupp1-v2"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.011</object-id><label>Figure 2—figure supplement 2.</label><caption><title>Learning curves for the multisensory integration task.</title><p>(<bold>A</bold>) Average reward per trial. Black indicates the network realization shown in the main text, gray additional realizations, i.e., trained with different random number generator seeds. (<bold>B</bold>) Percent correct, for trials on which the network made a decision (<inline-formula><mml:math id="inf183"><mml:mo>≥</mml:mo></mml:math></inline-formula>99% required for termination). Red: target performance (when training was terminated).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.011">http://dx.doi.org/10.7554/eLife.21492.011</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig2-figsupp2-v2"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.012</object-id><label>Figure 2—figure supplement 3.</label><caption><title>Learning curves for the parametric working memory task.</title><p>(<bold>A</bold>) Average reward per trial. Black indicates the network realization shown in the main text, gray additional realizations, i.e., trained with different random number generator seeds. (<bold>B</bold>) Percent correct, for trials on which the network made a decision (<inline-formula><mml:math id="inf184"><mml:mo>≥</mml:mo></mml:math></inline-formula>99% required for termination). Red: target performance (when training was terminated).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.012">http://dx.doi.org/10.7554/eLife.21492.012</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig2-figsupp3-v2"/></fig></fig-group></p><p>The context-dependent integration task (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) is based on <xref ref-type="bibr" rid="bib47">Mante et al. (2013)</xref>, in which monkeys were required to integrate one type of stimulus (the motion or color of the presented dots) while ignoring the other depending on a context cue. In training the network, we included both the 750 ms stimulus period and 300–1500 ms delay period following stimulus presentation. The delay consisted of 300 ms followed by a variable duration drawn from an exponential distribution with mean 300 ms and truncated at a maximum of 1200 ms. The network successfully learned to perform the task, which is reflected in the psychometric functions showing the percentage of trials on which the network chose R as a function of the signed motion and color coherences, where motion and color indicate the two sources of noisy information and the sign is positive for R and negative for L (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, left). As in electrophysiological recordings, units in the decision network show mixed selectivity when sorted by different combinations of task variables (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right). Learning curves for this and additional networks trained for the task are shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p><p>The multisensory integration task (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) is based on <xref ref-type="bibr" rid="bib64">Raposo et al. (2012</xref>, <xref ref-type="bibr" rid="bib65">2014)</xref>, in which rats used visual flashes and auditory clicks to determine whether the event rate was higher or lower than a learned threshold of 12.5 events per second. When both modalities were presented, they were congruent, which implied that the rats could improve their performance by combining information from both sources. As in the experiment, the network was trained with a 1000 ms stimulus period, with inputs whose magnitudes were proportional (both positively and negatively) to the event rate. For this task the input connection weights <inline-formula><mml:math id="inf185"><mml:msub><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf186"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>λ</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf187"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>γ</mml:mi></mml:msubsup></mml:math></inline-formula> were initialized so that a third of the <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>150</mml:mn></mml:mrow></mml:math></inline-formula> decision network units received visual inputs only, another third auditory inputs only, and the remaining third received neither. As shown in the psychometric function (percentage of high choices as a function of event rate, <xref ref-type="fig" rid="fig2">Figure 2B</xref>, left), the trained network exhibits multisensory enhancement in which performance on multisensory trials was better than on single-modality trials. Indeed, as for rats, the results are consistent with optimal combination of the two modalities,<disp-formula id="equ8"><label>(13)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>visual</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>auditory</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>multisensory</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf189"><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>visual</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf190"><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>auditory</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf191"><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>multisensory</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> are the variances obtained from fits of the psychometric functions to cumulative Gaussian functions for visual only, auditory only, and multisensory (both visual and auditory) trials, respectively (<xref ref-type="table" rid="tbl2">Table 2</xref>). As observed in electrophysiological recordings, moreover, decision network units exhibit a range of tuning to task parameters, with some selective to choice and others to modality, while many units showed mixed selectivity to all task variables (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, right). Learning curves for this and additional networks trained for the task are shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.<table-wrap id="tbl2" position="float"><object-id pub-id-type="doi">10.7554/eLife.21492.013</object-id><label>Table 2.</label><caption><p>Psychophysical thresholds <inline-formula><mml:math id="inf192"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>visual</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf193"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>auditory</mml:mtext></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf194"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>multisensory</mml:mtext></mml:msub></mml:math></inline-formula> obtained from fits of cumulative Gaussian functions to the psychometric curves in visual only, auditory only, and multisensory trials in the multisensory integration task, for six networks trained from different random initializations (first row, bold: network from main text, cf. <xref ref-type="fig" rid="fig2">Figure 2B</xref>). The last two columns show evidence of 'optimal' multisensory integration according to <xref ref-type="disp-formula" rid="equ8">Equation 13</xref> (<xref ref-type="bibr" rid="bib64">Raposo et al., 2012</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.013">http://dx.doi.org/10.7554/eLife.21492.013</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th><inline-formula><mml:math id="inf195"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>visual</mml:mtext></mml:msub></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf196"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>auditory</mml:mtext></mml:msub></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf197"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>multisensory</mml:mtext></mml:msub></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf198"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>visual</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>auditory</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula><break/></th><th><inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>multisensory</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mstyle></mml:math></inline-formula><break/></th></tr></thead><tbody><tr><td>2.124</td><td>2.099</td><td>1.451</td><td>0.449</td><td>0.475</td></tr><tr><td>2.107</td><td>2.086</td><td>1.448</td><td>0.455</td><td>0.477</td></tr><tr><td>2.276</td><td>2.128</td><td>1.552</td><td>0.414</td><td>0.415</td></tr><tr><td>2.118</td><td>2.155</td><td>1.508</td><td>0.438</td><td>0.440</td></tr><tr><td>2.077</td><td>2.171</td><td>1.582</td><td>0.444</td><td>0.400</td></tr><tr><td>2.088</td><td>2.149</td><td>1.480</td><td>0.446</td><td>0.457</td></tr></tbody></table></table-wrap></p><p>The parametric working memory task (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) is based on the vibrotactile frequency discrimination task of <xref ref-type="bibr" rid="bib69">Romo et al. (1999)</xref>, in which monkeys were required to compare the frequencies of two temporally separated stimuli to determine which was higher. For network training, the task epochs consisted of a 500 ms base stimulus with 'frequency' <inline-formula><mml:math id="inf200"><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, a 2700–3300 ms delay, and a 500 ms comparison stimulus with frequency <inline-formula><mml:math id="inf201"><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>; for the trials shown in <xref ref-type="fig" rid="fig2">Figure 2C</xref> the delay was always 3000 ms as in the experiment. During the decision period, the network had to indicate which stimulus was higher by choosing <inline-formula><mml:math id="inf202"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf203"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The stimuli were constant inputs with amplitudes proportional (both positively and negatively) to the frequency. For this task we set the learning rate to <inline-formula><mml:math id="inf204"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:math></inline-formula>; the network successfully learned to perform the task (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, left), and the individual units of the network, when sorted by the first stimulus (base frequency) <inline-formula><mml:math id="inf205"><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, exhibit highly heterogeneous activity (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, right) characteristic of neurons recorded in the prefrontal cortex of monkeys performing the task (<xref ref-type="bibr" rid="bib45">Machens et al., 2010</xref>). Learning curves for this and additional networks trained for the task are shown in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>.</p><p>Additional comparisons can be made between the model networks shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> and the neural activity observed in behaving animals, for example state-space analyses as in <xref ref-type="bibr" rid="bib47">Mante et al. (2013)</xref>, <xref ref-type="bibr" rid="bib11">Carnevale et al. (2015)</xref>, or <xref ref-type="bibr" rid="bib78">Song et al. (2016)</xref>. Such comparisons reveal that, as found previously in studies such as <xref ref-type="bibr" rid="bib2">Barak et al. 2013)</xref>, the model networks exhibit many, but not all, features present in electrophysiological recordings. <xref ref-type="fig" rid="fig2">Figure 2</xref> and the following make clear, however, that RNNs trained with reward feedback alone can already reproduce the mixed selectivity characteristic of neural populations in higher cortical areas (<xref ref-type="bibr" rid="bib66">Rigotti et al., 2010</xref>, <xref ref-type="bibr" rid="bib67">2013</xref>), thereby providing a valuable platform for future investigations of how such complex representations are learned.</p></sec><sec id="s2-4"><title>Confidence and perceptual decision-making</title><p>All of the tasks in the previous section have the property that the correct response on any single trial is a function only of the task condition, and, in particular, does not depend on the network’s state during the trial. In a postdecision wager task (<xref ref-type="bibr" rid="bib39">Kiani and Shadlen, 2009</xref>), however, the optimal decision depends on the animal’s (agent’s) estimate of the probability that its decision is correct, i.e., its confidence. As can be seen from the results, on a trial-by-trial basis this is not the same as simply determining the stimulus difficulty (a combination of stimulus duration and coherence); this makes it difficult to train with standard supervised learning, which requires a pre-determined target output for the network to reproduce; instead, we trained an RNN to perform the task by maximizing overall reward. This task extends the simple perceptual decision-making task (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) by introducing a 'sure' option that is presented during a 1200–1800 ms delay period on a random half of the trials; selecting this option results in a reward that is 0.7 times the size of the reward obtained when correctly choosing L or R. As in the monkey experiment, the network receives no information indicating whether or not a given trial will contain a sure option until the middle of the delay period after stimulus offset, thus ensuring that the network makes a decision about the stimulus on all trials (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). For this task the input connection weights <inline-formula><mml:math id="inf206"><mml:msub><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf207"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>λ</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf208"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>γ</mml:mi></mml:msubsup></mml:math></inline-formula> were initialized so that half the units received information about the sure target while the other half received evidence for L and R. All units initially received fixation input.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.21492.014</object-id><label>Figure 3.</label><caption><title>Perceptual decision-making task with postdecision wagering, based on <xref ref-type="bibr" rid="bib39">Kiani and Shadlen (2009)</xref>.</title><p>(<bold>A</bold>) Task structure. On a random half of the trials, a sure option is presented during the delay period, and on these trials the network has the option of receiving a smaller (compared to correctly choosing L or R) but certain reward by choosing the sure option (S). The stimulus duration, delay, and sure target onset time are the same as in <xref ref-type="bibr" rid="bib39">Kiani and Shadlen 2009)</xref>. (<bold>B</bold>) Probability of choosing the sure option (left) and probability correct (right) as a function of stimulus duration, for different coherences. Performance is higher for trials on which the sure option was offered but waived in favor of L or R (filled circles, solid), compared to trials on which the sure option was not offered (open circles, dashed). (<bold>C</bold>) Activity of an example decision network unit for non-wager (left) and wager (right) trials, sorted by whether the presented evidence was toward the unit’s preferred (black) or nonpreferred (gray) target as determined by activity during the stimulus period on all trials. Dashed lines show activity for trials in which the sure option was chosen.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.014">http://dx.doi.org/10.7554/eLife.21492.014</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.015</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Learning curves for the postdecision wager task.</title><p>(<bold>A</bold>) Average reward per trial. Black indicates the network realization shown in the main text, gray additional realizations, i.e., trained with different random number generator seeds. (<bold>B</bold>) Percent correct, for trials on which the network made a decision (<inline-formula><mml:math id="inf209"><mml:mo>≥</mml:mo></mml:math></inline-formula>99% required for termination). Red: target performance when the sure bet was accepted between 40–50% of the time.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.015">http://dx.doi.org/10.7554/eLife.21492.015</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig3-figsupp1-v2"/></fig></fig-group></p><p>The key behavioral features found in <xref ref-type="bibr" rid="bib39">Kiani and Shadlen (2009)</xref>; <xref ref-type="bibr" rid="bib96">Wei and Wang (2015)</xref> are reproduced in the trained network, namely the network opted for the sure option more frequently when the coherence was low or stimulus duration short (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left); and when the network was presented with a sure option but waived it in favor of choosing L or R, the performance was better than on trials when the sure option was not presented (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right). The latter observation is taken as indication that neither monkeys nor trained networks choose the sure target on the basis of stimulus difficulty alone but based on their internal sense of uncertainty on each trial.</p><p><xref ref-type="fig" rid="fig3">Figure 3C</xref> shows the activity of an example network unit, sorted by whether the decision was the unit’s preferred or nonpreferred target (as determined by firing rates during the stimulus period on all trials), for both non-wager and wager trials. In particular, on trials in which the sure option was chosen, the firing rate is intermediate compared to trials on which the network made a decision by choosing L or R. Learning curves for this and additional networks trained for the task are shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p></sec><sec id="s2-5"><title>Value-based economic choice task</title><p>We also trained networks to perform the simple economic choice task of <xref ref-type="bibr" rid="bib57">Padoa-Schioppa and Assad (2006)</xref> and examined the activity of the <italic>value</italic>, rather than decision, network. The choice patterns of the networks were modulated only by varying the reward contingencies (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, upper and lower). We note that, on each trial there is a 'correct' answer in the sense that there is a choice which results in greater reward. In contrast to the previous tasks, however, information regarding whether an answer is correct in this sense is not contained in the inputs but rather in the association between inputs and rewards. This distinguishes the task from the cognitive tasks discussed in previous sections: although the task can be transformed into a cognitive-type task by providing the associated rewards as inputs, training in this manner conflates external with 'internal,' learned inputs.<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.21492.016</object-id><label>Figure 4.</label><caption><title>Value-based economic choice task (<xref ref-type="bibr" rid="bib57">Padoa-Schioppa and Assad, 2006</xref>).</title><p>(<bold>A</bold>) Choice pattern when the reward contingencies are indifferent for roughly 1 'juice' of <bold>A</bold> and 2 'juices' of <bold>B</bold> (upper) or 1 juice of <bold>A</bold> and 4 juices of <bold>B</bold> (lower). (<bold>B</bold>) Mean activity of example value network units during the pre-choice period, defined here as the period 500 ms before the decision, for the 1A = 2B case. Units in the value network exhibit diverse selectivity as observed in the monkey orbitofrontal cortex. For 'choice' (last panel), trials were separated into choice <bold>A</bold> (red diamonds) and choice <bold>B</bold> (blue circles).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.016">http://dx.doi.org/10.7554/eLife.21492.016</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.017</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Fit of cumulative Gaussian with parameters <inline-formula><mml:math id="inf210"><mml:mi>μ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf211"><mml:mi>σ</mml:mi></mml:math></inline-formula> to the choice pattern in <xref ref-type="fig" rid="fig4">Figure 4</xref> (upper), and the deduced indifference point <inline-formula><mml:math id="inf212"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mi>B</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>A</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM299"><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM300"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.017">http://dx.doi.org/10.7554/eLife.21492.017</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig4-figsupp1-v2"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.018</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Fit of cumulative Gaussian with parameters <inline-formula><mml:math id="inf213"><mml:mi>μ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf214"><mml:mi>σ</mml:mi></mml:math></inline-formula> to the choice pattern in <xref ref-type="fig" rid="fig4">Figure 4A</xref> (lower), and the deduced indifference point <inline-formula><mml:math id="inf215"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mi>B</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>A</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM303"><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM304"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</title><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.018">http://dx.doi.org/10.7554/eLife.21492.018</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig4-figsupp2-v2"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.21492.019</object-id><label>Figure 4—figure supplement 3.</label><caption><title>Learning curves for the value-based economic choice task.</title><p>(<bold>A</bold>) Average reward per trial. Black indicates the network realization shown in the main text, gray additional realizations, i.e., trained with different random number generator seeds. (<bold>B</bold>) Percentage of trials on which the network chose the option that resulted in greater (or equal) reward, for trials where the network made a decision (<inline-formula><mml:math id="inf216"><mml:mo>≥</mml:mo></mml:math></inline-formula>99% required for termination). Note this is conceptually different from the previous tasks, where 'correct' depends on the sensory inputs, not the rewards. Red: target performance (when training was terminated).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.21492.019">http://dx.doi.org/10.7554/eLife.21492.019</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-21492-fig4-figsupp3-v2"/></fig></fig-group></p><p>Each trial began with a 750 ms fixation period; the offer, which indicated the 'juice' type and amount for the left and right choices, was presented for 1000–2000 ms, followed by a 750 ms decision period during which the network was required to indicate its decision. In the upper panel of <xref ref-type="fig" rid="fig4">Figure 4A</xref> the indifference point was set to 1A = 2.2B during training, which resulted in 1A = 2.0B when fit to a cumulative Gaussian (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), while in the lower panel it was set to 1A = 4.1B during training and resulted in 1A = 4.0B (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). The basic unit of reward, i.e., 1B, was 0.1. For this task we increased the initial value of the value network’s input weights, <inline-formula><mml:math id="inf217"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>v</mml:mi></mml:msubsup></mml:math></inline-formula>, by a factor of 10 to drive the value network more strongly.</p><p>Strikingly, the activity of units in the value network <inline-formula><mml:math id="inf218"><mml:msub><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> exhibits similar types of tuning to task variables as observed in the orbitofrontal cortex of monkeys, with some units (roughly 20% of active units) selective to chosen value, others (roughly 60%, for both A and B) to offer value, and still others (roughly 20%) to choice alone as defined in <xref ref-type="bibr" rid="bib57">Padoa-Schioppa and Assad (2006)</xref> (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The decision network also contained units with a diversity of tuning. Learning curves for this and additional networks trained for the task are shown in <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>. We emphasize that no changes were made to the network architecture for this value-based economic choice task. Instead, the same scheme shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, in which the value network is responsible for predicting future rewards to guide learning but is <italic>not</italic> involved in the execution of the policy, gave rise to the pattern of neural activity shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work we have demonstrated reward-based training of recurrent neural networks for both cognitive and value-based tasks. Our main contributions are twofold: first, our work expands the range of tasks and corresponding neural mechanisms that can be studied by analyzing model recurrent neural networks, providing a unified setting in which to study diverse computations and compare to electrophysiological recordings from behaving animals; second, by explicitly incorporating reward into network training, our work makes it possible in the future to more directly address the question of reward-related processes in the brain, for instance the role of value representation that is essential for learning, but not executing, a task.</p><p>To our knowledge, the specific form of the baseline network inputs used in this work has not been used previously in the context of recurrent policy gradients; it combines ideas from <xref ref-type="bibr" rid="bib97">Wierstra et al. (2009)</xref> where the baseline network received the same inputs as the decision network in addition to the selected actions, and <xref ref-type="bibr" rid="bib62">Ranzato et al. (2016)</xref>, where the baseline was implemented as a simple linear regressor of the activity of the decision network, so that the decision and value networks effectively shared the same recurrent units. Indeed, the latter architecture is quite common in machine learning applications (<xref ref-type="bibr" rid="bib54">Mnih et al., 2016</xref>), and likewise, for some of the simpler tasks considered here, models with a baseline consisting of a linear readout of the selected actions and decision network activity could be trained in comparable (but slightly longer) time (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). The question of whether the decision and value networks ought to share the same recurrent network parallels ongoing debate over whether choice and confidence are computed together or if certain areas such as OFC compute confidence signals locally, though it is clear that such 'meta-cognitive' representations can be found widely in the brain (<xref ref-type="bibr" rid="bib42">Lak et al., 2014</xref>). Computationally, the distinction is expected to be important when there are nonlinear computations required to determine expected return that are not needed to implement the policy, as illustrated in the perceptual decision-making task (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>Interestingly, a separate value network to represent the baseline suggests an explicit role for value representation in the brain that is essential for learning a task (equivalently, when the environment is changing), but not for executing an already learned task, as is sometimes found in experiments (<xref ref-type="bibr" rid="bib91">Turner and Desmurget, 2010</xref>; <xref ref-type="bibr" rid="bib72">Schoenbaum et al., 2011</xref>; <xref ref-type="bibr" rid="bib79">Stalnaker et al., 2015</xref>). Since an accurate baseline dramatically improves learning but is not <italic>required</italic>—the algorithm is less reliable and takes many samples to converge with a constant baseline, for instance—this baseline network hypothesis for the role of value representation may account for some of the subtle yet broad learning deficits observed in OFC-lesioned animals (<xref ref-type="bibr" rid="bib93">Wallis, 2007</xref>). Moreover, since expected reward is closely related to decision confidence in many of the tasks considered, a value network that nonlinearly reads out confidence information from the decision network is consistent with experimental findings in which OFC inactivation affects the ability to report confidence but not decision accuracy (<xref ref-type="bibr" rid="bib42">Lak et al., 2014</xref>).</p><p>Our results thus support the actor-critic picture for reward-based learning, in which one circuit directly computes the policy to be followed, while a second structure, receiving projections from the decision network as well as information about the selected actions, computes expected future reward to guide learning. Actor-critic models have a rich history in neuroscience, particularly in studies of the basal ganglia (<xref ref-type="bibr" rid="bib32">Houk et al., 1995</xref>; <xref ref-type="bibr" rid="bib14">Dayan and Balleine, 2002</xref>; <xref ref-type="bibr" rid="bib35">Joel et al., 2002</xref>; <xref ref-type="bibr" rid="bib56">O'Doherty et al., 2004</xref>; <xref ref-type="bibr" rid="bib87">Takahashi et al., 2008</xref>; <xref ref-type="bibr" rid="bib46">Maia, 2010</xref>), and it is interesting to note that there is some experimental evidence that signals in the striatum are more suitable for direct policy search rather than for updating action values as an intermediate step, as would be the case for purely value function-based approaches to computing the decision policy (<xref ref-type="bibr" rid="bib43">Li and Daw, 2011</xref>; <xref ref-type="bibr" rid="bib55">Niv and Langdon, 2016</xref>). Moreover, although we have used a single RNN each to represent the decision and value modules, using 'deep,' multilayer RNNs may increase the representational power of each module (<xref ref-type="bibr" rid="bib58">Pascanu et al., 2013a</xref>). For instance, more complex tasks than considered in this work may require hierarchical feature representation in the decision network, and likewise value networks can use a combination of the different features [including raw sensory inputs (<xref ref-type="bibr" rid="bib97">Wierstra et al., 2009</xref>)] to predict future reward. Anatomically, the decision networks may correspond to circuits in dorsolateral prefrontal cortex, while the value networks may correspond to circuits in OFC (<xref ref-type="bibr" rid="bib74">Schultz et al., 2000</xref>; <xref ref-type="bibr" rid="bib88">Takahashi et al., 2011</xref>) or basal ganglia (<xref ref-type="bibr" rid="bib29">Hikosaka et al., 2014</xref>). This architecture also provides a useful example of the hypothesis that various areas of the brain effectively optimize different cost functions (<xref ref-type="bibr" rid="bib48">Marblestone et al., 2016</xref>): in this case, the decision network maximizes reward, while the value network minimizes the prediction error for future reward.</p><p>As in many other supervised learning approaches used previously to train RNNs (<xref ref-type="bibr" rid="bib47">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>), the use of BPTT to compute the gradients (in particular, the eligibility) make our 'plasticity rule' not biologically plausible. As noted previously (<xref ref-type="bibr" rid="bib103">Zipser and Andersen, 1988</xref>), it is indeed somewhat surprising that the activity of the resulting networks nevertheless exhibit many features found in neural activity recorded from behaving animals. Thus our focus has been on learning from realistic feedback signals provided by the environment but not on its physiological implementation. Still, recent work suggests that exact backpropagation is not necessary and can even be implemented in 'spiking' stochastic units (<xref ref-type="bibr" rid="bib44">Lillicrap et al., 2016</xref>), and that approximate forms of backpropagation and SGD can be implemented in a biologically plausible manner (<xref ref-type="bibr" rid="bib71">Scellier and Bengio, 2016</xref>), including both spatially and temporally asynchronous updates in RNNs (<xref ref-type="bibr" rid="bib34">Jaderberg et al., 2016</xref>). Such ideas require further investigation and may lead to effective yet more neurally plausible methods for training model neural networks.</p><p>Recently, <xref ref-type="bibr" rid="bib52">Miconi (2016)</xref> used a 'node perturbation'-based (<xref ref-type="bibr" rid="bib19">Fiete and Seung, 2006</xref>; <xref ref-type="bibr" rid="bib20">Fiete et al., 2007</xref>; <xref ref-type="bibr" rid="bib30">Hoerzer et al., 2014</xref>) algorithm with an error signal at the end of each trial to train RNNs for several cognitive tasks, and indeed, node perturbation is closely related to the REINFORCE algorithm used in this work. On one hand, the method described in <xref ref-type="bibr" rid="bib52">Miconi (2016)</xref> is more biologically plausible in the sense of not requiring gradients computed via backpropagation through time as in our approach; on the other hand, in contrast to the networks in this work, those in <xref ref-type="bibr" rid="bib52">Miconi (2016)</xref> did not 'commit' to a discrete action and thus the error signal was a graded quantity. In this and other works (<xref ref-type="bibr" rid="bib23">Frémaux et al., 2010</xref>), moreover, the prediction error was computed by algorithmically keeping track of a stimulus (task condition)-specific running average of rewards. Here we used a concrete scheme (namely a value network) for approximating the average that automatically depends on the stimulus, without requiring an external learning system to maintain a separate record for each (true) trial type, which is not known by the agent with certainty.</p><p>One of the advantages of the REINFORCE algorithm for policy gradient reinforcement learning is that direct supervised learning can also be mixed with reward-based learning, by including only the eligibility term in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> without modulating by reward (<xref ref-type="bibr" rid="bib53">Mnih et al., 2014</xref>), i.e., by maximizing the log-likelihood of the desired actions. Although all of the networks in this work were trained from reward feedback only, it will be interesting to investigate this feature of the REINFORCE algorithm. Another advantage, which we have not exploited here, is the possibility of learning policies for continuous action spaces (<xref ref-type="bibr" rid="bib60">Peters and Schaal, 2008</xref>; <xref ref-type="bibr" rid="bib97">Wierstra et al., 2009</xref>); this would allow us, for example, to model arbitrary saccade targets in the perceptual decision-making task, rather than limiting the network to discrete choices.</p><p>We have previously emphasized the importance of incorporating biological constraints in the training of neural networks (<xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>). For instance, neurons in the mammalian cortex have purely excitatory or inhibitory effects on other neurons, which is a consequence of Dale’s Principle for neurotransmitters (<xref ref-type="bibr" rid="bib17">Eccles et al., 1954</xref>). In this work we did not include such constraints due to the more complex nature of our rectified GRUs (<xref ref-type="disp-formula" rid="equ7">Equations 9–12</xref>); in particular, the units we used are capable of dynamically modulating their time constants and gating their recurrent inputs, and we therefore interpreted the firing rate units as a mixture of both excitatory and inhibitory populations. Indeed, these may implement the 'reservoir of time constants' observed experimentally (<xref ref-type="bibr" rid="bib7">Bernacchia et al., 2011</xref>). In the future, however, comparison to both model spiking networks and electrophysiological recordings will be facilitated by including more biological realism, by explicitly separating the roles of excitatory and inhibitory units (<xref ref-type="bibr" rid="bib50">Mastrogiuseppe and Ostojic, 2016</xref>). Moreover, since both the decision and value networks are obtained by minimizing an objective function, additional regularization terms can be easily included to obtain networks whose activity is more similar to neural recordings (<xref ref-type="bibr" rid="bib84">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Song et al., 2016</xref>).</p><p>Finally, one of the most appealing features of RNNs trained to perform many tasks is their ability to provide insights into neural computation in the brain. However, methods for revealing neural mechanisms in such networks remain limited to state-space analysis (<xref ref-type="bibr" rid="bib82">Sussillo and Barak, 2013</xref>), which in particular does not reveal how the synaptic connectivity leads to the dynamics responsible for implementing the higher-level decision policy. General and systematic methods for analyzing trained networks are still needed and are the subject of ongoing investigation. Nevertheless, reward-based training of RNNs makes it more likely that the resulting networks will correspond closely to biological networks observed in experiments with behaving animals. We expect that the continuing development of tools for training model neural networks in neuroscience will thus contribute novel insights into the neural basis of animal cognition.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Policy gradient reinforcement learning with RNNs</title><p>Here we review the application of the REINFORCE algorithm for policy gradient reinforcement learning to recurrent neural networks (<xref ref-type="bibr" rid="bib98">Williams, 1992</xref>; <xref ref-type="bibr" rid="bib1">Baird and Moore, 1999</xref>; <xref ref-type="bibr" rid="bib86">Sutton et al., 2000</xref>; <xref ref-type="bibr" rid="bib4">Baxter and Bartlett, 2001</xref>; <xref ref-type="bibr" rid="bib60">Peters and Schaal, 2008</xref>; <xref ref-type="bibr" rid="bib97">Wierstra et al., 2009</xref>). In particular, we provide a careful derivation of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> following, in part, the exposition in <xref ref-type="bibr" rid="bib102">Zaremba and Sutskever (2016)</xref>.</p><p>Let <inline-formula><mml:math id="inf219"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> be the sequence of interactions between the environment and agent (i.e., the environmental states, observables, and agent actions) that results in the environment being in state <inline-formula><mml:math id="inf220"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> at time <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> starting from state <inline-formula><mml:math id="inf222"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> at time <inline-formula><mml:math id="inf223"><mml:mi>μ</mml:mi></mml:math></inline-formula>:<disp-formula id="equ9"><label>(14)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM104"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo rspace="7.5pt">,</mml:mo><mml:msub id="XM105"><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo rspace="7.5pt">,</mml:mo><mml:msub id="XM106"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For notational convenience in the following, we adopt the convention that, for the special case of <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the history <inline-formula><mml:math id="inf225"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> includes the initial state <inline-formula><mml:math id="inf226"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and excludes the meaningless inputs <inline-formula><mml:math id="inf227"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, which are not seen by the agent:<disp-formula id="equ10"><label>(15)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM107"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo rspace="7.5pt">,</mml:mo><mml:msub id="XM108"><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo rspace="7.5pt">,</mml:mo><mml:msub id="XM109"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>When <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, it is also understood that <inline-formula><mml:math id="inf229"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∅</mml:mi></mml:mrow></mml:math></inline-formula>, the empty set. A full history, or a trial, is thus denoted as<disp-formula id="equ11"><label>(16)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>≡</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM110"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo rspace="7.5pt">,</mml:mo><mml:msub id="XM111"><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo rspace="7.5pt">,</mml:mo><mml:msub id="XM112"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf230"><mml:mi>T</mml:mi></mml:math></inline-formula> is the end of the trial. Here we only consider the episodic, 'finite-horizon' case where <inline-formula><mml:math id="inf231"><mml:mi>T</mml:mi></mml:math></inline-formula> is finite, and since different trials can have different durations, we take <inline-formula><mml:math id="inf232"><mml:mi>T</mml:mi></mml:math></inline-formula> to be the maximum length of a trial in the task. The reward <inline-formula><mml:math id="inf233"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> at time <inline-formula><mml:math id="inf234"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> following actions <inline-formula><mml:math id="inf235"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> (we use <inline-formula><mml:math id="inf236"><mml:mi>ρ</mml:mi></mml:math></inline-formula> to distinguish it from the firing rates <inline-formula><mml:math id="inf237"><mml:mi mathvariant="bold">𝐫</mml:mi></mml:math></inline-formula> of the RNNs) is determined by this history, which we sometimes indicate explicitly by writing<disp-formula id="equ12"><label>(17)</label><mml:math id="m12"><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM113"><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As noted in the main text, we adopt the convention that the agent performs actions at <inline-formula><mml:math id="inf238"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM114">0</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM115" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM116">T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and receives rewards at <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM117">1</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM118" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow id="XM119"><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to emphasize that rewards follow the actions and are jointly determined with the next state (<xref ref-type="bibr" rid="bib85">Sutton and Barto, 1998</xref>). For notational simplicity, here and elsewhere we assume that any discount factor is already included in <inline-formula><mml:math id="inf240"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, i.e., in all places where the reward appears we consider <inline-formula><mml:math id="inf241"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mtext>reward</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf242"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>reward</mml:mtext></mml:msub></mml:math></inline-formula> is the time constant for discounting future rewards (<xref ref-type="bibr" rid="bib16">Doya, 2000</xref>); we included temporal discounting only for the reaction-time version of the simple perceptual decision-making task (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), where we set <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the remaining tasks, <inline-formula><mml:math id="inf244"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>reward</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Explicitly, a trial <inline-formula><mml:math id="inf245"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> comprises the following. At time <inline-formula><mml:math id="inf246"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the environment is in state <inline-formula><mml:math id="inf247"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf248"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM120"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The agent initially chooses a set of actions <inline-formula><mml:math id="inf249"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf250"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM121"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is determined by the parameters of the decision network, in particular the initial conditions <inline-formula><mml:math id="inf251"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and readout weights <inline-formula><mml:math id="inf252"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>out</mml:mtext><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> and biases <inline-formula><mml:math id="inf253"><mml:msubsup><mml:mi mathvariant="bold">𝐛</mml:mi><mml:mtext>out</mml:mtext><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ5">Equation 6</xref>). At time <inline-formula><mml:math id="inf254"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the environment, depending on its previous state <inline-formula><mml:math id="inf255"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and the agent’s actions <inline-formula><mml:math id="inf256"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, transitions to state <inline-formula><mml:math id="inf257"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf258"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The history up to this point is <inline-formula><mml:math id="inf259"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM128"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi id="XM129" mathvariant="normal">∅</mml:mi><mml:mo>,</mml:mo><mml:msub id="XM130"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf260"><mml:mi mathvariant="normal">∅</mml:mi></mml:math></inline-formula> indicates that no inputs have yet been seen by the network. The environment also generates reward <inline-formula><mml:math id="inf261"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, which depends on this history, <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM131"><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. From state <inline-formula><mml:math id="inf263"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> the environment generates observables (inputs to the agent) <inline-formula><mml:math id="inf264"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> with a distribution given by <inline-formula><mml:math id="inf265"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In response, the agent, depending on the inputs <inline-formula><mml:math id="inf266"><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> it receives from the environment, chooses the set of actions <inline-formula><mml:math id="inf267"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> according to the distribution <inline-formula><mml:math id="inf268"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The environment, depending on its previous states <inline-formula><mml:math id="inf269"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and the agent’s previous actions <inline-formula><mml:math id="inf270"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, then transitions to state <inline-formula><mml:math id="inf271"><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf272"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus <inline-formula><mml:math id="inf273"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM138"><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM139"><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub id="XM140"><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Iterating these steps, the history at time <inline-formula><mml:math id="inf274"><mml:mi>t</mml:mi></mml:math></inline-formula> is therefore given by <xref ref-type="disp-formula" rid="equ10">Equation 15</xref>, while a full history is given by <xref ref-type="disp-formula" rid="equ11">Equation 16</xref>.</p><p>The probability <inline-formula><mml:math id="inf275"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM141"><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of a particular sub-history <inline-formula><mml:math id="inf276"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> up to time <inline-formula><mml:math id="inf277"><mml:mi>τ</mml:mi></mml:math></inline-formula> occurring, under the policy <inline-formula><mml:math id="inf278"><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula> parametrized by <inline-formula><mml:math id="inf279"><mml:mi>θ</mml:mi></mml:math></inline-formula>, is given by<disp-formula id="equ13"><label>(18)</label><mml:math id="m13"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="260%" minsize="260%">[</mml:mo><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>τ</mml:mi></mml:munderover><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo maxsize="260%" minsize="260%">]</mml:mo></mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In particular, the probability <inline-formula><mml:math id="inf280"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM157">H</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of a history <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> occurring is<disp-formula id="equ14"><label>(19)</label><mml:math id="m14"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM158">H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="260%" minsize="260%">[</mml:mo><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo maxsize="260%" minsize="260%">]</mml:mo></mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐬</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>A key ingredient of the REINFORCE algorithm is that the policy parameters only indirectly affect the environment through the agent’s actions. The logarithmic derivatives of <xref ref-type="disp-formula" rid="equ13">Equation 18</xref> with respect to the parameters <inline-formula><mml:math id="inf282"><mml:mi>θ</mml:mi></mml:math></inline-formula> therefore do not depend on the unknown (to the agent) environmental dynamics contained in <inline-formula><mml:math id="inf283"><mml:mi class="ltx_font_mathcaligraphic">ℰ</mml:mi></mml:math></inline-formula>, i.e.,<disp-formula id="equ15"><label>(20)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>with the understanding that <inline-formula><mml:math id="inf284"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∅</mml:mi></mml:mrow></mml:math></inline-formula> (the empty set) and therefore <inline-formula><mml:math id="inf285"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐮</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The goal of the agent is to maximize the expected return at time <inline-formula><mml:math id="inf286"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, reproduced here)<disp-formula id="equ16"><label>(21)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM174">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub id="XM176"><mml:mo>𝔼</mml:mo><mml:mi>H</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="260%" minsize="260%">[</mml:mo><mml:mrow id="XM177"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM175"><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="260%" minsize="260%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we have used the time index <inline-formula><mml:math id="inf287"><mml:mi>τ</mml:mi></mml:math></inline-formula> for notational consistency with the following and made the history-dependence of the rewards explicit. In terms of the probability of each history <inline-formula><mml:math id="inf288"><mml:mi>H</mml:mi></mml:math></inline-formula> occurring, <xref ref-type="disp-formula" rid="equ14">Equation 19</xref>, we have<disp-formula id="equ17"><label>(22)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM178">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>H</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM179">H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="260%" minsize="260%">[</mml:mo><mml:mrow id="XM182"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub id="XM181"><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="260%" minsize="260%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the generic sum over <inline-formula><mml:math id="inf289"><mml:mi>H</mml:mi></mml:math></inline-formula> may include both sums over discrete variables and integrals over continuous variables. Since, for any <inline-formula><mml:math id="inf290"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM183">0</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM184" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM185">T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ18"><label>(23)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM186">H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(cf. <xref ref-type="disp-formula" rid="equ13">Equation 18</xref>), we can simplify <xref ref-type="disp-formula" rid="equ17">Equation 22</xref> to<disp-formula id="equ19"><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-24_75"><mml:mtext>(24)</mml:mtext></mml:mtd><mml:mtd><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>H</mml:mi></mml:munder></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-25_51"><mml:mtext>(25)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-26_48"><mml:mtext>(26)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This simplification is used below to formalize the intuition that present actions do not influence past rewards. Using the 'likelihood-ratio trick'<disp-formula id="equ20"><label>(27)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>we can write<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-28_5"><mml:mtext>(28)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-29_5"><mml:mtext>(29)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ15">Equation 20</xref> we therefore have<disp-formula id="equ22"><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-30_5"><mml:mtext>(30)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">[</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">]</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-31_5"><mml:mtext>(31)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-32_5"><mml:mtext>(32)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-33_5"><mml:mtext>(33)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">[</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where we have 'undone' <xref ref-type="disp-formula" rid="equ18">Equation 23</xref> to recover the sum over the full histories <inline-formula><mml:math id="inf291"><mml:mi>H</mml:mi></mml:math></inline-formula> in going from <xref ref-type="disp-formula" rid="equ22">Equation 30</xref> to <xref ref-type="disp-formula" rid="equ22">Equation 31</xref>. We then obtain the first terms of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> by estimating the sum over all <inline-formula><mml:math id="inf292"><mml:mi>H</mml:mi></mml:math></inline-formula> by <inline-formula><mml:math id="inf293"><mml:msub><mml:mi>N</mml:mi><mml:mtext>trials</mml:mtext></mml:msub></mml:math></inline-formula> samples from the agent’s experience.</p><p>In <xref ref-type="disp-formula" rid="equ17">Equation 22</xref> it is evident that, while subtracting any constant <inline-formula><mml:math id="inf294"><mml:mi>b</mml:mi></mml:math></inline-formula> from the reward <inline-formula><mml:math id="inf295"><mml:mrow><mml:mi>J</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM234">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will not affect the <italic>gradient</italic> with respect to <inline-formula><mml:math id="inf296"><mml:mi>θ</mml:mi></mml:math></inline-formula>, it can reduce the variance of the stochastic estimate (<xref ref-type="disp-formula" rid="equ22">Equation 33</xref>) from a finite number of trials. Indeed, it is possible to use this invariance to find an 'optimal' value of the constant baseline that minimizes the variance of the gradient estimate (<xref ref-type="bibr" rid="bib60">Peters and Schaal, 2008</xref>). In practice, however, it is more useful to have a history-dependent baseline that attempts to predict the future return at every time (<xref ref-type="bibr" rid="bib97">Wierstra et al., 2009</xref>; <xref ref-type="bibr" rid="bib53">Mnih et al., 2014</xref>; <xref ref-type="bibr" rid="bib102">Zaremba and Sutskever, 2016</xref>). We therefore introduce a second network, called the <italic>value network</italic>, that uses the selected actions <inline-formula><mml:math id="inf297"><mml:msub><mml:mi mathvariant="bold">𝐚</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the activity of the decision network <inline-formula><mml:math id="inf298"><mml:msubsup><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> to predict the future return <inline-formula><mml:math id="inf299"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> by minimizing the squared error (<xref ref-type="disp-formula" rid="equ4 equ4">Equations 4–5)</xref>. Intuitively, such a baseline is appealing because the terms in the gradient of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> are nonzero only if the actual return deviates from what was predicted by the value network.</p></sec><sec id="s4-2"><title>Discretized network equations and initialization</title><p>Carrying out the discretization of <xref ref-type="disp-formula" rid="equ7">Equations 9–12</xref> in time steps of <inline-formula><mml:math id="inf300"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, we obtain<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-34_5"><mml:mtext>(34)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-35_5"><mml:mtext>(35)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-36_5"><mml:mtext>(36)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">[</mml:mo></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-37_5"><mml:mtext>(37)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>for <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn id="XM255">1</mml:mn><mml:mo>,</mml:mo><mml:mi id="XM256" mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM257">T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf302"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf303"><mml:mrow><mml:mi mathvariant="bold">𝐍</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn id="XM258">0</mml:mn><mml:mo>,</mml:mo><mml:mn id="XM259">1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are normally distributed random numbers with zero mean and unit variance. We note that the rectified-linear activation function appears in different positions compared to standard GRUs, which merely reflects the choice of using 'synaptic currents' as the dynamical variable rather than directly using firing rates as the dynamical variable. One small advantage of this choice is that we can train the unconstrained initial conditions <inline-formula><mml:math id="inf304"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> rather than the non-negatively constrained firing rates <inline-formula><mml:math id="inf305"><mml:msub><mml:mi mathvariant="bold">𝐫</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>.</p><p>The biases <inline-formula><mml:math id="inf306"><mml:msup><mml:mi mathvariant="bold">𝐛</mml:mi><mml:mi>λ</mml:mi></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf307"><mml:msup><mml:mi mathvariant="bold">𝐛</mml:mi><mml:mi>γ</mml:mi></mml:msup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf308"><mml:mi mathvariant="bold">𝐛</mml:mi></mml:math></inline-formula>, as well as the readout weights <inline-formula><mml:math id="inf309"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>out</mml:mtext><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf310"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>out</mml:mtext><mml:mi>v</mml:mi></mml:msubsup></mml:math></inline-formula>, were initialized to zero. The biases for the policy readout <inline-formula><mml:math id="inf311"><mml:msubsup><mml:mi mathvariant="bold">𝐛</mml:mi><mml:mtext>out</mml:mtext><mml:mi>π</mml:mi></mml:msubsup></mml:math></inline-formula> were initially set to zero, while the value network bias <inline-formula><mml:math id="inf312"><mml:msubsup><mml:mi>b</mml:mi><mml:mtext>out</mml:mtext><mml:mi>v</mml:mi></mml:msubsup></mml:math></inline-formula> was initially set to the 'reward' for an aborted trial, −1. The entries of the input weight matrices <inline-formula><mml:math id="inf313"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>γ</mml:mi></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="inf314"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext><mml:mi>λ</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf315"><mml:msub><mml:mi>W</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:math></inline-formula> for both decision and value networks were drawn from a zero-mean Gaussian distribution with variance <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi>K</mml:mi><mml:mo>/</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. For the recurrent weight matrices <inline-formula><mml:math id="inf317"><mml:msub><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf318"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext><mml:mi>λ</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf319"><mml:msubsup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext><mml:mi>γ</mml:mi></mml:msubsup></mml:math></inline-formula>, the <inline-formula><mml:math id="inf320"><mml:mi>K</mml:mi></mml:math></inline-formula> nonzero entries in each row were initialized from a gamma distribution <inline-formula><mml:math id="inf321"><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM260">α</mml:mi><mml:mo>,</mml:mo><mml:mi id="XM261">β</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf322"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, with each entry multiplied randomly by <inline-formula><mml:math id="inf323"><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>; the entire matrix was then scaled such that the spectral radius—the largest absolute value of the eigenvalues—was exactly <inline-formula><mml:math id="inf324"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. Although we also successfully trained networks starting from normally distributed weights, we found it convenient to control the sign and magnitude of the weights independently. The initial conditions <inline-formula><mml:math id="inf325"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, which are also trained, were set to 0.5 for all units before the start of training. We implemented the networks in the Python machine learning library Theano (<xref ref-type="bibr" rid="bib89">The Theano Development Team, 2016</xref>).</p></sec><sec id="s4-3"><title>Adam SGD with gradient clipping</title><p>We used a recently developed version of stochastic gradient descent known as Adam, for <italic>ada</italic>ptive <italic>m</italic>oment estimation (<xref ref-type="bibr" rid="bib40">Kingma and Ba, 2015</xref>), together with gradient clipping to prevent exploding gradients (<xref ref-type="bibr" rid="bib26">Graves, 2013</xref>; <xref ref-type="bibr" rid="bib59">Pascanu et al., 2013b</xref>). For clarity, in this section we use vector notation <inline-formula><mml:math id="inf326"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula> to indicate the set of all parameters being optimized and the subscript <inline-formula><mml:math id="inf327"><mml:mi>k</mml:mi></mml:math></inline-formula> to indicate a specific parameter <inline-formula><mml:math id="inf328"><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. At each iteration <inline-formula><mml:math id="inf329"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, let<disp-formula id="equ24"><label>(38)</label><mml:math id="m24"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM262">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:mrow></mml:mfrac><mml:mo fence="true">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM263"><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>be the gradient of the objective function <inline-formula><mml:math id="inf330"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:math></inline-formula> with respect to the parameters <inline-formula><mml:math id="inf331"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula>. We first clip the gradient if its norm <inline-formula><mml:math id="inf332"><mml:mrow><mml:mo>|</mml:mo><mml:msup id="XM265"><mml:mi mathvariant="bold">𝐠</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM264">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> exceeds a maximum <inline-formula><mml:math id="inf333"><mml:mi mathvariant="normal">Γ</mml:mi></mml:math></inline-formula> (see <xref ref-type="table" rid="tbl1">Table 1</xref>), i.e.,<disp-formula id="equ25"><label>(39)</label><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Each parameter <inline-formula><mml:math id="inf334"><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is then updated according to<disp-formula id="equ26"><label>(40)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM273">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow id="XM274"><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:msubsup><mml:mi>m</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM275">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msqrt><mml:msubsup><mml:mi>v</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi id="XM276">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msqrt><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf335"><mml:mi>η</mml:mi></mml:math></inline-formula> is the base learning rate and the moving averages<disp-formula id="equ27"><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd id="mjx-eqn-41_3"><mml:mtext>(41)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd id="mjx-eqn-42_3"><mml:mtext>(42)</mml:mtext></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">[</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">g</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>estimate the first and second (uncentered) moments of the gradient. Initially, <inline-formula><mml:math id="inf336"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐦</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn id="XM295">0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn id="XM296">0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. These moments allow each parameter to be updated in <xref ref-type="disp-formula" rid="equ26">Equation 40</xref> according to adaptive learning rates, such that parameters whose gradients exhibit high uncertainty and hence small 'signal-to-noise ratio' lead to smaller learning rates.</p><p>Except for the base learning rate <inline-formula><mml:math id="inf337"><mml:mi>η</mml:mi></mml:math></inline-formula> (see <xref ref-type="table" rid="tbl1">Table 1</xref>), we used the parameter values suggested in <xref ref-type="bibr" rid="bib40">Kingma and Ba (2015)</xref>:<disp-formula id="equ28"><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ε</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-4"><title>Computer code</title><p>All code used in this work, including code for generating the figures, is available at <ext-link ext-link-type="uri" xlink:href="http://github.com/xjwanglab/pyrl">http://github.com/xjwanglab/pyrl</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank R Pascanu, K Cho, E Ohran, E Russek, and G Wayne for valuable discussions. This work was supported by Office of Naval Research Grant N00014-13-1-0297 and a Google Computational Neuroscience Grant.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>HFS, Conceptualization, Software, Formal analysis, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>GRY, Formal analysis, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>X-JW, Conceptualization, Formal analysis, Writing—original draft, Writing—review and editing</p></fn></fn-group></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baird</surname> <given-names>L</given-names></name><name><surname>Moore</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Gradient descent for general reinforcement learning</article-title><source>Advances in Neural Information Processing Systems</source><volume>11</volume><fpage>968</fpage><lpage>974</lpage><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/1576-gradient-descent-for-general-reinforcement-learning.pdf">https://papers.nips.cc/paper/1576-gradient-descent-for-general-reinforcement-learning.pdf</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From fixed points to chaos: three models of delayed discrimination</article-title><source>Progress in Neurobiology</source><volume>103</volume><fpage>214</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2013.02.002</pub-id><pub-id pub-id-type="pmid">23438479</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barto</surname><given-names>AG</given-names></name><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Anderson</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Neuronlike adaptive elements that can solve difficult learning control problems</article-title><source>IEEE Transactions on Systems, Man, and Cybernetics</source><volume>SMC-13</volume><fpage>834</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1109/TSMC.1983.6313077</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baxter</surname><given-names>J</given-names></name><name><surname>Bartlett</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Infinite-horizon policy-gradient estimation</article-title><source>The Journal of Artificial Intelligence Research</source><volume>15</volume><fpage>319</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1613/jair.806</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayer</surname><given-names>HM</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Midbrain dopamine neurons encode a quantitative reward prediction error signal</article-title><source>Neuron</source><volume>47</volume><fpage>129</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.05.020</pub-id><pub-id pub-id-type="pmid">15996553</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Simard</surname><given-names>P</given-names></name><name><surname>Frasconi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Learning long-term dependencies with gradient descent is difficult</article-title><source>IEEE Transactions on Neural Networks</source><volume>5</volume><fpage>157</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1109/72.279181</pub-id><pub-id pub-id-type="pmid">18267787</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernacchia</surname><given-names>A</given-names></name><name><surname>Seo</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A reservoir of time constants for memory traces in cortical neurons</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>366</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nn.2752</pub-id><pub-id pub-id-type="pmid">21317906</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brea</surname><given-names>J</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Does computational neuroscience need new synaptic learning paradigms?</article-title><source>Current Opinion in Behavioral Sciences</source><volume>11</volume><fpage>61</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2016.05.012</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosch</surname><given-names>T</given-names></name><name><surname>Neumann</surname><given-names>H</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reinforcement learning of linking and tracing contours in recurrent neural networks. </article-title><source>PLoS Computational Biology</source><volume>11</volume><elocation-id>e1004489</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004489</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Pinto</surname><given-names>N</given-names></name><name><surname>Ardila</surname><given-names>D</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003963</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carnevale</surname><given-names>F</given-names></name><name><surname>de Lafuente</surname><suffix>V</suffix></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Parga</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamic control of response criterion in premotor cortex during perceptual detection under temporal uncertainty</article-title><source>Neuron</source><volume>86</volume><fpage>1067</fpage><lpage>1077</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.04.014</pub-id><pub-id pub-id-type="pmid">25959731</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>van Merrienboer</surname><given-names>B</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Bougares</surname><given-names>F</given-names></name><name><surname>Schwenk</surname><given-names>H</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning phrase representations using RNN encoder-decoder for statistical machine translation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>J</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Empirical evaluation of gated recurrent neural networks on sequence modeling</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.3555">http://arxiv.org/abs/1412.3555</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Balleine</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Reward, motivation, and reinforcement learning</article-title><source>Neuron</source><volume>36</volume><fpage>285</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00963-7</pub-id><pub-id pub-id-type="pmid">12383782</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision theory, reinforcement learning, and the brain</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>8</volume><fpage>429</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.3758/CABN.8.4.429</pub-id><pub-id pub-id-type="pmid">19033240</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Reinforcement learning in continuous time and space</article-title><source>Neural Computation</source><volume>12</volume><fpage>219</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1162/089976600300015961</pub-id><pub-id pub-id-type="pmid">10636940</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eccles</surname><given-names>JC</given-names></name><name><surname>Fatt</surname><given-names>P</given-names></name><name><surname>Koketsu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Cholinergic and inhibitory synapses in a pathway from motor-axon collaterals to motoneurones</article-title><source>The Journal of Physiology</source><volume>126</volume><fpage>524</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1954.sp005226</pub-id><pub-id pub-id-type="pmid">13222354</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>TA</given-names></name><name><surname>Chaisangmongkon</surname><given-names>W</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Choice-correlated activity fluctuations underlie learning of neuronal category representation</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>6454</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7454</pub-id><pub-id pub-id-type="pmid">25759251</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Gradient learning in spiking neural networks by dynamic perturbation of conductances</article-title><source>Physical Review Letters</source><volume>97</volume><elocation-id>048104</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.97.048104</pub-id><pub-id pub-id-type="pmid">16907616</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>2038</fpage><lpage>2057</lpage><pub-id pub-id-type="doi">10.1152/jn.01311.2006</pub-id><pub-id pub-id-type="pmid">17652414</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>MJ</given-names></name><name><surname>Claus</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Anatomy of a decision: striato-orbitofrontal interactions in reinforcement learning, decision making, and reversal</article-title><source>Psychological Review</source><volume>113</volume><fpage>300</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.2.300</pub-id><pub-id pub-id-type="pmid">16637763</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Goal-Directed decision making with spiking neurons</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>1529</fpage><lpage>1546</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2854-15.2016</pub-id><pub-id pub-id-type="pmid">26843636</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frémaux</surname><given-names>N</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional requirements for reward-modulated spike-timing-dependent plasticity</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>13326</fpage><lpage>13337</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6249-09.2010</pub-id><pub-id pub-id-type="pmid">20926659</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>P</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>On simplicity and complexity in the brave new world of large-scale neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>148</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.04.003</pub-id><pub-id pub-id-type="pmid">25932978</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Generating sequences with recurrent neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1308.0850">http://arxiv.org/abs/1308.0850</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grondman</surname><given-names>I</given-names></name><name><surname>Busoniu</surname><given-names>L</given-names></name><name><surname>Lopes</surname><given-names>GAD</given-names></name><name><surname>Babuska</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A survey of actor-critic reinforcement learning: Standard and natural policy gradients</article-title><source>IEEE Transactions on Systems, Man, and Cybernetics, Part C</source><volume>42</volume><fpage>1291</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1109/TSMCC.2012.2218595</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title><source>Neuron</source><volume>82</volume><fpage>1394</fpage><lpage>1406</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id><pub-id pub-id-type="pmid">24945778</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hikosaka</surname><given-names>O</given-names></name><name><surname>Kim</surname><given-names>HF</given-names></name><name><surname>Yasuda</surname><given-names>M</given-names></name><name><surname>Yamamoto</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Basal ganglia circuits for reward value-guided behavior</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>289</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-013924</pub-id><pub-id pub-id-type="pmid">25032497</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoerzer</surname><given-names>GM</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Emergence of complex computational structures from chaotic neural networks through reward-modulated hebbian learning</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>677</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs348</pub-id><pub-id pub-id-type="pmid">23146969</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Explicit information for category-orthogonal object properties increases along the ventral stream</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>613</fpage><lpage>622</lpage><pub-id pub-id-type="doi">10.1038/nn.4247</pub-id><pub-id pub-id-type="pmid">26900926</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Houk</surname><given-names>JC</given-names></name><name><surname>Adams</surname><given-names>JL</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>A model of how the basal ganglia generates and uses neural signals that predict reinforcement</chapter-title><person-group person-group-type="editor"><name><surname>Houk</surname> <given-names>J. C</given-names></name><name><surname>Davis</surname> <given-names>J. L</given-names></name><name><surname>Beisberb</surname> <given-names>D. G</given-names></name></person-group><source>Models of Information Processing in the Basal Ganglia</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>249</fpage><lpage>274</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izhikevich</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Solving the distal reward problem through linkage of STDP and dopamine signaling</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2443</fpage><lpage>2452</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl152</pub-id><pub-id pub-id-type="pmid">17220510</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jaderberg</surname><given-names>M</given-names></name><name><surname>Czarnecki</surname><given-names>WM</given-names></name><name><surname>Osindero</surname><given-names>S</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoupled neural interfaces using synthetic gradients</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1608.05343">http://arxiv.org/abs/1608.05343</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joel</surname><given-names>D</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Ruppin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Actor-critic models of the basal ganglia: new anatomical and computational perspectives</article-title><source>Neural Networks</source><volume>15</volume><fpage>535</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(02)00047-3</pub-id><pub-id pub-id-type="pmid">12371510</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaelbling</surname><given-names>LP</given-names></name><name><surname>Littman</surname><given-names>ML</given-names></name><name><surname>Cassandra</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Planning and acting in partially observable stochastic domains</article-title><source>Artificial Intelligence</source><volume>101</volume><fpage>99</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/S0004-3702(98)00023-X</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Zariwala</surname><given-names>HA</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates, computation and behavioural impact of decision confidence</article-title><source>Nature</source><volume>455</volume><fpage>227</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nature07200</pub-id><pub-id pub-id-type="pmid">18690210</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Bounded integration in parietal cortex underlies decisions even when viewing duration is dictated by the environment</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>3017</fpage><lpage>3029</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4761-07.2008</pub-id><pub-id pub-id-type="pmid">18354005</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representation of confidence associated with a decision by neurons in the parietal cortex</article-title><source>Science</source><volume>324</volume><fpage>759</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1126/science.1169405</pub-id><pub-id pub-id-type="pmid">19423820</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adam: A method for stochastic optimization. Int. Conf. Learn. Represent</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laje</surname><given-names>R</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>925</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nn.3405</pub-id><pub-id pub-id-type="pmid">23708144</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Costa</surname><given-names>GM</given-names></name><name><surname>Romberg</surname><given-names>E</given-names></name><name><surname>Koulakov</surname><given-names>AA</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orbitofrontal cortex is required for optimal waiting based on decision confidence</article-title><source>Neuron</source><volume>84</volume><fpage>190</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.039</pub-id><pub-id pub-id-type="pmid">25242219</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Signals in human striatum are appropriate for policy update rather than value prediction</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>5504</fpage><lpage>5511</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6316-10.2011</pub-id><pub-id pub-id-type="pmid">21471387</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Cownden</surname><given-names>D</given-names></name><name><surname>Tweed</surname><given-names>DB</given-names></name><name><surname>Akerman</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id><pub-id pub-id-type="pmid">27824044</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional, but not anatomical, separation of &quot;what&quot; and &quot;when&quot; in prefrontal cortex</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>350</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3276-09.2010</pub-id><pub-id pub-id-type="pmid">20053916</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maia</surname><given-names>TV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Two-factor theory, the actor-critic model, and conditioned avoidance</article-title><source>Learning &amp; Behavior</source><volume>38</volume><fpage>50</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.3758/LB.38.1.50</pub-id><pub-id pub-id-type="pmid">20065349</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marblestone</surname><given-names>AH</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Toward an integration of deep learning and neuroscience</article-title><source>Frontiers in Computational Neuroscience</source><volume>10</volume><fpage>94</fpage><pub-id pub-id-type="doi">10.3389/fncom.2016.00094</pub-id><pub-id pub-id-type="pmid">27683554</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Martens</surname><given-names>J</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Learning recurrent neural networks with Hessian-free optimization</article-title><conf-name>Proceedings of the 28th International Conference on Machine Learning</conf-name><conf-loc>Washington, USA</conf-loc><ext-link ext-link-type="uri" xlink:href="http://www.icml-2011.org/papers/532_icmlpaper.pdf">http://www.icml-2011.org/papers/532_icmlpaper.pdf</ext-link></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Intrinsically-generated fluctuating activity in excitatory-inhibitory networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1605.04221">http://arxiv.org/abs/1605.04221</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazurek</surname><given-names>ME</given-names></name><name><surname>Roitman</surname><given-names>JD</given-names></name><name><surname>Ditterich</surname><given-names>J</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A role for neural integrators in perceptual decision making</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1257</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg097</pub-id><pub-id pub-id-type="pmid">14576217</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Miconi</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Biologically plausible learning in recurrent neural networks for flexible decision tasks</article-title><source>bioRxiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/057729">https://doi.org/10.1101/057729</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Hess</surname><given-names>N</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Recurrent models of visual attention</article-title><conf-name><italic>Advances in neural information processing systems</italic></conf-name><ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf">https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Harley</surname><given-names>T</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Asynchronous methods for deep reinforcement learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Langdon</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reinforcement learning with Marr</article-title><source>Current Opinion in Behavioral Sciences</source><volume>11</volume><fpage>67</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2016.04.005</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname><given-names>J</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Schultz</surname><given-names>J</given-names></name><name><surname>Deichmann</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title><source>Science</source><volume>304</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1126/science.1094285</pub-id><pub-id pub-id-type="pmid">15087550</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname><given-names>C</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurons in the orbitofrontal cortex encode economic value</article-title><source>Nature</source><volume>441</volume><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1038/nature04676</pub-id><pub-id pub-id-type="pmid">16633341</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>How to construct deep recurrent neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1312.6026">http://arxiv.org/abs/1312.6026</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Mikolov</surname><given-names>T</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>On the difficulty of training recurrent neural networks</article-title><conf-name>Proceedings of the 30th International Conference on Machine Learning</conf-name><ext-link ext-link-type="uri" xlink:href="http://jmlr.org/proceedings/papers/v28/pascanu13.pdf">http://jmlr.org/proceedings/papers/v28/pascanu13.pdf</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Schaal</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Reinforcement learning of motor skills with policy gradients</article-title><source>Neural Networks</source><volume>21</volume><fpage>682</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2008.02.003</pub-id><pub-id pub-id-type="pmid">18482830</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Recurrent network models of sequence generation and memory</article-title><source>Neuron</source><volume>90</volume><fpage>128</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.009</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ranzato</surname><given-names>M</given-names></name><name><surname>Chopra</surname><given-names>S</given-names></name><name><surname>Auli</surname><given-names>M</given-names></name><name><surname>Zaremba</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Sequence level training with recurrent neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1511.06732">http://arxiv.org/abs/1511.06732</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decision making under uncertainty: a neural model based on partially observable markov decision processes</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>146</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00146</pub-id><pub-id pub-id-type="pmid">21152255</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname><given-names>D</given-names></name><name><surname>Sheppard</surname><given-names>JP</given-names></name><name><surname>Schrater</surname><given-names>PR</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multisensory decision-making in rats and humans</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>3726</fpage><lpage>3735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4998-11.2012</pub-id><pub-id pub-id-type="pmid">22423093</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname><given-names>D</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A category-free neural population supports evolving demands during decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1784</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1038/nn.3865</pub-id><pub-id pub-id-type="pmid">25383902</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Ben Dayan Rubin</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Internal representation of task rules by recurrent dynamics: the importance of the diversity of neural responses</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>24</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00024</pub-id><pub-id pub-id-type="pmid">21048899</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roitman</surname><given-names>JD</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</article-title><source>Journal of Neuroscience</source><volume>22</volume><fpage>9475</fpage><lpage>9489</lpage><pub-id pub-id-type="pmid">12417672</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Lemus</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title><source>Nature</source><volume>399</volume><fpage>470</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1038/20939</pub-id><pub-id pub-id-type="pmid">10365959</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><chapter-title>Learning internal representations by error propagation</chapter-title><person-group person-group-type="editor"><name><surname>Rumelhart</surname> <given-names>DE</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><source>Parallel Distributed Processing</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><volume>1</volume><fpage>318</fpage><lpage>362</lpage></element-citation></ref><ref id="bib71"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Towards a biologically plausible backprop</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1602.05179">http://arxiv.org/abs/1602.05179</ext-link></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Takahashi</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>TL</given-names></name><name><surname>McDannald</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Does the orbitofrontal cortex signal value?</article-title><source>Annals of the New York Academy of Sciences</source><volume>1239</volume><fpage>87</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2011.06210.x</pub-id><pub-id pub-id-type="pmid">22145878</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Tremblay</surname><given-names>L</given-names></name><name><surname>Hollerman</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Reward processing in primate orbitofrontal cortex and basal ganglia</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>272</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.3.272</pub-id><pub-id pub-id-type="pmid">10731222</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission</article-title><source>Neuron</source><volume>40</volume><fpage>1063</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00761-X</pub-id><pub-id pub-id-type="pmid">14687542</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural mechanism for stochastic behaviour during a competitive game</article-title><source>Neural Networks</source><volume>19</volume><fpage>1075</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2006.05.044</pub-id><pub-id pub-id-type="pmid">17015181</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Synaptic computation underlying probabilistic inference</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>112</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1038/nn.2450</pub-id><pub-id pub-id-type="pmid">20010823</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Training excitatory-inhibitory recurrent neural networks for cognitive tasks: A simple and flexible framework</article-title><source>PLoS Computational Biology</source><volume>12</volume><elocation-id>e1004792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004792</pub-id><pub-id pub-id-type="pmid">26928718</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stalnaker</surname><given-names>TA</given-names></name><name><surname>Cooch</surname><given-names>NK</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>What the orbitofrontal cortex does not do</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>620</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1038/nn.3982</pub-id><pub-id pub-id-type="pmid">25919962</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugrue</surname><given-names>LP</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Choosing the greater of two goods: neural currencies for valuation and decision making</article-title><source>Nature Reviews Neuroscience</source><volume>6</volume><fpage>363</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1038/nrn1666</pub-id><pub-id pub-id-type="pmid">15832198</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural Computation</source><volume>25</volume><fpage>626</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00409</pub-id><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural circuits as computational dynamical systems</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.01.008</pub-id><pub-id pub-id-type="pmid">24509098</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning: An Introduction</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Mcallester</surname><given-names>D</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Mansour</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Policy gradient methods for reinforcement learning with function approximation</article-title><source>Advances in neural information processing systems</source><volume>12</volume><fpage>1057</fpage><lpage>1063</lpage><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf</ext-link></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>Y</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Silencing the critics: understanding the effects of cocaine sensitization on dorsolateral and ventral striatum in the context of an actor/critic model</article-title><source>Frontiers in Neuroscience</source><volume>2</volume><fpage>86</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.014.2008</pub-id><pub-id pub-id-type="pmid">18982111</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>YK</given-names></name><name><surname>Roesch</surname><given-names>MR</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Toreson</surname><given-names>K</given-names></name><name><surname>O'Donnell</surname><given-names>P</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Expectancy-related changes in firing of dopamine neurons depend on orbitofrontal cortex</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>1590</fpage><lpage>1597</lpage><pub-id pub-id-type="doi">10.1038/nn.2957</pub-id><pub-id pub-id-type="pmid">22037501</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="preprint"><person-group person-group-type="author"><collab>The Theano Development Team</collab></person-group><year iso-8601-date="2016">2016</year><article-title>Theano: A python framework for fast computation of mathematical expressions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1605.02688">http://arxiv.org/abs/1605.02688</ext-link></element-citation></ref><ref id="bib90"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Todd</surname><given-names>MT</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Learning to use working memory in partially observable environments through dopaminergic reinforcement</article-title><conf-name><italic>Advances in Neural Information Processing Systems</italic></conf-name><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/3508-learning-to-use-working-memory-in-partially-observable-environments-through-dopaminergic-reinforcement.pdf">http://papers.nips.cc/paper/3508-learning-to-use-working-memory-in-partially-observable-environments-through-dopaminergic-reinforcement.pdf</ext-link></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>RS</given-names></name><name><surname>Desmurget</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Basal ganglia contributions to motor control: a vigorous tutor</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.08.022</pub-id><pub-id pub-id-type="pmid">20850966</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reinforcement learning in populations of spiking neurons</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>250</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1038/nn.2264</pub-id><pub-id pub-id-type="pmid">19219040</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Orbitofrontal cortex and its contribution to decision-making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>31</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094334</pub-id><pub-id pub-id-type="pmid">17417936</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title><source>Neuron</source><volume>36</volume><fpage>955</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01092-9</pub-id><pub-id pub-id-type="pmid">12467598</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision making in recurrent neuronal circuits</article-title><source>Neuron</source><volume>60</volume><fpage>215</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.034</pub-id><pub-id pub-id-type="pmid">18957215</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Confidence estimation as a stochastic process in a neurodynamical system of decision making</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>99</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1152/jn.00793.2014</pub-id><pub-id pub-id-type="pmid">25948870</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Forster</surname><given-names>A</given-names></name><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Recurrent policy gradients</article-title><source>Logic Journal of IGPL</source><volume>18</volume><fpage>620</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1093/jigpal/jzp049</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title><source>Machine Learning</source><volume>8</volume><fpage>229</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1007/BF00992696</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>KF</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>1314</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3733-05.2006</pub-id><pub-id pub-id-type="pmid">16436619</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>K</given-names></name><name><surname>Ba</surname><given-names>JL</given-names></name><name><surname>Kiros</surname><given-names>R</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name><name><surname>Zemel</surname><given-names>RS</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Show, attend and tell: Neural image caption generation with visual attention</article-title><conf-name>Proceedings of the 32 nd International Conference on Machine Learning</conf-name><ext-link ext-link-type="uri" xlink:href="http://jmlr.org/proceedings/papers/v37/xuc15.pdf">http://jmlr.org/proceedings/papers/v37/xuc15.pdf</ext-link></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zaremba</surname><given-names>W</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reinforcement learning neural turing machines</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1505.00521">http://arxiv.org/abs/1505.00521</ext-link></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zipser</surname><given-names>D</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</article-title><source>Nature</source><volume>331</volume><fpage>679</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1038/331679a0</pub-id><pub-id pub-id-type="pmid">3344044</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.21492.020</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Behrens</surname><given-names>Timothy EJ</given-names></name><role>Reviewing editor</role><aff id="aff4"><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Reward-based training of recurrent neural networks for cognitive and value-based tasks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Sam Gershman (Reviewer #2) and Mattia Rigotti (Reviewer #3), and the evaluation has been overseen by Timothy Behrens as the Senior and Reviewing editor.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The authors propose to use reward-based training of recurrent neural networks in order to build models able to perform behavioral tasks that are being studied in animal models by the systems neuroscience and neuroelectrophysiology communities. The trained models are then shown to capture features of both the behavior and of electrophysiological data observed in primate experiments. The reward-based training procedure is more general and more realistic (in terms of mimicking the actual primate training) than the supervised training approaches that have been typically employed so far in computational works that compare trained recurrent neural networks to neural recordings. As a result, this training method promises to have a much wider applicability and reach, in terms of the experimental paradigms that can be modeled and the scientific questions that can be asked. The authors then give several examples of the utility of their framework by training models to reproduce a perceptual discrimination task, a context-dependent integration task, a multisensory integration task, a working memory task, and economic choice task.</p><p>The paper is well-written and demonstrates an approach to fit recurrent neural networks to neuroscience experiment that is much more general, principled and powerful than the mentioned supervised method typically used so far. Besides being more natural and of wider applicability, the reward-based training technique will also potentially allow investigators to model the evolution of the neural dynamics and behavior as a subject is learning to perform a task, as the authors nicely demonstrate in a handful of landmark primate decision making tasks. These points alone arguably make for a strong paper.</p><p>Essential revisions:</p><p>Despite the laudable aspects of the work described above, the reviewers unanimously agreed that more needs to be done to demonstrate that the approach will lead to clear new biological knowledge.</p><p>All three reviewers raised the issue that it is not clear that the network learning routine is novel from the machine learning perspective, and yet it is completely clear that assumptions made in the model render it only a distant approximation to a biological learning rule. Hence, the reviewers thought that the manuscript needed to be reframed to be clearer what the contribution was. Less emphasis should be placed on the biological realism of the network, and more emphasis should be placed on the biological utility of training a network based on rewards.</p><p>Comments from the reviews that address this point are as follows:</p><p>I am having problems assessing novelty. If the main novelty is to provide the first neural implementation of the standard &quot;actor-critic&quot; paradigm, then the authors would need to provide an implementation where learning (both in the actor and in the critic) happens through plausible learning rules, which is clearly not the case here (use of backpropagation through time as well as Gated Recurrent Units; the authors acknowledge these limitations in the Discussion). It is not clear that the problem remains learnable (even with the same &quot;teaching signals&quot;) under more realistic assumptions. If, on the other hand, the aim was to argue that jointly training a critic facilitates learning of the actor, the authors did not need a neural implementation to make that point, and in fact, it is already well known that policy-gradient methods benefit immensely from variance reduction through fine-grained reward prediction. So, this leaves us with a third alternative message, which is to use these insights + their (non-plausible) neural implementations to try and map the <italic>algorithm</italic> (as opposed to a particular implementation) to specific brain structures/processes by relating model activity to recorded brain activity and behaviour. I think this could have been the main strength of this paper, but is somehow underplayed (in fact, inspection of the activity of the value network is limited to a single page).</p><p>The network is quasi-biological in the sense that it is technically a neural network, but the biological constraints on architecture, dynamics and synaptic plasticity are really weak. Should we see this as essentially a phenomenological model, or is there deeper significance to the &quot;neural&quot; aspect? If the latter, then I think a stronger case needs to be made.</p><p>My main criticism with the paper is that some presentation choices might mislead some readers into thinking that the application of the policy gradient method to recurrent neural networks trained with backpropagation through time is novel. In fact, the training method used in this paper is essentially the &quot;recurrent policy gradient&quot; algorithm presented among others in one of the papers referenced by the authors (Wierstra et al. 2009). In that paper from the Schmidhuber group the authors utilize LSTMs, instead of GRUs, but otherwise the rest is essentially the same, including the use of a variance reduction method to the estimate of the gradient of the policy function, consisting in a &quot;baseline value function&quot;. In addition, in both this paper and in Wierstra et al. 2009 the baseline value function is an RNNs (in the paper under review, the input to the value function is the hidden state of the policy network, but this seems arbitrary).</p><p>In fact both ideas – using backpropagation to estimate the policy gradient and using a baseline reinforcement function to reduce the variance of the gradient estimate – are already present in the original Williams 1992 paper (referenced by the authors). The second idea was however probably mostly popularized by a series of papers by Baxter and Bartlett (that apply policy gradients to solving POMDPs), and by the VAPS (Value and Policy Search) algorithm by Baird and More (1999). I feel that the authors should cite these papers. I also suggest that the authors explicitly say that they're using (a modified version of) the &quot;recurrent policy gradient&quot; algorithm presented in the series of papers by Wierstra et al. Since the algorithm already has a name, it might be beneficial to use it. That would also help make the mentioned connections across the literature most transparent.</p><p>Whilst there was clear agreement that the biologically-realistic training regime opened up the possibility of asking new biological questions with the network, and that this was exciting, there was also clear agreement that the biological questions and predictions that you have actually made did not lead to the clear new biological insights that the reviewers were expecting. For example, comments from the reviews that spoke to this issue were:</p><p>I found the Results section somewhat weak – the Discussion arrived at the point in the Results that I thought was only the end of the warm-up phase. Up to <xref ref-type="fig" rid="fig3">Figure 3</xref>, I thought this is all nice but it looks like a sanity check that their architecture does learn what it's supposed to learn, i.e. a sort of reproduction (albeit in an actor-critic architecture) of the results of Song et al. (2016) where the same authors had used stochastic gradient descent to train RNNs on the same family of tasks. It is only in <xref ref-type="fig" rid="fig4">Figure 4</xref> that the authors start inspecting the value network. In light of what I wrote above (namely, knowing that the learning implementation isn't realistic, and that reward prediction is already known to help a lot in policy gradient learning), this was disappointing to me. I was expecting the authors to use their trained value networks to make more specific, experimentally testable predictions about the type of signals you expect to see (and where), the form of synergy predicted between the progress of learning in the task and the quality of reward predictions (by looking at the synergies in the simultaneous training of both nets), etc.</p><p>The main empirical observation from the section &quot;Tasks with simple input-output mappings&quot;, apart from the fact that the network learns the tasks, is that it exhibits mixed selectivity. This mixed selectivity observation shows up in several of the other sections, but I feel that it is not a particularly strong argument for this model. Many models could produce mixed selectivity, and in any case the prevalence of mixed selectivity is never quantified. Is this really the only empirical constraint from the neural data?</p><p>In general, need more explanation for <italic>why</italic> the network reproduces particular empirical phenomena. Which assumptions provide explanatory power? If one were to deviate from these assumptions, would the model no longer reproduce these phenomena? What is <italic>uniquely</italic> explained by this model?</p><p>The authors mention that the model allows the Markov property to be relaxed. This seems like an important observation, but there's no demonstration of this in simulations. What empirical phenomena speak to this issue?</p><p>The reward baseline idea is interesting, and the authors mention some empirical data possibly consistent with this idea, but they don't report any simulations of lesions, inactivation or pharmacological manipulations to reproduce these effects with their model.</p><p>All three reviewers reiterated this point as essential in the Discussion. The basic point is that <italic>eLife</italic> is a biology journal, not a machine learning journal. It needs to be clearer how the new ML advances have led to substantial new biological insight. There was also a clear suggestion in the Discussion that a major advantage of having a network that learns from rewards is the potential to analyse the dynamics of learning itself; the potential to elucidate the limits of task learnability under sparse delayed rewards, and to predict specific patterns of interaction between the learning of the task and the learning of the reward landscape. The reviewers thought that one potential avenue to strengthen the Results section was to focus on these dynamics.</p><p>There was also a discussion about deference to the existing literature. You can see several points above that demonstrate concerns along these lines. A further point was also raised:</p><p>The argument that actor-critic/policy gradient models are &quot;opposite of the way value functions are usually thought of in neuroscience&quot; (Discussion) seems extreme, since this only applies to value-based model-free algorithms like Q-learning and sarsa. But there is a long tradition of actor-critic models applied to the basal ganglia; see for example Houk et al. (1995), Dayan &amp; Balleine (2002), Joel et al. (2002), O'Doherty et al. (2004), Takahashi et al. (2008), Maia (2010), to name a few.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.21492.021</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>Despite the laudable aspects of the work described above, the reviewers unanimously agreed that more needs to be done to demonstrate that the approach will lead to clear new biological knowledge.</italic> </p><p><italic>All three reviewers raised the issue that it is not clear that the network learning routine is novel from the machine learning perspective, and yet it is completely clear that assumptions made in the model render it only a distant approximation to a biological learning rule. Hence, the reviewers thought that the manuscript needed to be reframed to be clearer what the contribution was. Less emphasis should be placed on the biological realism of the network, and more emphasis should be placed on the biological utility of training a network based on rewards.</italic> </p><p>We hope that the revised manuscript allays some of these concerns, please see below for more detailed responses to the editor and reviewers’ comments.</p><p>Here we note two points that were partially raised in the original manuscript and have now been expanded for emphasis and clarification:</p><p>We distinguish a learning rule and the neural activity that results from it. The plasticity rule used in this work is not biologically plausible in the sense that it is not one of the known synaptic plasticity rules. While several recent works have made progress toward biologically plausible backpropagation (or doing without BP altogether), we never considered this to be a point of debate. At present there is simply no biologically plausible learning rule capable of training networks for such tasks, in the same manner. What we argue is not that the physiological learning rule is biological, but rather that the resulting circuits, trained in an ethologically relevant manner, operate in a similar way to neural circuits in the brain as demonstrated by neural activity traces that are quite similar to those recorded from behaving animals. This is a nontrivial, even surprising, result, as we can think of no simple reason why this had to be [a point also raised in Zipser &amp; Andersen (1988) for the supervised learning case].</p><p>We did not invent the core learning algorithm used in this work, namely recurrent policy gradients with a baseline value network (responsible for computing expected values). Indeed, in some cases we opted not to include state-of-the-art techniques such as Asynchronous Advantage Actor-Critic (Mnih et al., 2016) due to biological <italic>impossibility</italic> (rather than implausibility). What is novel is the application of these techniques, with a few modifications (that are novel), to tasks relevant to systems neuroscience, particularly to the combined study of behavior and electrophysiology for a wide range of tasks. We have therefore reframed the overall paper around its main contributions as laid out at the beginning of the Discussion, to wit: “In this work we have demonstrated reward-based training of recurrent neural networks for both cognitive and value-based tasks. Our main contributions are twofold: first, our work expands the range of tasks and corresponding neural mechanisms that can be studied by analyzing model recurrent neural networks, providing a unified setting in which to study diverse computations and compare to electrophysiological recordings from behaving animals; second, by explicitly incorporating reward into network training, our work makes it possible in the future to more directly address the question of reward-related processes in the brain, for instance the role of value representation that is essential for learning, but not executing, a task.”</p><p><italic>Comments from the reviews that address this point are as follows:</italic> </p><p><italic>I am having problems assessing novelty. If the main novelty is to provide the first neural implementation of the standard &quot;actor-critic&quot; paradigm, then the authors would need to provide an implementation where learning (both in the actor and in the critic) happens through plausible learning rules, which is clearly not the case here (use of backpropagation through time as well as Gated Recurrent Units; the authors acknowledge these limitations in the Discussion). It is not clear that the problem remains learnable (even with the same &quot;teaching signals&quot;) under more realistic assumptions. If, on the other hand, the aim was to argue that jointly training a critic facilitates learning of the actor, the authors did not need a neural implementation to make that point, and in fact, it is already well known that policy-gradient methods benefit immensely from variance reduction through fine-grained reward prediction. So, this leaves us with a third alternative message, which is to use these insights + their (non-plausible) neural implementations to try and map the algorithm (as opposed to a particular implementation) to specific brain structures/processes by relating model activity to recorded brain activity and behaviour. I think this could have been the main strength of this paper, but is somehow underplayed (in fact, inspection of the activity of the value network is limited to a single page).</italic> </p><p>We agree that this is not the first neural network implementation of the “actor-critic” architecture – many variations of actor-critic are routinely used in machine learning, and as addressed further below, the concept of actor-critic has a rich history in neuroscience, particularly in models of the basal ganglia. We did not intend in any way to imply otherwise, and we have reframed and expanded portions of the manuscript to make this as clear as possible. Again, what is novel is to demonstrate a reinforcement learning framework of training RNNs to perform a number of cognitive and value-based tasks, which is of wide interest to systems neuroscience.</p><p>We have added the output of the value network (the expected reward) to <xref ref-type="fig" rid="fig1">Figure 1</xref> to show how the expected return/confidence is computed by performing an “absolute value”-like operation on the accumulated evidence. As we note in an expanded Discussion, however, it is also common in machine learning applications for the policy and value networks to share the same recurrent units, by using a weighted sum of the two losses (reward maximization and reward prediction error) to jointly train the two using a single loss. The question of whether the decision and value networks ought to share the same recurrent network parallels ongoing debate over whether choice and confidence are computed together, or if certain areas such as OFC compute confidence signals locally, and we view this problem as currently unresolved. Computationally, the distinction is expected to be important when there are nonlinear computations required to determine expected return that are not needed to implement the policy.</p><p>We were less focused on the <italic>learnability</italic> problem in this situation because there is no question that the tasks can be learned by animals under similar reward conditions, and we assumed that a sufficiently powerful RL algorithm should therefore be able to replicate this learning. We believe that in the future researchers will elucidate the corresponding mechanism in the brain.</p><p><italic>The network is quasi-biological in the sense that it is technically a neural network, but the biological constraints on architecture, dynamics and synaptic plasticity are really weak. Should we see this as essentially a phenomenological model, or is there deeper significance to the &quot;neural&quot; aspect? If the latter, then I think a stronger case needs to be made.</italic> </p><p>We agree that the biological constraints are weak, particularly the learning rule, but in our experience it would be a mistake to completely discard the “neural” aspect and view trained RNNs as a purely phenomenological model – after all, the activity of individual units in our trained networks exhibit strikingly similar features to biological neurons recorded in experiments. For instance, it is quite surprising that, after training with a value-based choice task, three major types of units (offer value, chosen value, and choice) found in OFC physiology naturally emerged in our network (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>Since no known biologically realistic learning rule exists with the flexibility to learn all the tasks explored in this work, our goal was to use this as a starting point for future development of increasingly biologically realistic models that can still perform the task.</p><p><italic>My main criticism with the paper is that some presentation choices might mislead some readers into thinking that the application of the policy gradient method to recurrent neural networks trained with backpropagation through time is novel. In fact, the training method used in this paper is essentially the &quot;recurrent policy gradient&quot; algorithm presented among others in one of the papers referenced by the authors (Wierstra et al. 2009). In that paper from the Schmidhuber group the authors utilize LSTMs, instead of GRUs, but otherwise the rest is essentially the same, including the use of a variance reduction method to the estimate of the gradient of the policy function, consisting in a &quot;baseline value function&quot;. In addition, in both this paper and in Wierstra et al. 2009 the baseline value function is an RNNs (in the paper under review, the input to the value function is the hidden state of the policy network, but this seems arbitrary).</italic> </p><p><italic>In fact both ideas</italic> – <italic>using backpropagation to estimate the policy gradient and using a baseline reinforcement function to reduce the variance of the gradient estimate</italic> – <italic>are already present in the original Williams 1992 paper (referenced by the authors). The second idea was however probably mostly popularized by a series of papers by Baxter and Bartlett (that apply policy gradients to solving POMDPs), and by the VAPS (Value and Policy Search) algorithm by Baird and More (1999). I feel that the authors should cite these papers. I also suggest that the authors explicitly say that they're using (a modified version of) the &quot;recurrent policy gradient&quot; algorithm presented in the series of papers by Wierstra et al. Since the algorithm already has a name, it might be beneficial to use it. That would also help make the mentioned connections across the literature most transparent.</italic> </p><p>It was not in any way our intention to imply that this was the first application of policy gradients to RNNs, and it was a big mistake on our part to have taken this to be obvious. We have changed the exposition to make this much clearer, and included the suggested citations. Please note that we were using Peters &amp; Schall (2008) as a sort of review of the GPOMDP method in addition to the Williams derivation. In any case, we now explicitly say that our work is based on Wierstra’s recurrent policy gradient, which again we felt was clear (please note that we used this term explicitly in the Discussion of the original manuscript) but could have been, and has been made, clearer.</p><p>We do think, however, that using the hidden state of the policy/decision network as inputs to the value network is 1) not completely arbitrary and 2) is, in fact, novel from the machine learning perspective. It is similar in spirit to jointly training the policy and value networks as one network with a single, combined loss (reward maximization and reward prediction error), while nevertheless treating the two networks separately. Of course, this was not explained well in the manuscript and we have also included additional discussion of this matter. In particular, we now point out how different choices for this architecture parallel ongoing debate over the question of whether decision and confidence are jointly computed in one brain area and read out by another, or if confidence can be computed locally by a different region from signals in the decision area. Within neuroscience this question remains unresolved, and we hope that our approach, and generalizations based on it, can help shed light on the matter.</p><p><italic>Whilst there was clear agreement that the biologically-realistic training regime opened up the possibility of asking new biological questions with the network, and that this was exciting, there was also clear agreement that the biological questions and predictions that you have actually made did not lead to the clear new biological insights that the reviewers were expecting. For example, comments from the reviews that spoke to this issue were:</italic> </p><p><italic>I found the Results section somewhat weak – the Discussion arrived at the point in the Results that I thought was only the end of the warm-up phase. Up to <xref ref-type="fig" rid="fig3">Figure 3</xref>, I thought this is all nice but it looks like a sanity check that their architecture does learn what it's supposed to learn, i.e. a sort of reproduction (albeit in an actor-critic architecture) of the results of Song et al. (2016) where the same authors had used stochastic gradient descent to train RNNs on the same family of tasks. It is only in <xref ref-type="fig" rid="fig4">Figure 4</xref> that the authors start inspecting the value network. In light of what I wrote above (namely, knowing that the learning implementation isn't realistic, and that reward prediction is already known to help a lot in policy gradient learning), this was disappointing to me. I was expecting the authors to use their trained value networks to make more specific, experimentally testable predictions about the type of signals you expect to see (and where), the form of synergy predicted between the progress of learning in the task and the quality of reward predictions (by looking at the synergies in the simultaneous training of both nets), etc.</italic> </p><p><italic>The main empirical observation from the section &quot;Tasks with simple input-output mappings&quot;, apart from the fact that the network learns the tasks, is that it exhibits mixed selectivity. This mixed selectivity observation shows up in several of the other sections, but I feel that it is not a particularly strong argument for this model. Many models could produce mixed selectivity, and in any case the prevalence of mixed selectivity is never quantified. Is this really the only empirical constraint from the neural data?</italic> </p><p><italic>In general, need more explanation for why the network reproduces particular empirical phenomena. Which assumptions provide explanatory power? If one were to deviate from these assumptions, would the model no longer reproduce these phenomena? What is uniquely explained by this model?</italic> </p><p><italic>The authors mention that the model allows the Markov property to be relaxed. This seems like an important observation, but there's no demonstration of this in simulations. What empirical phenomena speak to this issue?</italic> </p><p>The section on “Tasks with simple input-output mappings” was indeed meant as a sanity check, to confirm that any task that was previously trained using supervised learning could also be learned based on reward only. Please note a modified <xref ref-type="fig" rid="fig1">Figure 1</xref> to show the expected return predicted by the value network during the perceptual decision-making task as a function of time, which was previously <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>. It shows that the expected reward can be computed by an “absolute value”-like operation on the accumulated evidence to compute the expected reward. (It is not simply an absolute value because it also requires a shift, etc.). However, the confidence experiment (<xref ref-type="fig" rid="fig3">Figure 3</xref>) is new, not simulated in our previous work using supervised learning.</p><p>The tasks considered in this work are in some sense too simple for strong conclusions to be drawn about the role of the value network and we have avoided doing so, although we clearly speculate on possibilities that are of interest for future investigation. Moreover, we found the value network in the economic choice task of some interest precisely because electrophysiologists have recorded from the OFC of monkeys performing a similar task, but it is quite rare that PFC/PPC and OFC are recorded simultaneously so that we had little experimental constraint to work with – a situation that will, we hope, change in the future.</p><p>The relaxation of the Markov assumption is more relevant to ongoing work on certain games but we wanted to repeat this point, which was already made in Wierstra et al. (2009). We removed this comment to avoid confusion.</p><p><italic>The reward baseline idea is interesting, and the authors mention some empirical data possibly consistent with this idea, but they don't report any simulations of lesions, inactivation or pharmacological manipulations to reproduce these effects with their model.</italic> </p><p><italic>All three reviewers reiterated this point as essential in the Discussion. The basic point is that eLife is a biology journal, not a machine learning journal. It needs to be clearer how the new ML advances have led to substantial new biological insight. There was also a clear suggestion in the Discussion that a major advantage of having a network that learns from rewards is the potential to analyse the dynamics of learning itself; the potential to elucidate the limits of task learnability under sparse delayed rewards, and to predict specific patterns of interaction between the learning of the task and the learning of the reward landscape. The reviewers thought that one potential avenue to strengthen the Results section was to focus on these dynamics.</italic> </p><p>We were very mindful of the fact that <italic>eLife</italic> is a biology journal, and, importantly, that the primary intended audience of our work were neuroscientists. It would be a stretch to say that current research on RL in machine learning can be applied to neuroscience wholesale; we believe, however, that judiciously applying those methods and adapting the parts we can to be even incrementally more biologically plausible is extremely useful, and that was the main goal of our paper. We were less focused on the learnability problem in this situation because there is no question that the tasks can be learned by animals under similar reward conditions, and we assumed that a sufficiently powerful RL algorithm should therefore be able to replicate this learning.</p><p>We are very interested in comparing the dynamics of learning in our networks to animals under experimental conditions; however, we simply do not have the necessary data for meaningful comparison. We are working with experimental collaborators to make this a reality. We agree that this is a major advantage of the RL framework.</p><p><italic>There was also a discussion about deference to the existing literature. You can see several points above that demonstrate concerns along these lines. A further point was also raised:</italic> </p><p><italic>The argument that actor-critic/policy gradient models are &quot;opposite of the way value functions are usually thought of in neuroscience&quot; (Discussion) seems extreme, since this only applies to value-based model-free algorithms like Q-learning and sarsa. But there is a long tradition of actor-critic models applied to the basal ganglia; see for example Houk et al. (1995), Dayan &amp; Balleine (2002), Joel et al. (2002), O'Doherty et al. (2004), Takahashi et al. (2008), Maia (2010), to name a few.</italic> </p><p>This was at best a sloppy sentence; the sentiment expressed here merely reflects the first author’s own (cortical) bias in interacting with neuroscientists working on reinforcement learning. We are of course aware that actor-critic models have a long and distinguished history in neuroscience, especially in work on basal ganglia. We have added the suggested references along with a sentence in the Discussion acknowledging this history, and removed the problematic phrase in question.</p></body></sub-article></article>