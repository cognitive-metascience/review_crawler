<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47686</article-id><article-id pub-id-type="doi">10.7554/eLife.47686</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The representational space of observed actions</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-138275"><name><surname>Tucciarelli</surname><given-names>Raffaele</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0342-308X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-138307"><name><surname>Wurm</surname><given-names>Moritz</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4358-9815</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-138308"><name><surname>Baccolo</surname><given-names>Elisa</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2527-1953</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-138278"><name><surname>Lingnau</surname><given-names>Angelika</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8620-3009</contrib-id><email>angelika.lingnau@psychologie.uni-regensburg.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Psychology</institution><institution>Royal Holloway University of London</institution><addr-line><named-content content-type="city">Egham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Center for Mind/Brain Sciences (CIMeC)</institution><institution>University of Trento</institution><addr-line><named-content content-type="city">Rovereto</named-content></addr-line><country>Italy</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Institute of Psychology</institution><institution>University of Regensburg</institution><addr-line><named-content content-type="city">Regensburg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Lescroart</surname><given-names>Mark</given-names></name><role>Reviewing Editor</role><aff><institution>University of Nevada at Reno</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>05</day><month>12</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e47686</elocation-id><history><date date-type="received" iso-8601-date="2019-04-14"><day>14</day><month>04</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-11-21"><day>21</day><month>11</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Tucciarelli et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Tucciarelli et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47686-v1.pdf"/><abstract><p>Categorizing and understanding other people’s actions is a key human capability. Whereas there exists a growing literature regarding the organization of objects, the representational space underlying the organization of observed actions remains largely unexplored. Here we examined the organizing principles of a large set of actions and the corresponding neural representations. Using multiple regression representational similarity analysis of fMRI data, in which we accounted for variability due to major action components (body parts, scenes, movements, objects, sociality, transitivity) and three control models (distance between observer and actor, number of people, HMAX-C1), we found that the semantic dissimilarity structure was best captured by patterns of activation in the lateral occipitotemporal cortex (LOTC). Together, our results demonstrate that the organization of observed actions in the LOTC resembles the organizing principles used by participants to classify actions behaviorally, in line with the view that this region is crucial for accessing the meaning of actions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>action categories</kwd><kwd>action recognition</kwd><kwd>representational similarity analysis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Li 2840/1-1</award-id><principal-award-recipient><name><surname>Lingnau</surname><given-names>Angelika</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Li 2840/2-1</award-id><principal-award-recipient><name><surname>Lingnau</surname><given-names>Angelika</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Representational similarity analysis of human functional magnetic resonance imaging data demonstrates that the lateral occipitotemporal cortex represents action knowledge along dimensions that are in accordance with behavioural judgements.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans can perform and recognize a striking number of different types of actions, from hammering a nail to performing open heart surgery. However, most of what we know about the way we control and recognize actions is based on a rich literature on prehension movements in humans and non-human primates. This literature revealed a widespread network of fronto-parietal regions, with a preference along the dorso-medial stream and the dorso-lateral stream for reaching and grasping movements, respectively (for reviews see e.g. <xref ref-type="bibr" rid="bib13">Culham and Valyear, 2006</xref>; <xref ref-type="bibr" rid="bib55">Rizzolatti and Matelli, 2003</xref>; <xref ref-type="bibr" rid="bib63">Turella and Lingnau, 2014</xref>). Less is known regarding the organization of more complex actions (for exceptions, see <xref ref-type="bibr" rid="bib1">Abdollahi et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Jastorff et al., 2010</xref>; <xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>). According to which principles are different types of actions organized in the brain, and do these principles help us understand how we are able to tell that two actions, for example running and riding a bike, are more similar to each other than two other actions, for example riding a bike and reading a book? Are observed actions that we encounter on a regular basis organized according to higher-level semantic categorical distinctions (e.g. between locomotion, object manipulation, communication actions) and further overarching organizational dimensions? Note that higher-level semantic categories often covary with more basic action components (such as body parts, movement kinematics, and objects) of perceived action scenes. As an example, locomotion actions tend to involve the legs, consist of repetitive movements, can involve vehicles such as a bike and often take place outdoors, whereas communicative actions often involve mouth/lip movements, consist of small movements, can involve objects such as a mobile phone, and can take place in a variety of different scenes. Disentangling these levels neurally presents an analytical challenge that has not been addressed so far.</p><p>A number of recent studies used multivariate pattern analysis (MVPA) (<xref ref-type="bibr" rid="bib24">Haxby et al., 2001</xref>) to examine which brain areas are capable to distinguish between different observed actions (e.g. opening vs closing, slapping vs lifting an object, or cutting vs peeling) (<xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Wurm et al., 2016</xref>; <xref ref-type="bibr" rid="bib71">Wurm and Lingnau, 2015</xref>; <xref ref-type="bibr" rid="bib21">Hafri et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Oosterhof et al., 2010</xref>; <xref ref-type="bibr" rid="bib48">Oosterhof et al., 2012</xref>). The general results that emerged from these studies is that it is possible to distinguish between different actions on the basis of patterns of brain activation in the lateral occipito-temporal cortex (LOTC), the inferior parietal lobe (IPL) and the ventral premotor cortex (PMv). In line with this view, LOTC has been shown to contain action-related object properties (<xref ref-type="bibr" rid="bib7">Bracci and Peelen, 2013</xref>). LOTC and IPL, but not the PMv, furthermore showed a generalization across the way in which these actions were performed (e.g. performing the same action with different kinematics), suggesting that these areas represent actions at more general levels and thus possibly the meaning of the actions (<xref ref-type="bibr" rid="bib71">Wurm and Lingnau, 2015</xref>). However, previous studies could not unambiguously determine what kind of information was captured from observed actions: movement trajectories and body postures (<xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Oosterhof et al., 2010</xref>; <xref ref-type="bibr" rid="bib48">Oosterhof et al., 2012</xref>), certain action precursors at high levels of generality (<xref ref-type="bibr" rid="bib71">Wurm and Lingnau, 2015</xref>) (e.g. object state change), or more complex semantic aspects that go beyond the basic constituents of perceived actions and that represent the meaning of actions at higher integratory levels. In the latter case, the LOTC and the IPL should also reflect the semantic similarity structure of a wide range of actions: Running shares more semantic aspects with riding a bike than with reading; therefore, semantic representations of running and riding a bike should be more similar with each other than with the semantic representation of reading.</p><p>To determine the structure of the similarity in meaning (which we will refer to as <italic>semantic similarity</italic> in the remainder of this paper), we used inverse multidimensional scaling (MDS) (<xref ref-type="bibr" rid="bib33">Kriegeskorte and Mur, 2012</xref>) of a range of different actions (<xref ref-type="fig" rid="fig1">Figure 1</xref>). To test the prediction that the LOTC and the IPL reflect the semantic similarity structure determined behaviorally (using inverse MDS), we carried out an fMRI study in the same group of participants. To control for action components that often covary with action semantics, we carried out inverse MDS in the same group of participants for the similarity of actions with respect to (a) the body parts involved in the action (body model), (b) the scenes in which these actions typically take place (scene model), (c) movement kinematics involved in the actions (movement model), and (d) objects involved in these actions (object model).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimuli and behavioral task.</title><p>(<bold>A</bold>) Stimuli used in the behavioral and the fMRI experiment, depicting static images of 28 everyday actions. To increase perceptual variability, actions were shown from different viewpoints, in different scenes, using two different actors (see text for details) and different objects (for actions involving an object). For a full set of stimuli used in the fMRI experiment, see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. (<bold>B</bold>) Illustration of the behavioral experiment used for inverse multidimensional scaling. In the first trial of the experiment, participants were presented with an array of images arranged on the circumference of a gray circle (left panel). In each subsequent trial, an adaptive algorithm determined a subset of actions that provided optimal evidence for the pairwise dissimilarity estimates (see <xref ref-type="bibr" rid="bib33">Kriegeskorte and Mur, 2012</xref> and <italic>Materials and methods</italic>, for details). In different parts of the experiment, participants were asked to rearrange the images according to their perceived similarity with respect to a specific aspect of the action, namely, their meaning (or semantics), the body part(s) involved, the scene/context in which the action typically takes place, movement kinematics, and objects involved in the action. Right panel: Example arrangement resulting from the semantic task of one representative participant. Using inverse multidimensional scaling, we derived a behavioral model (see <xref ref-type="fig" rid="fig2">Figure 2</xref>) from this arrangement, individually for each participant, that we then used for the representational similarity analysis to individuate those brain regions that showed a similar representational geometry (for details, see <italic>Materials and methods</italic> section).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>All images used in the fMRI experiment.</title><p>Different actions are depicted in rows (for corresponding labels, see words printed in bold font in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Different exemplars for each action are shown in the columns. Exemplars varied in terms of the actor (2), scene (2), and viewing angle (3).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig1-figsupp1-v1.tif"/></fig></fig-group><p>To be able to relate our results to a previous study (<xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>) explicitly comparing actions directed towards an object and actions directed towards another person, we included two additional models capturing the <italic>transitivity</italic> and <italic>sociality</italic> of the actions, respectively (see Materials and methods, section <italic>Construction of Representational Dissimilarity Matrices</italic>, for details). Moreover, to account for the fact that some of the images depicting actions were photographed at a shorter distance to the actor(s) than other images, and since some actions included two actors, whereas the majority of actions depicted one actor only, we included additional models capturing the distance to the actor (near, medium, far) and the number of actors involved in the action (one vs two; see Materials and methods, section <italic>Construction of Representational Dissimilarity Matrices</italic> for details). Finally, to be able to rule out that any differences obtained in the RSA are not due to low-level differences between the actions, we included the second level (C1) of the HMAX model (<xref ref-type="bibr" rid="bib54">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib58">Serre et al., 2007</xref>) (see Methods, section <italic>Construction of Representational Dissimilarity Matrices</italic>, for details).</p><p>In the fMRI study, we examined which brain regions capture the semantic similarity structure determined in the behavioral experiment, using representational similarity analysis (<xref ref-type="bibr" rid="bib31">Kriegeskorte et al., 2008a</xref>) (RSA). Moreover, to examine which brain areas capture semantic similarity over and beyond the action components described above, and to control for potential confounds and low level differences in the stimulus material, we carried out a multiple regression RSA for each of the models while accounting for all the remaining models (see Materials and methods, section <italic>Representational Similarity Analysis</italic>, for details).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral</title><sec id="s2-1-1"><title>Inverse multidimensional scaling experiment (Behavioral)</title><p>To obtain representational dissimilarity matrices (RDMs) for the semantic model, we extracted the pairwise Euclidean distances from the participants’ inverse MDS arrangements (see Materials and methods, section <italic>Inverse Multidimensional Scaling</italic>, for details). <xref ref-type="fig" rid="fig2">Figure 2</xref> shows the resulting RDM (averaged across participants).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Behavioral Representational Dissimilarity Matrix for the semantic model.</title><p>Bluish colors indicate high similarity between pairwise combinations of actions, whereas reddish colors indicate high dissimilarity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Pairwise cross-correlation matrix across models.</title><p>For each participant, we computed the pairwise cross-correlation matrix across models. We then computed (<bold>A</bold>) the averaged correlation values across participants; and reported (<bold>B</bold>) the maximum and (<bold>C</bold>) the minimum correlation values across participants. For a corresponding analysis of collinearity using the variance inflation factor, see Materials and methods, multiple regression RSA.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig2-figsupp1-v1.tif"/></fig></fig-group><p>We found significant (all p-values were smaller than p&lt;0.0001 and survived false discovery rate correction) inter-observer correlations, that is the individual RDMs significantly correlated with the average RDM of the remaining participants (mean leave-one-subject-out correlation coefficient: 0.61, min individual correlation coefficient: 0.46, max individual correlation coefficient: 0.78), suggesting that the participants’ arrangements were reliable and based on comparable principles.</p></sec></sec><sec id="s2-2"><title>Principal Component Analysis (PCA) and Clustering analysis: K-means</title><p>To better characterize the dimensions along which the actions were organized and the clusters resulting from inverse MDS for the semantic task, we carried out a K-means clustering analysis and a Principal Component Analysis (PCA; see Materials and methods for details). A Silhouette analysis (see Materials and methods and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) revealed that the optimal number of clusters for the semantic task was six. As can be seen in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, the first three components account for the largest amount of variance. For ease of visualization, we show the first two components in <xref ref-type="fig" rid="fig3">Figure 3</xref>. A visualization of the first three principal components is shown in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>. The analysis revealed clusters related to locomotion (e.g. biking, running), social/communicative actions (e.g. handshaking, talking on the phone), leisure-related actions (e.g. painting, reading), food-related actions (e.g. eating, drinking), and cleaning-related actions (e.g. showering, washing the dishes).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cluster analysis.</title><p>Clusters resulting from the K-means clustering analysis for the semantic task. The 2D-plot shows component 1 and 2, the corresponding labels of individual actions and the suggested labels for the categories resulting from the K-means clustering. For a visualization of the organization of these actions along the third component, which explained 23.4% of the variance, please refer to <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Results silhouette analysis.</title><p>Silhouette analysis revealed that the optimal number of clusters for the Semantic model was 6. The analysis was performed using the fviz_nbclust function of the R package ‘factoextra’. The silhouette width is a measure of how well each item fits within its cluster with higher values indicating better fit. The average silhouette width is the average of the all items. The algorithm computes the average silhouette width as a function of the number of clusters (k). The highest value of the average silhouette width is taken and the corresponding k is selected as the optimal number of clusters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Eigenvalues obtained from the PCA of the semantic model.</title><p>The PCA shows that the first three components accounted for the largest amount of variance.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Results cluster analysis.</title><p>Clusters resulting from the K-means clustering analysis for the semantic model. (<bold>A</bold>) 2D-plot showing component 1 and 2, corresponding labels of individual actions and suggested labels for the categories resulting from the K-means clustering. (<bold>B</bold>) 2D-plot showing component 2 and 3. (<bold>C</bold>) 2D-plot showing component 1 and 3. (<bold>D</bold>) 3D-plot showing components 1–3.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>First two principal components of the control models.</title><p>Clusters were distinguished using K-means clustering as implemented in R. Number of clusters used as input were 6 (based on the Silhouette analysis of the semantic model; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Colors are used to distinguish between the six resulting clusters in each analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig3-figsupp4-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>RSA</title><p>To identify neural representations of observed actions that are organized according to semantic similarity, we performed a searchlight correlation-based RSA using the semantic model derived from the behavioral experiment. We thereby targeted brain regions in which the similarity of activation patterns associated with the observed actions matches the participants’ individual behavioral semantic similarity arrangement. We identified significant clusters in bilateral LOTC extending ventrally into inferior temporal cortex, bilateral posterior intraparietal sulcus (pIPS), and bilateral inferior frontal gyrus/ventral premotor cortex (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Standard RSA, Semantic model.</title><p>Group results of the searchlight-based RSA using the semantic model (standard RSA, that is second order correlation between neural data and behavioral model). Statistical maps only show the positive t values that survived a cluster-based nonparametric analysis with Monte Carlo permutation (cluster stat: max sum; initial pval &lt;0.001; <xref ref-type="bibr" rid="bib59">Stelzer et al., 2013</xref>). The resulting individual correlation maps were first Fisher transformed and then converted to t scores. After the correction, data were converted to z scores, and only values greater than 1.65 (one-tailed test) were considered as significant. This analysis revealed clusters in bilateral LOTC, bilateral IPL and bilateral precentral gyrus (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for details).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig4-v1.tif"/></fig><p>Given that a number of action components covary to some extent with semantic features (e.g. locomotion actions typically take place outdoors, cleaning-related actions involve certain objects, etc.; see also <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), it is impossible to determine precisely what kind of information drove the RSA effects in the identified regions on the basis of the correlation-based RSA alone. Hence, to test which brain areas contain action information in their activity patterns that can be predicted by the semantic model over and above the models for different action components and the three control models, we conducted a multiple regression RSA. We hypothesized that if actions were organized predominantly according to action components (captured in the body, scene, movement, object, sociality and transitivity models) or due to low level differences between conditions (captured in the distance, 1 vs 2 people and HMAX-C1 model; see <xref ref-type="fig" rid="fig5">Figure 5</xref> and section <italic>Construction of Representational Dissimilarity Matrices</italic> for details), this analysis should not reveal any remaining clusters. Therefore, the multiple regression RSA included ten predictors (semantic, body, scene, movement, object, sociality, transitivity, distance, 1 vs 2 people, HMAX-C1).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Model Representational Dissimilarity Matrices.</title><p>Group representational dissimilarity matrices for body, scene, movement, and object model, derived from inverse MDS carried out in the same group of participants after the fMRI experiment. The sociality, transitivity and distance model were based on ratings in a separate group of participants. For construction of the 1 vs 2 people and the HMAX-C1 model, and further details on the construction of the remaining models, see Materials and methods, section Construction of Representational Dissimilarity Matrices. Bluish colors indicate high similarity between pairwise combinations of actions, whereas reddish colors indicate high dissimilarity. For ease of comparison, we used the ordering of actions resulting from the semantic task (see <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig5-v1.tif"/></fig><p>As can be seen in <xref ref-type="fig" rid="fig6">Figure 6</xref>, the semantic model explained significant amounts of variance over and above the six models for the different action components and the three control models in the left anterior LOTC at the junction to the posterior middle temporal gyrus (see also <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> and <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Multiple regression RSA (semantic model).</title><p>Group results of the searchlight-based multiple regression RSA, in which the ten different models (see <italic>Materials and methods</italic>, section <italic>Construction of Representational Dissimilarity Matrices</italic>, for details) were used as regressors in a multiple regression RSA conducted at the individual level. The resulting beta estimates were converted to t scores across participants and then corrected for multiple comparisons using cluster-based nonparametric permutation analysis (<xref ref-type="bibr" rid="bib59">Stelzer et al., 2013</xref>) (see <italic>Materials and methods</italic> for details). Accounting for the behavioral (body, scene, movement, object, sociality, transitivity) and the control models (distance, 1 vs 2 People, HMAX-C1) in the multiple regression analysis, the semantic model explained observed variance in the left LOTC. For ease of comparison, the black outlines in the bottom part of the figure (flat maps) depict the clusters revealed by the standard RSA for the semantic model (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Multiple regression based RSA results, together with Glasser parcellation.</title><p>Significant clusters resulting from the searchlight multiple-regression RSA (see <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig7">7</xref>), with Glasser brain parcellation (<xref ref-type="bibr" rid="bib20">Glasser et al., 2016</xref>) superimposed on the statistical maps. The top panel shows the cluster revealed by the semantic model, and labels of ROIs as reported in <xref ref-type="bibr" rid="bib20">Glasser et al. (2016)</xref>. The remaining panels show the significant clusters explained by the other models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Results for the body model, together with EBA coordinates.</title><p>Cluster identified by the multiple regression-based RSA using the body model, with coordinates of the extrastriate body area (EBA) reported by a number of studies that used a functional localizer to identify this region. The coordinates for some studies (<xref ref-type="bibr" rid="bib14">Downing et al., 2001</xref>; <xref ref-type="bibr" rid="bib15">Downing et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Hodzic et al., 2009</xref>; <xref ref-type="bibr" rid="bib16">Downing et al., 2007</xref>; <xref ref-type="bibr" rid="bib52">Peelen and Downing, 2005</xref>; <xref ref-type="bibr" rid="bib51">Peelen et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Astafiev et al., 2004</xref>; <xref ref-type="bibr" rid="bib28">Jastorff and Orban, 2009</xref>; <xref ref-type="bibr" rid="bib29">Kontaris et al., 2009</xref>; <xref ref-type="bibr" rid="bib4">Atkinson et al., 2012</xref>; <xref ref-type="bibr" rid="bib61">Taylor et al., 2010</xref>) were taken from <xref ref-type="bibr" rid="bib17">Ferri et al. (2013)</xref>. The remaining foci were taken from <xref ref-type="bibr" rid="bib36">Ma et al. (2018)</xref>, <xref ref-type="bibr" rid="bib43">Myers and Sowden (2008)</xref>, <xref ref-type="bibr" rid="bib26">Hummel et al. (2013)</xref> and <xref ref-type="bibr" rid="bib2">Aleong and Paus (2010)</xref>. As shown on the inflated maps, EBA in both hemispheres is typically reported in a region of the posterior LOTC which is more ventral than the cluster that we observed in our study. However, In the left hemisphere, three of the selected studies (<xref ref-type="bibr" rid="bib14">Downing et al., 2001</xref>; <xref ref-type="bibr" rid="bib36">Ma et al., 2018</xref> and <xref ref-type="bibr" rid="bib43">Myers and Sowden, 2008</xref>) reported the EBA foci within or near the dorsal sector of the cluster reported in the current study. Furthermore, in the right hemisphere, two of the selected studies (<xref ref-type="bibr" rid="bib16">Downing et al., 2007</xref> and <xref ref-type="bibr" rid="bib36">Ma et al., 2018</xref>) also reported coordinates for EBA within the right cluster found in the current study.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig6-figsupp2-v1.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig7">Figure 7</xref> shows the results of the multiple regression RSA for the other models (for a visualization of the same results together with the outlines of the Glasser multi-modal parcellation [<xref ref-type="bibr" rid="bib20">Glasser et al., 2016</xref>] superimposed on the flat maps, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). All models except the sociality and the scene model revealed significant clusters (see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for details). The clusters obtained for the body, movement distance, and the 1 vs 2 people models that explained significant amounts of variance over and above the remaining models partially overlapped with the cluster revealed by the semantic model (<xref ref-type="fig" rid="fig6">Figure 6</xref>; outlines superimposed in black in <xref ref-type="fig" rid="fig7">Figure 7</xref> for ease of comparison). For the body model (<xref ref-type="fig" rid="fig7">Figure 7A</xref>), the cluster was obtained predominantly in the left angular gyrus, slightly more dorsal than the cluster revealed by the semantic model, and the right pMTG. Note that the cluster revealed by the body model in the left hemisphere was more dorsal than Extrastriate Body Area (EBA) (<xref ref-type="bibr" rid="bib14">Downing et al., 2001</xref>) (for a comparison with coordinates revealed by previous studies that used a functional localizer to identify EBA, see <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). The movement model revealed clusters in the left SPL, the left inferior occipital gyrus (posterior to the cluster revealed by the semantic model) and the right MTG (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The object model explained variance in the right intraparietal sulcus area 1 (IPS1; <xref ref-type="bibr" rid="bib22">Hagler et al., 2007</xref>) and the left fusiform gyrus area (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). The transitivity model revealed a small cluster in the left inferior occipital gyrus, posterior to the cluster revealed by the semantic cluster (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). The distance (<xref ref-type="fig" rid="fig7">Figure 7E</xref>) and the 1 vs 2 people model (<xref ref-type="fig" rid="fig7">Figure 7F</xref>) revealed a number of clusters, mostly posterior to the cluster revealed by the semantic model, both in the left and right hemisphere (for a full set of labels and peak coordinates, see <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). The HMAX-C1 model revealed clusters (distinct from the cluster revealed by the semantic model) in the calcarine cortex (left hemisphere), the right occipital fusiform gyrus, the right inferior occipital gyrus, the right lingual gyrus and the left fusiform gyrus (<xref ref-type="fig" rid="fig7">Figure 7G</xref> and <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Searchlight-based multiple regression RSA.</title><p>Searchlight-based multiple regression RSA results for the body (<bold>A</bold>), movement (<bold>B</bold>), object (<bold>C</bold>), transitivity (<bold>D</bold>), distance (<bold>E</bold>), 1 vs 2 people (<bold>F</bold>) and the HMAX-C1 (<bold>G</bold>) model. The resulting beta estimates were converted to t scores across participants and then corrected for multiple comparisons using cluster-based nonparametric permutation analysis (<xref ref-type="bibr" rid="bib59">Stelzer et al., 2013</xref>) (see <italic>Materials and methods</italic> for details). Results for the scene and the sociality model did not survive corrections for multiple corrections and thus are not shown here. Black outlines on the inflated brains and the flat maps depict significant clusters revealed by the multiple regression RSA for the semantic mode (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig7-v1.tif"/></fig><p>Note that the obtained RSA results are unlikely to be due to some low-level features of the images we used. First, to minimize the risk that results could be driven by trivial pixel-wise perceptual similarities, we introduced a substantial amount of variability for each of the 28 actions, using different exemplars in which we varied the actor, the viewpoint, the background/scene (e.g. kitchen A vs kitchen B), and the object (for actions that involved one). Second, as described above, to account for low-level perceptual similarities, we included three control models (distance, one vs two people, HMAX-C1) in the multiple regression analysis. The resulting clusters that show the variance explained by these control models can be seen in <xref ref-type="fig" rid="fig7">Figure 7 (E–G)</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>.</p><p>To have a better idea of the representational geometry encoded in the left LOTC cluster, we extracted the beta estimates associated with the 100 features neighboring the vertex with the highest T score in the cluster in the left LOTC revealed by the multiple regression RSA for the semantic model. For this visualization, we used exactly the same steps involved in the searchlight-based standard RSA (see <italic>Materials and methods</italic> section for details). We derived the RDM from the beta patterns extracted from the ROI and then correlated this neural RDM with the behavioral model RDMs (semantic, body, scene, movement, object, sociality, transitivity) and the control model RDMs (distance, 1 vs 2 People, HMAX-C1). The averaged correlations between the model RDMs and the neural RDM in the LOTC obtained from this analysis are reported in <xref ref-type="fig" rid="fig8">Figure 8A</xref>. The bar plot confirmed that the semantic model is the model that best correlates with the neural RDM (shown in <xref ref-type="fig" rid="fig8">Figure 8B</xref>). Not surprisingly, also the other models significantly correlated with the neural RDMs, with the exception of the scene, sociality, object and HMAX-C1 models. Note that the averaged correlations were quite low (semantic model: 0.0645, distance model: 0.0608, 1vs2People: 0.0563; all other models &lt; 0.05), in line with a number of previous studies (e.g. <xref ref-type="bibr" rid="bib6">Bracci and Op de Beeck, 2016</xref>; <xref ref-type="bibr" rid="bib37">Magri et al., 2019</xref>). To visualize how the data are organized in a two-dimensional space and thus to better understand the underlying representational geometry encoded in the LOTC, we conducted a classical multidimensional scaling (MDS) analysis on the neural RDMs averaged across participants. As shown in <xref ref-type="fig" rid="fig8">Figure 8C</xref>, the organization of the neural multidimensional patters associated with each action indeed resembles the clustering organization we observed for the behavioral semantic task (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). To facilitate the comparison with <xref ref-type="fig" rid="fig3">Figure 3</xref>, we assigned actions with the same color code as the corresponding clusters identified from the behavioral analysis. A hierarchical clustering analysis showed a similar result (see <xref ref-type="fig" rid="fig8">Figure 8D</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Neural RDM from LOTC.</title><p>(<bold>A</bold>) Kendall rank correlation coefficient between neural and model RDMs in the cluster in the LOTC revealed by the multiple regression RSA using the semantic model (see Figure 6). Error bars depict the standard error of the mean. Asterisks indicate that the correlation was greater than zero (one-tailed t-test, FDR corrected). (<bold>B</bold>) Neural RDM from the LOTC used for the analysis shown in panel (<bold>A</bold>). Values were separately rank-transformed and then rescaled to values between 0 and 100, as suggested in <xref ref-type="bibr" rid="bib45">Nili et al. (2014)</xref>. (<bold>C</bold>) Two-dimensional visualization of the beta patterns of the cluster in the LOTC resulting from classical MDS. Actions were assigned to the clusters resulting from the behavioral analysis as shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>D</bold>). Dendrogram resulting from the hierarchical clustering analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-fig8-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here we aimed to investigate the organizational principles of everyday actions and the corresponding neural representations. Using inverse MDS (<xref ref-type="bibr" rid="bib33">Kriegeskorte and Mur, 2012</xref>), we identified a number of clusters emerging in the arrangement of actions according to their meaning that were relatively stable across participants. These clusters corresponded to meaningful categories, namely locomotion, communicative actions, food-related actions, cleaning-related actions, and leisure-related actions (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Using multiple regression RSA (<xref ref-type="bibr" rid="bib31">Kriegeskorte et al., 2008a</xref>), we showed that this semantic similarity structure of observed actions was best captured in the patterns of activation in the left LOTC.</p><p>PCA suggested that the five categories revealed in the behavioral experiment appear to be organized along three main components that together explained around 78% of the observed variance. Whereas neither the PCA nor the k-means clustering provides objective labels of the main dimensions that might underlie the organization of actions into categories, it appears that clusters corresponding to semantic categories along the first principal component differed with respect to the type of change induced by the action (negative side of component 1: change of location, positive side of component 1: change of external/physical state, middle: change of internal/mental state). The second component seemed to distinguish actions that fulfil basic (or physiological) needs such as eating, drinking, cutting, or getting from one place to the other, and actions that fulfil higher (social belonging, self-fulfillment) needs such as hugging, talking to someone, reading, listening to music). Interestingly, this distinction shows some similarity with Maslow’s hierarchy structure of needs (<xref ref-type="bibr" rid="bib39">Maslow, 1943</xref>). The third component might capture the degree to which an action is directed towards another person (hugging, holding hands, talking, etc.) or not (running, swimming, playing video games, reading). To the best of our knowledge, the only study that explicitly examined behavioral dimensions underlying the organization of actions focused on tools and the way they are typically used. Using ratings of the use of tools depicted as static images, <xref ref-type="bibr" rid="bib66">Watson and Buxbaum (2014)</xref> found that the two components that best explained the variability of their ratings of tool-related actions were related to ‘hand configuration’ and ‘magnitude of arm movement’.</p><p>In the language domain, there exists a rich literature on the semantic structure of action representations. As an example, based on the examination of the relationship between verb meaning (e.g. to break, to appear) and verb behavior (e.g. whether or not a verb can be used transitively), a number of authors including <xref ref-type="bibr" rid="bib60">Talmy (1985)</xref>, <xref ref-type="bibr" rid="bib34">Levin (1993)</xref>, and <xref ref-type="bibr" rid="bib53">Pinker (1989)</xref> aimed to reveal the underlying semantic structure of verbs (for related work on the use of semantic feature production norms see for example <xref ref-type="bibr" rid="bib64">Vinson and Vigliocco, 2008</xref>). Based on their analyses, these authors proposed a number of <italic>semantic categories</italic> (<xref ref-type="bibr" rid="bib60">Talmy, 1985</xref>) or <italic>semantic fields</italic> (<xref ref-type="bibr" rid="bib34">Levin, 1993</xref>). The latter include, but are not limited to, <italic>change of location, communication, cooking</italic>, and <italic>change of state</italic>, which show similarities to the action categories <italic>locomotion</italic>, <italic>communicative actions</italic>, <italic>food-related actions</italic> and <italic>cleaning-related actions</italic> revealed in the current study. It is less clear how to map the category we labeled <italic>leisure-related actions</italic> onto semantic fields proposed by <xref ref-type="bibr" rid="bib34">Levin (1993)</xref> and others. Whereas it is not surprising that action categories revealed on the basis of visual stimuli to some degree resemble semantic fields derived from cross-linguistic comparisons, it is likely that there exist action categories that can be revealed by visual stimuli but not by language, and vice versa (see also <xref ref-type="bibr" rid="bib64">Vinson and Vigliocco, 2008</xref>, and <xref ref-type="bibr" rid="bib66">Watson and Buxbaum, 2014</xref>, for related discussions on this topic). As an example, actions depicted by visual stimuli are concrete depictions, whereas actions described by words by definition are symbolic and thus more abstract representations of actions. Based on the existing literature on semantic categories, we expect that future studies, using a similar approach as described in the current study with a wider range of actions, will reveal additional categories and overarching dimensions that show similarities, but are not necessarily identical to the semantic categories described in the literature so far.</p><p>To identify brain regions that encoded the similarity patterns revealed by the behavioral experiment, we conducted a searchlight-based RSA. We observed a significant correlation between the semantic model and the pattern of fMRI data in regions of the so-called action observation network (<xref ref-type="bibr" rid="bib10">Caspers et al., 2010</xref>), which broadly includes the LOTC, IPL, premotor and inferior frontal cortex. Using multiple regression RSA, we found that only the left LOTC contains action information as predicted by the semantic model over and above the remaining models. In line with these results, it has been demonstrated that it is possible to discriminate between observed actions based on the patterns of activation in the LOTC, generalizing across objects and kinematics (<xref ref-type="bibr" rid="bib67">Wurm et al., 2016</xref>), and at different levels of abstraction (<xref ref-type="bibr" rid="bib71">Wurm and Lingnau, 2015</xref>). Interestingly, studies using semantic tasks on actions using verbal stimuli (<xref ref-type="bibr" rid="bib65">Watson et al., 2013</xref>) or action classification across videos and written sentences (<xref ref-type="bibr" rid="bib69">Wurm and Caramazza, 2019a</xref>) tend to recruit anterior portions of the left LOTC. By contrast, studies using static pictures (<xref ref-type="bibr" rid="bib21">Hafri et al., 2017</xref>) or videos depicting actions (<xref ref-type="bibr" rid="bib21">Hafri et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Wurm et al., 2016</xref>; <xref ref-type="bibr" rid="bib71">Wurm and Lingnau, 2015</xref>) find LOTC bilaterally and more posteriorly, closer to the cluster in the MOG identified in the current study. Note that a previous study by <xref ref-type="bibr" rid="bib21">Hafri et al. (2017)</xref>, directly comparing areas in which it is possible to decode between actions depicted by static pictures and by dynamic videos, found a far more widespread network of areas decoding actions depicted by videos in comparison to static pictures. We thus cannot rule out that more dynamic action components, for example movement kinematics, are not well captured in the current study. At the same time, <xref ref-type="bibr" rid="bib21">Hafri et al. (2017)</xref> demonstrated that it is possible to decode actions across stimulus format in the posterior LOTC, in line with the view that action-related representations in this area does not necessarily require visual motion. Together, our findings suggest that this area captures semantic aspects of actions at higher-order visual levels, whereas anterior portions of the left LOTC might capture these aspects at stimulus-independent or verbal levels of representation (see also <xref ref-type="bibr" rid="bib35">Lingnau and Downing, 2015</xref> and <xref ref-type="bibr" rid="bib50">Papeo et al., 2019</xref>).</p><p>Whereas the focus of the multiple regression RSA was to examine the results for the semantic model while accounting for the variability explained by the remaining models, it is interesting to compare the clusters revealed by the remaining models and how they relate to the semantic model. Several models (body, movement, distance, 1 vs 2 people) revealed clusters that were partially distinct from and partially overlapped with the cluster revealed by the semantic model. By contrast, the transitivity model revealed a cluster in the ventral portion of the LOTC that did not overlap with the semantic model. The LOTC has been shown to be sensitive to categorical action distinctions, such as whether they are directed towards persons or objects (<xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>). The results obtained for the transitivity model are in line with the results reported by <xref ref-type="bibr" rid="bib68">Wurm et al. (2017)</xref> and <xref ref-type="bibr" rid="bib70">Wurm and Caramazza (2019b)</xref>, whereas we failed to obtain reliable results for the sociality model. Note, however, that the current study was not designed to test this model. In particular, only a small number of actions were directed towards another person, and these actions co-varied with the presence of another person, which was captured by the 1 vs 2 people model (which showed comparable results to those obtained for the sociality model in the study by <xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>; see also <xref ref-type="bibr" rid="bib70">Wurm and Caramazza, 2019b</xref> for an experimental segregation of sociality and 1 vs. 2 people models).</p><p>Together, the results of the multiple regression RSA obtained for the different models are in line with the view that the LOTC hosts a variety of different, partially overlapping representations of action components that are likely to be integrated flexibly according to task demands (see also <xref ref-type="bibr" rid="bib35">Lingnau and Downing, 2015</xref>). Multiple, possibly overlapping basic dimensions have been proposed to underlie the organization of these different action components within the LOTC, among them the input modality (visual versus non-visual; <xref ref-type="bibr" rid="bib35">Lingnau and Downing, 2015</xref>; <xref ref-type="bibr" rid="bib50">Papeo et al., 2019</xref>), the presence and orientation of a person, and the directedness of actions toward persons or other targets (<xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>; <xref ref-type="bibr" rid="bib70">Wurm and Caramazza, 2019b</xref>).</p><p>Whereas the standard RSA revealed strong clusters not only in the LOTC but also in the IPL and the precentral gyrus, the multiple regression RSA for the semantic model revealed a cluster in the left LOTC only. This observation suggests that the results obtained in the IPL and the precentral gyrus for the semantic model revealed by the standard RSA was due to some combination of the models accounted for in the multiple regression RSA, even if no individual model alone revealed a cluster in these two regions that survived corrections for multiple comparisons.</p><p>Our results call for a comparison with the object domain, where similar questions have been addressed for decades. In line with the results of the multiple regression-based RSA, the LOTC has been demonstrated to represent the similarity structure of object categories (<xref ref-type="bibr" rid="bib6">Bracci and Op de Beeck, 2016</xref>). Regarding the results of the cluster analysis, salient distinctions at the behavioral and neural level have been found between animate and inanimate objects (for example <xref ref-type="bibr" rid="bib32">Kriegeskorte et al., 2008b</xref>; <xref ref-type="bibr" rid="bib11">Chao et al., 1999</xref>; <xref ref-type="bibr" rid="bib9">Caramazza and Shelton, 1998</xref>) which have been further segregated into human and nonhuman objects (<xref ref-type="bibr" rid="bib42">Mur et al., 2013</xref>), and manipulable and non-manipulable objects (<xref ref-type="bibr" rid="bib41">Mecklinger et al., 2002</xref>), respectively. The division between animate and inanimate objects, supported by neuropsychological, behavioral and neuroimaging findings, has been suggested to have a special status, likely due to evolutionary pressures that favored fast and accurate recognition of animals (<xref ref-type="bibr" rid="bib9">Caramazza and Shelton, 1998</xref>; <xref ref-type="bibr" rid="bib42">Mur et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">New et al., 2007</xref>). We conjecture that similar evolutionary mechanisms might have produced the distinction between actions belonging to different categories, such as locomotion (which might indicate the approach of an enemy), food-related actions (which might be critical for survival) and communicative actions (critical for survival within a group).</p><sec id="s3-1"><title>Conclusions</title><p>Using a combination of behavioral and fMRI data, we identified a number of meaningful semantic categories according to which participants arrange observed actions. The corresponding similarity structure was captured in left LOTC over and above the major components of perceived actions (body parts, scenes, movements, and objects) and other related features of the observed action scenes, in line with the view that the LOTC hosts a variety of different, partially overlapping action components that can be integrated flexibly. Together, our results support the view that the LOTC plays a critical role in accessing the meaning of actions beyond the mere perceptual processing of action-relevant components.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Twenty healthy participants (13 females; mean age: 28 years; age range: 20–46) took part in an fMRI and a behavioral experiment carried out at the Combined Universities Brain Imaging Centre (CUBIC) at Royal Holloway University of London (RHUL). The experiment was approved by the ethics committee at the Department of Psychology, RHUL (REF 2015/088). Participants provided written informed consent before starting the experiment. All participants were right-handed with normal or corrected-to-normal vision and no history of neurological or psychiatric disease. All participants but one (RT, one of the authors) were naïve to the purpose of the study.</p></sec><sec id="s4-2"><title>Inverse multidimensional scaling</title><p>Participants sat in front of a monitor (LCD 16.2 × 19.2 inches; distance 60 cm). In trial 1, all action images (one exemplar per action) appeared on the screen in a circular arrangement (with the order of actions randomly selected; see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Participants were instructed to arrange the pictures by drag-and-drop using the mouse according to their perceived similarity in meaning (e.g. <italic>walking</italic> and <italic>running</italic> would be placed closer to each other than <italic>walking</italic> and <italic>drinking</italic>) and to press a button once they were satisfied with the arrangement. In each subsequent trial (trial two to N<sub>p</sub>, where N<sub>p</sub> is the total number of trials for participant <italic>p</italic>), a subset of stimuli was sampled from the original stimulus set. The subset of actions was defined using an adaptive algorithm that provided the optimal evidence for the pairwise dissimilarity estimates (which are inferred from the 2D arrangement of the items on the screen, see <xref ref-type="bibr" rid="bib33">Kriegeskorte and Mur, 2012</xref> for details). Participants were given 15 minutes in total to complete the task.</p></sec><sec id="s4-3"><title>Stimulus selection</title><p>In contrast to previous studies that used a small set of actions, we aimed to cover a wide range of actions that we encounter on a daily basis. To this aim, we initially carried out an online survey using Google Forms. The aim of the survey was to identify actions that are considered common by a large sample of people. We thus asked 36 participants (different from those that took part in the fMRI study) to spontaneously write down all the actions that came to their mind within 10 min that they or other people are likely to do or observe. As expected, participants often used different words to refer to similar meanings (e.g. talking and discussing) or used different specific objects associated with the same action (e.g. drinking coffee and drinking water). Two of the authors (EB, AL) thus assigned these different instances of similar actions to a unique label. Actions were selected if they were mentioned by at least 20% of the participants. In total, we identified 37 actions (see <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>).</p><p>As a next step, we selected a subset of actions that were best suitable for the fMRI experiment. Specifically, we aimed to choose a set of actions that were arranged consistently across participants according to their perceived similarities in meaning. To this aim, we retrieved images depicting the 37 actions from the Internet. Using these images, we carried out inverse MDS (see corresponding section and <xref ref-type="bibr" rid="bib33">Kriegeskorte and Mur, 2012</xref> for details) using 15 new participants. Each participant had 20 min to complete the arrangement. In three additional 20 min sessions, participants were furthermore instructed to arrange the actions according to the perceived similarity in terms of their meaning, the scenes in which these actions typically take place, movement kinematics, and the objects which are typically associated with these actions. The order in which these four tasks (semantics, scenes, movements, objects) were administered to participants was counterbalanced across participants. To rule out that any obtained arrangements were driven by the specific exemplars chosen for each action, we repeated the same experiment with a new group of people (N = 15) and an independent set of 37 images taken from the Internet.</p><p>To construct representational dissimilarity matrices (RDMs), we averaged the dissimilarity estimates for each pair of actions (e.g. the dissimilarity between biking and brushing teeth, etc.), separately for each participant and each task, across trials. For each participant and model, we then constructed dissimilarity matrices based on the Euclidean distance between each pair of actions that resulted from the inverse MDS experiment. The dissimilarity matrices were then normalized by dividing each value by the maximum value of each matrix. Each row of this matrix represented the dissimilarity judgment of one action with respect to every other action. To select the most suitable actions for the fMRI experiment, we aimed to evaluate which of the 37 actions were arranged similarly across participants in the different tasks. To this aim, we carried out a cosine distance analysis, which allowed us to determine, for each action, the similarity across all participants. The cosine distance evaluates the similarity of orientation between two vectors. It can be defined as one minus the cosine of the angle between two vectors of an inner product space: a cosine distance of 1 indicates that the two vectors are orthogonal to each other (maximum dissimilarity/minimum similarity); a cosine distance of zero indicates that the two vectors have the same orientation (maximum similarity/minimum dissimilarity). The cosine distance can therefore range between 0 and 1. In an RDM, each row (or column) represents the dissimilarity score between one action and every other action, ranging from 0 (minimum dissimilarity) to 1 (maximum dissimilarity). Therefore, each row of the matrix of each single participant was used to compute the pairwise cosine distances between this and the corresponding row of every other participant. For each action, a cosine distance close to zero would indicate that participants agreed on the geometrical configuration of that action with respect to every other action; a value close to one would indicate disagreement. For each action, we computed the mean across the pairwise cosine distances of all participants in both behavioral pilot experiments and kept only those actions that had a cosine distance within one standard deviation from the averaged cosine distance in all tasks and both stimulus sets. Thirty-one actions fulfilled this criterion, whereas five (<italic>getting dressed</italic>, <italic>cleaning floor</italic>, <italic>brushing teeth</italic>, <italic>singing</italic> and <italic>watering plants</italic>) had to be discarded. We also decided to remove two additional actions (<italic>grocery shopping</italic> and <italic>taking the train</italic>) because these could not be considered as single actions but implied a sequence of actions (e.g. <italic>entering the shop, choosing between products, etc.; waiting for the train, getting on the train, sitting on the train, etc.</italic>).</p><p>At the end of the procedure, we identified 30 actions that could be used for the next step, which consisted in creating the final stimulus dataset. To this aim, we took photos of 29 of the 30 actions using a Canon EOS 400D camera. To maximize perceptual variability within each action, and thus to minimize low-level feature differences between actions, we varied the actors (2), the scene (2) and perspectives (3), for a total of 12 exemplars per action. Exemplars for the action ‘swimming’ were collected from the Internet because of the difficulties in taking photos in a public swimming pool.</p><p>The distance between the camera and the actor was kept constant within each action (across exemplars). Since some actions consisted of hand-object interactions (such as painting, drinking) and thus required finer details, while other actions involved the whole body (such as dancing, running) and thus required a certain minimum distance to be depicted, it was not possible to maintain the same distance across all the actions. The two actors were instructed to maintain a neutral facial expression and were always dressed in similar neutral clothing. If an action involved an object, the actor used two different exemplars of the object (e.g. two different bikes for <italic>biking</italic>) or two different objects (e.g. a sandwich or an apple for <italic>eating</italic>). Furthermore, some actions required the presence of an additional actor (<italic>handshaking, hugging, talking</italic>). The brightness of all pictures was adjusted using PhotoPad Image Editor (<ext-link ext-link-type="uri" xlink:href="http://www.nchsoftware.com/photoeditor/">www.nchsoftware.com/photoeditor/</ext-link>). Pictures were then converted into grayscale and resized to 400 × 300 pixels using FastOne Photo (<ext-link ext-link-type="uri" xlink:href="http://www.faststone.org">www.faststone.org</ext-link>). In addition, we made the images equally bright using custom written Matlab code available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/cvrb2/">osf.io/cvrb2</ext-link> (<xref ref-type="bibr" rid="bib62">Tucciarelli, 2019</xref>) (mean brightness across all images was 115.80 with standard deviation equal to 0.4723).</p><p>To ensure that the final set of pictures were comprehensible and identified as the actions we intended to investigate, we furthermore validated the stimuli through an online survey using Qualtrics and Amazon Mechanical Turk involving 30 participants. Specifically, the 30*12 = 360 pictures were randomly assigned to three groups of 120 images. Each group was assigned to ten participants that had to name the actions depicted in the images. For each participant, the images were presented in a random sequence. Since most of the participants failed to correctly name some of the exemplars of <italic>making coffee</italic> and <italic>switching on lights</italic>, these actions were excluded from the stimulus set. Therefore, the final number of actions chosen for the main experiment was 28 (see <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>).</p><p>Note that we decided to use static images instead of videos of actions for two reasons. First of all, we wished to avoid systematic differences between conditions based on the kinematic profiles of videos of actions. Second, we aimed to use stimuli that are suitable both for the fMRI experiment and for the inverse MDS experiment. We considered static stimuli more suitable for the latter, given that the participant had to judge the similarity of a large set of actions simultaneously. We examine the consequences of this choice in the discussion.</p></sec><sec id="s4-4"><title>Experimental design and setup</title><p>The fMRI experiment consisted of twelve functional runs and one anatomical sequence halfway through the experiment. Each functional run started and ended with 15 s of fixation. In between runs, the participants could rest.</p><p>Stimuli were back-projected onto a screen (60 Hz frame rate) via a projector (Sanyo, PLC-XP-100L) and viewed through a mirror mounted on the head coil (distance between mirror and eyes: about 12 cm). The background of the screen was uniform gray. Stimulus presentation and response collection was controlled using ASF (<xref ref-type="bibr" rid="bib57">Schwarzbach, 2011</xref>), a toolbox based on the Matlab Psychophysics toolbox (<xref ref-type="bibr" rid="bib8">Brainard, 1997</xref>).</p><p>Each functional run consisted of 56 experimental trials (28 exemplars of the action categories performed by each of the two actors) and 18 null events (to enhance design efficiency) presented in a pseudorandomized order (preventing that the same action was shown in two consecutive trials, except during catch trials, see next paragraph). A trial consisted of the presentation of an action image for 1 s followed by 3 s of fixation. A null event consisted of the presentation of a fixation cross for 4 s. Within run 1–6, each possible combination of action types (28) x exemplars (12) was presented once, for a total of 336 trials. For runs 7–12, the randomization procedure was repeated such that each possible combination was presented another time. In this way, each participant saw each exemplar twice during the entire experiment (and thus each action was presented 24 times). A full balancing of all combinations of action, scene, and actor within each run was not possible with 28 actions, therefore the experiment was quasi-balanced: in each run, if actor one performed an action in scene A, actor two performed the same action in scene B, and vice versa.</p><p>To ensure that participants paid attention to the stimuli, we included seven (out of 63; 4.41%) catch trials in each run which consisted in the presentation of an image depicting the same action (but not the same exemplar) as the action presented in trial N-1 (e.g. <italic>eating</italic> an apple, actor A, scene A, followed by <italic>eating</italic> a sandwich, actor B, scene B). Participants were instructed to press a button with their right index finger whenever they detected an action repetition. Within the entire experimental session, all 28 actions could serve as catch trials and each action was selected randomly without replacement such that the same action could not be used as a catch trial within the same run. After a set of 4 runs all 28 actions were used as catch trial once, thus the selection process started from scratch. Catch trials were discarded from multivariate data analysis.</p><p>Before entering the scanner, participants received written instructions about their task and familiarized with the stimulus material for a couple of minutes. Next, participants carried out a short practice run to ensure that they properly understood the task.</p></sec><sec id="s4-5"><title>MRI data acquisition</title><p>Functional and structural data were acquired using a Siemens TIM Trio 3T MRI scanner. For the acquisition of the functional images, we used a T2*-weighted gradient EPI sequence. The repetition time (TR) was 2.5 s, the echo time (TE) was 30 milliseconds, the flip angle was 85°, the field of view was 192 × 192 mm, the matrix size was 64 × 64, and the voxel resolution was 3 × 3 × 3 mm. A total of 37 slices were acquired in ascending interleaved order. Each functional run lasted 5 min and 55 s and consisted of 142 volumes.</p><p>For the structural data, we used a T1*-weighted MPRAGE sequence (image size 256 × 256 × 176 voxels, voxel size 1 × 1 × 1 mm, TR 1.9 s, TE 3.03, flip angle 11), lasting 5 min and 35 s.</p></sec><sec id="s4-6"><title>MRI data preprocessing</title><p>Anatomical data were segmented using FreeSurfer (<xref ref-type="bibr" rid="bib18">Fischl et al., 1999</xref>). Preprocessing of the functional data was carried out using SPM12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/software/spm12/">http://www.fil.ion.ucl.ac.uk/spm/software/spm12/</ext-link>). The slices of each functional volume were slice time corrected and then spatially realigned to correct for head movements. Functional volumes were then coregistered to the individual anatomical image. Analyses were conducted in individual volume space, but using the inner and outer surfaces obtained with FreeSurfer as a constraint to select the voxels included in each searchlight as implemented in CoSMoMVPA (<xref ref-type="bibr" rid="bib47">Oosterhof et al., 2011</xref>; <xref ref-type="bibr" rid="bib49">Oosterhof et al., 2016</xref>). The resulting maps were resampled to the surface level on the Human Connectome Project common space <italic>FS_LR 32 k</italic> (<xref ref-type="bibr" rid="bib19">Glasser et al., 2013</xref>) using FreeSurfer and workbench connectome (<xref ref-type="bibr" rid="bib38">Marcus et al., 2011</xref>). Multivariate analyses were conducted using unsmoothed data.</p></sec><sec id="s4-7"><title>Behavioral experiment</title><p>Following the fMRI experiment, either on the same or the next day, participants took part in an additional behavioral experiment in which they carried out an inverse MDS task using similar procedures as described above (see Materials and methods section and <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The actions were similar to those used during the fMRI experiment. In separate blocks of the experiment, participants were asked to arrange the actions according to their perceived similarity in terms of (a) meaning (referred to as 'semantics' in the remainder of the text), (b) the body part(s) involved, (c) scene, (d) movement kinematics, (e) the object involved.</p><p>The order of blocks was counterbalanced across participants. Participants were provided with the following written instructions:</p><list list-type="bullet"><list-item><p>In the semantic similarity task, you will be asked to arrange the images with respect of their meaning: for example, sewing and ironing should be placed closer to each other than sewing and smoking.</p></list-item><list-item><p>In the body parts task, actions typically involving the same/similar body parts, for example kicking a ball and walking, should be placed closer to each other than kicking a ball and smiling.</p></list-item><list-item><p>In the context similarity task, actions typically taking place in the same/similar context, for example cutting bread and preparing tea, should be placed closer to each other than cutting bread and cutting hair.</p></list-item><list-item><p>In the type of movement task, you will be asked to arrange the images with respect to the type of movement usually involved in each action. For example, actions like grasping and reaching would be more close to each other than grasping and kicking.</p></list-item><list-item><p>In the type of object task, actions involving similar objects, for example catching a football and throwing a tennis ball, should be placed closer to each other than throwing a tennis ball and throwing a pillow at another person.</p></list-item></list><p>To further characterize the structure that emerged from the inverse MDS, we adopted principal component analysis (PCA) as implemented in the R package <italic>cluster</italic> to individuate the principal components along which the actions were organized. To characterize the observed clusters, we furthermore used a model-based approach using the <italic>K-means</italic> (<xref ref-type="bibr" rid="bib23">Hartigan and Wang, 1979</xref>) clustering method. The K-means method requires the number of clusters as an input, which was one of the parameters we wished to estimate from the data. To this aim, we used the Silhouette method (<xref ref-type="bibr" rid="bib56">Rousseeuw, 1987</xref>) as implemented in the R package <italic>factoextra</italic> to estimate the optimal number of clusters. Specifically, this method provides an estimate of the averaged distance between clusters as a function of the number of clusters used and selects the value which provides the maximal distance.</p></sec><sec id="s4-8"><title>Construction of representational dissimilarity matrices (RDMs)</title><p>To construct RDMs for the semantic, body, scene, movement, and object model used in the behavioral experiment, we used the same procedure described in the section <bold>Stimulus selection</bold>, that is we determined the Euclidean distance between each pair of actions that resulted from inverse MDS, and normalized the dissimilarity matrices by dividing each value by the maximum value of each matrix. Individual dissimilarity matrices were used as a model for the multiple regression-based representational similarity analysis of fMRI data (see section <italic>Representational Similarity Analysis</italic>). We found significant (all p-values were smaller than p&lt;0.0001 and survived false discovery rate correction) inter-observer correlations, that is the individual RDMs significantly correlated with the average RDMs of the remaining participants (mean leave-one-subject-out correlation coefficients [min – max individual correlation coefficients]; semantic model: 0.61 [0.46–0.78], body model: 0.57 [0.31–0.70]; scene model: 0.63 [0.40–0.78]; movement model: 0.47 [0.26–0.67]; object model: 0.51 [0.22–0.71]. Clusters obtained from the body, scene, movement, and object model using PCA can be found in <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4A–D</xref>.</p><p>To construct RDMs for the sociality and the transitivity models (<xref ref-type="bibr" rid="bib68">Wurm et al., 2017</xref>), we conducted an online experiment using Qualtrics and Amazon Mechanical Turn in which we asked a separate group of N = 20 participants to judge on a Likert scale (1: not at all; 7: very much) the sociality (i.e. the interaction between the actors involved) and transitivity (i.e. the use of an object) of each action. To construct the RDM for the distance model, we asked another group of N = 20 participants to judge the distance (1: within reaching distance; 2: not within reaching distance, but &lt;= 3 meters, 3:&gt;3 meters) from the observer at which each action takes place. For each participant, we derived a RDM from the judgment scores and then averaged the individual RDMs for the sociality, transitivity and distance model.</p><p>The model controlling for the number of people depicted in the action was constructed by coding one person involved as 0 and two people involved as 1. The HMAX-C1 model (<xref ref-type="bibr" rid="bib58">Serre et al., 2007</xref>) was derived similarly to <xref ref-type="bibr" rid="bib12">Connolly et al. (2012)</xref> in the following way: we computed the C1 representation of each stimulus image (i.e. each exemplar of an action) and then averaged across the exemplar response vectors to obtain one C1 vector for each action. The 28 HMAX-C1 representations were then used to compute the RDM to be used as a predictor in the multiple regression RSA.</p><p>To examine correlations between the different model representational dissimilarity matrices, we computed correlations between each pairwise comparison of RDMs, both averaged across participants (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>) and separately for each participant (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B,C</xref>). As can be seen, not surprisingly, there are modest correlations between the different models (in particular when averaged across participants). However, as shown in the Materials and methods section on multiple regression RSA, the Variance Inflation Factor indicated a low risk of collinearity and thus justified the use of these models for multiple regression RSA.</p></sec><sec id="s4-9"><title>Representational Similarity Analysis (RSA)</title><p>The aim of the RSA was to individuate those brain regions that were best explained by the models obtained behaviorally and thus to infer the representational geometry that these areas encoded. We therefore conducted an RSA over the entire cortical surface using a searchlight approach (<xref ref-type="bibr" rid="bib30">Kriegeskorte et al., 2006</xref>) at the individual brain space. Each searchlight consisted of 100 features (one central vertex + 99 neighbors) and was approximately 12 mm in radius. All multivariate analyses were carried out using custom written Matlab functions (available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/cvrb2/">osf.io/cvrb2</ext-link> ; <xref ref-type="bibr" rid="bib62">Tucciarelli, 2019</xref>) and CoSMoMVPA (<xref ref-type="bibr" rid="bib49">Oosterhof et al., 2016</xref>).</p><p>For the multivariate analysis, the design matrix consisted of 142 volumes X 28 predictors of interest (resulting from the 28 actions) plus nuisance predictors consisting of the catch trials, the parameters resulting from motion correction, and a constant term. Thus, for each participant, we obtained 28 beta maps at the volume level. We adopted two approaches for the representational similarity analysis, a standard and a multiple regression RSA approach.</p></sec><sec id="s4-10"><title>Standard RSA</title><p>First, to identify clusters in which the neural data reflected the dissimilarity pattern captured by the semantic model, we conducted a standard RSA. For this analysis, the beta maps were averaged across runs. For each searchlight, we derived the normalized RDM using squared Euclidian distance as distance metric (note that we chose this distance metric, rather than Spearman correlation, for a more straightforward comparison of the results of the standard and multiple regression RSA; see also next paragraph). For each searchlight, the RDM was correlated with the normalized RDM of the semantic model. The correlation values were assigned to the central node of each searchlight, thus leading to a correlation map for the semantic model, separately for each participant. The correlation maps of this first-level analysis were then resampled to the common space and Fisher transformed to normalize the distribution across participants to run a second-level analysis. Specifically, the correlation maps for the semantic model of all N participants were tested against zero using a one-tailed <italic>t</italic>-test at each vertex. The resulting t maps were corrected using a cluster-based nonparametric Monte Carlo permutation analysis (5000 iterations; initial threshold p&lt;0.001 ; <xref ref-type="bibr" rid="bib59">Stelzer et al., 2013</xref>).</p></sec><sec id="s4-11"><title>Multiple regression RSA</title><p>To determine clusters in which the neural RDM correlated with a given RDM (with a specific focus on the semantic model) while accounting for the remaining RDMs, we conducted a multiple regression RSA at each searchlight. For this analysis, the beta patterns were first normalized across images and then averaged across runs. Before computing the neural RDM, the betas of a searchlight were also normalized across features. Following the procedure used by <xref ref-type="bibr" rid="bib5">Bonner and Epstein (2018)</xref>, the neural RDM for each searchlight was computed using the squared Euclidean distance. Using the squared Euclidian distance as distance metric for multiple regression RSA, which models distances from a brain RDM as linear combinations of the distances from a number of predictor RDMs, guarantees that the distance metric sums linearly.</p><p>As predictors of the multiple regression analysis, we used the RDMs described in the section <italic>Construction of representational dissimilarity matrices</italic> (semantic, body, movement, object, scene, sociality, transitivity, distance, 1 vs 2 People, HMAX-C1; see also <xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><p>To estimate potential risks of collinearity, we computed the Variance Inflation Factor (VIF) for each participant to have a measure of the <italic>inflation</italic> of the estimated variance of the <italic>ith</italic> regression coefficient (computed as 1/ (1−R<sup>2</sup><sub>i</sub>), where <italic>i</italic> indicates a variable and R<sup>2</sup> is the coefficient of determination), assuming this coefficient being independent from the others. The VIFs were relatively small (average VIF semantic model: 1.61, body model: 1.37, scene model: 1.63, movement model: 1.35, object model: 1.38, sociality model: 1.54, transitivity model: 1.31, distance model: 1.22, 1 vs 2 People model: 1.37, HMAX-C1 model: 1.06), indicating a low risk of multicollinearity (<xref ref-type="bibr" rid="bib40">Mason et al., 2003</xref>) and thus justifying the use of multiple regression RSA.</p><p>For each participant, the multiple regression-based RSA provided us with beta maps for each of the ten predictors that were then entered in a second-level (group) analysis to test the individual beta maps against zero. The procedure for multiple comparisons correction was the same as described in the section <italic>Standard RSA</italic>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by Royal Holloway University of London and a grant awarded to Angelika Lingnau (Heisenberg-Professorship, German Research Foundation, Li 2840/1–1 and Li 2840/2–1). We are grateful to Niko Kriegeskorte and Marieke Mur for providing code for the inverse MDS experiment, to Beatrice Agostini for helping with stimulus creation, and to Nick Oosterhof for advice on data analysis.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants provided written informed consent in line with local ethics and MRI protocols. The study was approved by the Ethics Committee at the Department of Psychology, Royal Holloway University of London (REF 2015/088).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Cluster table standard RSA (semantic model).</title><p>Cluster table standard RSA (semantic model). List of clusters resulting from the standard RSA for the semantic model which survived correction for multiple comparisons (cluster p-value&lt;0.05; see <italic>Materials and methods</italic> and <xref ref-type="fig" rid="fig4">Figure 4</xref>). Coordinates are in MNI space. Labels are based on MRI scans that originated from the OASIS project (<ext-link ext-link-type="uri" xlink:href="http://www.oasis-brains.org/">http://www.oasis-brains.org/</ext-link>) and were provided by Neuromorphometrics, Inc (<ext-link ext-link-type="uri" xlink:href="http://www.neuromorphometrics.com/">http://www.neuromorphometrics.com/</ext-link>) under academic subscription provided in SPM12 and Glasser’s surface-based atlas (<xref ref-type="bibr" rid="bib20">Glasser et al., 2016</xref>).</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-47686-supp1-v1.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Cluster table multiple regression RSA.</title><p>Cluster table multiple regression RSA. List of clusters resulting from the multiple regression RSA for the eight different models (semantic, body, movement, object, transitivity, distance, 1 vs 2 people, HMAX-C1) which survived correction for multiple comparisons (cluster p-value&lt;0.05; see <italic>Materials and methods</italic> and <xref ref-type="fig" rid="fig6">Figure 6</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Coordinates are in MNI space. Labels are based on MRI scans that originated from the OASIS project (<ext-link ext-link-type="uri" xlink:href="http://www.oasis-brains.org/">http://www.oasis-brains.org/</ext-link>) and were provided by Neuromorphometrics, Inc (<ext-link ext-link-type="uri" xlink:href="http://www.neuromorphometrics.com/">http://www.neuromorphometrics.com/</ext-link>) under academic subscription provided in SPM12 and Glasser’s surface-based atlas (<xref ref-type="bibr" rid="bib20">Glasser et al., 2016</xref>).</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-47686-supp2-v1.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>List of actions identified using the online survey.</title><p>We initially kept actions that were mentioned by at least 20% of the participants. The final selected twenty-eight actions used for the study are the ones highlighted. See <italic>Materials and methods</italic> section for the procedure used to select these actions.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-47686-supp3-v1.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-47686-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data and codes have been archived at the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/cvrb2/">https://osf.io/cvrb2/</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Raffaele</surname><given-names>Tucciarelli</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>RESPACT: The representational space of observed actions</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/cvrb2/">cvrb2</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdollahi</surname> <given-names>RO</given-names></name><name><surname>Jastorff</surname> <given-names>J</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Common and segregated processing of observed actions in human SPL</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>2734</fpage><lpage>2753</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs264</pub-id><pub-id pub-id-type="pmid">22918981</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aleong</surname> <given-names>R</given-names></name><name><surname>Paus</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural correlates of human body perception</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>482</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21211</pub-id><pub-id pub-id-type="pmid">19309293</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astafiev</surname> <given-names>SV</given-names></name><name><surname>Stanley</surname> <given-names>CM</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Extrastriate body area in human occipital cortex responds to the performance of motor actions</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>542</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nn1241</pub-id><pub-id pub-id-type="pmid">15107859</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atkinson</surname> <given-names>AP</given-names></name><name><surname>Vuong</surname> <given-names>QC</given-names></name><name><surname>Smithson</surname> <given-names>HE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Modulation of the face- and body-selective visual regions by the motion and emotion of point-light face and body stimuli</article-title><source>NeuroImage</source><volume>59</volume><fpage>1700</fpage><lpage>1712</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.08.073</pub-id><pub-id pub-id-type="pmid">21924368</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonner</surname> <given-names>MF</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Computational mechanisms underlying cortical responses to the affordance properties of visual scenes</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006111</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006111</pub-id><pub-id pub-id-type="pmid">29684011</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Op de Beeck</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>432</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2314-15.2016</pub-id><pub-id pub-id-type="pmid">26758835</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname> <given-names>S</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Body and object effectors: the organization of object representations in high-level visual cortex reflects body-object interactions</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>18247</fpage><lpage>18258</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1322-13.2013</pub-id><pub-id pub-id-type="pmid">24227734</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Shelton</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Domain-specific knowledge systems in the brain the animate-inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><volume>10</volume><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1162/089892998563752</pub-id><pub-id pub-id-type="pmid">9526080</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspers</surname> <given-names>S</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name><name><surname>Laird</surname> <given-names>AR</given-names></name><name><surname>Eickhoff</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>ALE meta-analysis of action observation and imitation in the human brain</article-title><source>NeuroImage</source><volume>50</volume><fpage>1148</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.112</pub-id><pub-id pub-id-type="pmid">20056149</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname> <given-names>LL</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Attribute-based neural substrates in temporal cortex for perceiving and knowing about objects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>913</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1038/13217</pub-id><pub-id pub-id-type="pmid">10491613</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Guntupalli</surname> <given-names>JS</given-names></name><name><surname>Gors</surname> <given-names>J</given-names></name><name><surname>Hanke</surname> <given-names>M</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Wu</surname> <given-names>YC</given-names></name><name><surname>Abdi</surname> <given-names>H</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The representation of biological classes in the human brain</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>2608</fpage><lpage>2618</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5547-11.2012</pub-id><pub-id pub-id-type="pmid">22357845</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culham</surname> <given-names>JC</given-names></name><name><surname>Valyear</surname> <given-names>KF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Human parietal cortex in action</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>205</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.03.005</pub-id><pub-id pub-id-type="pmid">16563735</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Jiang</surname> <given-names>Y</given-names></name><name><surname>Shuman</surname> <given-names>M</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A cortical area selective for visual processing of the human body</article-title><source>Science</source><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1126/science.1063414</pub-id><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Chan</surname> <given-names>AW</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Dodds</surname> <given-names>CM</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Domain specificity in visual cortex</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>1453</fpage><lpage>1461</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj086</pub-id><pub-id pub-id-type="pmid">16339084</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Wiggett</surname> <given-names>AJ</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Functional magnetic resonance imaging investigation of overlapping lateral occipitotemporal activations using multi-voxel pattern analysis</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>226</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3619-06.2007</pub-id><pub-id pub-id-type="pmid">17202490</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferri</surname> <given-names>S</given-names></name><name><surname>Kolster</surname> <given-names>H</given-names></name><name><surname>Jastorff</surname> <given-names>J</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The overlap of the EBA and the MT/V5 cluster</article-title><source>NeuroImage</source><volume>66</volume><fpage>412</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.060</pub-id><pub-id pub-id-type="pmid">23108274</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical Surface-Based analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>195</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Sotiropoulos</surname> <given-names>SN</given-names></name><name><surname>Wilson</surname> <given-names>JA</given-names></name><name><surname>Coalson</surname> <given-names>TS</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Andersson</surname> <given-names>JL</given-names></name><name><surname>Xu</surname> <given-names>J</given-names></name><name><surname>Jbabdi</surname> <given-names>S</given-names></name><name><surname>Webster</surname> <given-names>M</given-names></name><name><surname>Polimeni</surname> <given-names>JR</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>The minimal preprocessing pipelines for the human connectome project</article-title><source>NeuroImage</source><volume>80</volume><fpage>105</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id><pub-id pub-id-type="pmid">23668970</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Coalson</surname> <given-names>TS</given-names></name><name><surname>Robinson</surname> <given-names>EC</given-names></name><name><surname>Hacker</surname> <given-names>CD</given-names></name><name><surname>Harwell</surname> <given-names>J</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Andersson</surname> <given-names>J</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafri</surname> <given-names>A</given-names></name><name><surname>Trueswell</surname> <given-names>JC</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural representations of observed actions generalize across static and dynamic visual input</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>3056</fpage><lpage>3071</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2496-16.2017</pub-id><pub-id pub-id-type="pmid">28209734</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagler</surname> <given-names>DJ</given-names></name><name><surname>Riecke</surname> <given-names>L</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Parietal and superior frontal visuospatial maps activated by pointing and saccades</article-title><source>NeuroImage</source><volume>35</volume><fpage>1562</fpage><lpage>1577</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.01.033</pub-id><pub-id pub-id-type="pmid">17376706</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartigan</surname> <given-names>JA</given-names></name><name><surname>Wang</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>A k-means clustering algorithm</article-title><source>Journal of the Royal Statistical Society</source><volume>28</volume><fpage>100</fpage><lpage>108</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Furey</surname> <given-names>ML</given-names></name><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Schouten</surname> <given-names>JL</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodzic</surname> <given-names>A</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Stirn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Cortical responses to self and others</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>951</fpage><lpage>962</lpage><pub-id pub-id-type="doi">10.1002/hbm.20558</pub-id><pub-id pub-id-type="pmid">18381769</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname> <given-names>D</given-names></name><name><surname>Rudolf</surname> <given-names>AK</given-names></name><name><surname>Brandi</surname> <given-names>ML</given-names></name><name><surname>Untch</surname> <given-names>KH</given-names></name><name><surname>Grabhorn</surname> <given-names>R</given-names></name><name><surname>Hampel</surname> <given-names>H</given-names></name><name><surname>Mohr</surname> <given-names>HM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural adaptation to thin and fat bodies in the fusiform body area and middle occipital gyrus: an fMRI adaptation study</article-title><source>Human Brain Mapping</source><volume>34</volume><fpage>3233</fpage><lpage>3246</lpage><pub-id pub-id-type="doi">10.1002/hbm.22135</pub-id><pub-id pub-id-type="pmid">22807338</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jastorff</surname> <given-names>J</given-names></name><name><surname>Begliomini</surname> <given-names>C</given-names></name><name><surname>Fabbri-Destro</surname> <given-names>M</given-names></name><name><surname>Rizzolatti</surname> <given-names>G</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Coding observed motor acts: different organizational principles in the parietal and premotor cortex of humans</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>128</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1152/jn.00254.2010</pub-id><pub-id pub-id-type="pmid">20445039</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jastorff</surname> <given-names>J</given-names></name><name><surname>Orban</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Human functional magnetic resonance imaging reveals separation and integration of shape and motion cues in biological motion processing</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>7315</fpage><lpage>7329</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4870-08.2009</pub-id><pub-id pub-id-type="pmid">19494153</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kontaris</surname> <given-names>I</given-names></name><name><surname>Wiggett</surname> <given-names>AJ</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dissociation of extrastriate body and biological-motion selective Areas by manipulation of visual-motor congruency</article-title><source>Neuropsychologia</source><volume>47</volume><fpage>3118</fpage><lpage>3124</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.07.012</pub-id><pub-id pub-id-type="pmid">19643118</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Information-based functional brain mapping</article-title><source>PNAS</source><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Inverse MDS: inferring dissimilarity structure from multiple item arrangements</article-title><source>Frontiers in Psychology</source><volume>3</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00245</pub-id><pub-id pub-id-type="pmid">22848204</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Levin</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1993">1993</year><source>English Verb Classes and Alternations</source><publisher-loc>Chicago</publisher-loc><publisher-name>The University of Chicago Press</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lingnau</surname> <given-names>A</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The lateral occipitotemporal cortex in action</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>268</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.03.006</pub-id><pub-id pub-id-type="pmid">25843544</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>F</given-names></name><name><surname>Xu</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>X</given-names></name><name><surname>Wang</surname> <given-names>P</given-names></name><name><surname>Wang</surname> <given-names>B</given-names></name><name><surname>Liu</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Investigating the neural basis of basic human movement perception using multi-voxel pattern analysis</article-title><source>Experimental Brain Research</source><volume>236</volume><fpage>907</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1007/s00221-018-5175-9</pub-id><pub-id pub-id-type="pmid">29362830</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magri</surname> <given-names>C</given-names></name><name><surname>Fabbri</surname> <given-names>S</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Lingnau</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Directional tuning for eye and arm movements in overlapping regions in human posterior parietal cortex</article-title><source>NeuroImage</source><volume>191</volume><fpage>234</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.029</pub-id><pub-id pub-id-type="pmid">30769145</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcus</surname> <given-names>DS</given-names></name><name><surname>Harwell</surname> <given-names>J</given-names></name><name><surname>Olsen</surname> <given-names>T</given-names></name><name><surname>Hodge</surname> <given-names>M</given-names></name><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Prior</surname> <given-names>F</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Laumann</surname> <given-names>T</given-names></name><name><surname>Curtiss</surname> <given-names>SW</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Informatics and data mining tools and strategies for the human connectome project</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fninf.2011.00004</pub-id><pub-id pub-id-type="pmid">21743807</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maslow</surname> <given-names>AH</given-names></name></person-group><year iso-8601-date="1943">1943</year><article-title>A theory of human motivation</article-title><source>Psychological Review</source><volume>50</volume><fpage>370</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1037/h0054346</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mason</surname> <given-names>RL</given-names></name><name><surname>Gunst</surname> <given-names>RF</given-names></name><name><surname>Hess</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Statistical Design and Analysis of Experiments</source><publisher-name>John Wiley &amp; Sons, Inc</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mecklinger</surname> <given-names>A</given-names></name><name><surname>Gruenewald</surname> <given-names>C</given-names></name><name><surname>Besson</surname> <given-names>M</given-names></name><name><surname>Magnié</surname> <given-names>MN</given-names></name><name><surname>Von Cramon</surname> <given-names>DY</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Separable neuronal circuitries for manipulable and non-manipulable objects in working memory</article-title><source>Cerebral Cortex</source><volume>12</volume><fpage>1115</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1093/cercor/12.11.1115</pub-id><pub-id pub-id-type="pmid">12379600</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Meys</surname> <given-names>M</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Human Object-Similarity judgments reflect and transcend the Primate-IT object representation</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>128</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00128</pub-id><pub-id pub-id-type="pmid">23525516</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname> <given-names>A</given-names></name><name><surname>Sowden</surname> <given-names>PT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Your hand or mine? the extrastriate body area</article-title><source>NeuroImage</source><volume>42</volume><fpage>1669</fpage><lpage>1677</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.05.045</pub-id><pub-id pub-id-type="pmid">18586108</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>New</surname> <given-names>J</given-names></name><name><surname>Cosmides</surname> <given-names>L</given-names></name><name><surname>Tooby</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Category-specific attention for animals reflects ancestral priorities, not expertise</article-title><source>PNAS</source><volume>104</volume><fpage>16598</fpage><lpage>16603</lpage><pub-id pub-id-type="doi">10.1073/pnas.0703913104</pub-id><pub-id pub-id-type="pmid">17909181</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Wingfield</surname> <given-names>C</given-names></name><name><surname>Walther</surname> <given-names>A</given-names></name><name><surname>Su</surname> <given-names>L</given-names></name><name><surname>Marslen-Wilson</surname> <given-names>W</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id><pub-id pub-id-type="pmid">24743308</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Wiggett</surname> <given-names>AJ</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name><name><surname>Tipper</surname> <given-names>SP</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Surface-based information mapping reveals crossmodal vision-action representations in human parietal and occipitotemporal cortex</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>1077</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1152/jn.00326.2010</pub-id><pub-id pub-id-type="pmid">20538772</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Wiestler</surname> <given-names>T</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A comparison of volume-based and surface-based multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>56</volume><fpage>593</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.04.270</pub-id><pub-id pub-id-type="pmid">20621701</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Tipper</surname> <given-names>SP</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Viewpoint (in)dependence of action representations: an MVPA study</article-title><source>Journal of Cognitive Neuroscience</source><volume>24</volume><fpage>975</fpage><lpage>989</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00195</pub-id><pub-id pub-id-type="pmid">22264198</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname> <given-names>NN</given-names></name><name><surname>Connolly</surname> <given-names>AC</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CoSMoMVPA: multi-modal multivariate pattern analysis of neuroimaging data in matlab/GNU octave</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id><pub-id pub-id-type="pmid">27499741</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papeo</surname> <given-names>L</given-names></name><name><surname>Agostini</surname> <given-names>B</given-names></name><name><surname>Lingnau</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Large-Scale organization of gestures and words in the middle temporal gyrus</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>5966</fpage><lpage>5974</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2668-18.2019</pub-id><pub-id pub-id-type="pmid">31126999</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Wiggett</surname> <given-names>AJ</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Patterns of fMRI activity dissociate overlapping functional brain Areas that respond to biological motion</article-title><source>Neuron</source><volume>49</volume><fpage>815</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.02.004</pub-id><pub-id pub-id-type="pmid">16543130</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname> <given-names>MV</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Selectivity for the human body in the fusiform gyrus</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>603</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1152/jn.00513.2004</pub-id><pub-id pub-id-type="pmid">15295012</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pinker</surname> <given-names>SL</given-names></name></person-group><year iso-8601-date="1989">1989</year><source>Cognition: The Acquisition of Argument Structure.</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname> <given-names>M</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname> <given-names>G</given-names></name><name><surname>Matelli</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Two different streams form the dorsal visual system: anatomy and functions</article-title><source>Experimental Brain Research</source><volume>153</volume><fpage>146</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1007/s00221-003-1588-0</pub-id><pub-id pub-id-type="pmid">14610633</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseeuw</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title><source>Journal of Computational and Applied Mathematics</source><volume>20</volume><fpage>53</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/0377-0427(87)90125-7</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarzbach</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A simple framework (ASF) for behavioral and neuroimaging experiments based on the psychophysics toolbox for MATLAB</article-title><source>Behavior Research Methods</source><volume>43</volume><fpage>1194</fpage><lpage>1201</lpage><pub-id pub-id-type="doi">10.3758/s13428-011-0106-8</pub-id><pub-id pub-id-type="pmid">21614662</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname> <given-names>T</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A feedforward architecture accounts for rapid categorization</article-title><source>PNAS</source><volume>104</volume><fpage>6424</fpage><lpage>6429</lpage><pub-id pub-id-type="doi">10.1073/pnas.0700622104</pub-id><pub-id pub-id-type="pmid">17404214</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stelzer</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Turner</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Statistical inference and multiple testing correction in classification-based multi-voxel pattern analysis (MVPA): random permutations and cluster size control</article-title><source>NeuroImage</source><volume>65</volume><fpage>69</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.063</pub-id><pub-id pub-id-type="pmid">23041526</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talmy</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Lexicalization patterns: semantic structure in lexical forms</article-title><source>Language Typology and Syntactic Description</source><volume>3</volume><fpage>36</fpage><lpage>149</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname> <given-names>JC</given-names></name><name><surname>Wiggett</surname> <given-names>AJ</given-names></name><name><surname>Downing</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>fMRI-adaptation studies of viewpoint tuning in the extrastriate and fusiform body Areas</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>1467</fpage><lpage>1477</lpage><pub-id pub-id-type="doi">10.1152/jn.00637.2009</pub-id><pub-id pub-id-type="pmid">20032242</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Tucciarelli</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>RESPACT</data-title><source>Open Science Framework</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/cvrb2/">https://osf.io/cvrb2/</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turella</surname> <given-names>L</given-names></name><name><surname>Lingnau</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural correlates of grasping</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>686</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00686</pub-id><pub-id pub-id-type="pmid">25249960</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinson</surname> <given-names>DP</given-names></name><name><surname>Vigliocco</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Semantic feature production norms for a large set of objects and events</article-title><source>Behavior Research Methods</source><volume>40</volume><fpage>183</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.3758/BRM.40.1.183</pub-id><pub-id pub-id-type="pmid">18411541</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname> <given-names>CE</given-names></name><name><surname>Cardillo</surname> <given-names>ER</given-names></name><name><surname>Ianni</surname> <given-names>GR</given-names></name><name><surname>Chatterjee</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Action concepts in the brain: an activation likelihood estimation meta-analysis</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1191</fpage><lpage>1205</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00401</pub-id><pub-id pub-id-type="pmid">23574587</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname> <given-names>CE</given-names></name><name><surname>Buxbaum</surname> <given-names>LJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Uncovering the architecture of action semantics</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>40</volume><fpage>1832</fpage><lpage>1848</lpage><pub-id pub-id-type="doi">10.1037/a0037449</pub-id><pub-id pub-id-type="pmid">25045905</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname> <given-names>MF</given-names></name><name><surname>Ariani</surname> <given-names>G</given-names></name><name><surname>Greenlee</surname> <given-names>MW</given-names></name><name><surname>Lingnau</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding concrete and abstract action representations during explicit and implicit conceptual processing</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3390</fpage><lpage>3401</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv169</pub-id><pub-id pub-id-type="pmid">26223260</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname> <given-names>MF</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name><name><surname>Lingnau</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Action categories in lateral occipitotemporal cortex are organized along sociality and transitivity</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>562</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1717-16.2016</pub-id><pub-id pub-id-type="pmid">28100739</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname> <given-names>MF</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Distinct roles of temporal and frontoparietal cortex in representing actions across vision and language</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>289</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08084-y</pub-id><pub-id pub-id-type="pmid">30655531</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname> <given-names>MF</given-names></name><name><surname>Caramazza</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Lateral occipitotemporal cortex encodes perceptual components of social actions rather than abstract representations of sociality</article-title><source>NeuroImage</source><volume>202</volume><elocation-id>116153</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116153</pub-id><pub-id pub-id-type="pmid">31491524</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname> <given-names>MF</given-names></name><name><surname>Lingnau</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Decoding actions at different levels of abstraction</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>7727</fpage><lpage>7735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0188-15.2015</pub-id><pub-id pub-id-type="pmid">25995462</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47686.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Lescroart</surname><given-names>Mark</given-names></name><role>Reviewing Editor</role><aff><institution>University of Nevada at Reno</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Lescroart</surname><given-names>Mark</given-names> </name><role>Reviewer</role><aff><institution>University of Nevada at Reno</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Hafri</surname><given-names>Alon</given-names> </name><role>Reviewer</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Many regions in the human brain have been implicated in the representation of observed actions, but the literature on action recognition has focused largely on a few classes of actions. Here, the authors seek to understand the representation of actions by studying how the common underlying aspects of many different actions are reflected in brain responses. Perhaps the best aspect of this paper is the large number of actions used as stimuli. The realistic visual variation in these actions grants the work an important increment in external validity compared to much past work. The authors also cope well with the complexity of their stimuli by considering multiple models of the underlying components (or features) of actions, which capture multiple ways that actions can be similar. These models do not exactly rule out alternative explanations for the results in the paper. Rather, the authors show how different aspects of observed actions (or visual features correlated with actions) are represented in partly overlapping regions of human lateral occipito-temporal cortex. The comparison with the other models makes the finding that semantic aspects of actions best capture responses in a particular parcel of left lateral occipito-temporal cortex interestingly specific and compelling.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;The representational space of observed actions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Mark Lescroart as the Reviewing Editor, and the evaluation has been overseen by Richard Ivry as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Alon Hafri.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The goal of the present study was to find which areas of the brain contain a representational space that reflects the semantic similarity space of actions. Using a set of photographic images of 28 everyday actions, inverse MDS, and multiple regression RSA of fMRI data, the authors found that semantic similarity ratings were best matched to representations in LOTC and left pIPS, over and above other models related to body parts, objects, movement, and scene contexts. The authors propose that these regions may act as the input for how humans represent the meaning of actions.</p><p>This work is significant in that it is the first to show that a punctate and limited set of regions (LOTC and pIPS) may be the substrate for human representation of action meanings from visual content. The broad stimulus set also provides an important generalization and expansion of past work on action representation. Consequently, the reviewers were unanimous in their enthusiasm for the impact and general quality of the study. However, the reviewers also raised concerns that must be addressed before the paper can be accepted.</p><p>Major comments:</p><p>1) The authors acknowledge some confounds in their experiment: some actions were photographed nearer than others, and some actions included two bodies where most included only one. The reviewers support the inclusion of these stimuli in the interest of broad sampling. However, the authors should create models to assess the severity of these potential confounds. Specifically, the authors should include a one- vs. two-person model and a near vs. far model as sixth and seventh models in the multiple regression RSA analysis.</p><p>2) The authors should also consider at least one image-derived model to assure that any differences they measure in the brain are not due to low-level differences between their conditions. The authors should choose a model that provides a better proxy for V1 processing than does the pixel energy model they currently use (e.g. a Gist model, Gabor model, HMax, or the first layer of nearly any convolutional neural network). Use of a better low-level model is necessary both to assess correlations with the other models and to be included in the multiple regression RSA analysis. Particularly if the RDM-RDM correlations are small (see below), there is the potential that only one or two outlying conditions in low-level similarity space could affect the results.</p><p>3) The authors should provide more information about the subjects' instructions in their behavioral rating sessions. Specifically, the authors should clarify whether &quot;meaning&quot; was defined for participants, and whether (and which) examples were given. These instructions strongly influence the interpretation of the models used in the study.</p><p>4) The authors should indicate whether some or all of the subjects were naive to the purpose of the study. This is necessary because the models of interest are all based on subjective ratings that might be influenced by knowledge of the purpose of the experiment.</p><p>5) Probabilistic regions, or at least probabilistic centroids of regions, for hMT+, EBA, and V3A/B should be labeled on the flatmaps. Such labels are available from published atlases and other work and would require no additional data collection. Currently, anatomical details are only reported for maxima of activation in Supplementary file 3. Labels on the flatmaps will make it clearer how the extent of activation in each condition relates to hMT+, EBA, and V3A/B (and by extension, to OPA/TOS). This is important given the extent to which anatomy figures in the Discussion.</p><p>6) The authors should report how large the model-RDM-to-brain-RDM correlations are in individual subjects, or at least an average and range of these correlations across subjects. The authors should also include some visualization of the brain-derived RDM(s) for an area or areas of interest to facilitate qualitative comparison with the model RDMs in Figure 2. One of the hazards of RSA is that correlation can be strongly influenced by a few outlying data points, so if a given model only captures one or two cells of the RDM well, small but above-chance correlation values can still result. If the overall correlation between the model RDMs and the brain RDMs is high (say, over 0.4), this is not a concern. However, in many RSA studies, the correlations between model- and brain-derived RDMs is quite low (r &lt; 0.05), which is in the range that a few outlying values could affect results. If this were true here, it would have consequences for the interpretation of the models, so some assurance is necessary that this is not the case.</p><p>7) The authors should report the searchlight radius.</p><p>8) Instead of correlation distance, the authors should use squared Euclidean distance as their RDM distance metric, as in (Bonner and Epstein, 2018; Khaligh-Razavi and Kriegeskorte, 2014). Since the distances from one brain RDM are modeled as linear combinations of the distances from a set of predictor RDMs, a distance metric that sums linearly is more in line with the modeling assumptions.</p><p>9) The reviewers do not object to use of static images. However, the authors should more explicitly justify the use of static images instead of videos, and discuss the consequences of not using moving stimuli. (This topic merits than the one sentence currently present around the third paragraph of the Discussion.)</p><p>10) A substantial portion of the Discussion is dedicated to the results in pIPS. This section is fairly long and strays from the data reported in the paper somewhat more than the other sections. Since the analysis is performed at a group level and there are no functional localizers reported, and since the location of OPA in particular can vary across individuals and is not tightly localized to the Temporal Occipital Sulcus (Dilks et al., 2013), it is difficult to say what the relation of the IPS result is as compared to OPA or other regions. This section should be re-written to more clearly indicate which parts are speculative, and potentially reduced in length.</p><p>11) Currently, discussion of the relationship between the recovered action similarity space and other work on this topic is confined to a few lines in the Discussion. The authors' point that their space depends on the actions they chose to include is well taken, but nonetheless the authors should discuss whether and how the dimensions of their similarity space relate to other hypotheses about the semantic structure of action representation. Work in language has thought at length on this issue (e.g. Pinker, 1989; Talmy, 1985; Kemmerer).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47686.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Major comments:</p><p>1) The authors acknowledge some confounds in their experiment: some actions were photographed nearer than others, and some actions included two bodies where most included only one. The reviewers support the inclusion of these stimuli in the interest of broad sampling. However, the authors should create models to assess the severity of these potential confounds. Specifically, the authors should include a one- vs. two-person model and a near vs. far model as sixth and seventh models in the multiple regression RSA analysis.</p></disp-quote><p>We thank the reviewers for this comment and we agree that the severity of these potential confounds should be assessed. To this aim, we carried out the multiple regression RSA with a number of additional models, as suggested by the reviewers. To construct the model depicting the distance between the observer and the action, we carried out an additional online rating study in N = 20 participants, asking participants to judge the distance between the observer and the actor (1: within reaching distance, 2: not within reaching distance, but within 3 meters, 3: further away than 3 meters). Moreover, we added a model depicting the number of people present in the action (1 vs. 2). To be able to compare our results with a previous study reporting sensitivity of the dorsal and ventral portion of the LOTC to sociality and transitivity, respectively (as also suggested by one of the reviewers), we carried out an additional online rating study to construct these two models as well. Finally, as suggested in Point 2, we also added the HMAX-C1 model as a regressor to account for any low-level differences between the conditions.</p><p>The results of the multiple regression RSA using the additional models described above are shown in Figures 7 and Figure 6—figure supplement 1 of the revised manuscript. Importantly, whereas the multiple regression RSA reported in the original version of the manuscript revealed clusters for the semantic model in the left parietal and left and right lateral occipito-temporal areas, the new analysis, including models for sociality, transitivity, distance, number of people, and the HMAX C1 model, only revealed the cluster in the left LOTC. This suggests that the results observed in the left parietal and the right LOTC in the previous version of the manuscript were indeed due to some of the variability accounted for in the models that we added to the new multiple regression RSA. By contrast, we can conclude with more confidence that the neural dissimilarity structure obtained in the left LOTC captured the semantic dissimilarity structure obtained behaviorally, over and beyond the dissimilarity captured in the nine other models.</p><p>Note that in contrast to the previous analysis, the multiple regression analysis for the scene model did not reveal any clusters that survived corrections for multiple comparisons, suggesting that the results we obtained for this model previously were accounted for in some of the models we added to the multiple regression RSA.</p><p>The transitivity model revealed a cluster that is in line with the cluster reported by Wurm, Caramazza, and Lingnau, 2017, whereas the sociality model did not reveal any clusters that survived corrections for multiple comparisons (note, however, that the current study was not designed to test this model, with only a small number of actions directed towards another person).</p><p>As can be seen in Figure 2—figure supplement 1 of the revised manuscript, there is a high correlation between the one- vs. two-person model and the sociality and transitivity models. However, the Variance Inflation Factor (VIF), reported in more detail in the manuscript, indicated a low risk of multicollinearity, thus justifying the use of a multiple regression approach.</p><p>As a consequence of these changes, we added the following paragraphs in the main manuscript:</p><p>Results section</p><p>“Given that a number of action components covary to some extent with semantic features (e.g. locomotion actions typically take place outdoors, cleaning-related actions involve certain objects, etc.; see also Figure 2—figure supplement 1), it is impossible to determine precisely what kind of information drove the RSA effects in the identified regions on the basis of the correlation-based RSA alone. […] Therefore, the multiple-regression RSA included ten predictors (semantic, body, scene, movement, object, sociality, transitivity, distance, 1 vs. 2 people, HMAX-C1).”</p><p>Materials and methods section:</p><p>“As predictors of the multiple-regression analysis, we used the RDMs described in the section Construction of representational dissimilarity matrices (semantic, body, movement, object, scene, sociality, transitivity, distance, 1 vs. 2 People, HMAX-C1; see also Figure 5).”</p><disp-quote content-type="editor-comment"><p>2) The authors should also consider at least one image-derived model to assure that any differences they measure in the brain are not due to low-level differences between their conditions. The authors should choose a model that provides a better proxy for V1 processing than does the pixel energy model they currently use (e.g. a Gist model, Gabor model, HMax, or the first layer of nearly any convolutional neural network). Use of a better low-level model is necessary both to assess correlations with the other models and to be included in the multiple regression RSA analysis. Particularly if the RDM-RDM correlations are small (see below), there is the potential that only one or two outlying conditions in low-level similarity space could affect the results.</p></disp-quote><p>We thank the reviewers for raising this point. As suggested, we constructed a model for early visual areas using the HMAX C1 model and added it as a regressor to the multiple regression RSA (see also response to point 1 above, and Figures 7 and Figure 6—figure supplement 1 of the revised manuscript). The correlations between the HMAX C1 model and the remaining models are shown in Figure 2—figure supplement 1 of the revised manuscript. Importantly, the HMAX C1 model showed no systematic correlations with the remaining models.</p><p>As a consequence of these changes, we updated the manuscript accordingly in the following sections:</p><p>Results section:</p><p>Original version:</p><p>“in bilateral anterior LOTC at the junction to posterior middle temporal gyrus, right posterior superior temporal sulcus, and left pIPS.”</p><p>Revised version:</p><p>“in the left anterior LOTC at the junction to the posterior middle temporal gyrus”.</p><p>Discussion section:</p><p>Original version:</p><p>…“we showed that this semantic similarity structure of observed actions was best captured in the patterns of activation in the LOTC (bilaterally) and left posterior IPL.”</p><p>Revised version:</p><p>…“we showed that this semantic similarity structure of observed actions was best captured in the patterns of activation in the left LOTC”.</p><disp-quote content-type="editor-comment"><p>3) The authors should provide more information about the subjects' instructions in their behavioral rating sessions. Specifically, the authors should clarify whether &quot;meaning&quot; was defined for participants, and whether (and which) examples were given. These instructions strongly influence the interpretation of the models used in the study.</p></disp-quote><p>Participants received the following written instructions (and examples):</p><p>- “- In the semantic similarity task, you will be asked to arrange the images with respect of their meaning: for example, sewing and ironing should be placed closer to each other than sewing and smoking […] - In the type of object task, actions involving similar objects, e.g. catching a football and throwing a tennis ball, should be placed closer to each other than throwing a tennis ball and throwing a pillow at another person.”</p><p>We added these written instructions to the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>4) The authors should indicate whether some or all of the subjects were naive to the purpose of the study. This is necessary because the models of interest are all based on subjective ratings that might be influenced by knowledge of the purpose of the experiment.</p></disp-quote><p>All participants but one (RT, one of the authors) were naïve to the purpose of the study. We added this information to the Materials and methods section.</p><disp-quote content-type="editor-comment"><p>5) Probabilistic regions, or at least probabilistic centroids of regions, for hMT+, EBA, and V3A/B should be labeled on the flatmaps. Such labels are available from published atlases and other work and would require no additional data collection. Currently, anatomical details are only reported for maxima of activation in Supplementary file 3. Labels on the flatmaps will make it clearer how the extent of activation in each condition relates to hMT+, EBA, and V3A/B (and by extension, to OPA/TOS). This is important given the extent to which anatomy figures in the Discussion.</p></disp-quote><p>We thank the reviewers for this suggestion. To provide labels for hMT+ and V3A/B, we added Figure 6—figure supplement 1 in which we superimposed the Glasser surface based atlas (Glasser et al., 2016) on top of the flat maps depicting the results of the multiple regression RSA. Labels for EBA are harder to establish since, to our knowledge, this area is not currently captured in any probabilistic atlas. However, for ease of interpretation of the results of the body model, we added Figure 6—figure supplement 2 in which we superimposed the coordinates of a number of previous studies that used specific EBA localizers on the flat maps showing the results of the multiple regression RSA for the body part model.</p><disp-quote content-type="editor-comment"><p>6) The authors should report how large the model-RDM-to-brain-RDM correlations are in individual subjects, or at least an average and range of these correlations across subjects. The authors should also include some visualization of the brain-derived RDM(s) for an area or areas of interest to facilitate qualitative comparison with the model RDMs in Figure 2. One of the hazards of RSA is that correlation can be strongly influenced by a few outlying data points, so if a given model only captures one or two cells of the RDM well, small but above-chance correlation values can still result. If the overall correlation between the model RDMs and the brain RDMs is high (say, over 0.4), this is not a concern. However, in many RSA studies, the correlations between model- and brain-derived RDMs is quite low (r &lt; 0.05), which is in the range that a few outlying values could affect results. If this were true here, it would have consequences for the interpretation of the models, so some assurance is necessary that this is not the case.</p></disp-quote><p>We thank the reviewers for this suggestion. To evaluate the representational geometry of the semantic cluster, for each participant we selected the 100 voxels neighboring the maximum T-scores for the semantic models using the multiple regression RSA.</p><p>Next, for each participant, we computed the neural DSM and compared it with the model DSMs using Kendall’s tau rank correlation coefficient. This was the same procedure used for each searchlight as described in the Materials and methods section “Representational Similarity Analysis (RSA) – Standard RSA”. The correlation scores were averaged across the 20 participants, and results are shown in Figure 8A of the revised manuscript.</p><p>Several aspects are worth noting in Figure 8A. First, as expected, the bar plot confirmed that the semantic model is the model that best correlates with the neural RDM (shown in Figure 8B of the revised manuscript). Not surprisingly (and in line with the results shown in Figure 7), also some of the other models significantly correlated with the neural RDM in the left LOTC with the exception of the scene model, the sociality model, the object model and the HMAX-C1 model. As noted by the reviewers, the averaged correlations between neural and model RDMs were quite low (semantic model: 0.0645, distance model: 0.0608, 1 vs. 2 people model: 0.0563; all other models &lt; 0.05). However, these were significantly greater than zero for 6 out of the 10 models (marked by an asterisk in Figure 8A). To rule out the possibility that the significant correlation between the neural DSM and the semantic DSM was due to only a few outliers, we conducted a classical multidimensional scaling (MDS) on the averaged neural DSM to visualize the representational geometry of the data. The result of this analysis is reported in Figure 8C, and the results of the k-means cluster analysis carried out on the behavioral data in Figure 3. For ease of comparison, actions were assigned to the same color code as the corresponding clusters identified from the behavioral analysis. As evident, the neural DSM of this region appears to encode a similar geometry as the one observed at the behavioral level for the semantic task. Similar results were obtained using hierarchical clustering analysis (see Figure 8D).</p><p>We have updated the manuscript in accordance with this new analysis:</p><p>“To have a better idea of the representational geometry encoded in the left LOTC cluster, we extracted the beta estimates associated with the 100 features neighboring the vertex with the highest T score in the cluster in the left LOTC revealed by the multiple regression RSA for the semantic model. […] To facilitate the comparison with Figure 3, we assigned actions with the same color code as the corresponding clusters identified from the behavioral analysis. A hierarchical clustering analysis showed a similar result (see Figure 8D).”</p><disp-quote content-type="editor-comment"><p>7) The authors should report the searchlight radius.</p></disp-quote><p>The radius of the searchlights was approximately 12 mm in radius on average. We now report this in the Materials and methods section:”… and was approximately 12 mm in radius”.</p><disp-quote content-type="editor-comment"><p>8) Instead of correlation distance, the authors should use squared Euclidean distance as their RDM distance metric, as in (Bonner and Epstein, 2018; Khaligh-Razavi and Kriegeskorte, 2014). Since the distances from one brain RDM are modeled as linear combinations of the distances from a set of predictor RDMs, a distance metric that sums linearly is more in line with the modeling assumptions.</p></disp-quote><p>We thank the reviewers for this important comment, and for suggesting a way to address this. After consulting the papers suggested by the reviewers, we re-ran the multiple regression RSA using the squared Euclidean distance as metric. The results are shown in Figure 6 (semantic model) and Figure 7 and Figure 6—figure supplement 1 (all remaining models) of the revised manuscript.</p><p>In general, results are qualitatively similar and the Squared Euclidean metric seems to have improved the estimation of the betas as noted by the larger T scores compared to the ones obtained when using Spearman correlation as distance metric. However, there are also important qualitative differences between the two approaches: the left IPL cluster and the more anterior STS cluster for the semantic model were not present anymore when using the squared Euclidean distance. Likewise, the cluster revealed for the scene model in the previous analysis was no longer present. For the body model, a new cluster in the right posterior LOTC survived corrections multiple-comparisons correction.</p><p>Note that in light of the additional models (and correspondingly, additional figures) we added to the manuscript, and given that the multiple regression RSA in the current study in our view really shows the key results, we decided to remove the results of the standard RSA for all models except for the semantic model (which we kept in for a direct comparison of the clusters revealed with and without controlling for the remaining models).</p><p>It is obvious from the references above (and from the comments of the reviewers) that it is more appropriate to use squared Euclidian distance as our RDM distance metric. By contrast, we are under the assumption that Spearman correlation should be the recommended RDM distance metric for the standard RSA. While the squared Euclidian distances should be linearly proportional to correlation distances (see also Bonner and Epstein, 2018), we wished to examine how the choice of the RDM distance metric (Spearman versus squared Euclidian distance) affects the results of the standard RSA. This comparison is shown in <xref ref-type="fig" rid="respfig1">Author response image 1</xref>. As can be seen, the statistical map revealed by the standard RSA using the two distance metrics look very similar, but not identical. In particular, the standard RSA using squared Euclidian distance reveals additional clusters in left and right dorsal premotor cortex that are not revealed by the standard RSA using Spearman correlation. Moreover, Tvalues in posterior parietal cortex appear to be strongest in parietal cortex and somewhat weaker in the LOTC, whereas the opposite appears to be true for the standard RSA using the squared Euclidian distance. Given these differences, we decided to show the results of the standard RSA using the squared Euclidian distance in the manuscript since we were concerned that differences between the standard and the multiple regression RSA would otherwise reflect a difference between using the semantic model only (standard RSA, Spearman) versus controlling for the remaining models plus differences induced by using another distance metric (multiple regression RSA, squared Euclidian distance).</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Comparison of the results of the standard RSA using Spearman correlation (top panel) and squared Euclidian distance (right panel) as distance metric.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47686-resp-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>9) The reviewers do not object to use of static images. However, the authors should more explicitly justify the use of static images instead of videos, and discuss the consequences of not using moving stimuli. (This topic merits than the one sentence currently present around the third paragraph of the Discussion.)</p></disp-quote><p>As suggested by the reviewers, we now explained why we used static images instead of videos (see Materials and methods section, “Stimulus selection”, last paragraph) and discuss the consequences of using static images instead of moving stimuli in the Discussion (fourth paragraph).</p><disp-quote content-type="editor-comment"><p>10) A substantial portion of the Discussion is dedicated to the results in pIPS. This section is fairly long and strays from the data reported in the paper somewhat more than the other sections. Since the analysis is performed at a group level and there are no functional localizers reported, and since the location of OPA in particular can vary across individuals and is not tightly localized to the Temporal Occipital Sulcus (Dilks et al., 2013), it is difficult to say what the relation of the IPS result is as compared to OPA or other regions. This section should be re-written to more clearly indicate which parts are speculative, and potentially reduced in length.</p></disp-quote><p>In light of the results revealed by the new analysis, we decided to delete this paragraph.</p><disp-quote content-type="editor-comment"><p>11) Currently, discussion of the relationship between the recovered action similarity space and other work on this topic is confined to a few lines in the Discussion. The authors' point that their space depends on the actions they chose to include is well taken, but nonetheless the authors should discuss whether and how the dimensions of their similarity space relate to other hypotheses about the semantic structure of action representation. Work in language has thought at length on this issue (e.g. Pinker, 1989; Talmy, 1985; Kemmerer).</p></disp-quote><p>We agree with the reviewers that this point should be made more explicit. We have added a new section on this aspect in the Discussion:</p><p>“To the best of our knowledge, the only study that explicitly examined dimensions underlying the organization of actions focused on tools and the way they are typically used. […] Based on the existing literature on semantic categories, we expect that future studies, using a similar approach as described in the current study with a wider range of actions, will reveal additional categories and overarching dimensions that show similarities, but are not necessarily identical to the semantic categories described in the literature so far.”</p></body></sub-article></article>