<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">55964</article-id><article-id pub-id-type="doi">10.7554/eLife.55964</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Automated task training and longitudinal monitoring of mouse mesoscale cortical circuits using home cages</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-63833"><name><surname>Murphy</surname><given-names>Timothy H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0093-4490</contrib-id><email>thmurphy@mail.ubc.ca</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177621"><name><surname>Michelson</surname><given-names>Nicholas J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177622"><name><surname>Boyd</surname><given-names>Jamie D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177623"><name><surname>Fong</surname><given-names>Tony</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-151531"><name><surname>Bolanos</surname><given-names>Luis A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-151532"><name><surname>Bierbrauer</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-151533"><name><surname>Siu</surname><given-names>Teri</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177624"><name><surname>Balbi</surname><given-names>Matilde</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-151535"><name><surname>Bolanos</surname><given-names>Federico</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177625"><name><surname>Vanni</surname><given-names>Matthieu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">†</xref></contrib><contrib contrib-type="author" id="author-151530"><name><surname>LeDue</surname><given-names>Jeff M</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychiatry, Kinsmen Laboratory of Neurological Research</institution><addr-line><named-content content-type="city">Vancouver</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution>Djavad Mowafaghian Centre for Brain Health, University of British Columbia</institution><addr-line><named-content content-type="city">Vancouver</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kleinfeld</surname><given-names>David</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, San Diego</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>University of Montreal, Montreal, Canada</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>15</day><month>05</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e55964</elocation-id><history><date date-type="received" iso-8601-date="2020-02-12"><day>12</day><month>02</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-05-07"><day>07</day><month>05</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Murphy et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Murphy et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-55964-v2.pdf"/><abstract><p>We report improved automated open-source methodology for head-fixed mesoscale cortical imaging and/or behavioral training of home cage mice using Raspberry Pi-based hardware. Staged partial and probabilistic restraint allows mice to adjust to self-initiated headfixation over 3 weeks’ time with ~50% participation rate. We support a cue-based behavioral licking task monitored by a capacitive touch-sensor water spout. While automatically head-fixed, we acquire spontaneous, movement-triggered, or licking task-evoked GCaMP6 cortical signals. An analysis pipeline marked both behavioral events, as well as analyzed brain fluorescence signals as they relate to spontaneous and/or task-evoked behavioral activity. Mice were trained to suppress licking and wait for cues that marked the delivery of water. Correct rewarded go-trials were associated with widespread activation of midline and lateral barrel cortex areas following a vibration cue and delayed frontal and lateral motor cortex activation. Cortical GCaMP signals predicted trial success and correlated strongly with trial-outcome dependent body movements.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cortex</kwd><kwd>homecage</kwd><kwd>automation</kwd><kwd>mesoscale</kwd><kwd>imaging</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000024</institution-id><institution>Canadian Institutes of Health Research</institution></institution-wrap></funding-source><award-id>FDN-143209</award-id><principal-award-recipient><name><surname>Murphy</surname><given-names>Timothy H</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001674</institution-id><institution>Fondation Leducq</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Murphy</surname><given-names>Timothy H</given-names></name><name><surname>Balbi</surname><given-names>Matilde</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Canadian Neurophotonics Platform</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Murphy</surname><given-names>Timothy H</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000245</institution-id><institution>Michael Smith Foundation for Health Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Balbi</surname><given-names>Matilde</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21</award-id><principal-award-recipient><name><surname>Murphy</surname><given-names>Timothy H</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new methodology and protocol use automated brain imaging and task training technology to probe links between behavior and mouse cortical circuits.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Variability in animal experiments can be reduced through automation of training and data acquisition procedures in combined behavioral assessment and brain imaging of headfixed mice. Automation has been extended to complex headfixed visual tasks with a relatively good success rate in mice (<xref ref-type="bibr" rid="bib2">Aoki et al., 2017</xref>) and headfixed rats (<xref ref-type="bibr" rid="bib44">Scott et al., 2013</xref>). While the work described in Aoki et al in mice was impressive for introducing a complex visual task to home cage training, this work did not autonomously gather longitudinal mesoscale brain imaging data, or use with animals in a group and relied on daily supervised 2-photon imaging. Such supervised imaging following home cage training of individual animals will permit high-resolution microscopy in the context of extended automated training (<xref ref-type="bibr" rid="bib2">Aoki et al., 2017</xref>). Recently, our lab and others have begun to automate the acquisition of mesoscale functional brain imaging (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) and motor actions data using compact and cost-effective Linux-based Raspberry Pi or other single board computers (<xref ref-type="bibr" rid="bib3">Ardesch et al., 2017</xref>; <xref ref-type="bibr" rid="bib49">Silasi et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Woodard et al., 2017</xref>). This work extends previous automated analysis of animal behaviors within home cages (<xref ref-type="bibr" rid="bib8">Bollu et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Robinson et al., 2013</xref>; <xref ref-type="bibr" rid="bib42">Robinson and Riedel, 2014</xref>; <xref ref-type="bibr" rid="bib41">Robinson et al., 2018</xref>) to the more complex brain circuit level analysis during behavioral tasks in combination with mesoscale imaging. Such automated studies offer potential for 24/7 data acquisition from large numbers of animals in a relatively unperturbed manner. While automated mouse (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) and rat brain imaging has recently been described (<xref ref-type="bibr" rid="bib44">Scott et al., 2013</xref>), the work presented here provides a significant improvement over the initial mouse-based designs (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) and to led to higher rates of head fixation, greater data throughput, and coupling to a behavioral task. Furthermore, we provide extensive documentation on how to construct these cages and implement a head-fixing and task training protocol. Our refined cage allows the acquisition of datasets that co-register behavioral and functional brain imaging data, providing the basis for a flexible open-source tool for chronic experiments that could assess aspects of human neurological disorders modelled in mice.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Improvements to the cage hardware</title><p>We describe results and procedures to build and use a new generation of home cages with emphasis on training for simple headfixed tasks during simultaneous autonomous acquisition of mesoscale brain imaging data (<xref ref-type="fig" rid="fig1">Figure 1A,B</xref> for cage scheme and <xref ref-type="fig" rid="fig2">Figure 2</xref> for training). We have tested 44 adult male GCaMP6 transgenic mice (<xref ref-type="table" rid="table1">Table 1</xref> and <xref ref-type="fig" rid="fig3">Figure 3</xref>) and an additional cohort of 8 female mice that were trained for ~7 h/day that yielded similar levels of participation (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Mesoscale widefield in vivo imaging maps activation of cortex over a field-of-view that can extend bilaterally from olfactory bulb to visual cortex (<xref ref-type="bibr" rid="bib31">Mohajerani et al., 2013</xref>). In contrast to single cell imaging needing high resolution in the X-Y plane and in depth, wide field imaging is typically done with larger focal volume (~2–3 mm) to image curved brain surfaces and lower spatial resolution (pixels binned to ~32–49 μm) to sum more photons over a larger area (<xref ref-type="bibr" rid="bib24">Lim et al., 2012</xref>). Thus, the intrinsically large depth of field and reduced resolution provided by the small lenses employed by compact cameras such as the Raspberry Pi Camera Module (<xref ref-type="fig" rid="fig1">Figure 1B,C</xref>) are not expected to degrade signals. While we emphasize single photon mesoscale widefield imaging within these cages, our system would also be suitable for training animals on complex headfixed tasks where imaging could be performed using parallel manual headfixation and advanced two-photon imaging or electrophysiological procedures (<xref ref-type="bibr" rid="bib12">Chen et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Le Merre et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Prsa et al., 2017</xref>), as was already elegantly applied by others (<xref ref-type="bibr" rid="bib2">Aoki et al., 2017</xref>). We include an extensive supplemental guide with full construction illustrations, parts lists (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> and <xref ref-type="table" rid="table2">Table 2</xref>), share files with Python acquisition and analysis code, and various CAD and electronics schematics in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref> or as online resources.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Tandem autoheadfixation and brain imaging home cage.</title><p>(<bold>A</bold>) 3D graphic of the home cage system with 8 mice. Two cages are connected via a tunnel with an RFID sensor underneath and designed to automatically monitor the weight of the traversing mice. (<bold>B</bold>) Side view of the imaging chamber with a head-fixed mouse. An RFID sensor identifies the specific mouse. The snout of the mouse breaking the IR beam initiates the servo motor to head-fix the mouse by using the mounted aluminum T-piece to press the head-bar against the metal reinforced end of the chamber. Imaging LEDs, brain camera, and body cameras which record the behavior (eye, face and body) of the head-fixed animal are then activated. The spout provides the water rewards and detects licks for the task via a capacitance sensor. The Raspberry Pi processes inputs from the lick sensor and controls task related electronics (water solenoid, stimulus buzzer, auditory feedback). (<bold>C</bold>) Photo of the assembled major cage components. The Raspberry Pi and a printed circuit board (PCB) containing transistor logic switches and servo driving hardware are covered in a metal case (upper right). Electronic devices are plugged into headers on the PCB breakout (Water solenoid, Servo Motor) or into the Raspberry Pi directly (HDMI, USB RFID Reader). Picam with triple band filter 69013 m Chroma was used for brain imaging. IR beam breakers, auditory feedback buzzer, LEDs, vibration motor and licking sensor adapters (need short wiring to reduce noise) can be plugged via unique cables to the breakout extension board and to the Raspberry Pi. The breakout also supplies several regulated supply voltages and contains a small breadboard for use in developing future behavioral tasks.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig1-v2.tif"/></fig><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Scheme of the home cage training procedure leading to self-initiated head-fixing and task-based brain imaging.</title><p>(<bold>A</bold>) Mice were kept in a training double-cage with a connecting tunnel to allow acclimation of pooled liters to achieve 8–10 mice of a particular genotype, sex, and age and receive ad lib water via a bottle (protocol step 1–2). After relocation to the home cage mice learned that water was provided from a spout near the entrance of the imaging chamber. Over time the spout is moved towards the end of the chamber (step 3). Once mice break the IR beam at the end of the chamber they were rewarded with more water. During this beam break training basic task trials can be introduced (step 4). Mice need to withhold licking for a short time to get a water drop after a buzz cue and receive auditory feedback for inappropriate licking. Requested lick withhold time can be increased each day. After mice learn the basic concept of lick withholding the servo motor was engaged and they are loose head-fixed for approximately 20 s with a random probability of 50%. The ratio of loose head-fixed trials was increased during the next week to ~70% and servo motor travel was increased further reducing head movement (step 5). When servo is tight enough to press the head-bar against the chamber metal stop the mice were trained to detect a stimulus (step 6). A licking response is now required in order to trigger water delivery. Additionally, the mouse has to delay its licking for some time after the stimulus (vibration) was given. During the next weeks head-fix duration and delay time are increased. In some cases no-go trials were introduced for cages with long runtime (steps 7–8). The acquisition software is organized by Python code stimulator objects that achieve specific broad training goals (these are termed stages: stage 1 rewards, stage 2 lick with-hold, stage 3 cued licking, stage 4 go and no-go). (<bold>B</bold>) General structure of a task trial within a headfixed session. To initiate a trial the mouse has to withhold its licking (blue) for an interval termed ‘lick withhold time’. A vibration cue follows the cued licking (green arrow can be go or no-go based on buzz frequency). After the stimulus the mouse had to delay its licking for self-timed period (red) until the response licking period. Delay time was increased during step 6–8. After the delay time has passed the mouse needs to lick within 1.25 s in Go trials. If the mouse correctly responded to the go stimulus water is provided after the 1.25 s. In no-go trials the mouse has to further suppress licking waiting for the next stimulus. Auditory feedback was given for inappropriate licks in no-go trials. Delay time was first introduced in group 2. Groups 2–3 faced constant delay times (0.5 s and 0.25 s). Groups 4–5 experienced delay time training.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig2-v2.tif"/></fig><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Entry, licking, and headfixing statistic for all mice tested.</title><p>(<bold>A</bold>) Entries into the head-fix chamber, (<bold>B</bold>) spout licks, (<bold>C</bold>) head-fixes and (<bold>D</bold>) total time spent under head-fixation. All plots display participation relative to the starting day of loose head-fixing protocol (day 0). Entries are counted when the RFID sensor initially detects the presence of an RFID tag (implanted within the abdomen of a mouse). The lick detector in the first cage (group 1) was installed 6 days after beginning of the record, but before loose head-fixing protocol started. The 5 cage groups had slightly different runtimes and external interruptions of the systems (i.e. day 13–17 of Group 3 were not collected due to a power outage at the animal facility, animals were manually given water).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Headfixing and Go-trial statistics for 8 female mice training using ~7 h/day laboratory-based unsupervised training.</title><p>(<bold>A</bold>) Entries/day for each of 5 mice that were active in the task. (<bold>B</bold>) Head-fixes/day, (<bold>C</bold>) Licks/day, (<bold>D</bold>) total task trials presented with any outcome, (<bold>E</bold>) Successful trials/day, and (<bold>F</bold>) total licks for each trial outcome across all female mice, note due to timing errors a small number of licks to appear in the lick withhold period.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig3-figsupp1-v2.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Headfixation statistics for 23 of 44 male and 5 of 8 female mice (lower panel) that were good performers.</title><p>6 of the 23 shown male mice were subsequently removed from the task due to health issues (2/6) or because they suddenly stopped performing (4/6). All together 23 of 44 mice were removed from the task, due to issues with the window and/or health (6/23), sideways entries (3/23) or poor participation (14/23) while 21 of 44 continued to perform. 4 mice of these latter 14 already stopped participating before headfixation protocol started. All behavioral data are available online in database where animal specific queries can be made (see Methods).Genotypes: Ai94-GCaMP6s (Group 1,3), Thy1-GCaMP6s (Group 2), TTA-GCaMP6s (Group 4,5, 6 females). Headfixed stats were calculated for the number of days with headfixation as indicated and only headfixed trials were used for calculation of task success rate.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Male Mice Facility trained 24/7</th><th valign="bottom">Group</th><th valign="bottom">Days under headfix protocol</th><th valign="bottom">Days with head-fixation</th><th valign="bottom">Total head-fixes</th><th valign="bottom">Total hours head-fixed</th><th valign="bottom">Head-fixes <break/><break/>/day</th><th valign="bottom">Minutes <break/><break/>head-fixed <break/><break/>/day</th><th valign="bottom">Success rate <break/><break/>GO trials of <break/><break/>last 5 days [%]</th><th valign="top">Number of trials daily average based on 5 days at max success, all outcomes</th></tr></thead><tbody><tr><td valign="bottom">2016080026</td><td valign="bottom">1</td><td valign="bottom">90</td><td valign="bottom">60</td><td valign="bottom">933</td><td valign="bottom">9.8</td><td valign="bottom">15.6</td><td valign="bottom">9.8</td><td valign="bottom">91.3</td><td valign="top">17.6</td></tr><tr><td valign="bottom">201608466</td><td valign="bottom">1</td><td valign="bottom">93</td><td valign="bottom">91</td><td valign="bottom">2440</td><td valign="bottom">26</td><td valign="bottom">26.8</td><td valign="bottom">17.1</td><td valign="bottom">87.8</td><td valign="top">46.8</td></tr><tr><td valign="bottom">201608468</td><td valign="bottom">1</td><td valign="bottom">93</td><td valign="bottom">93</td><td valign="bottom">8360</td><td valign="bottom">97.7</td><td valign="bottom">89.9</td><td valign="bottom">63</td><td valign="bottom">8.6</td><td valign="top">1615.2</td></tr><tr><td valign="bottom">201608481</td><td valign="bottom">1</td><td valign="bottom">92</td><td valign="bottom">91</td><td valign="bottom">2197</td><td valign="bottom">24.2</td><td valign="bottom">24.1</td><td valign="bottom">15.9</td><td valign="bottom">92.8</td><td valign="top">222.6</td></tr><tr><td valign="bottom">201609136</td><td valign="bottom">1</td><td valign="bottom">90</td><td valign="bottom">82</td><td valign="bottom">1469</td><td valign="bottom">16.1</td><td valign="bottom">17.9</td><td valign="bottom">11.8</td><td valign="bottom">56.5</td><td valign="top">335</td></tr><tr><td valign="bottom">201609336</td><td valign="bottom">1</td><td valign="bottom">93</td><td valign="bottom">90</td><td valign="bottom">1344</td><td valign="bottom">11.9</td><td valign="bottom">14.9</td><td valign="bottom">8</td><td valign="bottom">70.4</td><td valign="top">23</td></tr><tr><td valign="bottom">210608298</td><td valign="bottom">1</td><td valign="bottom">92</td><td valign="bottom">88</td><td valign="bottom">2369</td><td valign="bottom">25.4</td><td valign="bottom">26.9</td><td valign="bottom">17.3</td><td valign="bottom">95.2</td><td valign="top">209.8</td></tr><tr><td valign="bottom">2016090793</td><td valign="bottom">2</td><td valign="bottom">43</td><td valign="bottom">32</td><td valign="bottom">1397</td><td valign="bottom">11.5</td><td valign="bottom">43.7</td><td valign="bottom">21.6</td><td valign="bottom">41.4</td><td valign="top">162.4</td></tr><tr><td valign="bottom">2016090943</td><td valign="bottom">2</td><td valign="bottom">47</td><td valign="bottom">37</td><td valign="bottom">1118</td><td valign="bottom">9.2</td><td valign="bottom">30.2</td><td valign="bottom">14.9</td><td valign="bottom">34.6</td><td valign="top">184.8</td></tr><tr><td valign="bottom">2016091112</td><td valign="bottom">2</td><td valign="bottom">13</td><td valign="bottom">7</td><td valign="bottom">141</td><td valign="bottom">1.1</td><td valign="bottom">20.1</td><td valign="bottom">9.1</td><td valign="bottom">6.1</td><td valign="top">28</td></tr><tr><td valign="bottom">2016090629</td><td valign="bottom">3</td><td valign="bottom">36</td><td valign="bottom">34</td><td valign="bottom">763</td><td valign="bottom">9.4</td><td valign="bottom">22.4</td><td valign="bottom">16.5</td><td valign="bottom">78.3</td><td valign="top">196.2</td></tr><tr><td valign="bottom">2016090797</td><td valign="bottom">3</td><td valign="bottom">33</td><td valign="bottom">30</td><td valign="bottom">866</td><td valign="bottom">10.8</td><td valign="bottom">28.9</td><td valign="bottom">21.7</td><td valign="bottom">75.8</td><td valign="top">173.2</td></tr><tr><td valign="bottom">2016090882</td><td valign="bottom">3</td><td valign="bottom">14</td><td valign="bottom">12</td><td valign="bottom">170</td><td valign="bottom">2</td><td valign="bottom">14.2</td><td valign="bottom">9.8</td><td valign="bottom">62.3</td><td valign="top">96.6</td></tr><tr><td valign="bottom">2016090964</td><td valign="bottom">3</td><td valign="bottom">9</td><td valign="bottom">8</td><td valign="bottom">144</td><td valign="bottom">1.4</td><td valign="bottom">18</td><td valign="bottom">10.6</td><td valign="bottom">58.8</td><td valign="top">100.5</td></tr><tr><td valign="bottom">2016090965</td><td valign="bottom">3</td><td valign="bottom">35</td><td valign="bottom">31</td><td valign="bottom">1024</td><td valign="bottom">12.8</td><td valign="bottom">33</td><td valign="bottom">24.8</td><td valign="bottom">68.8</td><td valign="top">185.8</td></tr><tr><td valign="bottom">2016090985</td><td valign="bottom">3</td><td valign="bottom">36</td><td valign="bottom">34</td><td valign="bottom">2072</td><td valign="bottom">25.9</td><td valign="bottom">60.9</td><td valign="bottom">45.7</td><td valign="bottom">0.4</td><td valign="top">581</td></tr><tr><td valign="bottom">2016091183</td><td valign="bottom">3</td><td valign="bottom">13</td><td valign="bottom">12</td><td valign="bottom">288</td><td valign="bottom">3.2</td><td valign="bottom">24</td><td valign="bottom">15.9</td><td valign="bottom">40.7</td><td valign="top">130.2</td></tr><tr><td valign="bottom">2016080252</td><td valign="bottom">4</td><td valign="bottom">85</td><td valign="bottom">73</td><td valign="bottom">1314</td><td valign="bottom">14.6</td><td valign="bottom">18</td><td valign="bottom">12</td><td valign="bottom">74.9</td><td valign="top">74.8</td></tr><tr><td valign="bottom">201608423</td><td valign="bottom">4</td><td valign="bottom">91</td><td valign="bottom">84</td><td valign="bottom">2513</td><td valign="bottom">29.9</td><td valign="bottom">29.9</td><td valign="bottom">21.4</td><td valign="bottom">73.1</td><td valign="top">359.8</td></tr><tr><td valign="bottom">201608474</td><td valign="bottom">4</td><td valign="bottom">35</td><td valign="bottom">33</td><td valign="bottom">632</td><td valign="bottom">6.7</td><td valign="bottom">19.2</td><td valign="bottom">12.1</td><td valign="bottom">42.9</td><td valign="top">199</td></tr><tr><td valign="bottom">801010205</td><td valign="bottom">5</td><td valign="bottom">22</td><td valign="bottom">21</td><td valign="bottom">368</td><td valign="bottom">2.8</td><td valign="bottom">17.5</td><td valign="bottom">8</td><td valign="bottom">31.5</td><td valign="top">92.8</td></tr><tr><td valign="bottom">801010219</td><td valign="bottom">5</td><td valign="bottom">36</td><td valign="bottom">34</td><td valign="bottom">803</td><td valign="bottom">7</td><td valign="bottom">23.6</td><td valign="bottom">12.4</td><td valign="bottom">34.6</td><td valign="top">153.4</td></tr><tr><td valign="bottom">801010270</td><td valign="bottom">5</td><td valign="bottom">34</td><td valign="bottom">27</td><td valign="bottom">362</td><td valign="bottom">3</td><td valign="bottom">13.4</td><td valign="bottom">6.7</td><td valign="bottom">30.2</td><td valign="top">130.4</td></tr><tr><td valign="bottom">AVG</td><td valign="bottom"/><td valign="bottom"><bold>53.3</bold></td><td valign="bottom"><bold>48</bold></td><td valign="bottom"><bold>1438.6</bold></td><td valign="bottom"><bold>15.8</bold></td><td valign="bottom"><bold>27.5</bold></td><td valign="bottom"><bold>17.7</bold></td><td valign="bottom"><bold>54.7</bold></td><td valign="top"><bold>231.3</bold></td></tr><tr><td valign="bottom">STDEV</td><td valign="bottom"/><td valign="bottom"><bold>32.4</bold></td><td valign="bottom"><bold>31.0</bold></td><td valign="bottom"><bold>1690.9</bold></td><td valign="bottom"><bold>19.9</bold></td><td valign="bottom"><bold>17.3</bold></td><td valign="bottom"><bold>12.9</bold></td><td valign="bottom"><bold>28.4</bold></td><td valign="top"><bold>326.8</bold></td></tr><tr><td valign="bottom">SUM</td><td valign="bottom"/><td valign="bottom"><bold>1225</bold></td><td valign="bottom"><bold>1104</bold></td><td valign="bottom"><bold>33087</bold></td><td valign="bottom"><bold>362.4</bold></td><td valign="bottom"><bold>633.1</bold></td><td valign="bottom"><bold>406.1</bold></td><td valign="bottom"/><td valign="top"><bold>5318.9</bold></td></tr><tr><td colspan="10" valign="bottom">Female Mice lab trained ~ 7 h/day</td></tr><tr><td valign="bottom">801010240</td><td valign="bottom">6</td><td valign="bottom">83</td><td valign="bottom">62</td><td valign="bottom">953</td><td valign="bottom">9.9</td><td valign="bottom">13.4</td><td valign="bottom">9.6</td><td valign="bottom">42.9</td><td valign="top">78</td></tr><tr><td valign="bottom">2018121234</td><td valign="bottom">6</td><td valign="bottom">83</td><td valign="bottom">39</td><td valign="bottom">812</td><td valign="bottom">8.8</td><td valign="bottom">14.5</td><td valign="bottom">13.6</td><td valign="bottom">43.5</td><td valign="top">93.4</td></tr><tr><td valign="bottom">2018121244</td><td valign="bottom">6</td><td valign="bottom">83</td><td valign="bottom">57</td><td valign="bottom">818</td><td valign="bottom">8.1</td><td valign="bottom">11.9</td><td valign="bottom">8.6</td><td valign="bottom">30.6</td><td valign="top">123.4</td></tr><tr><td valign="bottom">2018121245</td><td valign="bottom">6</td><td valign="bottom">83</td><td valign="bottom">68</td><td valign="bottom">1308</td><td valign="bottom">14.2</td><td valign="bottom">18.4</td><td valign="bottom">12.5</td><td valign="bottom">36.8</td><td valign="top">125.4</td></tr><tr><td valign="bottom">2018121379</td><td valign="bottom">6</td><td valign="bottom">83</td><td valign="bottom">70</td><td valign="bottom">1532</td><td valign="bottom">16.2</td><td valign="bottom">21.6</td><td valign="bottom">13.9</td><td valign="bottom">34.4</td><td valign="top">148.2</td></tr><tr><td valign="bottom">AVG</td><td valign="bottom"/><td valign="bottom"><bold>83</bold></td><td valign="bottom"><bold>59.2</bold></td><td valign="bottom"><bold>1084.6</bold></td><td valign="bottom"><bold>11.5</bold></td><td valign="bottom"><bold>16.0</bold></td><td valign="bottom"><bold>11.6</bold></td><td valign="bottom"><bold>37.6</bold></td><td valign="top"><bold>113.68</bold></td></tr><tr><td valign="bottom">STDEV</td><td valign="bottom"/><td valign="bottom"><bold>0</bold></td><td valign="bottom"><bold>12.4</bold></td><td valign="bottom"><bold>321.2</bold></td><td valign="bottom"><bold>3.6</bold></td><td valign="bottom"><bold>4.0</bold></td><td valign="bottom"><bold>2.4</bold></td><td valign="bottom"><bold>5.5</bold></td><td valign="top"><bold>27.9</bold></td></tr><tr><td valign="bottom">SUM</td><td valign="bottom"/><td valign="bottom"><bold>415</bold></td><td valign="bottom"><bold>355.2</bold></td><td valign="bottom"><bold>5423</bold></td><td valign="bottom"><bold>57.3</bold></td><td valign="bottom"><bold>79.8</bold></td><td valign="bottom"><bold>58.2</bold></td><td valign="bottom"/><td valign="top"><bold>568.4</bold></td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Parts for Headfixing System.</title></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4" valign="bottom">Description</th><th valign="bottom">#</th><th valign="bottom">Manufacturer</th><th valign="bottom">Part Number</th></tr></thead><tbody><tr><td colspan="4" valign="bottom">Cage</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">1/4–20 Bolts, Setscrews, Nuts, Washers</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">8/32 Bolts, Nuts, Washers</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">M2 and M3 Screws, Nuts, and Washers</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Cage 7.5’ X 11.5’ X 5’</td><td valign="bottom">2</td><td valign="bottom">Lab Products</td><td valign="bottom">10027</td></tr><tr><td colspan="4" valign="bottom">Aluminum Breadboard 18’ x 24’ x 1/2’, 1/4’−20 Taps</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">MB1824</td></tr><tr><td colspan="4" valign="bottom">Ø1’ Pillar Posts with 1/4’−20 Taps, 2’</td><td valign="bottom">4</td><td valign="bottom">Thorlabs</td><td valign="bottom">RS2</td></tr><tr><td colspan="4" valign="bottom">Ø1’ Pillar Posts with 1/4’−20 Taps, 3’</td><td valign="bottom">8</td><td valign="bottom">Thorlabs</td><td valign="bottom">RS3</td></tr><tr><td colspan="4" valign="bottom">Ø1’ Pillar Posts with 1/4’−20 Taps, 6’</td><td valign="bottom">8</td><td valign="bottom">Thorlabs</td><td valign="bottom">RS6</td></tr><tr><td colspan="4" valign="bottom">Clamping Fork, 1.24’ Counterbored Slot, Universal</td><td valign="bottom">4</td><td valign="bottom">Thorlabs</td><td valign="bottom">CF125</td></tr><tr><td colspan="4" valign="bottom">Ø1/2’ Pedestal Post Holder</td><td valign="bottom">3</td><td valign="bottom">Thorlabs</td><td valign="bottom">PH2E</td></tr><tr><td colspan="4" valign="bottom">Ø1/2’ Optical Post, SS, 8–32 Setscrew, 1/4’−20 Tap, L = 8’</td><td valign="bottom">3</td><td valign="bottom">Thorlabs</td><td valign="bottom">TR8</td></tr><tr><td colspan="4" valign="bottom">Ø1/2’ Optical Post, SS, 8–32 Setscrew, 1/4’−20 Tap, L = 12’</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">TR12</td></tr><tr><td colspan="4" valign="bottom">Right-Angle Clamp for Ø1/2’ Posts, 3/16’ Hex</td><td valign="bottom">2</td><td valign="bottom">Thorlabs</td><td valign="bottom">RA90</td></tr><tr><td colspan="4" valign="bottom">Ø25 mm Post Spacer, Thickness = 3 mm</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">RS3M</td></tr><tr><td colspan="4" valign="bottom">Ø1.25’ Studded Pedestal Base Adapter, 1/4’−20 Thread</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">BE1</td></tr><tr><td colspan="4" valign="bottom">Glass cut to 93 mm X 31 mm X 3 mm rectangle</td><td valign="bottom">1</td><td valign="bottom">Superglass</td><td valign="bottom">Custom</td></tr><tr><td colspan="4" valign="bottom">HS-645MG High Torque, Metal Gear Premium Sport Servo</td><td valign="bottom">1</td><td valign="bottom">Hitec</td><td valign="bottom">32645S</td></tr><tr><td colspan="4" valign="bottom">HSB-9475SH brushless motor servo superior performance</td><td valign="bottom">1</td><td valign="bottom">Hitec</td><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">RFID Reader ID-20LA (125 kHz)</td><td valign="bottom">2</td><td valign="bottom">IDInnovations</td><td valign="bottom">ID-20LA</td></tr><tr><td colspan="4" valign="bottom">RFID Reader Breakout</td><td valign="bottom">2</td><td valign="bottom">Sparkfun</td><td valign="bottom">13030</td></tr><tr><td colspan="4" valign="bottom">IR Break Beam Sensor - 5 mm LEDs</td><td valign="bottom">1</td><td valign="bottom">Adafruit</td><td valign="bottom">2168</td></tr><tr><td colspan="4" valign="bottom">NIR-Blocking Filters CALFLEX X</td><td valign="bottom">1</td><td valign="bottom">Qioptiq</td><td valign="bottom">G380227033</td></tr><tr><td colspan="4" valign="bottom">RPi Camera (F), Supports Night Vision, Adjustable-Focus</td><td valign="bottom">2</td><td valign="bottom">Waveshare</td><td valign="bottom">10299</td></tr><tr><td colspan="4" valign="bottom">Flex Cable for Raspberry Pi Camera or Display - 2 meters</td><td valign="bottom">2</td><td valign="bottom">Adafruit</td><td valign="bottom">2144</td></tr><tr><td colspan="4" valign="bottom">Load Cell Amplifier - HX711</td><td valign="bottom">1</td><td valign="bottom">Sparkfun</td><td valign="bottom">SEN-13879</td></tr><tr><td colspan="4" valign="bottom">Micro Load Cell (0–100 g) - CZL639HD</td><td valign="bottom">1</td><td valign="bottom">Bonad/Alibaba</td><td valign="bottom">CZL639M</td></tr><tr><td colspan="4" valign="bottom">Capacitance sensor MRP121</td><td valign="bottom">1</td><td valign="bottom">Adafruit</td><td valign="bottom">1982</td></tr><tr><td colspan="4" valign="bottom">Machined Parts (Stainless Steel)</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Head_fix_servo_coupler_PROFESH_revised_no_pinch.ipt</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom">Servo T-arm</td></tr><tr><td colspan="4" valign="bottom">ahf_contact_plate_L.stp</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom">Tube ending</td></tr><tr><td colspan="4" valign="bottom">ahf_contact_plate_R.stp</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom">Tube ending</td></tr><tr><td colspan="4" valign="bottom">3D Printed Parts (Black PLA)</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Camera_Mount_V2.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">head_bar_grabbing_plate_extended_6 mm_hole_in_bottom v25_reinforced.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Tunnel_Coupler.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Tunnel_Guider.stl</td><td valign="bottom">2</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Tunnel_V2.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">RFID_Holder.stl</td><td valign="bottom">2</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Bottle_Holder.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">AHF_HeadStraightener_25 mm.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">AHF_HeadStraightener_50 mm.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">3D Printed Parts (Protolabs Watershed plastic)</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">head_bar_grabbing_plate_extended_6 mm_hole_in_bottom V24 barrier1mmgreater_cut_front_bottom.stl</td><td valign="bottom">1</td><td valign="bottom">Protolabs special order</td><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Water reward parts valve, tubing etc.</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Ø1’ Pillar Post, 1/4’−20 Taps, L = 12’</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">RS12</td></tr><tr><td colspan="4" valign="bottom">Water Solenoid</td><td valign="bottom">1</td><td valign="bottom">Gems Sensor</td><td valign="bottom">45M6131</td></tr><tr><td colspan="4" valign="bottom">Male Luer 1/16</td><td valign="bottom">2</td><td valign="bottom">Component Supply Co.</td><td valign="bottom">LN-ML-062</td></tr><tr><td colspan="4" valign="bottom">Polyurethane Tubing 1/8&quot;ID X 3/16&quot;OD</td><td valign="bottom">1</td><td valign="bottom">Component Supply Co.</td><td valign="bottom">PUT-02A</td></tr><tr><td colspan="4" valign="bottom">Med/Surgical Tubing 1/16&quot;ID X 1/8&quot;OD</td><td valign="bottom">1</td><td valign="bottom">Component Supply Co.</td><td valign="bottom">TND65-062A</td></tr><tr><td colspan="4" valign="bottom">22 Gauge Needle 1.5 Inch</td><td valign="bottom">1</td><td valign="bottom">Becton Dickinson</td><td valign="bottom">Z192473</td></tr><tr><td colspan="4" valign="bottom">250 ml Water Bottle</td><td valign="bottom">1</td><td valign="bottom">Thermo Fisher Nalgene</td><td valign="bottom">2003–0008</td></tr><tr><td colspan="4" valign="bottom">3D Printed Parts (Black PLA)</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Triple Light Guide and Imaging Parts</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Triple Bandpass Filter (camera)</td><td valign="bottom">1</td><td valign="bottom">Chroma</td><td valign="bottom">69013 m</td></tr><tr><td colspan="4" valign="bottom">Liquid Light Guide</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">LLG0338-4</td></tr><tr><td colspan="4" valign="bottom">SM1 Adapter for Liquid Light Guide</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">AD3LLG</td></tr><tr><td colspan="4" valign="bottom">SM1 Lens Tube, 3.00’ Thread Depth</td><td valign="bottom">3</td><td valign="bottom">Thorlabs</td><td valign="bottom">SM1L30</td></tr><tr><td colspan="4" valign="bottom">SM1 Lens Tube, 1.00’ Thread Depth</td><td valign="bottom">3</td><td valign="bottom">Thorlabs</td><td valign="bottom">SM1L10</td></tr><tr><td colspan="4" valign="bottom">SM1 Lens Tube, 2.00’ Thread Depth</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">SM1L20</td></tr><tr><td colspan="4" valign="bottom">SM1 Retaining Rings</td><td valign="bottom">2</td><td valign="bottom">Thorlabs</td><td valign="bottom">SM1RR-P10</td></tr><tr><td colspan="4" valign="bottom">Dichroic Cage Cube</td><td valign="bottom">2</td><td valign="bottom">Thorlabs</td><td valign="bottom">CM1-DC</td></tr><tr><td colspan="4" valign="bottom">Cage Cube Connector</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">CM1-CC</td></tr><tr><td colspan="4" valign="bottom">Compact Clamp with Variable Height</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">CL3</td></tr><tr><td colspan="4" valign="bottom">Bi-Convex Lens</td><td valign="bottom">4</td><td valign="bottom">Thorlabs</td><td valign="bottom">LB1761</td></tr><tr><td colspan="4" valign="bottom">AT455DC Size: 26 * 38 mm</td><td valign="bottom">1</td><td valign="bottom">Chroma</td><td valign="bottom">AT455DC</td></tr><tr><td colspan="4" valign="bottom">25 mm x 36 mm Longpass Dichroic Mirror, 550 nm Cutoff</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">DMLP550R</td></tr><tr><td colspan="4" valign="bottom">Ø1’ Bandpass Filter, CWL = 620 ± 2 nm, FWHM = 10 ± 2 nm</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">FB620-10</td></tr><tr><td colspan="4" valign="bottom">ET480/30x Size: 25mmR R = Mounted in Ring</td><td valign="bottom">1</td><td valign="bottom">Chroma</td><td valign="bottom">ET480/30x</td></tr><tr><td colspan="4" valign="bottom">Ø1’ Bandpass Filter, CWL = 440 ± 2 nm, FWHM = 10 ± 2 nm</td><td valign="bottom">1</td><td valign="bottom">Thorlabs</td><td valign="bottom">FB440-10</td></tr><tr><td colspan="4" valign="bottom">Royal-Blue (448 nm) Rebel LED</td><td valign="bottom">1</td><td valign="bottom">Luxeon Star</td><td valign="bottom">SP-01-V4</td></tr><tr><td colspan="4" valign="bottom">Blue (470 nm) Rebel LED</td><td valign="bottom">1</td><td valign="bottom">Luxeon Star</td><td valign="bottom">SP-01-B6</td></tr><tr><td colspan="4" valign="bottom">Red-Orange (617 nm) Rebel LED</td><td valign="bottom">1</td><td valign="bottom">Luxeon Star</td><td valign="bottom">SP-01-E6</td></tr><tr><td colspan="4" valign="bottom">Machined Parts (Stainless Steel)</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Milled as-1.50_2_v2.SLDPRT</td><td valign="bottom">3</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Spacer_with_wire_hole_as-.500_v2.SLDPRT</td><td valign="bottom">3</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">LED_mount_as-1.50_v2.SLDPRT</td><td valign="bottom">3</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">3D Printed Parts (Black PLA)</td><td valign="bottom"/><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">TripleLEDLightGuide_Base.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr><tr><td colspan="4" valign="bottom">Light_Guide_Mount_V2.stl</td><td valign="bottom">1</td><td valign="bottom"/><td valign="bottom"/></tr></tbody></table></table-wrap><p>The home cage platform captures wide-scale transitions in brain activity from windows of 8.2 × 8.2 mm with video rate temporal resolution (30 Hz) for several months using a double cage system (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Our original system for automatically head fixing mice, while effective (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>), did not result in daily durations of head fixation that would rival those in manually head-fixed mice for behavioral training or the integration of behavior with brain imaging of more complex tasks (which are in the order of 15–40 min sessions/day) (<xref ref-type="bibr" rid="bib18">Gilad et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Guo et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Prsa et al., 2017</xref>). We have optimized the original mouse voluntary head fixing system to improve the number of head fixes per day and the total time head fixed for each cage (<xref ref-type="table" rid="table1">Table 1</xref>). To accomplish this, we have used several strategies: doubling the cage size (tandem cage) to include up to 8–10 animals and improving the mechanical fixation mechanism which made the process more palatable for mice (<xref ref-type="fig" rid="fig1">Figure 1A,B</xref>). The initial design (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) employed electromagnetic solenoids. Using this system, it was impossible to control the degree of restraint and it produced a significant audible and vibration stimulus. As an improvement to this, we developed a servo-based system (Hitec HF-645 or HSB-9475SH; use the latter for even less noise and vibration) which allows variable degrees of headfixation with an initial training period where mice were loosely headfixed (achieved by controlling the servo travel) (<xref ref-type="fig" rid="fig1">Figure 1B,C</xref>). Loose headfixation allows mice to move significantly and grow accustomed to restraint. This loose fixation period, along with the less jarring mechanics has resulted in significantly higher rates of headfixation than previous work (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>). In a total of 5 improved cages that were tested, we observed average of 28+−17 head fixations per day per animal for a subset of animals tested (23/44) that exhibited high rates of headfixation (for supplemental database of all mice see methods and repositories). The time headfixed for these 23 good performers averaged 18+−13 min/day per animal (<xref ref-type="table" rid="table1">Table 1</xref>, behavioral data from all mice are available in an online database, see methods). In the current cage the cut-off for a poor performer was &lt;20 min of total headfixation time, while good performers averaged 15.8+−19.9 hr (summed time over all daily sessions). A difference over previous work (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) was removing mice from the cage that were poor performers, or insisted on entering the headfixing tube sideways. Assuming a session length of ~40 s, which was the typical range in the previous system (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>), taking the top 50% of headfixing mice would result in an average of 6.5+−4 min of daily data acquisition or training per mouse using the initial design. In the improved cages, animals exhibit much longer daily durations of head fixation which leads to more spontaneous brain activity imaging data, but also the possibility to assess mesoscale functional circuits using GCaMP imaging as animal’s train in simple detection or choice tasks. In total we were able to record 34,114 videos of brain activity in headfix sessions, where 29,097 were task related. An additional 5423 sessions were recorded from a cage with 8 female mice during daily lab-based 7 h/day training where 5 advanced to high levels of participation (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Moreover, the improved cage employs infrared beam breaks (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) to determine mouse position for fixation rather than, as in the previous version (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>), electrical contact with the fixation bar where buildup of insulating dirt/oil can reduce sensitivity. A capacitance-based lick detector based around the MPR-121 that we employed previously in a mouse tapered beam assay (<xref ref-type="bibr" rid="bib3">Ardesch et al., 2017</xref>) provides a system to confirm that mice are reaching the water spout and that they can register an output during a licking-based behavioral task. To relate the brain activity to the (currently) more than 7.5 million behavioral events (such as licking) and training circumstances we also implemented a relational database, allowing us to effectively analyze the data by pipelining plots and statistical tests to adaptive queries (see Materials and methods for how to access the database where we share all behavioral data collected).</p></sec><sec id="s2-2"><title>Initial training for headfixation and combined mesoscale imaging</title><p>For initial training 8–10 post-surgical male mice (2 cages of 10 and 3 cages of 8 mice) with RFID tags (<xref ref-type="bibr" rid="bib7">Bolaños et al., 2017</xref>) and transcranial windows that were at least 45 days of age (<xref ref-type="bibr" rid="bib48">Silasi et al., 2016</xref>) were kept in their double training cage for several weeks with ad libitum water from a bottle (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, and see protocol steps in Methods). A group of female 8 mice was also trained in a similar manner (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This ensures that the mice are well acclimated to the double cage (and each other) before headfixed training begins. Training begins by placing the mice within the headfixing cage and removing the water bottle overnight (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Mice were weighed on the subsequent day and this weight is the baseline by which 15% wt loss criteria were determined (for water supplementation). Mice received water from a spout for having their RFID tag read by a tag reader placed on the underside of the imaging chamber (see Materials and methods for details on spout positioning and strategies to prevent sideways entry).</p><p>After determining that the majority of mice entered the chamber (by reading RFID tags and lick detection) we progressively moved the water spout until it is 7–10 mm from the back wall of the headfixation tube (typically this takes 5–18 days) (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). At this point mice are be able to break the infrared beams (<xref ref-type="fig" rid="fig1">Figure 1B,C</xref>) with their snouts and trigger blue LED lights used for mesoscale imaging producing what are termed no-fix trials. Once the infrared beams are broken a no-fix trial is initiated the mouse receives up to 5 large water rewards for just getting at the end of the tube. Once most of the remaining mice achieved this level (mice with very low entries were removed from the cage) we start the headfix training. If mice break the beam the servo now travels enough to prevent mice from leaving the tube, but the headbars can still freely move up to 20 mm along the rails of the tube (<xref ref-type="fig" rid="fig2">Figure 2</xref> and protocol steps Materials and methods). This step is termed loose headfixation. Initially we set a 50% probability of loose headfixation. Next, over 7–10 days we gradually tightened the fixation mechanism (increase servo forward travel, fixed setting) until the headbar was tightly pressed against the metal reinforcement at the end of the rails. Additionally, during this period we gradually increased the probability of headfixation until mice were fully headfixed 75–90% of the time for about 19–45 s. Headfixing probability was never set to 100% as a potential safety mechanism allowing animals that for some reason could not un-break the IR beams in time (parameter called skedaddle time in software) to be prevented from re-fixation. A text messaging service was also setup to send automated warnings if a mouse was present in the headfixing tube for longer than 600–800 s. Once mice were fully headfixed we continued using a single entrance reward as well as adding a licking task (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Plotting entry rates, time headfixed, and headfixing rates for all mice tested indicated a peak of early entries just after the mice were placed in the cage (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). At these early times of training (even in the second day in the cage) mice were actively licking the waterspout (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). This was followed by a transition to headfixation which began with a loose restraint and was progressively tightened over 4–7 days to the point where mesoscale imaging data could be acquired with minimal motion artifact (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The records of headfixation for all 44 male mice are displayed over a 45 day period (<xref ref-type="fig" rid="fig3">Figure 3A–D</xref>) (note, not all cages were examined for all 45 days and cage 3 was shut down due to a power outage during day 13–17). The graphs illustrated that the mice entered less frequently after the first 10 days of headfixation once they transitioned from entrance rewards to receiving multiple task rewards (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) and collectively the mice of the 5 cages could yield up to 4 hr headfixed training or imaging a day (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><p>To quantify the observed clustering of headfixation events within and between mice we determined the time between headfixes for individual mice, as well as the time between a single headfixing mouse and any other mouse headfixing. Consistent with previous findings (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) mice headfixed repeatedly over short time intervals (<xref ref-type="fig" rid="fig4">Figure 4A,B</xref>, red) with an average median interval of 5.8+- 4.1 s (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, red line). Surprisingly, plotting histograms for the time interval between different mice headfixing (<xref ref-type="fig" rid="fig4">Figure 4A,B</xref>, black) also showed short interval repetitions with a second different mouse: average median interval of 305+- 575 s indicating linked behaviors between mice. Plotting the median separately for each group (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, black line) showed large differences in inter-mouse headfixing times between cages, indicating potential group dynamics, however given the small sample size of cages (n = 5) we did not investigate statistically. Cluster analysis indicated most headfixes occurred in large temporal groups that could involve 1 or several mice (<xref ref-type="fig" rid="fig4">Figure 4C,D,E,F</xref>). Plotting headfix events on a 24 hr timescale with each day stacked on the Y-axis (<xref ref-type="fig" rid="fig4">Figure 4D,F</xref>) indicated that headfixes are not done randomly, but often form dense clusters of activity and 69+−11% of headfixes were performed at night (n = 5 cages); with the exception of cage 6 tested during the day only. Testing the time interval between headfixes revealed that they were not uniformly or normally distributed across a day (Anderson-Darling test; test statistic 14137, critical value for 1% level is 1.092, indicating a very low probability of normality) consistent with the temporal clusters we observed.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Group dynamics during headfixed behavior.</title><p>(<bold>A</bold>) Histograms show the inter-headfixation interval for individual mice (same mouse) or for two different mice. Individual mice were most likely to start a new head-fix session within the first 10 s after the previous session. In about 90% of cases other mice were found to also enter within 60 s of an individual mouse (overlap darker red). Displayed bin size is 5 s. (<bold>B</bold>) Histogram plotted over longer time scale. After 10 min was more likely that the next head-fixation was done by another mouse than the same mouse as the gray area is greater (displayed bin size is 5 min) and after 2 hr the probability becomes equal again. Histograms include data from all 5 mouse groups during the time under head-fixing protocol. Group specific medians of the head-fix intervals can be found in C) using the same color code and the left (log) scale. (<bold>C</bold>) Distributions of different values over the 5 groups. Medians are displayed in red and black lines using the left (log) scale. The bar plots display the proportion of headfix clusters that include the activity of one, two or more mice, each bar sums up to 100%. (<bold>D</bold>) Group dynamics and diurnal variation in headfixation rates for cage group 4. Day 0 represents the beginning of the loose head-fixing protocol. Each circle represents a head-fix session and the colors correspond to unique RFID tags. (<bold>E</bold>) Count of head-fixes that were associated in clusters of distinct size resulting from KDE cluster analysis. Note that the displayed binning of cluster sizes was not linear and counts of larger cluster sizes were grouped together. (<bold>F</bold>) Data plotted for cage 1, note cage 1 started with 10 mice Ai-94 genotype.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig4-v2.tif"/></fig></sec><sec id="s2-3"><title>Task training during combined cortical mesoscale imaging</title><p>Since headfixation was done in number of relatively short trials, mice were initially anticipating water delivery by vigorous licking following entrance rewards (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). These high rates of licking can interfere with using licks within a behavioral task and will lead to brain activity largely reflecting frontal motor areas with the tongue (<xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>). Therefore, the first stage of task training is called lick suppression where water is dispensed only when licking is withheld for 0.5–2 s. We added a 0.5 s mean random interval to this time to help ensure mice wait for a cue rather than use internal timing (<xref ref-type="fig" rid="fig5">Figure 5A</xref>); see group data on licking counts for all mice (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Licking at an inappropriate time was met with auditory feedback: a tone of 0.3 kHz was given to provide immediate feedback using piezo buzzer (Adafruit product #1739). Once lick suppression training was initiated mice were given a vibratory (Adafruit Vibe motor #1201) cue (500 ms continuous or pulsed 3 × 200 ms, 100 ms duty cycle) to signal that water will be dispensed after the lick suppression period is met. In cages 1 and 2 lick suppression began during headfixation, while in cages 3–5 it was started 2–4 days prior to loose headfixation. Over time the period of lick suppression was increased to 2 s (+- 0.5 s random interval) and water would only be dispensed when a mouse licks within a time window after the cue. As the goal was to extend training to a go and no-go task (<xref ref-type="bibr" rid="bib18">Gilad et al., 2018</xref>), where mice will need to process and discriminate between two similar vibratory cues, we also added a self-timed delay period after the cue (0.3–1.1 s). Mice should not lick between the cue and the end of the delay period. When using the delay period, water was delivered following go-stimuli if the licking response occurred between the end of the delay period and 1.25 s after that (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). For example, with a 1 s delay a mouse must lick between 1 and 2.25 s after the cue to receive a reward which comes 2.25 s from the cue. Initially, mice licked immediately after the cue and these errors were not rewarded and these task outcomes were termed lick early (coded −4) trials and signaled by auditory feedback using the same tone as for lick suppression (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Over time the delay period is increased from 300 ms by ~50–100 ms a day to an eventual 1100 ms. While we have termed these go and no-go trials, the addition of delay period (which in trained mice can be up to 1100 ms) makes the primary response after the vibratory conditioned stimulus (CS) a withholding of licking until after the delay period. Using this design all trials could be considered no-go and they would occur in the context of discriminating rewarded (CS+) and non-rewarded vibratory conditioning stimuli (CS-) that vary in frequency.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Example of a full headfixation session and group licking behavior during go-trial detection task.</title><p>(<bold>A</bold>) The entering TTA-GCaMP6s mouse (RFID tag ID 201608423, Group 4, day 49 of head-fixation) was rewarded with a water drop and was head-fixed when it broke the infrared beam and brain and body behavioral video recording was started. The blue light LED directed to the brain window for GCaMP epi-fluorescence was turned on 3 s after the beginning of head-fixation and was switched off 3 s before the servomotor releases the mouse. Trial outcomes included in the displayed session scheme (in order left to right): GO = 1, GO = 2, GO = 2, GO=-4, GO=-4, GO = 2, GO=-1, GO = 2 (outcome shortcuts specified in <xref ref-type="fig" rid="fig6">Figure 6B</xref>). (<bold>B</bold>) Time distribution of licks of different outcomes of GO trials under head-fixation, note the delay period increases as training progresses. Plot displays all licks from 20 mice of groups 2–5. Mice of group 1 did not have a self-timed lick delay time and thus no GO=-4 outcomes and were excluded from the plot. Y-axis displays the raw count of licks (binned in intervals of 0.1 s) between −5 and 5 s after stimulus. Licks between two stimuli are displayed twice, considered as a lick before and after a stimulus (multiple stimulus trials up to 7 are given in a single headfixed session). Number of trials and licks due to outcome (trials/displayed licks/unique licks): GO=-2 (6181/65225/58538), GO=-4 (14501/295624/263467), GO = 2 (30005/823637/661619). Outcome shortcuts: (GO=) 2: successful go trial, −2: unsuccessful go trial (no/late response), −4: unsuccessful go trial (early response, licked within delay time). (<bold>C</bold>) Repeated measurements two-way ANOVA comparing success rates of go trials between head-fixed and not head-fixed trials. Mice of group 2–5 had a random chance (30%) of getting a session without head-fixation. Procedure and task were the same as in head-fixed sessions. Success rates of mice that performed over a period of 30 days (n = 8) were binned in 5 day intervals. The ANOVA shows a significant lower success rate of non-fixed go trials.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig5-v2.tif"/></fig></sec><sec id="s2-4"><title>Go task headfixed and non-headfixed task performance</title><p>Typically, we did not set the probability of head fixing to be 100%. In most cages headfixing probability was initially set low 25–50% and then moved to 70–90%. Having the head fixed probability lower than 100% was thought to aid to some mice which preferred to do the task in non-head fixed manner. Surprisingly, analysis of success rate data for the Go detection task indicated that most mice actually had higher success rates while head-fixed than non-head fixed (RM 2WAY ANOVA, p=0.0026, n = 8) (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). All data on task performance presented in <xref ref-type="table" rid="table1">Table 1</xref> and used to calculate reported d’ were calculated for fully headfixed trials. We have not investigated the nature of this difference further, although it could suggest that head fixation helps to constrain mouse behaviors similar to that observed in reaching tasks that were not impaired by headfixation (<xref ref-type="bibr" rid="bib17">Galiñanes et al., 2018</xref>). While the anecdotes of better task performance in headfixed mice are interesting, another explanation is that because there were more headfixed trials than in the non-headfixed state and animals were simply less practiced and not fully engaged during these trials.</p><p>Having longitudinal data also allowed us to assess whether mouse performance differed in the day or night period. In each of the 21 mice that performed the go task for more than 10 days we assessed whether they had a consistent diurnal preference for the licking task resulting in a significant difference in success rate. Overall, the ratio of success rate for day and night was similar across mice (1.03+0.17, t-test: p=0.40, maximum ratios: 0.56 and 1.39, n = 21 mice that performed the go-task under headfixation for more than 10 days). We also assessed whether any individual mouse had a consistent day-night preference (t-test for each mouse using day/night success ratios for each day) and found no instances of a significant bias (p&gt;0.05), except M985 that had a very low success rate (see <xref ref-type="table" rid="table1">Table 1</xref>) and therefore produced a very inconsistent daily day and night success rate ratio.</p></sec><sec id="s2-5"><title>Lick detection and training of a single mouse over an early, intermediate, and advanced days of in cage training</title><p>In <xref ref-type="fig" rid="fig6">Figure 6</xref> we show examples of lick detection and training of a single mouse over an early, intermediate, and advanced day of in cage training. The results of training are scored automatically for 6 possible task outcomes (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Mice were first trained on the go detection task and over the first 1–4 days could already pause and lick correctly (Go=+2 code) after the delay period. Even early in training there was relatively little non-rewarded early licking (Go=-4 code for early licking in the go trial (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). The proportion of early licks (Go=-4) and ignored cues (Go=-2) was further reduced after 55–68 days of training even with a much longer delay period of up to 1 s (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). No-go trials were introduced after 14 days of go-training and initially mice had trouble distinguishing the 2 trial types and would lick after the delay period as in a go-trial leading to a −1 code error (or −3 when licking early during the delay period for a no-go trial). Throughout training, there was marked reduction in licking after the no-go cue as the mice learned to distinguish the two stimuli (No-go trial correct rejection is No-go=+1) (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Go and No-Go Task training for an individual mouse.</title><p>(<bold>A</bold>) Shows the number of trials performed, outcome software shortcuts and their consideration in the d’ analysis of displayed trials for TTA-GCaMP6s mouse RFID tag ID 201608423. Go=-3 trials are not displayed in D) due to their rare occurrence. (<bold>B</bold>) Lick histograms for go trials on different training days. If the mouse withholds its licking for a given amount of time (up to 2 +- 0.5 s) a new trial is initiated by a randomly given go (or no-go) cue. Correct no-go trials are unrewarded, correct go trials are rewarded if the mouse delays its licking response for a given delay time. After the delay time has passed the mouse has 1.25 s to respond with licking. A water reward is given when the 1.25 s have passed and the mouse responded within this 1.25 s time window. A new cue is given when the mouse withholds its licking again for approximately 2 s. Blue bars show the lengthening of the delay time. Water drops indicate when the reward is given. (<bold>C</bold>) Delay times used and (<bold>D</bold>) No-go trial lick histograms.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig6-v2.tif"/></fig><p>While we appreciate the power of a choice task, the sample sizes for the no-go task were relatively small (n = 10 mice) and only 5 of these mice had a sensitivity index (also called d’) greater than 2 during their best task training days. d’ is a z-transformed measure of the discrimination between go and no-go stimuli (hit rate (correct)z – false alarm (incorrect)z), with a value of 0 corresponding to no discrimination. In the d’ analysis we disregarded failures due to the delay time early lick errors (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) to calculate the hit rate and false alarm rate as these nonetheless reflected stimulus detection. The 5 mice that were good performers had a d' that averaged 1.37+−0.53 over their days of testing and they reached a maximal daily d’ that averaged 2.3 +−0.52 and ranged from 1.9 to 3.21 (M252, M423, M026, M098, and M336). The statistic itself can be used as an estimate of how far values are from the hypothetical mean or 0, with 1.37 and 2.3 being in the 92% and 97% tails of the distribution. The other 5 mice performing the go and no-go task had lower average d’=0.2+−0.54 and only reached a mean d’ of 0.98+−1.01 in maximal daily training values.</p></sec><sec id="s2-6"><title>Mesoscale cortical functional imaging within home cages</title><p>Mesoscale GCaMP signals were collected from the dorsal cortex every time the mouse headfixed and examples are shown in <xref ref-type="fig" rid="fig7">Figure 7A</xref>. While the focus of this manuscript is on the construction of cages and the training of mice, we present cortical GCaMP6 imaging examples to illustrate that high-throughput automated mesoscale imaging is possible and can be integrated with behavior. The name of each brain imaging file which contains both the mouse ID and the time stamp can be found in the SQL database (RFIDtag_xxxx_timestamp.raw; see Materials and methods for URL and hosted as a full text file archive on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3268838">https://doi.org/10.5281/zenodo.3268838</ext-link>) for the 5 cages of male mice and cage 6 female mice <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5683/SP2/9RFXRP">https://doi.org/10.5683/SP2/9RFXRP</ext-link>. GCaMP cortical signals allow functional connectivity to be assessed using correlational methods such as seed-pixel analysis of the hindlimb somatosensory cortex (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The location of hindlimb cortex was estimated by overlaying an averaged cortical anatomical parcellation based on the Allen Institute for Brain Science mouse connectivity atlas and the approximate location of the bregma landmark (<xref ref-type="bibr" rid="bib54">Vanni et al., 2017</xref>). In addition to the brain video showing normalized cortical functional GCaMP6 signals, up to 3 other video cameras were used to obtain behavioral data under infrared illumination (<xref ref-type="fig" rid="fig7">Figure 7C,D,E</xref>). Circuity was added to cages 3–5 to turn-off infrared beam breaks once the mouse was headfixed as the infrared beam-breaks interfere with the behavioral cameras. Behavioral videos and GCaMP signals were synchronized using a master TTL switch that controls the infrared and blue LEDs used for GCaMP excitation or hemodynamic correction. As a means of illustrating the potential relationship between moving body parts and cortical mesoscale networks we made maps of the correlation between brain GCaMP signal pixels and normalized behavioral video ROIs for tongue, whiskers, and hindlimbs with a lag time of zero (see example <xref ref-type="fig" rid="fig7">Figure 7F</xref>). This analysis indicated a network of areas around limb and facial cortices that extended into frontal motor areas associated with licking (<xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>). Plotting movement from the video analysis ROIs alongside licking rates (from the capacitance sensor) and mesoscale GCaMP signal for anterolateral motor cortex indicated that trial-associated licking bouts were accompanied by large scale body movements involving oral-facial, hindlimbs, and whiskers and GCaMP signals on the level of individual trials (<xref ref-type="fig" rid="fig7">Figure 7G,H</xref>), for group data analysis see Figure 9.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Mesoscale imaging of task-dependent GCaMP6 activity in automated home cages.</title><p>(<bold>A</bold>) Raw fluorescence image of GCaMP and transcranial window, overlay represents a re-drawing from the <xref ref-type="bibr" rid="bib37">Oh et al., 2014</xref>, note we have down-sampled the original data (256 × 256 pixels) to 64 × 64 pixels to speed processing. (<bold>B</bold>) Seed-pixel correlation map with seeded region in the right hind limb somatosensory cortex for mouse M8423 for data from 09252018 to 09292018 over which 151 headfixed sessions occurred (total imaging time of ~88 min, concatenated headfixed trials), red circle on the right hemisphere in the area of high correlation was the presumed hind limb area, the second red circle located more anterior was used for plotting ROI values in panel G. (<bold>C, D, E</bold>) Examples of behavioral ROIs for mouse tongue face, whisker, and hind leg, regions of interest (ROIs colored coded as in F). Analyses in panels B and F used global signal regression on cortical GCaMP signal to emphasize local networks, this step was removed for all other analysis. (<bold>F</bold>) Correlation maps (zero lag time) between behavioral activity (gradient pixel analysis of videos, see methods) and brain GCaMP-6 calcium-dependent activity made over the 151 concatenated head-fixed sessions. Pixel by pixel brain GCaMP correlation with three different behavioral ROIs are shown. (<bold>G</bold>) Plots of behavioral ROI values on the same timescale as mesoscale GCaMP signal from frontal motor cortex, the gradient values were auto-scaled based on min/max movements. During each cycle of movement that typically involved multiple body parts there was a large GCaMP signal within frontal motor cortex (2nd head-fixed session from 09252018 for M8423, 38–76 s of the dataset), ROI is the anterior red circle in panel B. (<bold>H</bold>) Single trial pseudo colored changes in GCaMP6 fluorescence with during licking task (cue time indicated) for 50 consecutive rewarded Go hit trials that received a code of 2 for mouse 8423 using data from 08252018 (each image reflects the maximal value of 400 ms time bins).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig7-v2.tif"/></fig><p>Mesoscale wide-field imaging can inform the areal extent of cortical activation, but the procedure is not without its limitations. Imaging green epifluorescence is sensitive to interference from hemoglobin (both the amount and oxygenation content are factors) (<xref ref-type="bibr" rid="bib26">Ma et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Ma et al., 2016</xref>; <xref ref-type="bibr" rid="bib56">Wekselblatt et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Xiao et al., 2017</xref>). While blood dependent alteration in signals is less of a factor for newer generation GCaMPs with larger change in fluorescence divided by baseline (dF/Fo) (<xref ref-type="bibr" rid="bib55">Vanni and Murphy, 2014</xref>; <xref ref-type="bibr" rid="bib58">Xiao et al., 2017</xref>), it can nonetheless be a significant confound. Recent work suggests several correction strategies for green epifluorescence involving reflected light signals that serve as reference wavelengths and are the basis of most corrective schemes (<xref ref-type="bibr" rid="bib26">Ma et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Ma et al., 2016</xref>; <xref ref-type="bibr" rid="bib56">Wekselblatt et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Xiao et al., 2017</xref>). Within this improved home cage system we simultaneously capture a short blue wavelength reflected light (440 nm; near the hemoglobin isosbestic point) signal (into the blue RGB channel of the picam) to estimate these and other artifacts and correct for movements that would have parallel effects on short blue reflected light and green epi- fluorescence. The collection of this reference signal was made possible by using a triple band emission filter (<xref ref-type="table" rid="table1">Table 1</xref>). These artifacts were evaluated by determining both green epi-fluorescence and short blue reflected light dF/Fo and subtracting equaled scaled short blue reflection changes from the green epi-fluorescence dF/Fo at each pixel and time point studied. We acknowledge that future extensions could tune the weighting of these signals or add additional wavelengths (<xref ref-type="bibr" rid="bib26">Ma et al., 2017</xref>). Typically, these changes in short blue reflection were &lt;20% of the green GCaMP signal amplitude (<xref ref-type="fig" rid="fig7">Figure 7G</xref>) and produced relatively small decreases in reflected light signal apparent during averaged task dependent activity (<xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplements 1</xref>–<xref ref-type="fig" rid="fig8s7">7</xref>). Technical considerations associated with using the Picam and its rolling shutter prevented us from collecting a green reflected light signal in alternating frames. While we acknowledge that a multispectral and model based strategy (<xref ref-type="bibr" rid="bib26">Ma et al., 2017</xref>) would be an improvement, the procedure we employ nevertheless correlates well with the green reflected light (<xref ref-type="bibr" rid="bib58">Xiao et al., 2017</xref>) strobing correction as in <xref ref-type="bibr" rid="bib56">Wekselblatt et al., 2016</xref> and was simpler to employ in the context of the home cage. To link mesoscale GCaMP activity with particular phases of a behavioral task, we used the lick detector as a means of registering a behavioral output. Signals on the lick detector were thresholded and recorded as binary events and assigned unix timestamps within a text file which accompanied each imaging session. Using Matlab software, we associated these text file time stamps with sequences of brain images also identified by a starting timestamp and confirmed by measuring the timing of the illumination LED on and off signals. To assess spatial extent of single trial signals we made montages of blue reflectance corrected GCaMP dF/F<sub>0</sub> images that were binned over 400 ms (<xref ref-type="fig" rid="fig7">Figure 7H</xref>). These sequences of dF/F<sub>0</sub> images even in single trials indicated remarkable fidelity around the delivery of the stimulus during go-trials and a wide cortical network activated during correctly scored go-trials.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Averaged brain GCaMP6 and behavioral ROI data for licking task.</title><p>(<bold>A</bold>) Trial averaged pseudo-colored GCaMP cortical image montage binned every 500 ms (pixelwise to show maximum values) during go-task activity. Data from a 2.5 s period before and after vibratory go-cue presentation (500 ms vibration, begins at time 0) for mouse tag 8423 and 49 days since the start of the go task training and 36 days from no-go. Large image shows the maximal activation over this period (difference between pre-cue minimum and post-cue maximum values for images binned over 100 ms). Upper panel plot normalized behavioral gradient signal for indicated body region ROIs. Bottom panel plots: average GCaMP signal for 470 trials of successful licking Go-trials (lick rate plotted in blue thin lines) for a pixel in anterolateral motor cortex (white circle coordinates 52, 27 in the 64 × 64 image shown, note image was binned spatially from 256 × 256 raw data). By examining the lick rate plot, the lick withhold time (with random 500 ms interval) can be seen when licks plummet to 0 just before trial start. The response window after the self-timed delay (900 ms) is also visible with licks being detected again and then a second peak of licks at the reward delivery is observed. (<bold>B</bold>) Identical plots as A for trials where hindlimb movements 0–2.5 s before the vibration were &lt;0.4 stdev (ROI 1 = tongue/mouth, 2 = whiskers, and 3 = hindlegs). This criterion makes pre-stimulus baseline activity more constant but otherwise shows similar dynamics. (<bold>C</bold>) Identical plots as B but for trials where a no-go stimulus (3, 100 ms vibrations with 200 ms inter stimulus interval) was given. These data were autonomously obtained from 9/25/2018 to 9/29/2018 and are the average of all head-fixed trials of given outcome type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Averaged GCaMP6 and behavioral ROI data for licking task early training data.</title><p>(<bold>A</bold>) Average pseudo-colored GCaMP cortical image montage binned every 500 ms go-task activity over a 2.5 s period before and after vibratory go-cue presentation (500 ms vibration, begins at time 0) for mouse tag 8423 and 17 days since the start of the go task training and 4 days from no-go. Large image shows the maximal activation over this period (difference between pre-cue minimum and post-cue maximum values for images binned over 100 ms)). Upper panel plot normalized behavioral gradient signal for indicated body region ROIs. Bottom panel plots: average GCaMP signal for 416 trials of successful go licking trial (lick rate plotted in blue thin lines) for a pixel in anterolateral motor cortex (white circle coordinates 52, 27 in the 64 × 64 image shown). By examining the lick rate plot, the lick withhold time (with random 500 ms interval) can be seen when licks plummet to 0 just before trial start. The response window after the self-timed delay (600 ms) is also visible with licks being detected again and then a second peak of licks at the reward delivery is observed. (<bold>B</bold>) Identical plots as A, but for trials where hindlimb movements 0–2.5 s before the vibration were &lt;0.4 stdev (ROI 1 = tongue, 2 = whiskers, and 3 = hindlegs). This criterion makes pre-stimulus baseline activity more constant but otherwise shows similar dynamics. (<bold>C</bold>) Identical plots as B but for trials where a no-go stimulus (3, 100 ms vibrations with 200 ms inter-stimulus interval) was given. These data were obtained autonomously from 8/24/2018 to 8/29/2018 and are the average of all head-fixed trials of given outcome type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Averaged GCaMP6 and behavioral ROI data for licking task early training data from mouse 8474, Go and No-Go.</title><p>(<bold>A</bold>) Average pseudo-colored GCaMP cortical image montage binned every 500 ms go-task activity over a 2.5 s period before and after vibratory go-cue presentation (500 ms vibration) 17 days since the start of the go task training and 4 days from no-go. Large image shows the maximal activation over this period (difference between pre-cue minimum and post-cue maximum values for images binned over 100 ms). Upper panel plot normalized behavioral gradient signal for indicated body region ROIs. Bottom panel plots: average GCaMP signal of successful go licking trial (lick rate plotted in blue thin lines) for a pixel in anterolateral motor cortex (white circle coordinates 52, 27 in the 64 × 64 image shown). (<bold>B</bold>) Identical plots as A, but for trials where hindlimb movements 0–2.5 s before the vibration were &lt;0.5 stdev (ROI 1 = tongue, 2 = whiskers, and 3 = hindlegs). This criterion makes pre-stimulus baseline activity more constant but otherwise shows similar dynamics. (<bold>C</bold>) Identical plots as B but for trials where a no-go stimulus (3, 100 ms vibrations with 200 ms inter-stimulus interval) was given. These data were obtained autonomously from 8/24/2018 to 8/29/2018 and are the average of all head-fixed trials of given outcome type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig8-figsupp2-v2.tif"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 3.</label><caption><title>Averaged GCaMP6 and behavioral ROI data for licking task early training data from mouse 0252, Go and No-Go.</title><p>(<bold>A</bold>) Average pseudo-colored GCaMP cortical image montage binned every 500 ms go-task activity over a 2.5 s period before and after vibratory go-cue presentation (500 ms vibration) 17 days since the start of the go task training and 4 days from no-go. Large image shows the maximal activation over this period (difference between pre-cue minimum and post-cue maximum values for images binned over 100 ms). Upper panel plot normalized behavioral gradient signal for indicated body region ROIs. Bottom panel plots: average GCaMP signal of successful go licking trial (lick rate plotted in blue thin lines) for a pixel in anterolateral motor cortex (white circle coordinates 52, 27 in the 64 × 64 image shown). (<bold>B</bold>) Identical plots as A, but for trials where hindlimb movements 0–2.5 s before the vibration were &lt;0.5 stdev (ROI 1 = tongue, 2 = whiskers, and 3 = hindlegs). This criterion makes pre-stimulus baseline activity more constant but otherwise shows similar dynamics. (<bold>C</bold>) Identical plots as B but for trials where a no-go stimulus (3, 100 ms vibrations with 200 ms inter-stimulus interval) was given. These data were obtained autonomously from 8/24/2018 to 8/29/2018 and are the average of all head-fixed trials of given outcome type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig8-figsupp3-v2.tif"/></fig><fig id="fig8s4" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 4.</label><caption><title>Averaged GCaMP6 and behavioral ROI data for licking task error trials for dataset presented in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</title><p>(<bold>A</bold>) Average pseudo-colored GCaMP cortical image montage binned every 500 ms go-task activity over a 2.5 s period before and after vibratory go-cue presentation (500 ms vibration) for mouse tag 8423. Large image shows the maximal activation over this period (difference between pre-cue minimum and post-cue maximum values for images binned over 100 ms)). Upper panel plot normalized behavioral gradient signal for indicated body region ROIs. Bottom panel plots: average GCaMP signal for 3 trials of unsuccessful go licking trial (no licking in the response window) for a pixel in anterolateral motor cortex (white circle coordinates 52, 27 in the 64 × 64 image shown). (<bold>B</bold>) Similar plots as A, but for trials a No-Go false alarm event occurred (mouse licked in the correct time window but for a non-rewarded cue). (<bold>C</bold>) Similar plots as A but for trials where a go stimulus was given but the mouse licked too early and was not rewarded. These data were obtained autonomously from 9/25/2018 to 9/29/2018 and are the average of all head-fixed trials of given outcome type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig8-figsupp4-v2.tif"/></fig><fig id="fig8s5" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 5.</label><caption><title>Averaged GCaMP6 and behavioral ROI data for licking task: error trials for dataset presented in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>.</title><p>(<bold>A</bold>) Average pseudo-colored GCaMP cortical image montage binned every 500 ms go-task activity over a 2.5 s period before and after vibratory go-cue presentation (500 ms vibration) for mouse tag 8423 at a time early in training (beginning 17 days after the start of training). Large image shows the maximal activation over this period (difference between pre-cue minimum and post-cue maximum values for images binned over 100 ms). Upper panel plot normalized behavioral gradient signal for indicated body region ROIs. Bottom panel plots: average GCaMP signal for 3 trials of unsuccessful go licking trial (no licking in the response window) for a pixel in anterolateral motor cortex (white circle coordinates 52, 27 in the 64 × 64 image shown). (<bold>B</bold>) Similar plots as A but for trials a No-Go false alarm event occurred (mouse liked in the correct time window but for a non-rewarded cue). (<bold>C</bold>) Similar plots as A, but for trials where a go stimulus was given but the mouse licked too early and was not rewarded. These data were obtained autonomously from 8/24/2018 to 8/29/2018 and are the average of all head-fixed trials of given outcome type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig8-figsupp5-v2.tif"/></fig><fig id="fig8s6" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 6.</label><caption><title>Averaged GCaMP6 and behavioral ROI data for licking task error trials for mouse 8474.</title><p>(<bold>A</bold>) Average pseudo-colored GCaMP cortical image montage binned every 500 ms go-task activity over a 2.5 s period before and after vibratory go-cue presentation (500 ms vibration) for mouse tag 0252. Large image shows the maximal activation over this period (difference between pre-cue minimum and post-cue maximum values for images binned over 100 ms). Upper panel plot normalized behavioral gradient signal for indicated body region ROIs. Bottom panel plots: average GCaMP signal for 3 trials of unsuccessful go licking trial (no licking in the response window) for a pixel in anterolateral motor cortex (white circle coordinates 52, 27 in the 64 × 64 image shown). (<bold>B</bold>) Similar plots as A but for trials a No-Go false alarm event occurred (mouse liked in the correct time window but for a non-rewarded cue). (<bold>C</bold>) Similar plots as A but for trials where a go stimulus was given but the mouse licked too early and was not rewarded. These data were obtained autonomously from 8/24/2018 to 8/29/2018 and are the average of all head-fixed trials of given outcome type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig8-figsupp6-v2.tif"/></fig><fig id="fig8s7" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 7.</label><caption><title>Averaged GCaMP6 and behavioral ROI data for licking task error trials for mouse 0252.</title><p>(<bold>A</bold>) Average pseudo-colored GCaMP cortical image montage binned every 500 ms go-task activity over a 2.5 s period before and after vibratory go-cue presentation (500 ms vibration) for mouse tag 8474. Large image shows the maximal activation over this period (difference between pre-cue minimum and post-cue maximum values for images binned over 100 ms). Upper panel plot normalized behavioral gradient signal for indicated body region ROIs. Bottom panel plots: average GCaMP signal for 3 trials of unsuccessful go licking trial (no licking in the response window) for a pixel in anterolateral motor cortex (white circle coordinates 52, 27 in the 64 × 64 image shown). (<bold>B</bold>) Similar plots as A but for trials a No-Go false alarm event occurred (mouse licked in the correct time window but for a non-rewarded cue). (<bold>C</bold>) Similar plots as A but for trials where a go stimulus was given but the mouse licked too early and was not rewarded. These data were obtained autonomously from 8/24/2018 to 8/29/2018 and are the average of all head-fixed trials of given outcome type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig8-figsupp7-v2.tif"/></fig><media id="fig8video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-55964-fig8-video1.mp4"><label>Figure 8—video 1.</label><caption><title>Correct response hit-trial Go-trial (Go = 2 code), averaged pseudo-colored GCaMP cortical images over a 2.5 s period before and after vibratory go-cue presentation for mouse M8423.</title><p>Recorded cortical images were concatenated as video files after spatial and temporal binning into 64 × 64 pixels every 100 ms (spatial binning is done by averaging, temporal binning uses the maximum of dF/Fo values). The 500 ms vibration cue is indicated with a red bar in the upper part of the video starting at 0 s and date, mouse and outcome-code of the trials shown are displayed above the image data. In case of successful Go trials (Go = 2) water reward is approximately given at 2 s, all other trials are unrewarded. Data corresponds to <xref ref-type="fig" rid="fig8">Figure 8</xref> in trial number and other factors.</p></caption></media><media id="fig8video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-55964-fig8-video2.mp4"><label>Figure 8—video 2.</label><caption><title>Correct rejection no-response No-Go-trial (Go = 1 code), averaged pseudo-colored GCaMP cortical images over a 2.5 s period before and after vibratory cue presentation for mouse M8423, not rewarded.</title><p>Recorded cortical images were concatenated as video files after spatial and temporal binning into 64 × 64 pixels every 100 ms (spatial binning is done by averaging, temporal binning uses the maximum of dF/Fo values). The 500 ms vibration cue is indicated with a red bar in the upper part of the video starting at 0 s and date, mouse and outcome-code of the shown trials are displayed above the image data. In case of successful Go trials (Go = 2) water reward is approximately given at 2 s, all other trials are unrewarded (as was this averaged data). Data correspond to <xref ref-type="fig" rid="fig8">Figure 8</xref> in trial number and other factors.</p></caption></media><media id="fig8video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-55964-fig8-video3.mp4"><label>Figure 8—video 3.</label><caption><title>In-correct response Go-trial licked early (−4 code, considered hit because they responded to correct cue), averaged pseudo-colored GCaMP cortical images over a 2.5 s period before and after vibratory go-cue presentation for mouse M8423, not rewarded.</title><p>Recorded cortical images were concatenated as video files after spatial and temporal binning into 64 × 64 pixels every 100 ms (spatial binning is done by averaging, temporal binning uses the maximum of dF/Fo values). The 500 ms vibration cue is indicated with a red bar in the upper part of the video starting at 0 s and date, mouse and outcome-code of the shown trials are displayed above the image data. In case of successful Go trials (Go = 2) water reward is approximately given at 2 s, all other trials are unrewarded. Data correspond to <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p></caption></media><media id="fig8video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-55964-fig8-video4.mp4"><label>Figure 8—video 4.</label><caption><title>In-correct response false alarm No-Go-trial (licked in correct time window for No-Go −1 code), averaged pseudo-colored GCaMP cortical images over a 2.5 s period before and after vibratory go-cue presentation for mouse M8423, not rewarded.</title><p>Recorded cortical images were concatenated as video files after spatial and temporal binning into 64 × 64 pixels every 100 ms (spatial binning is done by averaging, temporal binning uses the maximum of dF/Fo values). The 500 ms vibration cue is indicated with a red bar in the upper part of the video starting at 0 s and date, mouse and outcome-code of the shown trials are displayed above the image data. In case of successful Go hit trials (Go = 2) water reward is approximately given at 2 s, all other trials are unrewarded. Data correspond to <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p></caption></media></fig-group></sec><sec id="s2-7"><title>Go and No-go task headfixed and non-headfixed task acquisition</title><p>In a mouse that was trained in the Go and No-go task for 49 and 36 days respectively we created average maps of cortical activity aligned to the presentation of the cue stimulus (<xref ref-type="fig" rid="fig8">Figure 8A</xref>); for other individual mouse examples see <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplements 2</xref> and <xref ref-type="fig" rid="fig8s3">3</xref> (mouse 8474 and 0252). In <xref ref-type="fig" rid="fig8">Figure 8</xref> the mouse was trained to suppress licking for a 2 s +- 0.5 s random interval and then was presented with a vibration to its left paw after which it needed to wait for a delay period (self-timed, 900 ms) and then lick to be given a reward 1.25 s later (as described in <xref ref-type="fig" rid="fig2">Figure 2B</xref>). <xref ref-type="fig" rid="fig8">Figure 8</xref> shows an example of average cortical maps of task and reward-dependent mesoscale GCaMP functional signal from cortex during rewarded trials (<xref ref-type="fig" rid="fig8">Figure 8A,B</xref> and <xref ref-type="video" rid="fig8video1">Figure 8—Video 1</xref>). We observed quiescence within brain networks during the lick withhold period (pre-cue stimulus) when the animals must suppress licking and then a much larger response in sensori-motor cortex associated with preparation (during the delay period after the cue), execution of licking (<xref ref-type="bibr" rid="bib12">Chen et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>), and collection of the reward. By making averages of the GCaMP data and plotting regions of interest from motor cortex, we also observed changes in GCaMP6 signal before the presentation of the cue. It is possible that some of this brain activity is related to ongoing body movement dynamics associated with the task. To address this, we examined regions of interest from synchronized behavioral cameras directed on the body of the animal. Using the hindlimb behavioral ROI we compared all trials collected to those where hindlimb activity was low for 2 s preceding the stimuli (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). This constraint led to the elimination of about 40% of the trials and removed a portion of the pre-trial brain activity from the average (see −2.5 and −2 s images, Fig, 8A/B and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). This finding is consistent with ongoing movements contributing to significant brain activity, a variable which must be addressed in any home cage or head-fixed behavioral task (<xref ref-type="bibr" rid="bib34">Musall et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Salkoff et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Clancy et al., 2019</xref>; <xref ref-type="bibr" rid="bib53">Stringer et al., 2019</xref>). This issue was further addressed in a group data analysis present in <xref ref-type="fig" rid="fig9">Figure 9</xref> where movements before and after the cue strongly correlate with GCaMP dynamics.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Brain-behavior correlations on grouped mouse data during successful Go trials.</title><p>(<bold>A</bold>) Z-scored cortical fluorescence values plotted for each of 13 mice, first averaged spatially over the cortical region, then averaged across trials, for hind limb (HL) and anterior lateral motor (ALM) cortical areas. (<bold>B</bold>) Averaged z-scored fluorescence signal (black) and behavioral gradients (blue) across mice in hindlimb (HL), anterior lateral motor (ALM), and barrel cortex (BC) areas. Mean +/- SEM. (<bold>C</bold>) Correlation between single trial fluorescence activity and the averaged fluorescence signal across mice in hind limb (blue) and anterior lateral motor cortex (black). Markers indicate mean +/- SEM. Mice from different cages are grouped by background color (females last 4 mice, #10-13). (<bold>D</bold>) Correlations between average fluorescence signal and behavior gradients. Blue violins show correct pairings, and black violins show mismatched pairings across mice. * indicates p&lt;0.05, Wilcoxon signed rank test.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig9-v2.tif"/></fig><p>The ability to group trials based on task type and outcome allowed us to assess the difference in brain activation between average go and no-go trials in mouse M423 which exhibited good performance during the task (<xref ref-type="fig" rid="fig8">Figure 8C</xref> and <xref ref-type="video" rid="fig8video2">Figure 8—Video 2</xref>). No-go trials were associated with minimal changes in brain activity and body movements, with most activity around the time of cue processing (1 s after cue presentation) and little activation associated with the advance to licking phase (as they did not lick in correct no-go trials).</p><p>In <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> we plot data using the same format as <xref ref-type="fig" rid="fig8">Figure 8</xref> for an earlier timepoint during training (30 days earlier) for the licking task (after 17 and 4 days of training in the go and no-go task respectively) from the same mouse M8423. In this example similar maps and dynamics were observed, but the self-timed delay period over which post-stimulus licking suppression occurred was shorter. In <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplements 2</xref> and <xref ref-type="fig" rid="fig8s3">3</xref> we show examples of averaged (correct) Go and No trial data from 2 additional mice from cage 4 (tags M8474 and M0252) and group average data on 13 mice in <xref ref-type="fig" rid="fig9">Figures 9</xref> and <xref ref-type="fig" rid="fig10">10</xref> for the Go-detection phase of the task.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Prediction of task outcome from cortical fluorescence signals using a ridge regression model.</title><p>(<bold>A</bold>) Z-scored fluorescence maps over the course of the trial (averaged across 454 trials averaged over 13 mice for each trial outcome). Dotted region indicates time when cue is delivered. (<bold>B</bold>) Locations of cortical regions used for the general linear model (scale bar: 1 mm). (<bold>C</bold>) Task outcome prediction accuracy vs time. Black dotted line indicates the time when the stimulus was delivered. (<bold>D</bold>) Coefficient weights as a function of time for correct vs early (blue) and correct vs late (red) models. (<bold>E</bold>) Violin plots show maximum accuracy for full and reduced models. Solid lines show effect sizes for reduced models compared to the full model. Model types denoted by (-) indicate that the region specified was randomly permuted, whereas (+) indicates that all regions except for the specified region were randomly permuted (e.g. –V1 indicates V1 calcium signal trials were shuffled, whereas +V1 indicates that all calcium signal trials except those from V1 were shuffled). (p&lt;0.05, repeated measures one way ANOVA with post-hoc Bonferroni correction for all groups except those labeled not significant (NS)).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-55964-fig10-v2.tif"/></fig></sec><sec id="s2-8"><title>Brain-behavior correlation</title><p>Z-scored calcium activity from HL and ALM cortical areas were averaged across trials for each of 13 mice during successful GO trials (<xref ref-type="fig" rid="fig9">Figure 9A</xref>; we pooled data from all animals that had at least 20 trials). These traces were then used to compute an across-mouse averaged record (<xref ref-type="fig" rid="fig9">Figure 9B</xref>, black traces). To determine which mice exhibited different cortical activity patterns during the task, the Pearson correlation coefficient was calculated between each mouse’s single trial calcium activity and the across-mouse averaged trace (<xref ref-type="fig" rid="fig9">Figure 9C</xref>). Some mice exhibited similar activity patterns during the trial (e.g. correlation &gt;0.5, <xref ref-type="fig" rid="fig9">Figure 9C</xref>), while other mice had a different strategy consistent with recent work (<xref ref-type="bibr" rid="bib18">Gilad et al., 2018</xref>). Surprisingly, the cage of female mice trained using slightly different training parameters in the laboratory had good correlation to the overall GCaMP signal dynamics in the pooled group (<xref ref-type="fig" rid="fig9">Figure 9C</xref>, see far right points). To compare calcium signal dynamics (<xref ref-type="fig" rid="fig9">Figure 9B</xref>, black traces) to behavioral activity, the behavior gradient across the tongue, leg, and face areas were z-scored and then averaged across trials (<xref ref-type="fig" rid="fig9">Figure 9B</xref>, blue traces). The Pearson correlation coefficient was calculated between the behavioral gradient and the GCaMP calcium activity in the corresponding region for each mouse (<xref ref-type="fig" rid="fig9">Figure 9D</xref>, blue violins). The correlations indicated that a significant amount of covariation in the two signals. Signals from HL and ALM cortical regions were chosen for their association to body movements and licking, respectively. Brain-behavior pairs were then shuffled (across mice), where all pairs of cortical and behavioral activity were considered except for those which include the correct pairing of mice (e.g. correct pair correlation mouseA<sub>brain</sub> ALM, mouseA<sub>behavior</sub> tongue). The Pearson correlation coefficient was then calculated for shuffled brain-behavior pairs (<xref ref-type="fig" rid="fig9">Figure 9d</xref>, black violins). Shuffling brain-behavior pairs across mice resulted in a significant decrease in brain-behavior correlation in HL and ALM regions, but not BC (p&lt;0.05, Wilcoxon signed rank test) suggesting tight coupling between the brain task GCaMP and the movement of tongue or hindlimb body parts.</p></sec><sec id="s2-9"><title>Decoding of trial outcome</title><p>Z-scored ΔF/F<sub>0</sub> montages, averaged across mice (454 single trials over 13 mice, for each trial outcome), show different cortical calcium dynamics associated with each Go-trial outcome: correct (code +2), lick late (code −2), or lick early (code −4) (<xref ref-type="fig" rid="fig10">Figure 10A</xref>). Activity from various cortical regions were used to predict task outcome using a generalized linear model with ridge regression (<xref ref-type="bibr" rid="bib38">Pinto et al., 2019</xref>). Seed pixel locations and labels are shown over a binary mask outlining the bilateral cortical image (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). The model achieved a maximum prediction accuracy of approximately 80% (<xref ref-type="fig" rid="fig10">Figure 10C</xref>). Anterior regions (ALM and M2) generally had greater coefficient weights during the stimulus response period than posterior regions (<xref ref-type="fig" rid="fig10">Figure 10D</xref>). To examine the effect of specific cortical regions on prediction accuracy, we created reduced models by randomly permuting fluorescence activity of the corresponding region(s) with respect to trial outcome (<xref ref-type="bibr" rid="bib43">Salkoff et al., 2020</xref>). Reduced models with individual regions randomly permuted showed small changes in maximum prediction accuracy (<xref ref-type="fig" rid="fig10">Figure 10E</xref>, violins), where permutation of ALM trials resulted in the greatest decrease in model accuracy compared to any other individual region (p&lt;0.001, repeated measures ANOVA with post-hoc Bonferroni correction). Similarly, running the model using only ALM fluorescence signals (+ALM) yielded significantly higher accuracy than when using only M2 (+M2) or V1 (+V1) signals (<xref ref-type="fig" rid="fig10">Figure 10E</xref>, violins, p&lt;0.001, repeated measures ANOVA with post-hoc Bonferroni correction).</p></sec><sec id="s2-10"><title>Supplemental task error analysis examples</title><p>Using the text file tags, we examined various types of licking related error trials, including trials where a mouse failed to respond to a stimulus (<xref ref-type="fig" rid="fig8s4">Figure 8—figure supplements 4A</xref> and <xref ref-type="fig" rid="fig8s5">5A</xref> for two timepoints in training for mouse M8423 and <xref ref-type="fig" rid="fig8s6">Figure 8—figure supplements 6A</xref> and <xref ref-type="fig" rid="fig8s7">7A</xref> for M8474 and M0252). These included no-go errors where the mouse was given a no-go cue, responded by licking, but was not rewarded (GO=-1) (<xref ref-type="fig" rid="fig8s4">Figure 8—figure supplements 4B</xref> and <xref ref-type="fig" rid="fig8s5">5B</xref> for two timepoints in training for mouse M8423 and (<xref ref-type="fig" rid="fig8s6">Figure 8—figure supplements 6B</xref> and <xref ref-type="fig" rid="fig8s7">7B</xref> for M8474 and M0252, <xref ref-type="video" rid="fig8video4">Figure 8—Video 4</xref>). Also of interest were trials in which the mouse licked early, during the delay period during a go-trial, and was not rewarded. Analysis of averaged brain images during early response trials (−4 code) indicated that the errors were associated with brain activity during the period prior to the trial, and that this activity was also spilling over into the delay period ((<xref ref-type="fig" rid="fig8s4">Figure 8—figure supplements 4C</xref> and <xref ref-type="fig" rid="fig8s5">5C</xref> and (<xref ref-type="video" rid="fig8video3">Figure 8—Video 3</xref>). As expected, in lick too early trials, was a robust early increase in frontal cortical activity associated with tongue motor circuits. While mice were found to perform well in the go detection task, discriminating between go- and no-go stimuli was harder for them in practice and was limited to small number of examples and was not the focus of our study. For example, we found that some mice would lick for both go- and no-go trials in response to cues. These mice apparently did not consider a no-go trial as negative reinforcement (time out was small and no overt punishment applied) and merely licked during these trials in addition to the go-trials. In cases where a mouse licked in a no-go trial there was an initial brain response in limb and motor areas but the mouse would fail to show high rates of licking and associated frontal motor cortex brain activity later in the trial as no reward was given (<xref ref-type="fig" rid="fig8s4">Figure 8—figure supplements 4B</xref>, <xref ref-type="fig" rid="fig8s5">5B</xref>, <xref ref-type="fig" rid="fig8s6">6B</xref> and <xref ref-type="fig" rid="fig8s7">7B</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>A way to conceptualize brain circuit organization is into micro, meso, and macro length scales (<xref ref-type="bibr" rid="bib6">Bohland et al., 2008</xref>). To enable study of mesoscale circuit interactions, wide field imaging (<xref ref-type="bibr" rid="bib16">Ferezou et al., 2007</xref>; <xref ref-type="bibr" rid="bib22">Kenet et al., 2003</xref>; <xref ref-type="bibr" rid="bib29">Mitra et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Mohajerani et al., 2013</xref>; <xref ref-type="bibr" rid="bib30">Mohajerani et al., 2010</xref>) permits the definition of functional map-level networks in genetically specified cell populations (<xref ref-type="bibr" rid="bib10">Carandini et al., 2015</xref>; <xref ref-type="bibr" rid="bib18">Gilad et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Ma et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Ma et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Madisen et al., 2015</xref>; <xref ref-type="bibr" rid="bib55">Vanni and Murphy, 2014</xref>; <xref ref-type="bibr" rid="bib59">Zhuang et al., 2017</xref>). Complementing wide field imaging are methods for spatial and temporal control of cortical circuits through the activation of selective opsins (<xref ref-type="bibr" rid="bib4">Ayling et al., 2009</xref>; <xref ref-type="bibr" rid="bib8">Bollu et al., 2019</xref>; <xref ref-type="bibr" rid="bib17">Galiñanes et al., 2018</xref>; <xref ref-type="bibr" rid="bib20">Guo et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>). An emerging theme is that task-evoked cortical activity is not necessarily locally confined, but is widely disseminating across mesoscale cortical networks (<xref ref-type="bibr" rid="bib1">Allen et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Chen et al., 2013</xref>; <xref ref-type="bibr" rid="bib16">Ferezou et al., 2007</xref>; <xref ref-type="bibr" rid="bib21">Guo et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Makino et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Shimaoka et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Steinmetz et al., 2019</xref>). The relatively small brain size of the mouse and its relatively transparent skull (<xref ref-type="bibr" rid="bib15">Drew et al., 2010</xref>; <xref ref-type="bibr" rid="bib48">Silasi et al., 2016</xref>) greatly facilitates mesoscale connectivity studies. However, the need for large scale optical access restricts these experiments to headfixed animals as only larger animals such as rats can carry current widefield sensors (<xref ref-type="bibr" rid="bib45">Scott et al., 2018</xref>). While studying cortical mesoscale circuits during active tasks is a subject of many laboratories, training headfixed mice can extend over months (<xref ref-type="bibr" rid="bib18">Gilad et al., 2018</xref>; <xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>) making home cage approaches increasingly desirable (<xref ref-type="bibr" rid="bib8">Bollu et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Silasi et al., 2018</xref>).</p><p>We present a versatile system for collection of automated mesoscale brain activity data from voluntarily head-fixed mice. The system allows integration of various data streams including behavioral video and behavioral output using a capacitance-based lick detector. While fully automated rodent in vivo brain imaging is still early in development (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib44">Scott et al., 2013</xref>), we anticipate that these systems will have key advantages for the future. These include less animal stress and lower variance due to more consistent animal treatment and experimental design. Recent experimental observations imply that rodent behavioral results can vary based on the sex of the investigator (<xref ref-type="bibr" rid="bib51">Sorge et al., 2014</xref>) confirming that the experimenter perturbs the measurement process. We expect that automated head fixation will remove or at the very least, reduce handler sex bias and other potential procedural biases.</p><sec id="s3-1"><title>Group versus single housing and training</title><p>In all our experiments we have employed group housing of animals in automated home cages. Group housing has certain advantages in that it is proven to reduce animal stress (<xref ref-type="bibr" rid="bib5">Bartolomucci et al., 2003</xref>), it is cost-effective and creates the potential for higher throughput. Typically, mice in our system were only engaged in head fixation for less than 20 min a day each. Even with 8–10 mice in the cage there still will be enough time for all mice to obtain water without there being competition for this resource. In cases where we have removed poor performing mice, competition at the water spout becomes even less of a problem. One potential improvement for the system would be to physically isolate the mouse undergoing head fixation from the others in the cage (currently the hindquarters is within the open chamber but tail is exposed). We currently have a prototype door system for the head fixing tube which fully encloses the mouse and its tail once automated head fixation is initiated. The door is designed to not injure any animal which is potentially caught at its entrance. While we have designed this partition system, we do not feel it is required at this point as we have lengthened the chamber in relation to the original design helping to isolate the headfixed mouse from its cagemates.</p></sec><sec id="s3-2"><title>Optimizing behavioral training for trial number and success rate</title><p>In mouse home cages combined with mesoscale imaging we have attempted two kinds of behavioral tasks. The first is the simple go-trial or detection task where mice after a waiting period will respond to a vibratory cue by licking. We find that home cage mice are quite good at learning this task and that this can potentially report activity in circuits associated with the cue detection, as well as motor output associated with licking. We’ve also employed more advanced training strategies, including a go- and no-go task. In this case we had more variable rates of performance, in particular home cage mice have trouble with suppressing output during no-go trials. We attribute some of the lack of success in no-go training to be due to the nature of home cage training itself. In our home cage task there is no penalty for an animal doing an incorrect trial other than a short time penalty of a few seconds. Because of the potentially aversive nature of the headfixing experience, we decided to not actively punish animals for wrong responses using air-puffs or extended time-outs. A better task for home cage mice to perform would be a discrimination task with two rewarded outcomes, and an animal would specify them by licking from two different water spouts (<xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>). Alternatively, a lever or wheel could be used to support the choice of 2 rewarded outcomes (<xref ref-type="bibr" rid="bib9">Burgess et al., 2017</xref>). Other ways to motivate mice to do a more difficult task would be to make rewards in some ways more palatable, perhaps adding a choice of regular water versus sugar water. Another scheme might be to limit the total number of trials an animal could perform. With limited trials mice might be more motivated to exhibit a higher success rate rather than the lower rate they exhibit with unlimited time.</p><p>The time headfixed for these 23 good male performers averaged 18+−13 min/day (<xref ref-type="table" rid="table1">Table 1</xref>, behavioral data from all mice is available in an online database, see methods). A difference over previous work (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) was removing mice from the cage that were poor performers, or insisted on entering the headfixing tube sideways. Other gains in performance might be obtained through mouse specific training criteria. In the current version of the cage software (used for cages 1–5) all animals are advanced through the go and no-go task together. While we report individual mouse results, it was not possible to selectively tailor the task to individual mice. For cage 6 (and the software we will supply going forward) there was the option to advance mice at different rates using animal specific task information. In other work we have done using a home cage lever task in non-headfixed mice, we did have the ability to alter task criteria based on individual animal performance (<xref ref-type="bibr" rid="bib49">Silasi et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Woodard et al., 2017</xref>). Here, if mice had difficulty with a phase of training, it was possible to downgrade them and make the task easier, allowing them get back on track rather than left thirsty and requiring supplemental water. The auto-headfixing cage will benefit from such individualized training criteria and it will be instituted in all future cages. While we discuss various improvements in trial design to lead to more complex tasks and even better animal engagement, the current hardware and software is capable of monitoring mesoscale circuits in a behavioral or movement based context. Emphasis in our lab has been around mesoscale imaging, notably the cage is also an ideal platform for mesoscale circuit control using optogenetics as transcranial windows readily support the optical control of targeted brain regions (<xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Silasi et al., 2013</xref>; <xref ref-type="bibr" rid="bib48">Silasi et al., 2016</xref>).</p><p>While we have had success automating headfixing and training, it is clear that some mice are not interested in the system as headfixation is voluntary. In our system we have intentionally kept headfixed time short, no more than 45 s so that mice will have an opportunity to leave the device. However, during these short trials we find that most mice will re-headfix at a short interval &lt;30 s, indicating a pattern associated with drinking, or some degree of acceptance of the task. It is anticipated that there could optimization of headfixed duration as other work in mice (<xref ref-type="bibr" rid="bib2">Aoki et al., 2017</xref>) did use much longer but fewer blocks of restrained training using a latching mechanism. Other optimization could involve smaller reward sizes (<xref ref-type="bibr" rid="bib2">Aoki et al., 2017</xref>) as we used a ~ 7–10 μl reward and always gave one task-independent entry reward. In this case we would not expect mice to perform more than 120 successful trials to obtain a daily 1 ml total that is typical for mouse training (<xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>). In our case short bouts of headfixation led to mice performing about 100–200 licking task trials a day (males 24/7 training averaged 200 trials/day, females ~ 7 h/day 100 trials). While 100–200 trials/day/mouse is a much lower than some non-head-fixed home cage tasks where 500–2000 trials/mouse/day were observed (<xref ref-type="bibr" rid="bib8">Bollu et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">Silasi et al., 2018</xref>), one needs to take into consideration that multiple animals are in the same cage and totals can approach 400–800 trials/day when all mice are considered (see <xref ref-type="table" rid="table1">Table 1</xref>). Furthermore, in our case by combining the task with mesoscale imaging each trial needs to have at least 3 s of data collected before and after the cue given the relatively slow GCaMP kinetics making these trials longer than some motor tasks that were less than 2 s (<xref ref-type="bibr" rid="bib49">Silasi et al., 2018</xref>) or evaluated reaching trajectories (<xref ref-type="bibr" rid="bib8">Bollu et al., 2019</xref>).</p><p>In summary, we present an open-source tool that is capable of training group-house mice for simple detection tasks while performed mesoscale imaging. We provide an extensive supplemental cage assembly guide and all documentation for hardware, software, and electronics and encourage users to further customize the cage to their needs.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Supplemental material and data availability</title><p>We include an extensive supplemental guide with full construction illustrations and parts lists (see Home cageGuide_Murphy_lab_June_2019.docx). As a companion to this guide are CAD files for machined and 3-D printed parts, as well electronics schematics and software for data acquisition and data analysis also present on the journal website. All text file behavioral data are included online as well as image data for <xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> are found on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3243572">https://doi.org/10.5281/zenodo.3243572</ext-link>, all data files and code for <xref ref-type="fig" rid="fig9">Figures 9</xref> and <xref ref-type="fig" rid="fig10">10</xref> are found in <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5683/SP2/ZTOPUM">https://doi.org/10.5683/SP2/ZTOPUM</ext-link> and female mouse behavioral data <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5683/SP2/9RFXRP">https://doi.org/10.5683/SP2/9RFXRP</ext-link>. All Python data acquisition code can be found on <ext-link ext-link-type="uri" xlink:href="https://github.com/jamieboyd/AutoHeadFix/">https://github.com/jamieboyd/AutoHeadFix/</ext-link> (<xref ref-type="bibr" rid="bib33">Murphy, 2020</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/AutoHeadFix">https://github.com/elifesciences-publications/AutoHeadFix</ext-link>) and <ext-link ext-link-type="uri" xlink:href="https://github.com/ubcbraincircuits/AutoHeadFix">https://github.com/ubcbraincircuits/AutoHeadFix</ext-link>.</p></sec><sec id="s4-2"><title>Animals</title><p>Forty four male and 8 female transgenic C57BL/6 mice expressing GCaMP6s were used. Cage 1 and 3 were Ai94 from Allen Institute for Brain Science, crossed to Emx1–cre and CaMK2-tTA line, Jackson Labs) (<xref ref-type="bibr" rid="bib27">Madisen et al., 2015</xref>), cage 2 Thy-1 GCaMP6s line 4.3 (HHMI Janelia Research Campus) (<xref ref-type="bibr" rid="bib14">Dana et al., 2014</xref>; <xref ref-type="bibr" rid="bib50">Sofroniew et al., 2016</xref>), and cages 4,5, and 6 were tetO-GCaMP6s x CAMK tTA (<xref ref-type="bibr" rid="bib56">Wekselblatt et al., 2016</xref>). These animals underwent surgery, water deprivation/restriction and training as described below. All procedures were conducted with approval from the University of British Columbia Animal Care Committee and in accordance with guidelines set forth by the Canadian Council for Animal Care. Mice were housed in a conventional facility in plastic cages with micro-isolator tops and kefpt a normal 12 hr light cycle with lights on at 7 AM. When mice were placed in auto-head-fixing cages the bedding was removed and substituted by paper towels to prevent the fine pieces of bedding interfering with the head-fixing or weighing apparatus. The lack of bedding was a precaution; we anticipate that it will be possible to use bedding in the future based on bedding being employed in other similar home cages (<xref ref-type="bibr" rid="bib49">Silasi et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Woodard et al., 2017</xref>).</p></sec><sec id="s4-3"><title>Animal surgery, chronic transcranial window preparation and RFID implantation</title><p>Animals were anesthetized with isoflurane and a transcranial window was installed as previously described (<xref ref-type="bibr" rid="bib48">Silasi et al., 2016</xref>; <xref ref-type="bibr" rid="bib55">Vanni and Murphy, 2014</xref>) and in an amended and more extensive protocol described below. A sterile field was created by placing a surgical drape over the previously cleaned surgical table, and surgical instruments were sterilized with a hot bead sterilizer for 20 s (Fine Science Tools; Model 18000–45). Mice were anesthetized with isoflurane (2% induction, 1.5% maintenance in air) and then mounted in a stereotactic frame in with the skull level between lambda and bregma. The eyes were treated with eye lubricant (Lacrilube; <ext-link ext-link-type="uri" xlink:href="http://www.well.ca">www.well.ca</ext-link>) to keep the cornea moist, and body temperature was maintained at 37°C using a feedback-regulated heating pad monitored by a rectal probe. Lidocaine (0.1 ml, 0.2%) was injected under the scalp, and mice also received a 0.5 ml subcutaneous injection of a saline solution containing burprenorphine (2 mg/ml), atropine (3 μg/ml), and glucose (20 mM). The fur on the head of the mouse (from cerebellar plate to near the eyes) was removed using a fine battery powered beard trimmer, and the skin was prepared with a triple scrub of 0.1% Betadine in water followed by 70% ethanol. Respiration rate and response to toe pinch was checked every 10–15 min to maintain surgical anesthetic plane.</p><p>Prior to starting the surgery, a No. 1 circular cover-glass (Marienfeld, Lauda-Konigshofen, Germany; Cat#:0111520) was cut with a diamond pen (ThorLabs, Newton, NJ, USA; Cat#: S90W) to the size of the final cranial window (~9 mm diameter). A skin flap extending over both hemispheres approximately 8 mm in diameter (3 mm anterior to bregma to posterior end of skull and down lateral to eye level) was cut and removed. A #10 scalpel (curved) and sterile cotton tips were used to gently wipe off any fascia or connective tissue on the skull surface making sure it was completely clear of debris and dry before proceeding. The clear version of C and B-Metabond (Parkell, Edgewood, NY, USA; Product: C and B Metabond) dental cement was prepared by mixing 1 scoop of C and B Metabond powder (Product: S399), 6 drops of C and B Metabond Quick Base (Product: S398) and one drop of C and B Universal catalyst (Product: S371) in a ceramic or glass dish (do not use plastic). Once the mixture reaches a consistency that makes it stick to the end of a wooden stir stick, a titanium fixation bar (22.2 × 2.7×3.2 mm) was placed so that there was a 4 mm posterior space between the bar edge and bregma, by applying a small amount of dental cement and holding it pressed against the skull until the cement partially dried (1–2 min). With the bar in place, a layer of dental adhesive was applied directly on the intact skull. The precut cover glass was gently placed on top of the mixture before it solidifies (within 1 min) taking care to avoid bubble formation. If necessary, extra dental cement was applied around the edge of the cover slip to ensure that all of the exposed bone was covered, and that the incision site was sealed at the edges. The skin naturally tightens itself around the craniotomy and sutures are not necessary. The mixture remains transparent after it solidifies and one should be able to clearly see large surface veins and arteries at the end of the procedure. In the same surgery a small incision was made over the abdomen and an RFID tag (125 kHz glass Sparkfun) was implanted and secured by a suture in a similar manner as described previously (<xref ref-type="bibr" rid="bib7">Bolaños et al., 2017</xref>). Once the dental cement around the coverslip is completely solidified (up to 20 min), the animal received a second sub-cutaneous injection of saline (0.5 ml) with 20 mM of glucose, and allowed to recover in the home cage with an overhead heat lamp and intermittent monitoring (hourly for the first 4 hr and every 4–8 hr thereafter for activity level). Allow the mouse to rest for approximately 7 days before experimenting with the mouse.</p></sec><sec id="s4-4"><title>Camera parameters</title><p>To focus the image at ~15 mm from the sample the Picam lens was screwed to its maximal counterclockwise position (threads engaged ~1/2 turn) and was secured with glue. The depth of field was determined in the same way as in previous reports (<xref ref-type="bibr" rid="bib24">Lim et al., 2012</xref>) and was found to be ~3 mm. This provided both a large focal volume over which to collect fluorescence and makes the system less sensitive to potential changes in z-axis position. To reduce image file size, we binned data at 256 × 256 pixels on the camera for brain imaging data. Brain images were saved in raw 24 bit RGB. We manually fixed camera frame rate to 30 Hz, turned off automatic exposure and auto white balance and set white balance gains to unity. To register frames within and between recording sessions, images were aligned using shifts computed by cross-correlation which was calculated directly (Matlab) or via the FFT to estimate shifts. For both green epifluoresccence and blue reflection channels we adjusted the intensity of illumination so that all values were below 190 out of 256 grey levels (higher levels increase chance of cross talk and saturation). Synchronization between multiple behavioral cameras and brain imaging camera was obtained using a BrainLEDON and BrainLEDOFF text file event which was placed ~3 s from the beginning and end of the headfixed session (duration varies depending on behavioral trial, i.e. the mouse is allowed to finish a trial that has been initiated). Due to small variability in Picam framerates that can accumulate when long sequences of imaging data are concatenated we corrected imported brain and behavioral videos using interpolation in Matlab to yield a framerate of 30 Hz. To assess behavior and movements additional Picams were used that were linked by UDP triggers and also recorded BrainLEDON and BrainLEDOFF text file events to assist in aligning different imaging streams based on interpolation. Regions of interest were drawn over body parts in down-sampled behavioral videos. In the case of HL, this ROI may have also contained contributions to tail or hind quarter movements.</p></sec><sec id="s4-5"><title>Training of mice for self-head-fixation</title><p>Male or female mice that were at least 45 days of age were used for all experiments (see step by step protocol below and cage building supplemental guide). We performed a surgery to install a chronic transcranial window and head fixation bar (<xref ref-type="bibr" rid="bib48">Silasi et al., 2016</xref>) that was similar to previous windows but without bone thinning (<xref ref-type="bibr" rid="bib15">Drew et al., 2010</xref>). 5–21 days after the surgery, the animals were placed on a schedule of water deprivation and had access to a training cage. Given variation in weight due to ad libitum consumption, the initial mouse weight was defined 24 hr after the start of water restriction. If mice did not progress well through training they were still given up to 1 ml of water daily, there was also a 15% maximal weight loss criterion used for supplementation (see detailed protocol). Once mice have learned the head-fixed task, we allowed them to automatically head-fix themselves and consume water ad libitum. Successfully head-fixing animals were able to maintain body weight and gained weight towards pre-surgery and water restriction values.</p></sec><sec id="s4-6"><title>Initial home cage training: optimizing participation, waterspout position, and preventing sideways entry</title><p>In initial cages the water spout tip was mounted within 20 mm of the entrance (cages 1 and 2), but we found problems with accurate RFID tag reading since mice would not enter far enough into the imaging chamber to reliably trigger the tag reader. In cages 3–5 the water spout was initially placed 65 mm from the headfixing tube entrance ensuring that animals would come in far enough to have their RFID tag accurately read and that would allow us to confirm that all animals were involved in the initial phases of training and they were able to lick the spout. Because we noticed that some animals would enter the headfixing tube improperly (sideways), we installed removable 3D-printed plastic barriers to prevent an animal with a vertical bar from reaching the water spout if they enter the head fixing tube sideways (see <xref ref-type="table" rid="table2">Table 2</xref>, i.e. AHF_HeadStraightener_50 mm.stl). Two barriers were used as the water spout position was manually advanced in training. The first barrier was 50 mm long and the water spout was placed 15–25 mm after the end of this barrier. Based on the advancing spout positions, we updated mice to a smaller 25 mm barrier and then completely removed the barrier once the water spout is in the final position and it was unlikely that a mouse entering sideways could still reach the spout at this point.</p><p>Mice not fully engaged in the task (or continuing to enter the tube sideways) at these early stages of training (first 3 weeks) were removed from the cage. During early phases of training (first 1–3 weeks), we make use of video streaming to continuously monitor the cage and did not require the mice to perform any task to obtain water. By entering the head fixing tube and having RFID tag read, mice received what was termed as an entrance reward. Initially, the volume of rewards would be large (~20 μl) and this amount is gated by the duration of the valve opening (up to 500 ms). Once the mice advance significantly in training, the volume of the entrance rewards can be reduced by shortening the valve opening time (100–400 ms, drop now ~7–10 μl). We then included additional large task water rewards for breaking the infrared beams which would indicate that the animal has entered the head fixing tube far enough to potentially trigger the headfixation mechanism (these are termed no-fix trials, see below). At this initial stage of testing we also monitored animal weight using the parallel weighing cage technology (<xref ref-type="bibr" rid="bib36">Noorshams et al., 2017</xref>). However, since we individually inspect their health once a day and in the process manually weigh animals, we have relied on manual weights for management of water supplementation for this study. Female mice were trained in automated daily lab-based sessions of ~7 hr duration and lacked auditory feedback for early licking.</p></sec><sec id="s4-7"><title>Raspberry Pi and electronics interface</title><p>To control the task, we used the Raspberry Pi 3 single board computer. This internet accessible device incorporates HDMI video output and was able to both control the task and capture data. To collect images of mouse entry and exit from the cage, a second Raspberry Pi camera was employed. Assigning each Raspberry Pi a unique IP address allowed us to assess the cages remotely through the internet to monitor whether any animals were in difficulty during chronic imaging or not receiving sufficient water rewards. UDP protocol triggers were used to synchronize behavioral and brain imaging cameras using unique IP addresses (all cameras record a master triggered lights on and off signal as well).</p></sec><sec id="s4-8"><title>Electronics</title><p>Electronics were produced as described in the schematics present within the supplemental material.</p></sec><sec id="s4-9"><title>Ridge regression model</title><p>We trained a linear model to predict trial outcome (correct or incorrect GO trials) based on calcium activity recorded during the trial. Incorrect trial outcomes consisted of errors where mice licked during the delay period (before the correct response period) and where they licked after the correct response period (or not at all), hereafter named early and late errors. Calcium activity (454 single trials across 13 mice) from 16 bilateral cortical regions of interest (anterior lateral motor cortex (ALM), secondary motor cortex (M2), primary motor cortex (M1), forelimb area (FL), hindlimb area (HL), barrel cortex (BC), retrosplenial cortex (RS), and primary visual cortex (V1)) was z-scored and then spatially averaged within each region over a 0.7 × 0.7 mm area and across hemispheres. To account for multicollinearity, we used a ridge regression model previously described (<xref ref-type="bibr" rid="bib38">Pinto et al., 2019</xref>)<sup>1</sup>. The model was fit with 50 runs of 3-fold cross validation for each frame of calcium activity in the trial. Briefly, the model can be described as:<disp-formula id="equ1"><mml:math id="m1"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mfenced close="‖" open="‖" separators="|"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>Where y is a Nx1 vector of outcomes (1 or -1 for correct or incorrect trials respectively) where N is the number of trials, <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is an offset term, X is a NxR matrix of z-scored ΔF/F<sub>0</sub> values where R is the number of ROIs, β is a Rx1 vector of coefficient values, <inline-formula><mml:math id="inf2"><mml:mfenced close="‖" open="‖" separators="|"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the Euclidean norm of the coefficient values, and λ is a penalty term. The optimal value for λ was determined through cross validation (<xref ref-type="bibr" rid="bib38">Pinto et al., 2019</xref>). Prediction accuracy was determined as the proportion of outcomes from the test set that were correctly classified. To evaluate each region’s contribution to the model’s prediction accuracy, we created reduced models for each predictor (calcium activity from individual cortical region) from the model. Predictors were eliminated by randomly permuting trials of the ΔF/F<sub>0</sub> time series from an individual region (1000 iterations). We also compared the models to a randomly permuted model, in which the trial labels (e.g. correct or incorrect) were randomly permuted (1000 iterations). The difference in prediction accuracy between the full model and the reduced model was the unique contribution of that region’s calcium activity to prediction accuracy (<xref ref-type="bibr" rid="bib35">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib43">Salkoff et al., 2020</xref>).</p></sec><sec id="s4-10"><title>Statistics</title><p>Results are presented as mean +- standard deviation unless noted otherwise. Experimenters were not blinded during the experiment or the analysis, although automated acquisition and analysis procedures were employed. Some animals were excluded from the results as indicated (based on poor head-fixation performance or health concerns) and no method of randomization was used. Sample size measurements for analysis of imaging data were consistent with previous work (<xref ref-type="bibr" rid="bib31">Mohajerani et al., 2013</xref>).</p><p>Behavioral events were printed within textfiles with timestamp and RFID tag information and then uploaded to an online MySQL relational database. Raw events were further analyzed in Python and processed into detailed summary information about each chamber entry, head-fix session, task driven trial, lick, earned reward and video file. These newly stored tables were further sourced to generate pipelines for daily monitoring of the animals, statistics and figures, allowing their direct update by adjusting the query sent to the database. The database allows the selection and download of data with SQL queries with libraries like e.g. pymysql (Python) or similar SQL connectors and can be setup locally. For data on cages 1–5 see Zenodo <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3268838">https://doi.org/10.5281/zenodo.3268838</ext-link> and cage 6 female mice see <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5683/SP2/9RFXRP">https://doi.org/10.5683/SP2/9RFXRP</ext-link>, Scholars Portal Dataverse.</p><p>Clustering of the behavior was done using 1D kernel density estimation using the timestamp as the variable. Head-fix start times were substituted by a Gaussian with standard deviation of 50 s, which is the average time a mouse needs for a single head-fixation including entering and exiting the chamber. The signal minima that appear between two head-fixes were thresholded at approximately 5% of the peak amplitude of an isolated head-fixation signal to identify the end of a cluster. This corresponds to 2.4 sigma (99% probability) and results in cluster breaks after 4–5 min without head-fixation.</p><p>The headfixation task-trial software had provisions to allow mice extra time to finish a partial trial. In a small number of cases (less than 5% of trials) when the last task trial occurred beyond the point of specified headfixation duration there were cases where the trial lacked the usual lick withhold time. In these instances a small number of licks would appear in the lick withhold period. We have removed these trials from the lick analysis and they were typically not included in averaged brain imaging GCaMP data as we only included trials where there was 3 s of data after the stimulus and before the brain LED turns off.</p><p>Analysis of brain GCaMP dynamics was performed in Matlab using an analysis pipeline that was similar to that previously reported (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>), but with the addition of a 440 nm light reflectance image reference subtraction (<xref ref-type="bibr" rid="bib26">Ma et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Ma et al., 2016</xref>; <xref ref-type="bibr" rid="bib56">Wekselblatt et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Xiao et al., 2017</xref>). Plots in <xref ref-type="fig" rid="fig8">Figure 8</xref>, and <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplements 2</xref>–<xref ref-type="fig" rid="fig8s7">7</xref> include the mean +-SEM. For analysis of behavioral data we employed ROIs over specific body parts and expressed movement as the absolute value of the gradient between two adjacent frames that are scaled according to the max and min value of all the ROIs.</p></sec><sec id="s4-11"><title>Step by step protocol: Auto head fixation: animal care, surgery, training, behavioral task, archiving and analysis. Assemble of a Raspberry Pi-controlled cage is described in the cage assembly addendum (see home cageGuide and <xref ref-type="table" rid="table2">Table 2</xref>)</title><sec id="s4-11-1"><title>Step 1. acclimate group-housed male (or female) mice to large number housing</title><p>Note the bulk of the data we report here uses male mice, it is expected that females would train in a similar manner as our previous work used all females (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) and training of a small cohort of female mice yielded similar levels of head-fixed training as the males (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Select 8 to 10 male or female mice of C57bl6 background that are post-weaning and pre-pubescent to form a single cage. The goal is to pool different litters to have 8 to 10 animals of similar age (within 2 weeks of each other) and sex for pre-acclimation to a double automatic head fixation training cage. These animals live together within a double training cage for a 2–6 week period prior to transfer to the autoheadfixation cage over which time they undergo and recover from surgery for RFID tags (<xref ref-type="bibr" rid="bib7">Bolaños et al., 2017</xref>) and transcranial windows when they are least 45 days of age (<xref ref-type="bibr" rid="bib48">Silasi et al., 2016</xref>) (see step 2). Potentially aggressive mice will be noted and removed. Note, we have not removed any mice at the double training cage stage (in the 6 cages we report here), but we nonetheless mention this as a potential safeguard.</p></sec><sec id="s4-11-2"><title>Step 2 combined RFID tag and window surgery</title><p>Once acclimated to the double cage, the cohort of 8 to 10 mice undergo a combined surgery for both RFID tag implantation and cranial window installation (typically the 8–10 surgeries are performed over 1–3 days). Over this time, the animals are checked every other day for signs of aggression, wounding or barbering, or poor sealing or infection at the margins of their transcranial windows. We employ previously the published protocol from our group (<xref ref-type="bibr" rid="bib48">Silasi et al., 2016</xref>) with the modification of installing a flat titanium bar (McMaster Carr for stock titanium #9039K24) of dimensions as described (<xref ref-type="bibr" rid="bib32">Murphy et al., 2016</xref>) instead of the 4–40 stud bolt. Prior to installation of the bar, the corners are rounded with a file to reduce the chance of a sharp edge catching the walls of the headfixing tube. Cages are configured to use a text message server to message a set of phone numbers when a set duration (for a RFID tag) in the chamber is exceeded in the unlikely, but possible, scenario of a mouse being stranded within the fixation tube. Once the animals have recovered from the surgery (~7–21 days), they are removed as a group from the training cage and placed in the auto-headfixation cage for the start of water restriction.</p></sec><sec id="s4-11-3"><title>Step 3 Non-head fixed water spout training within head fixation cage</title><p>Once in the autoheadfixation cage, we remove the water bottle and weigh the mice 24 hr after water restriction. This weight is the baseline for all weight criteria during auto-headfixation training. Any mouse exhibiting weight loss of more than 15% or exhibiting signs of dehydration is supplemented with water and monitored daily for weight, appearance, and behavior (<xref ref-type="bibr" rid="bib19">Guo et al., 2014</xref>). The cage contains a RFID triggered water spout (tip) placed 65 mm from the head fixation tube opening (measured from the opening to the cage) which dispenses water ‘Entrance Rewards’ upon reading a tag. In this case, it is important to place the water spout and the tag reader so that the RFID tags will be read as the mouse approaches the water spout. In current versions of the cage, we include capacitive lick detection (<xref ref-type="bibr" rid="bib3">Ardesch et al., 2017</xref>) which provides confirmation that the mouse is indeed licking the water spout and has good spout placement for later headfixed trials. An appropriate starting position for the tag reader is centered 20 mm from the headfixing tube entrance; if ‘Licks’ occur after ‘Exits’ in the text file, the reader is moved further from the cage (deeper into the tunnel) to prevent periodic loss of tag reading. If the water spout is placed too close to the entrance, the mouse will reach the spout before the tag is read and no water will be dispensed; in this case, with lick detection, licks will be observed on the capacitance sensor but no entries. We recommend investigators use video streaming to confirm appropriate placement of the water spout as the mice enter. During the first day of non-headfixed water spout training, we monitor the reading of RFID tags and confirmation of licking in most mice tested (use Python code stimulator class AHF_Stimulator_Rewards). Any mice not entering the task will be given supplemental water (to a maximum of 1 ml/day, unless they are below the cut-off where the amount can be up to 1 ml plus any deficit) to maintain their weight above the specified limit (85% of baseline). If only one or two mice are not reaching the spout it is best to identify them and mark their tails using black lab marker so they can be easily found and supplemented with water outside of the cage with less disturbance of the other mice. In these other mice water intake can be confirmed automatically using lick detection and automatic weighing (<xref ref-type="bibr" rid="bib36">Noorshams et al., 2017</xref>). Over the next 6–14 days the water spout is moved a total of 40 mm until a reaches a final length of ~7–9 mm from the headfixing chamber back wall or 110 mm from the entrance. At this time first a removable 50 mm and then a 25 mm blocking structure (see CAD parts AHF_HeadStraightener_25 mm.stl and AHF_HeadStraightener_50 mm.stl <xref ref-type="table" rid="table2">Table 2</xref>) were adding to prevent mice from reaching the waterspout when sideways.</p></sec><sec id="s4-11-4"><title>Step 4 beam break training and lick suppression training</title><p>Over 7–14 days mice are gradually advanced towards head fixation. For the first 3–8 days mice are trained to move to the end of the chamber with their bar on the overhead rails breaking the IR beam and eliciting the mesoscale brain imaging excitation LEDs to activate (use Python code stimulator class AHF_Stimulator_Rewards). Here mice are given rewards every 5 s for merely breaking the beam and will learn to associate the blue brain imaging LEDs with greater rewards (at this point headfixing probability is at 0). When the mice are going to the tube end the tag reader should be moved so its center is 40 mm from the headfixing tube entrance as the mice advance and the 25 mm blocker (CAD file) used instead of the 50 mm version. When most mice reach this point (typically within 2–4 days of beam breaking) we begin to train mice to suppress repetitive licking which is the first step in task training. Repetitive licking is flagged by an audible tone: with intervals between licks of less than &lt;1 s met by the tone (use Python code stimulator class AHF_Stimulator_LickWitholdSpeaker). Over 2–4 days mice will learn to suppress repetitive licking creating 1–2 s periods without touching the water spout that are then rewarded by water delivery. At this point in training mice are given rewards regardless of subsequent lick timing (they only need to learn to pause licking) and a vibratory cue that marks the end of the no licking period (and later used for go and no-go training) is also introduced. During this time the water sprout is continually manually retracted (and secured with hot melt glue) and its length is measured from the far wall of the head fixation chamber. The final point for water spout placement is 7–8 mm from the back wall and will make it very likely that the mouse will break the infrared beam triggering head fixation (or a no-fix trial) during spout licking. Lick suppression was begun before loose headfixing in cages 3–5, while it was added after in cages 1 and 2.</p></sec><sec id="s4-11-5"><title>Step 5 gradual head fixation task training</title><p>The servo motor head fixation system has the ability to partially restrain mice allowing significant movement of their head and movement forward and backwards (15–20 mm) within the head fixation tube. Initially, this loose restraint is given with a random probability of 50%. Within a 7 day period, the travel of the servo head fixation device is gradually increased until the mouse is tightly head fixed at the end of this period (adjust servo travel using the hardware tester found in the main code interface). Adjustments of the servo position should made in small increments as it is possilbe to damage the system by over-travel due to over-heating of the motor or the mechanical force itself. During both partial and full head fixation trials, as well as non-head fixed trials (where the mouse still receives water rewards), we cue blue LED lights which are required for mesoscale imaging at the point of fixation and during the entire trial during in mesoscale GCaMP imaging. These blue lights help the mouse associate the head fixation trial and task with the availability of water. Initially, mice are given rewards for merely triggering a head fixed imaging session. In this case water is dispensed for a 100–500 ms period at regular intervals (every 5–10 s). These initial trials are relatively short in duration being 15–35 s in length. At this initial time mice are not required to do a complex behavior task, but are merely rewarded regularly for being either loose or tightly head fixed. Mice are also rewarded for breaking the infrared beams that trigger the head fixation process during no-fix (non-headfixed) trials. It is important to view the mice on streaming video and to make sure that the water spout is positioned such that they come far enough forward to break the infrared beam. The position of the mouse tongue with respect to the spout needs to be inspected while head fixed to make sure that there are no mice whose head bar placement prevents them from obtaining water when restrained (nose is angled up or down). Such animals need to be removed, or have their bar repositioned. Of note, the infrared beam break can interfere with making behavioral videos using infrared sensitive CMOS cameras. Therefore, we have included electronics that automatically turn off the infrared beam once the mouse undergoes head fixation.</p></sec><sec id="s4-11-6"><title>Step 6 head fixed task training: lick suppression/cued licking</title><p>Once mice are exhibiting significant rates of head fixation of (typically &gt;10 min total/day/mouse) we will advance them to a task which can be performed while head fixed. While our initial goal has been to optimize the training for head fixation, we have also addressed whether mice can be trained for a task in the context of the potential stress that the headfixation may bring. A simple detection task was developed to permit the collection of cortical calcium dynamics. A common occurrence is that once head fixed, mice will lick the water spout in expectation of water rewards. As described in Step 4 we train to suppress over-licking and provide auditory feedback to discourage high rates of spout licking and do not present stimuli until a lick suppression period is met. To signal the end of the lick suppression period and that licking will now lead to water delivery we add a vibration cue at this point and reward licking that occurs after at least 200-300 ms after the cue (this is termed the delay period) and before the reward dispensing time. Water rewards are dispensed 1.25–1.5 s after the response withholding interval plus the cue response window (for a 1.25 s water delivery add response period of 0.3 s so reward at 1.55 s) (use Python code stimulator class AHF_Stimulator_LickNoLickDisc).</p></sec><sec id="s4-11-7"><title>Step 7 head fixed training go and No-go task with detection task</title><p>Over a period of several weeks animals are advanced into a task once they learn to suppress continuous licking and they are trained to respond to a cue that indicates the end of the lick lockout period (go-trial). We have used a small vibration (that can vary in frequency) of the headfixing tube as the cue for both go and no-go trials. After the cue mice are initially rewarded with water and in a subsequent step are required to lick the water spout after a delay period to receive a reward (as in step 6), for this correct response the software awards a +2 code. We find that head fixed mice have difficulty initially suppressing licking and will lick too early in response to the cue which gives a −4 code or will miss the cue all together −2 code (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Training mice to delay before responding to the cue is important for future development of a choice task where mice discriminate two related sensory stimuli (in our case we employ two types of vibration a continuous 500 ms or three 200 ms vibrations 100 ms apart). We find that over a period of days we can gradually lengthen this delay period from an initial 300 ms to over 1 s. Correspondingly, we lengthen the period over which mice must suppress licking from 1 s to several seconds (pre-cue period). We also add randomization (+−0.5 s) to the lick suppression interval, so that mice do not learn to rely on internal timing. Over this time, all animals are monitored and the cage is progressed as a group in delay period interval over time. Typically, small increases in delay period are made daily 50–100 ms. The software which runs autoheadfixation reports a series of text codes which indicate whether the mouse licked in response to the cue or not (−2, +2, or −4 described above for Go trials). Once this training is going well a no-go trial is initiated with a different, but related stimulus (pulsed versus continuous vibration). Here the correct response is to withhold licking over the response interval which gives a code of +1, licking during correct time interval (after the stimulus) but during a no-go trial is an error and receives a code of −1, if the mouse licks too early during a no-go trial this a −3 code error (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p></sec><sec id="s4-11-8"><title>Step 8 on-going daily data acquisition</title><p>Mice are manually weighed as automatic weighing (<xref ref-type="bibr" rid="bib36">Noorshams et al., 2017</xref>), while feasible was not used as the mice were each manually inspected and weighed. Animals are also inspected for any health changes, including ruffled fur, wounding, or signs of dehydration or trauma. Typically, once animals have learned the task, those which headfix require very little supplemental water, whereas non-headfixing animals require daily supplementation of 0.5–1 ml or need to be removed from the cage. We generally find that animals which fail to head fix 2 weeks after the initiation of head fixed training are unlikely to change and adopt headfixation. Secure file transfer protocol (sftp) is used to download text files made daily for each cage, both a detailed record of all events with time stamps and tag numbers, and a brief total for each mouse of head fixes, rewards given, and task performance.</p></sec><sec id="s4-11-9"><title>Step 9 data analysis</title><p>Each day automated, cage-based mesoscale imaging can generate an average 15 min of data/mouse of head fixed mesoscale data acquisition. Taking advantage of the nocturnal nature of mice, we typically back up data during the light cycle. It is important to note that Raspberry Pi external USB mechanical hard disks can drop frames if a hard disk is simultaneously writing new files and being backed up. Using Matlab, we import text files which indicate behavioral performance and licking along with RFID tag and time stamps and use them to parse raw file video of brain activity or H264 files of behavior. Each image file is saved with a timestamp and the RFID tag in the filename. An important step in data analysis is to use a common timing signal that can be viewed in all data streams: text file, behavioral video, and brain video. Typically, we achieve this by making an LED light turn on and off signal. Between lights on and off, we establish the time base for the various camera or text file inputs, this event is also printed within the text file as BrainLEDON and BrainLEDOFF.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by a Canadian Institutes of Health Research (CIHR) THM FDN-143209, NIH R21 to THM, M Balbi and T Murphy were supported by a Fondation Leducq team grant, from Brain Canada for the Canadian Neurophotonics Platform to THM, and a Canadian Partnership for Stroke Recovery Catalyst grant. CIHR or Brain Canada had no involvement in the research or decision to publish. We thank Pumin Wang for help with surgery and Cindy Jiang for animal colony management and Lucas Pinto for providing the code used for the ridge regression model.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Software, Formal analysis, Methodology</p></fn><fn fn-type="con" id="con4"><p>Software, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con5"><p>Resources, Visualization, Methodology</p></fn><fn fn-type="con" id="con6"><p>Resources, Data curation, Software, Formal analysis, Investigation, Methodology</p></fn><fn fn-type="con" id="con7"><p>Software, Formal analysis</p></fn><fn fn-type="con" id="con8"><p>Investigation, Drew figures</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Software, Formal analysis, Investigation, Methodology</p></fn><fn fn-type="con" id="con10"><p>Data curation, Software, Formal analysis, Validation, Investigation, Methodology</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All procedures were conducted with approval from the University of British Columbia Animal Care Committee and in accordance with guidelines set forth by the Canadian Council for Animal Care.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Illustrated supplemental cage construction guide.</title></caption><media mime-subtype="pdf" mimetype="application" xlink:href="elife-55964-supp1-v2.pdf"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>CAD for 3D printed and machined parts.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-55964-supp2-v2.zip"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Electronics schematics.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-55964-supp3-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-55964-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The name of each brain imaging file which contains both the mouse ID and the time stamp can be found in the SQL database (RFIDtag_xxxx_timestamp.raw; see Methods for URL and hosted as a full text file archive on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3268838">https://doi.org/10.5281/zenodo.3268838</ext-link>) for the 5 cages of male mice that compose figures 1-7 and cage 6 female mice <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5683/SP2/9RFXRP">https://doi.org/10.5683/SP2/9RFXRP</ext-link>. All text file behavioral data is included online as well as image data for Figure 8 and Figure 8—figure supplement 1 are found on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3243572">https://doi.org/10.5281/zenodo.3243572</ext-link>, all data files and code for Figures 9 and 10 are found in <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5683/SP2/ZTOPUM">https://doi.org/10.5683/SP2/ZTOPUM</ext-link> and female mouse behavioral data <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5683/SP2/9RFXRP">https://doi.org/10.5683/SP2/9RFXRP</ext-link>. All Python data acquisition code can be found on <ext-link ext-link-type="uri" xlink:href="https://github.com/jamieboyd/AutoHeadFix/">https://github.com/jamieboyd/AutoHeadFix/</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/AutoHeadFix">https://github.com/elifesciences-publications/AutoHeadFix</ext-link>) and <ext-link ext-link-type="uri" xlink:href="https://github.com/ubcbraincircuits/AutoHeadFix">https://github.com/ubcbraincircuits/AutoHeadFix</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Homecage data for Figures 8 and 9 and behavioral database</data-title><source>Zenodo</source><pub-id assigning-authority="Zenodo" pub-id-type="doi">10.5281/zenodo.3243572</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Code and. mat files for automated homecage paper (Figs 9 and 10)</data-title><source>Scholars Portal Dataverse</source><pub-id assigning-authority="other" pub-id-type="doi">10.5683/SP2/ZTOPUM</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Home cage data female mice cage 6 all text file and database information</data-title><source>Scholars Portal Dataverse</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://doi.org/10.5683/SP2/9RFXRP">9RFXRP</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname> <given-names>WE</given-names></name><name><surname>Kauvar</surname> <given-names>IV</given-names></name><name><surname>Chen</surname> <given-names>MZ</given-names></name><name><surname>Richman</surname> <given-names>EB</given-names></name><name><surname>Yang</surname> <given-names>SJ</given-names></name><name><surname>Chan</surname> <given-names>K</given-names></name><name><surname>Gradinaru</surname> <given-names>V</given-names></name><name><surname>Deverman</surname> <given-names>BE</given-names></name><name><surname>Luo</surname> <given-names>L</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Global representations of Goal-Directed behavior in distinct cell types of mouse neocortex</article-title><source>Neuron</source><volume>94</volume><fpage>891</fpage><lpage>907</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.017</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aoki</surname> <given-names>R</given-names></name><name><surname>Tsubota</surname> <given-names>T</given-names></name><name><surname>Goya</surname> <given-names>Y</given-names></name><name><surname>Benucci</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An automated platform for high-throughput mouse behavior and physiology with voluntary head-fixation</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1196</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01371-0</pub-id><pub-id pub-id-type="pmid">29084948</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ardesch</surname> <given-names>DJ</given-names></name><name><surname>Balbi</surname> <given-names>M</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automated touch sensing in the mouse tapered beam test using raspberry pi</article-title><source>Journal of Neuroscience Methods</source><volume>291</volume><fpage>221</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.08.030</pub-id><pub-id pub-id-type="pmid">28860079</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayling</surname> <given-names>OG</given-names></name><name><surname>Harrison</surname> <given-names>TC</given-names></name><name><surname>Boyd</surname> <given-names>JD</given-names></name><name><surname>Goroshkov</surname> <given-names>A</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Automated light-based mapping of motor cortex by photoactivation of channelrhodopsin-2 transgenic mice</article-title><source>Nature Methods</source><volume>6</volume><fpage>219</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1303</pub-id><pub-id pub-id-type="pmid">19219033</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolomucci</surname> <given-names>A</given-names></name><name><surname>Palanza</surname> <given-names>P</given-names></name><name><surname>Sacerdote</surname> <given-names>P</given-names></name><name><surname>Ceresini</surname> <given-names>G</given-names></name><name><surname>Chirieleison</surname> <given-names>A</given-names></name><name><surname>Panerai</surname> <given-names>AE</given-names></name><name><surname>Parmigiani</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Individual housing induces altered immuno-endocrine responses to psychological stress in male mice</article-title><source>Psychoneuroendocrinology</source><volume>28</volume><fpage>540</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1016/S0306-4530(02)00039-2</pub-id><pub-id pub-id-type="pmid">12689611</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bohland</surname> <given-names>JW</given-names></name><name><surname>Bokil</surname> <given-names>H</given-names></name><name><surname>Allen</surname> <given-names>CB</given-names></name><name><surname>Mitra</surname> <given-names>PP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The brain atlas concordance problem: quantitative comparison of anatomical parcellations</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e7200</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0007200</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolaños</surname> <given-names>F</given-names></name><name><surname>LeDue</surname> <given-names>JM</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cost effective raspberry pi-based radio frequency identification tagging of mice suitable for automated in vivo imaging</article-title><source>Journal of Neuroscience Methods</source><volume>276</volume><fpage>79</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.11.011</pub-id><pub-id pub-id-type="pmid">27899319</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bollu</surname> <given-names>T</given-names></name><name><surname>Whitehead</surname> <given-names>SC</given-names></name><name><surname>Prasad</surname> <given-names>N</given-names></name><name><surname>Walker</surname> <given-names>J</given-names></name><name><surname>Shyamkumar</surname> <given-names>N</given-names></name><name><surname>Subramaniam</surname> <given-names>R</given-names></name><name><surname>Kardon</surname> <given-names>B</given-names></name><name><surname>Cohen</surname> <given-names>I</given-names></name><name><surname>Goldberg</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Automated home cage training of mice in a hold-still center-out reach task</article-title><source>Journal of Neurophysiology</source><volume>121</volume><fpage>500</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1152/jn.00667.2018</pub-id><pub-id pub-id-type="pmid">30540551</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname> <given-names>CP</given-names></name><name><surname>Lak</surname> <given-names>A</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Zatka-Haas</surname> <given-names>P</given-names></name><name><surname>Bai Reddy</surname> <given-names>C</given-names></name><name><surname>Jacobs</surname> <given-names>EAK</given-names></name><name><surname>Linden</surname> <given-names>JF</given-names></name><name><surname>Paton</surname> <given-names>JJ</given-names></name><name><surname>Ranson</surname> <given-names>A</given-names></name><name><surname>Schröder</surname> <given-names>S</given-names></name><name><surname>Soares</surname> <given-names>S</given-names></name><name><surname>Wells</surname> <given-names>MJ</given-names></name><name><surname>Wool</surname> <given-names>LE</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High-Yield methods for accurate Two-Alternative visual psychophysics in Head-Fixed mice</article-title><source>Cell Reports</source><volume>20</volume><fpage>2513</fpage><lpage>2524</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.08.047</pub-id><pub-id pub-id-type="pmid">28877482</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Shimaoka</surname> <given-names>D</given-names></name><name><surname>Rossi</surname> <given-names>LF</given-names></name><name><surname>Sato</surname> <given-names>TK</given-names></name><name><surname>Benucci</surname> <given-names>A</given-names></name><name><surname>Knöpfel</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Imaging the awake visual cortex with a genetically encoded voltage Indicator</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>53</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0594-14.2015</pub-id><pub-id pub-id-type="pmid">25568102</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>JL</given-names></name><name><surname>Carta</surname> <given-names>S</given-names></name><name><surname>Soldado-Magraner</surname> <given-names>J</given-names></name><name><surname>Schneider</surname> <given-names>BL</given-names></name><name><surname>Helmchen</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Behaviour-dependent recruitment of long-range projection neurons in somatosensory cortex</article-title><source>Nature</source><volume>499</volume><fpage>336</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/nature12236</pub-id><pub-id pub-id-type="pmid">23792559</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>T-W</given-names></name><name><surname>Li</surname> <given-names>N</given-names></name><name><surname>Daie</surname> <given-names>K</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A map of anticipatory activity in mouse motor cortex</article-title><source>Neuron</source><volume>94</volume><fpage>866</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.005</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clancy</surname> <given-names>KB</given-names></name><name><surname>Orsolic</surname> <given-names>I</given-names></name><name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Locomotion-dependent remapping of distributed cortical networks</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>778</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0357-8</pub-id><pub-id pub-id-type="pmid">30858604</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dana</surname> <given-names>H</given-names></name><name><surname>Chen</surname> <given-names>TW</given-names></name><name><surname>Hu</surname> <given-names>A</given-names></name><name><surname>Shields</surname> <given-names>BC</given-names></name><name><surname>Guo</surname> <given-names>C</given-names></name><name><surname>Looger</surname> <given-names>LL</given-names></name><name><surname>Kim</surname> <given-names>DS</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Thy1-GCaMP6 transgenic mice for neuronal population imaging in vivo</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e108697</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0108697</pub-id><pub-id pub-id-type="pmid">25250714</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drew</surname> <given-names>PJ</given-names></name><name><surname>Shih</surname> <given-names>AY</given-names></name><name><surname>Driscoll</surname> <given-names>JD</given-names></name><name><surname>Knutsen</surname> <given-names>PM</given-names></name><name><surname>Blinder</surname> <given-names>P</given-names></name><name><surname>Davalos</surname> <given-names>D</given-names></name><name><surname>Akassoglou</surname> <given-names>K</given-names></name><name><surname>Tsai</surname> <given-names>PS</given-names></name><name><surname>Kleinfeld</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Chronic optical access through a polished and reinforced thinned skull</article-title><source>Nature Methods</source><volume>7</volume><fpage>981</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1530</pub-id><pub-id pub-id-type="pmid">20966916</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferezou</surname> <given-names>I</given-names></name><name><surname>Haiss</surname> <given-names>F</given-names></name><name><surname>Gentet</surname> <given-names>LJ</given-names></name><name><surname>Aronoff</surname> <given-names>R</given-names></name><name><surname>Weber</surname> <given-names>B</given-names></name><name><surname>Petersen</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatiotemporal dynamics of cortical sensorimotor integration in behaving mice</article-title><source>Neuron</source><volume>56</volume><fpage>907</fpage><lpage>923</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.007</pub-id><pub-id pub-id-type="pmid">18054865</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galiñanes</surname> <given-names>GL</given-names></name><name><surname>Bonardi</surname> <given-names>C</given-names></name><name><surname>Huber</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Directional reaching for water as a Cortex-Dependent behavioral framework for mice</article-title><source>Cell Reports</source><volume>22</volume><fpage>2767</fpage><lpage>2783</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.02.042</pub-id><pub-id pub-id-type="pmid">29514103</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilad</surname> <given-names>A</given-names></name><name><surname>Gallero-Salas</surname> <given-names>Y</given-names></name><name><surname>Groos</surname> <given-names>D</given-names></name><name><surname>Helmchen</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Behavioral strategy determines frontal or posterior location of Short-Term memory in neocortex</article-title><source>Neuron</source><volume>99</volume><fpage>814</fpage><lpage>828</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.029</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname> <given-names>ZV</given-names></name><name><surname>Li</surname> <given-names>N</given-names></name><name><surname>Huber</surname> <given-names>D</given-names></name><name><surname>Ophir</surname> <given-names>E</given-names></name><name><surname>Gutnisky</surname> <given-names>D</given-names></name><name><surname>Ting</surname> <given-names>JT</given-names></name><name><surname>Feng</surname> <given-names>G</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Flow of cortical activity underlying a tactile decision in mice</article-title><source>Neuron</source><volume>81</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.020</pub-id><pub-id pub-id-type="pmid">24361077</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname> <given-names>JZ</given-names></name><name><surname>Graves</surname> <given-names>AR</given-names></name><name><surname>Guo</surname> <given-names>WW</given-names></name><name><surname>Zheng</surname> <given-names>J</given-names></name><name><surname>Lee</surname> <given-names>A</given-names></name><name><surname>Rodríguez-González</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>N</given-names></name><name><surname>Macklin</surname> <given-names>JJ</given-names></name><name><surname>Phillips</surname> <given-names>JW</given-names></name><name><surname>Mensh</surname> <given-names>BD</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name><name><surname>Hantman</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortex commands the performance of skilled movement</article-title><source>eLife</source><volume>4</volume><elocation-id>e10774</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10774</pub-id><pub-id pub-id-type="pmid">26633811</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname> <given-names>ZV</given-names></name><name><surname>Inagaki</surname> <given-names>HK</given-names></name><name><surname>Daie</surname> <given-names>K</given-names></name><name><surname>Druckmann</surname> <given-names>S</given-names></name><name><surname>Gerfen</surname> <given-names>CR</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Maintenance of persistent activity in a frontal thalamocortical loop</article-title><source>Nature</source><volume>545</volume><fpage>181</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1038/nature22324</pub-id><pub-id pub-id-type="pmid">28467817</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kenet</surname> <given-names>T</given-names></name><name><surname>Bibitchkov</surname> <given-names>D</given-names></name><name><surname>Tsodyks</surname> <given-names>M</given-names></name><name><surname>Grinvald</surname> <given-names>A</given-names></name><name><surname>Arieli</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Spontaneously emerging cortical representations of visual attributes</article-title><source>Nature</source><volume>425</volume><fpage>954</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1038/nature02078</pub-id><pub-id pub-id-type="pmid">14586468</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Merre</surname> <given-names>P</given-names></name><name><surname>Esmaeili</surname> <given-names>V</given-names></name><name><surname>Charrière</surname> <given-names>E</given-names></name><name><surname>Galan</surname> <given-names>K</given-names></name><name><surname>Salin</surname> <given-names>P-A</given-names></name><name><surname>Petersen</surname> <given-names>CCH</given-names></name><name><surname>Crochet</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reward-Based learning drives rapid sensory signals in medial prefrontal cortex and dorsal Hippocampus necessary for Goal-Directed behavior</article-title><source>Neuron</source><volume>97</volume><fpage>83</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.031</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname> <given-names>DH</given-names></name><name><surname>Mohajerani</surname> <given-names>MH</given-names></name><name><surname>Ledue</surname> <given-names>J</given-names></name><name><surname>Boyd</surname> <given-names>J</given-names></name><name><surname>Chen</surname> <given-names>S</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>In vivo Large-Scale cortical mapping using Channelrhodopsin-2 stimulation in transgenic mice reveals asymmetric and reciprocal relationships between cortical Areas</article-title><source>Frontiers in Neural Circuits</source><volume>6</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2012.00011</pub-id><pub-id pub-id-type="pmid">22435052</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>Y</given-names></name><name><surname>Shaik</surname> <given-names>MA</given-names></name><name><surname>Kozberg</surname> <given-names>MG</given-names></name><name><surname>Kim</surname> <given-names>SH</given-names></name><name><surname>Portes</surname> <given-names>JP</given-names></name><name><surname>Timerman</surname> <given-names>D</given-names></name><name><surname>Hillman</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Resting-state hemodynamics are spatiotemporally coupled to synchronized and symmetric neural activity in excitatory neurons</article-title><source>PNAS</source><volume>113</volume><fpage>E8463</fpage><lpage>E8471</lpage><pub-id pub-id-type="doi">10.1073/pnas.1525369113</pub-id><pub-id pub-id-type="pmid">27974609</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>Y</given-names></name><name><surname>Shaik</surname> <given-names>MA</given-names></name><name><surname>Kim</surname> <given-names>SH</given-names></name><name><surname>Kozberg</surname> <given-names>MG</given-names></name><name><surname>Thibodeaux</surname> <given-names>DN</given-names></name><name><surname>Zhao</surname> <given-names>HT</given-names></name><name><surname>Yu</surname> <given-names>H</given-names></name><name><surname>Hillman</surname> <given-names>EMC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Correction to ‘Wide-field optical mapping of neural activity and brain haemodynamics: considerations and novel approaches’</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>0539</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0539</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madisen</surname> <given-names>L</given-names></name><name><surname>Garner</surname> <given-names>AR</given-names></name><name><surname>Shimaoka</surname> <given-names>D</given-names></name><name><surname>Chuong</surname> <given-names>AS</given-names></name><name><surname>Klapoetke</surname> <given-names>NC</given-names></name><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>van der Bourg</surname> <given-names>A</given-names></name><name><surname>Niino</surname> <given-names>Y</given-names></name><name><surname>Egolf</surname> <given-names>L</given-names></name><name><surname>Monetti</surname> <given-names>C</given-names></name><name><surname>Gu</surname> <given-names>H</given-names></name><name><surname>Mills</surname> <given-names>M</given-names></name><name><surname>Cheng</surname> <given-names>A</given-names></name><name><surname>Tasic</surname> <given-names>B</given-names></name><name><surname>Nguyen</surname> <given-names>TN</given-names></name><name><surname>Sunkin</surname> <given-names>SM</given-names></name><name><surname>Benucci</surname> <given-names>A</given-names></name><name><surname>Nagy</surname> <given-names>A</given-names></name><name><surname>Miyawaki</surname> <given-names>A</given-names></name><name><surname>Helmchen</surname> <given-names>F</given-names></name><name><surname>Empson</surname> <given-names>RM</given-names></name><name><surname>Knöpfel</surname> <given-names>T</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Transgenic mice for intersectional targeting of neural sensors and effectors with high specificity and performance</article-title><source>Neuron</source><volume>85</volume><fpage>942</fpage><lpage>958</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.02.022</pub-id><pub-id pub-id-type="pmid">25741722</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makino</surname> <given-names>H</given-names></name><name><surname>Ren</surname> <given-names>C</given-names></name><name><surname>Liu</surname> <given-names>H</given-names></name><name><surname>Kim</surname> <given-names>AN</given-names></name><name><surname>Kondapaneni</surname> <given-names>N</given-names></name><name><surname>Liu</surname> <given-names>X</given-names></name><name><surname>Kuzum</surname> <given-names>D</given-names></name><name><surname>Komiyama</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Transformation of Cortex-wide emergent properties during motor learning</article-title><source>Neuron</source><volume>94</volume><fpage>880</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.015</pub-id><pub-id pub-id-type="pmid">28521138</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitra</surname> <given-names>A</given-names></name><name><surname>Kraft</surname> <given-names>A</given-names></name><name><surname>Wright</surname> <given-names>P</given-names></name><name><surname>Acland</surname> <given-names>B</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Rosenthal</surname> <given-names>Z</given-names></name><name><surname>Czerniewski</surname> <given-names>L</given-names></name><name><surname>Bauer</surname> <given-names>A</given-names></name><name><surname>Snyder</surname> <given-names>L</given-names></name><name><surname>Culver</surname> <given-names>J</given-names></name><name><surname>Lee</surname> <given-names>JM</given-names></name><name><surname>Raichle</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spontaneous Infra-slow brain activity has unique spatiotemporal dynamics and laminar structure</article-title><source>Neuron</source><volume>98</volume><fpage>297</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.015</pub-id><pub-id pub-id-type="pmid">29606579</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohajerani</surname> <given-names>MH</given-names></name><name><surname>McVea</surname> <given-names>DA</given-names></name><name><surname>Fingas</surname> <given-names>M</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mirrored bilateral slow-wave cortical activity within local circuits revealed by fast bihemispheric voltage-sensitive dye imaging in anesthetized and awake mice</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>3745</fpage><lpage>3751</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6437-09.2010</pub-id><pub-id pub-id-type="pmid">20220008</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohajerani</surname> <given-names>MH</given-names></name><name><surname>Chan</surname> <given-names>AW</given-names></name><name><surname>Mohsenvand</surname> <given-names>M</given-names></name><name><surname>LeDue</surname> <given-names>J</given-names></name><name><surname>Liu</surname> <given-names>R</given-names></name><name><surname>McVea</surname> <given-names>DA</given-names></name><name><surname>Boyd</surname> <given-names>JD</given-names></name><name><surname>Wang</surname> <given-names>YT</given-names></name><name><surname>Reimers</surname> <given-names>M</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spontaneous cortical activity alternates between motifs defined by regional axonal projections</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1426</fpage><lpage>1435</lpage><pub-id pub-id-type="doi">10.1038/nn.3499</pub-id><pub-id pub-id-type="pmid">23974708</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>TH</given-names></name><name><surname>Boyd</surname> <given-names>JD</given-names></name><name><surname>Bolaños</surname> <given-names>F</given-names></name><name><surname>Vanni</surname> <given-names>MP</given-names></name><name><surname>Silasi</surname> <given-names>G</given-names></name><name><surname>Haupt</surname> <given-names>D</given-names></name><name><surname>LeDue</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>High-throughput automated home-cage mesoscopic functional imaging of mouse cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11611</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11611</pub-id><pub-id pub-id-type="pmid">27291514</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>AutoHeadFix</data-title><source>GitHub</source><version designator="3.0">3.0</version><ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/AutoHeadFix">https://github.com/elifesciences-publications/AutoHeadFix</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Musall</surname> <given-names>S</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Gluf</surname> <given-names>S</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Movement-related activity dominates cortex during sensory-guided decision making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/308288</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname> <given-names>S</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Juavinett</surname> <given-names>AL</given-names></name><name><surname>Gluf</surname> <given-names>S</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noorshams</surname> <given-names>O</given-names></name><name><surname>Boyd</surname> <given-names>JD</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Automating mouse weighing in group homecages with raspberry pi micro-computers</article-title><source>Journal of Neuroscience Methods</source><volume>285</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.05.002</pub-id><pub-id pub-id-type="pmid">28476590</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname> <given-names>SW</given-names></name><name><surname>Harris</surname> <given-names>JA</given-names></name><name><surname>Ng</surname> <given-names>L</given-names></name><name><surname>Winslow</surname> <given-names>B</given-names></name><name><surname>Cain</surname> <given-names>N</given-names></name><name><surname>Mihalas</surname> <given-names>S</given-names></name><name><surname>Wang</surname> <given-names>Q</given-names></name><name><surname>Lau</surname> <given-names>C</given-names></name><name><surname>Kuan</surname> <given-names>L</given-names></name><name><surname>Henry</surname> <given-names>AM</given-names></name><name><surname>Mortrud</surname> <given-names>MT</given-names></name><name><surname>Ouellette</surname> <given-names>B</given-names></name><name><surname>Nguyen</surname> <given-names>TN</given-names></name><name><surname>Sorensen</surname> <given-names>SA</given-names></name><name><surname>Slaughterbeck</surname> <given-names>CR</given-names></name><name><surname>Wakeman</surname> <given-names>W</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Feng</surname> <given-names>D</given-names></name><name><surname>Ho</surname> <given-names>A</given-names></name><name><surname>Nicholas</surname> <given-names>E</given-names></name><name><surname>Hirokawa</surname> <given-names>KE</given-names></name><name><surname>Bohn</surname> <given-names>P</given-names></name><name><surname>Joines</surname> <given-names>KM</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Hawrylycz</surname> <given-names>MJ</given-names></name><name><surname>Phillips</surname> <given-names>JW</given-names></name><name><surname>Hohmann</surname> <given-names>JG</given-names></name><name><surname>Wohnoutka</surname> <given-names>P</given-names></name><name><surname>Gerfen</surname> <given-names>CR</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Bernard</surname> <given-names>A</given-names></name><name><surname>Dang</surname> <given-names>C</given-names></name><name><surname>Jones</surname> <given-names>AR</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A mesoscale connectome of the mouse brain</article-title><source>Nature</source><volume>508</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1038/nature13186</pub-id><pub-id pub-id-type="pmid">24695228</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname> <given-names>L</given-names></name><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>DePasquale</surname> <given-names>B</given-names></name><name><surname>Thiberge</surname> <given-names>SY</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Task-Dependent changes in the Large-Scale dynamics and necessity of cortical regions</article-title><source>Neuron</source><volume>104</volume><fpage>810</fpage><lpage>824</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.08.025</pub-id><pub-id pub-id-type="pmid">31564591</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prsa</surname> <given-names>M</given-names></name><name><surname>Galiñanes</surname> <given-names>GL</given-names></name><name><surname>Huber</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Rapid integration of artificial sensory feedback during operant conditioning of motor cortex neurons</article-title><source>Neuron</source><volume>93</volume><fpage>929</fpage><lpage>939</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.023</pub-id><pub-id pub-id-type="pmid">28231470</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname> <given-names>L</given-names></name><name><surname>Plano</surname> <given-names>A</given-names></name><name><surname>Cobb</surname> <given-names>S</given-names></name><name><surname>Riedel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Long-term home cage activity scans reveal lowered exploratory behaviour in symptomatic female rett mice</article-title><source>Behavioural Brain Research</source><volume>250</volume><fpage>148</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2013.04.041</pub-id><pub-id pub-id-type="pmid">23643691</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname> <given-names>L</given-names></name><name><surname>Spruijt</surname> <given-names>B</given-names></name><name><surname>Riedel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Between and within laboratory reliability of mouse behaviour recorded in home-cage and open-field</article-title><source>Journal of Neuroscience Methods</source><volume>300</volume><fpage>10</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.11.019</pub-id><pub-id pub-id-type="pmid">29233658</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname> <given-names>L</given-names></name><name><surname>Riedel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparison of automated home-cage monitoring systems: emphasis on feeding behaviour, activity and spatial learning following pharmacological interventions</article-title><source>Journal of Neuroscience Methods</source><volume>234</volume><fpage>13</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.06.013</pub-id><pub-id pub-id-type="pmid">24949557</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salkoff</surname> <given-names>DB</given-names></name><name><surname>Zagha</surname> <given-names>E</given-names></name><name><surname>McCarthy</surname> <given-names>E</given-names></name><name><surname>McCormick</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Movement and performance explain widespread cortical activity in a visual detection task</article-title><source>Cerebral Cortex</source><volume>30</volume><fpage>421</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz206</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>BB</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cellular resolution functional imaging in behaving rats using voluntary head restraint</article-title><source>Neuron</source><volume>80</volume><fpage>371</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.002</pub-id><pub-id pub-id-type="pmid">24055015</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>BB</given-names></name><name><surname>Thiberge</surname> <given-names>SY</given-names></name><name><surname>Guo</surname> <given-names>C</given-names></name><name><surname>Tervo</surname> <given-names>DGR</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name><name><surname>Karpova</surname> <given-names>AY</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Imaging cortical dynamics in GCaMP transgenic rats with a Head-Mounted widefield macroscope</article-title><source>Neuron</source><volume>100</volume><fpage>1045</fpage><lpage>1058</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.050</pub-id><pub-id pub-id-type="pmid">30482694</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shimaoka</surname> <given-names>D</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The impact of bilateral ongoing activity on evoked responses in mouse cortex</article-title><source>eLife</source><volume>8</volume><elocation-id>e43533</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43533</pub-id><pub-id pub-id-type="pmid">31038456</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silasi</surname> <given-names>G</given-names></name><name><surname>Boyd</surname> <given-names>JD</given-names></name><name><surname>Ledue</surname> <given-names>J</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Improved methods for chronic light-based motor mapping in mice: automated movement tracking with accelerometers, and chronic EEG recording in a bilateral thin-skull preparation</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>123</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00123</pub-id><pub-id pub-id-type="pmid">23966910</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silasi</surname> <given-names>G</given-names></name><name><surname>Xiao</surname> <given-names>D</given-names></name><name><surname>Vanni</surname> <given-names>MP</given-names></name><name><surname>Chen</surname> <given-names>AC</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Intact skull chronic windows for mesoscopic wide-field imaging in awake mice</article-title><source>Journal of Neuroscience Methods</source><volume>267</volume><fpage>141</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.04.012</pub-id><pub-id pub-id-type="pmid">27102043</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silasi</surname> <given-names>G</given-names></name><name><surname>Boyd</surname> <given-names>JD</given-names></name><name><surname>Bolanos</surname> <given-names>F</given-names></name><name><surname>LeDue</surname> <given-names>JM</given-names></name><name><surname>Scott</surname> <given-names>SH</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Individualized tracking of self-directed motor learning in group-housed mice performing a skilled lever positioning task in the home cage</article-title><source>Journal of Neurophysiology</source><volume>119</volume><fpage>337</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1152/jn.00115.2017</pub-id><pub-id pub-id-type="pmid">29070625</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sofroniew</surname> <given-names>NJ</given-names></name><name><surname>Flickinger</surname> <given-names>D</given-names></name><name><surname>King</surname> <given-names>J</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</article-title><source>eLife</source><volume>5</volume><elocation-id>e14472</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14472</pub-id><pub-id pub-id-type="pmid">27300105</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorge</surname> <given-names>RE</given-names></name><name><surname>Martin</surname> <given-names>LJ</given-names></name><name><surname>Isbester</surname> <given-names>KA</given-names></name><name><surname>Sotocinal</surname> <given-names>SG</given-names></name><name><surname>Rosen</surname> <given-names>S</given-names></name><name><surname>Tuttle</surname> <given-names>AH</given-names></name><name><surname>Wieskopf</surname> <given-names>JS</given-names></name><name><surname>Acland</surname> <given-names>EL</given-names></name><name><surname>Dokova</surname> <given-names>A</given-names></name><name><surname>Kadoura</surname> <given-names>B</given-names></name><name><surname>Leger</surname> <given-names>P</given-names></name><name><surname>Mapplebeck</surname> <given-names>JC</given-names></name><name><surname>McPhail</surname> <given-names>M</given-names></name><name><surname>Delaney</surname> <given-names>A</given-names></name><name><surname>Wigerblad</surname> <given-names>G</given-names></name><name><surname>Schumann</surname> <given-names>AP</given-names></name><name><surname>Quinn</surname> <given-names>T</given-names></name><name><surname>Frasnelli</surname> <given-names>J</given-names></name><name><surname>Svensson</surname> <given-names>CI</given-names></name><name><surname>Sternberg</surname> <given-names>WF</given-names></name><name><surname>Mogil</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Olfactory exposure to males, including men, causes stress and related analgesia in rodents</article-title><source>Nature Methods</source><volume>11</volume><fpage>629</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2935</pub-id><pub-id pub-id-type="pmid">24776635</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Zatka-Haas</surname> <given-names>P</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distributed coding of choice, action and engagement across the mouse brain</article-title><source>Nature</source><volume>576</volume><fpage>266</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1787-x</pub-id><pub-id pub-id-type="pmid">31776518</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname> <given-names>C</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Steinmetz</surname> <given-names>N</given-names></name><name><surname>Reddy</surname> <given-names>CB</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>eaav7893</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanni</surname> <given-names>MP</given-names></name><name><surname>Chan</surname> <given-names>AW</given-names></name><name><surname>Balbi</surname> <given-names>M</given-names></name><name><surname>Silasi</surname> <given-names>G</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mesoscale mapping of mouse cortex reveals Frequency-Dependent cycling between distinct macroscale functional modules</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7513</fpage><lpage>7533</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3560-16.2017</pub-id><pub-id pub-id-type="pmid">28674167</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanni</surname> <given-names>MP</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Mesoscale transcranial spontaneous activity mapping in GCaMP3 transgenic mice reveals extensive reciprocal connections between Areas of somatomotor cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>15931</fpage><lpage>15946</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1818-14.2014</pub-id><pub-id pub-id-type="pmid">25429135</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wekselblatt</surname> <given-names>JB</given-names></name><name><surname>Flister</surname> <given-names>ED</given-names></name><name><surname>Piscopo</surname> <given-names>DM</given-names></name><name><surname>Niell</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Large-scale imaging of cortical dynamics during sensory perception and behavior</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2852</fpage><lpage>2866</lpage><pub-id pub-id-type="doi">10.1152/jn.01056.2015</pub-id><pub-id pub-id-type="pmid">26912600</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodard</surname> <given-names>CL</given-names></name><name><surname>Bolaños</surname> <given-names>F</given-names></name><name><surname>Boyd</surname> <given-names>JD</given-names></name><name><surname>Silasi</surname> <given-names>G</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name><name><surname>Raymond</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An automated Home-Cage system to assess learning and performance of a skilled motor task in a mouse model of Huntington's Disease</article-title><source>Eneuro</source><volume>4</volume><elocation-id>ENEURO.0141-17.2017</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0141-17.2017</pub-id><pub-id pub-id-type="pmid">28929129</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname> <given-names>D</given-names></name><name><surname>Vanni</surname> <given-names>MP</given-names></name><name><surname>Mitelut</surname> <given-names>CC</given-names></name><name><surname>Chan</surname> <given-names>AW</given-names></name><name><surname>LeDue</surname> <given-names>JM</given-names></name><name><surname>Xie</surname> <given-names>Y</given-names></name><name><surname>Chen</surname> <given-names>AC</given-names></name><name><surname>Swindale</surname> <given-names>NV</given-names></name><name><surname>Murphy</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping cortical mesoscopic networks of single spiking cortical or sub-cortical neurons</article-title><source>eLife</source><volume>6</volume><elocation-id>e19976</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19976</pub-id><pub-id pub-id-type="pmid">28160463</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname> <given-names>J</given-names></name><name><surname>Ng</surname> <given-names>L</given-names></name><name><surname>Williams</surname> <given-names>D</given-names></name><name><surname>Valley</surname> <given-names>M</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Garrett</surname> <given-names>M</given-names></name><name><surname>Waters</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An extended retinotopic map of mouse cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e18372</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18372</pub-id><pub-id pub-id-type="pmid">28059700</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.55964.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kleinfeld</surname><given-names>David</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, San Diego</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Murphy and co-workers have advanced the field of high throughput imaging coupled with behavior to a new level of automation – demonstrating around 2,000 self-initiated imaging sessions per day per mouse for all mice in a colony- that incorporates an online SQL database to segue with the ambitions of a multitude of community-wide efforts in data sharing. This work significantly extends the 2013 pioneering effort of the Brody and Tank collaboration, as well as prior work by the Benucci, Goldberg and Murphy laboratories, among others, to create effective, efficient, and open source tools for longitudinal studies of the neuronal basis of behavior.</p><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;Automated task training and longitudinal monitoring of mouse mesoscale cortical circuits using home cages&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: James Ackman (Reviewer #3).</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>All three reviewers of your &quot;Tools and Resources&quot; article, as well as the Reviewing Editor, are serious players in the field of cortical dynamics and imaging from awake animals. They and I recognize the continued impact of your work on automated behavior and imaging with modestly priced equipment to promote high throughput experiments and enable adventurous queries. Yet the content in the current paper, which includes technical advances and demonstration imaging data, is judged to be too incremental to warrant publication. The claims will gain in impact if the increase in throughput is used to solve a narrow but nontrivial scientific issue, or make a novel kind of observation, as opposed to making a solely demonstration measurement.</p><p>We kept in mind the criteria for the Tools and Resources category in reaching this decision. This category highlights tools or resources that are especially important for their respective fields and have the potential to accelerate discovery. Tools and Resources articles do not have to report major new biological insights or mechanisms, but it must be clear that they will enable such advances to take place. The concensus was that, while there are many laudable aspects to your approach, the current approach is not a sufficiently large advance over the previous approach to warrant publication in <italic>eLife</italic>.</p><p>We would welcome a new research article that involves an application of this new tool. The Reviewing Editor provided some suggestions, but of course, we recognize that you are already engaged in a vigorous research program that might encompass these or serve as an alternative. I pass them along for your consideration.</p><p>1) An expanded imaging study during the Go/NoGo task, i.e., the use of many mice to establish cortical patterns instead of the one mouse as shown in Figures 8 and 9.</p><p>2) Activation of the orofacial regions (forepaw, jaw, tongue, vibrissa) in motor and sensory cortices are certainly of interest.</p><p>3) An imaging study from multi-animals study that looks at potential gender differences, with reasonable statistical bounds and controls, could fulfil this role.</p><p>4) Other extensions, e.g., plasticity of cortical responses during learning, may be more feasible.</p><p><italic>Reviewer #1:</italic></p><p>In this &quot;Tools and Resources&quot; article, the authors report on an improved methodology for doing widefield imaging in the home cage. The opportunity to measure behavior and neural activity is important, and the authors highlight reasons such as reduced stress for the animal, and removal of experimenter-to-experimenter variability. Further, the chance for automated fixation in conjunction with widefield imaging is exciting, as widefield imaging is turning out to be an informative and high-throughput way to measure neural activity during behavior.</p><p>The method proposed here is an improvement over a previous automated head-fixing apparatus that the authors reported a few years ago. Here, the mice are willing to have daily fixation durations that are much longer. In about half of the mice, the observed 28+/-17 headfixations/day (18+/-13 mins/day).</p><p>The paper has a lot of strengths. It includes data from 44 mice and a total of &gt;29K task related videos. The authors also created a relational database to allow analysis pipelining. This information, along with a lot of other useful information, such as the drawings for the head straightener that prevents the mouse from going in sideways, are all provided. There are also a lot of nice touches in the system, such as the text messaging system that alerts the experimenter if animals are in the tube too long; this is thoughtful from the point of view of animal welfare.</p><p>There were also a number of observations based on this large-scale approach that were interesting. These include the linked behaviors between mice (Figure 4A, B) and the fact that mice had higher success rates while headfixed although they preferred to be head free (Figure 5C). The fact that performance was similar over the 24-hour cycle was also interesting.</p><p>Finally, to deal with hemodynamic contamination of the widefield data, the authors captured 440 nm light in addition to the GCaMP signal. They then subtracted one from the other. The authors acknowledged the really beautiful alternative methods for this developed by the Hillman lab, but justified their use of a simpler method based on some constraints of the device, which I thought was fine. Other papers in the literature have entirely omitted hemodynamic correction so I appreciated the efforts to do this.</p><p>First, I was concerned that the authors didn't make sufficiently clear how this work is an advance on the Aoki, Benucci et al. approach that was published in 2017. The text states that, &quot;While an advance for training, this work did not longitudinally gather brain imaging data in an autonomous manner, nor were the systems of a footprint or cost appropriate for running at scale.&quot; But in the Aoki paper, 2-photon data was collected for 21 days (Figure 5D). The measurements were not taken in the home cage; if the authors feel that is a critical difference, they should state why. I also wasn't sure how to compare the cost difference with that approach and the one presented here. The Aoki apparatus does appear to be commercially available (http://ohara-time.co.jp/wordpress/wp-content/uploads/SelfHead-restraintOperantTestSystem_Pamphlet2017.pdf), but I wasn't sure of its price or the total price of the setup in the current paper so it was hard to compare.</p><p>Second, the authors included no female mice in their study. There seems to be no reason that a cage of all female mice might be tested as well. Inclusion of both sexes in a study is now required by many funding agencies, and is good scientific practice because it ensures that scientific conclusions are not made that only apply to one sex and not the other.</p><p>Third, I wasn't sure whether the total fixation time of the mice provided sufficient time for measurement of behavior and neural activity. It seemed that the total fixation time wasn't very long – on order of about 18±5 minutes/day (subsection “Improvements to the cage hardware”) for the good performers. It would be helpful to see the distribution of total daily head-fix times for all mice, as well as the distribution of within-day times, at least for some of the good mice. It would also be helpful to see the distribution of total daily completed trials. The Bollu paper (which admittedly isn't a totally fair comparison) reported &gt;2000 trials/day and the Aoki paper reported ~1000 trials/day. The reason I bring this up is that widefield measurements can be very noisy and so high trial counts are extremely helpful. This is especially true given that, as the authors point out, the signals reflect movements, the timing and magnitude of which vary a lot from trial-to-trial. If the approach in this paper is to be useful for sensory or cognitive tasks, large trial counts are needed.</p><p>A related point, I couldn't quite understand the sentence comparing good and bad performers (see the aforementioned subsection). It stated that bad performers had less than 20 min of head-fixed time while good performers had while &quot;good performers averaged 15.8+-19.9 h.&quot; I think h can't mean hours since the animals didn't headfix 16 hours/day. But it can't mean minutes either, because that would imply that the good performers were fixating less than the poor performers.</p><p>Finally, I found that the behavioral/widefield data included at the end (Figure 8 and the section titled, &quot;Task training error analysis examples&quot;) was not very useful. The problem is that it is hard to conclude very much from such a small sample size. There is only 1 animal shown in Figure 8 and so the comparison between go and no-go trials (Figure 8B vs. Figure 8C) is not that informative. Further, the data didn't really add that much to the paper. The reason is that this paper is meant to be a tool and a resource. It does that well. The scientific observations in Figure 8 weren't really in the service of that goal. It might be better to include that (potentially interesting) data in a different paper with a larger cohort of subjects and a lot more analyses.</p><p><italic>Reviewer #2:</italic></p><p>Achieving high-throughput behavioral training and subsequent automated task administration with physiology is a significant aim for the field of systems neuroscience. The Murphy lab and others have made notable contributions to this aim, which are described in the Introduction to this manuscript. The Murphy group, specifically, has previously reported automated methods for home cage behavioral training and for head-fixation with mesoscopic calcium imaging (in the absence of a specific behavioral paradigm). Here, they describe improvements on their previous head-fixation and imaging method and integration with behavioral training and task performance.</p><p>Small improvements in the design of the head-fixation system and refinement of their training protocol yield longer and more frequent head-fixations. Using their updated system, they successfully train 21/44 mice to undergo head-fixation to obtain water rewards. While head-fixed, the mice are trained to perform a detection task with varying degrees of success. Further, 10 mice progressed to a go/no-go task, but only 5 of those mice were able to perform the task (as assessed by a relatively lenient d' criterion), and there is no detailed presentation of this behavioral data in the manuscript. The mesoscopic imaging data presented reflects examples from a few selected mice without any analysis of results across mice, and the major benefits of the system (longitudinal mesoscopic imaging across training) are completely unexplored.</p><p>The challenge with this manuscript is that it does not present a compelling methodological result, nor does it describe a compelling experimental finding (it's neither fish nor fowl). The methodology is very promising and the authors should be applauded for the impressive level of detail with which they describe the construction of their system, however the methodology as presented appears to be an incremental improvement on the Murphy lab's previously published results (Murphy et al., 2016), and it's very difficult to tell how much of an improvement it actually represents. The first sentence of the Abstract notes a '4X improved automated methodology', but 4X of what? On the other hand, if the manuscript were focused on an experimental finding or data set, it falls short because of the rather incomplete data (one mouse in many experimental groups, little group level or longitudinal analysis), and further the somewhat opaque manner in which data are presented make it difficult to assess the results and compare to previous work by this group and others.</p><p>1) The criterions employed, if any, to move mice between training stages are not described, hampering the reproducibility of the study.</p><p>2) It is unclear how the length of individual head-fixations is determined. Can mice exit trials at any time? Further, there are no behavioral measures (other than repeated, &quot;clustered&quot; head-fixation) that indicate whether mice are comfortable in the head-fixation system. A lack of comfort could underlie the poor performance of many of the mice in the detection task.</p><p>3) The study pools data from several iterations of the home cage system with different conditions across the cages. Further, cage 3 experienced an unexplained outage. This makes the presentation of data in Figure 3 confusing and difficult to interpret. Clarity would be greatly improved by reporting results for a group of cages with consistent and optimal conditions. At the very least, data should be separated clearly by mouse to make it easier to assess changes in performance. Further, the 23 mice that perform well in training should be presented separately from those that did not progress. Additionally, given that the training periods were of variable length, it is not possible to assess the data (as currently presented) relative to the different training milestones.</p><p>4) The inclusion of data from non-head-fixed trials detracts from the manuscript. Given that the system is designed for imaging during behavior (which necessitates head-fixation), only data for head-fixed trials should be presented. There is no comparison of behavioral or imaging data collected with automated (infrequent, short-bout) head-fixation and extended manual head-fixation for comparison.</p><p>5) Group-level and longitudinal analysis of behavioral data from the detection task (which should have sufficient N), would strengthen the manuscript. The inclusion of limited go/no-go behavioral data detracts from the manuscript, as it does not appear that the current training regimen has been optimized for performance of this task.</p><p>6) In section 1, the authors claim that use of a Raspberry Pi camera module is not inferior, but they do not present any data to support this claim.</p><p>7) The mesoscopic imaging presented is sufficient to demonstrate that imaging can be performed in their home cage system. However, that was already established by their previously published manuscript (Murphy et al., 2016). At the very least, group level or longitudinal analysis should be performed to facilitate comparison with results published by other groups with conventional imaging paradigms.</p><p><italic>Reviewer #3:</italic></p><p>The authors improved upon a home cage system they reported on previously in Murphy et al., 2016. Docking system was improved for greater mouse participation, with a more effective training period. Additional monitoring of animal's behavioral state was added. Functional data is provided for a go/no-go licking task.</p><p>All materials are provided, along with schematics and acquisition code, and are a benefit to the greater research community. Automated longitudinal population studies are important for furthering our understanding of the neural basis of behavior.</p><p>1) Much of this work has already been published (Murphy et al., 2016). Details provided here are improvements upon the original methods, rather than new findings or techniques.</p><p>2) Home cage system is described to be optimized for capturing long-term functional changes in cortex, yet the data showed no functional changes after training. Described results did not elucidate benefits of using this system.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.55964.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…] First, I was concerned that the authors didn't make sufficiently clear how this work is an advance on the Aoki, Benucci et al. approach that was published in 2017. The text states that, &quot;While an advance for training, this work did not longitudinally gather brain imaging data in an autonomous manner, nor were the systems of a footprint or cost appropriate for running at scale.&quot; But in the Aoki paper, 2-photon data was collected for 21 days (Figure 5D). The measurements were not taken in the home cage; if the authors feel that is a critical difference, they should state why. I also wasn't sure how to compare the cost difference with that approach and the one presented here. The Aoki apparatus does appear to be commercially available (http://ohara-time.co.jp/wordpress/wp-content/uploads/SelfHead-restraintOperantTestSystem_Pamphlet2017.pdf), but I wasn't sure of its price or the total price of the setup in the current paper so it was hard to compare.</p></disp-quote><p>We have revised the statement around the Aoki paper and no longer mention cost as an issue as our system is open source and all parts are listed. “Automation has been extended to complex headfixed visual tasks with a relatively good success rate in mice (Aoki et al., 2017) and headfixed rats (Scott et al., 2013). […] Such supervised imaging following home cage training of individual animals will permit high-resolution microscopy in the context of extended automated training (Aoki et al., 2017).” The device sold by Ohara-Time is $32,000 USD/cage based a quote I just received, typically only 2 mice train in one of these cages at a time (not up to 8).</p><p>Automation of behavioral and imaging experiments is of keen interest to many in the field and has distinct advantages from an animal welfare standpoint as it may reduce variability leading to the use of fewer animals. We have produced a paper which describes an improved system incorporating many new features (over published work): including an online SQL database, lick detection, auto weight measurement, triggered behavioral cameras, and integration of mesoscale imaging during behavioral task. These features were not present in the original version. Furthermore, published work by others does not support multiple animals within the same cage identified by RFID. There is also little published data on headfixation across mice statistics (now at n=52 mice for our work) in the previous Aoki et al., 2017 work, only that it had been applied, but no daily headfixing records or stats on uptake for specific mice.</p><disp-quote content-type="editor-comment"><p>Second, the authors included no female mice in their study. There seems to be no reason that a cage of all female mice might be tested as well. Inclusion of both sexes in a study is now required by many funding agencies, and is good scientific practice because it ensures that scientific conclusions are not made that only apply to one sex and not the other.</p></disp-quote><p>We now include a cohort of 8 female mice that are presented as Figure 3—figure supplement 1. These mice were successfully trained in the lab during daily 8 h unsupervised sessions (rather than 24/7 animal facility training). Data from these mice are now in group mesoscale GCAMP analysis in figures 9 and 10.</p><p>From Materials and methods, “Step 1. Acclimate group-housed male (or female) mice to large number housing; note the bulk of the data we report here uses male mice, it is expected that females would train in a similar manner as our previous work used all females (Murphy et al., 2016) and training of a small cohort of female mice yielded similar levels of head-fixed training as the males (Figure 3—figure supplement 1).” In total we show data from a total of 52 mice that went through this procedure and include the SQL database online with all behavioral data available. Female mice show similar levels of engagement.</p><disp-quote content-type="editor-comment"><p>Third, I wasn't sure whether the total fixation time of the mice provided sufficient time for measurement of behavior and neural activity. It seemed that the total fixation time wasn't very long – on order of about 18±5 minutes/day (subsection “Improvements to the cage hardware”) for the good performers. It would be helpful to see the distribution of total daily head-fix times for all mice, as well as the distribution of within-day times, at least for some of the good mice. It would also be helpful to see the distribution of total daily completed trials. The Bollu paper (which admittedly isn't a totally fair comparison) reported &gt;2000 trials/day and the Aoki paper reported ~1000 trials/day. The reason I bring this up is that widefield measurements can be very noisy and so high trial counts are extremely helpful. This is especially true given that, as the authors point out, the signals reflect movements, the timing and magnitude of which vary a lot from trial-to-trial. If the approach in this paper is to be useful for sensory or cognitive tasks, large trial counts are needed.</p><p>A related point, I couldn't quite understand the sentence comparing good and bad performers (see the aforementioned subsection). It stated that bad performers had less than 20 min of head-fixed time while good performers had while &quot;good performers averaged 15.8+-19.9 h.&quot; I think h can't mean hours since the animals didn't head fix 16 hours/day. But it can't mean minutes either, because that would imply that the good performers were fixating less than the poor performers.</p></disp-quote><p>We now provide the successful trial number for all mice in Table 1 for the last 5</p><p>days of training. Typically, 5-6 trials are done in a 40 sec session of headfixation so on average mice were presented with about 100 trials/day. The average trials complete per day is now added in a new column in Table 1. We also have added discussion of trial number, see below.</p><p>While we appreciate the elegant design of the Aoki et al., 2017, there is only data shown on n=2 mice in that paper (there is mention that 8 of 12 mice learned the visual task) and no detailed presentation of trial number data for individual mice could be found.</p><p>We have also clarified the statement about total time of headfixation. The number in hours was the summed time headfixed over all sessions. “In the current cage the cutoff for a poor performer was &lt;20 min of total headfixation time, while good performers averaged 15.8+-19.9 h (summed time over all daily sessions).”</p><p>We have added the following discussion of headfixed trial number “While we have had success automating headfixing and training, it is clear that some mice are not interested in the system as headfixation is voluntary. […] Furthermore, in our case by combining the task with mesoscale imaging each trial needs to have at least 3 s of data collected before and after the cue given the relatively slow GCaMP kinetics making these trials longer than some motor tasks that were less than 2 s (Silasi et al., 2018) or evaluated reaching trajectories (Bollu et al., 2019)”</p><disp-quote content-type="editor-comment"><p>Finally, I found that the behavioral/widefield data included at the end (Figure 8 and the section titled, &quot;Task training error analysis examples&quot;) was not very useful. The problem is that it is hard to conclude very much from such a small sample size. There is only 1 animal shown in Figure 8 and so the comparison between go and no-go trials (Figure 8B vs. Figure 8C) is not that informative. Further, the data didn't really add that much to the paper. The reason is that this paper is meant to be a tool and a resource. It does that well. The scientific observations in Figure 8 weren't really in the service of that goal. It might be better to include that (potentially interesting) data in a different paper with a larger cohort of subjects and a lot more analyses.</p></disp-quote><p>There seems to be conflicting responses from the editor and the reviewers since we</p><p>understand that the paper was likely rejected as there was not clear group data showing a finding that was supported by mesoscale imaging. The intention of the paper was to: introduce a cage with additional features, detailed instructions on how to build it, describe the training of animals, and show example data to prove that it works and can be used to derive some reported task-dependent activity. We have removed Figure 9 to Figure 8—figure supplement 1 as this was showing a single mouse at 2 training time points and have now add 2 new figures with group data from 13 mice total for the Go detection task.</p><p>The new group data figures show that different mice have unique regional patterns of cortical activation during this task and show strong correlations between movements and mesoscale GCAMP signals (Figure 9, subsection “Go and No-go task headfixed and non-headfixed task acquisition”). Also, consistent with recent data is the finding that individual mice have different strategies for task completion. This work also consistent with findings from the Helmchen lab (Gilad et al., 2018). In total this new group analysis of functional imaging data suggests that our home cage is a well-characterized system that can produce results linking mesoscale networks to behavior. The mesoscale GCAMP signals are also used to predict single trial-outcome using a model-based regression approach (Figure 10). Furthermore, the work provides information on the relative contribution of different cortical areas to the predictive value in the regression model extending very recent findings from multiple labs to the context of the home cage (Figure 10). We have also provided group data on the relationship between body movements and mesoscale brain activity. Here, consistent with other findings, there is a strong correlation between movement and mesoscale activity.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…] The challenge with this manuscript is that it does not present a compelling methodological result, nor does it describe a compelling experimental finding (it's neither fish nor fowl). The methodology is very promising and the authors should be applauded for the impressive level of detail with which they describe the construction of their system, however the methodology as presented appears to be an incremental improvement on the Murphy lab's previously published results (Murphy et al., 2016), and it's very difficult to tell how much of an improvement it actually represents. The first sentence of the Abstract notes a '4X improved automated methodology', but 4X of what? On the other hand, if the manuscript were focused on an experimental finding or data set, it falls short because of the rather incomplete data (one mouse in many experimental groups, little group level or longitudinal analysis), and further the somewhat opaque manner in which data are presented make it difficult to assess the results and compare to previous work by this group and others.</p></disp-quote><p>We have removed the statement of a 4x improvement in headfixation from the Abstract and now present this argument in the Results only.</p><p>“The time headfixed for these 23 good performers averaged 18+-13 min/day (Table 1, behavioral data from all mice is available in an online database, see Materials and methods). […] Assuming a session length of ~40 sec, which was the typical range in the previous system (Murphy et al., 2016), taking the top 50% of headfixing mice would result in an average of 6.5+-4 min of daily data acquisition or training per mouse using the older design.”</p><disp-quote content-type="editor-comment"><p>1) The criterions employed, if any, to move mice between training stages are not described, hampering the reproducibility of the study.</p></disp-quote><p>Admittedly, the system was still under development during the testing of these 44 (plus 8 female) mice so we did not have strict criteria for advancement between stages. In general, we waited for the majority of mice to acquire a particular level i.e. entries into the fixation tube and drinking before advancement. The first version of the software used for cages 1-5 and Murphy et al., 2016 required that all mice be subjected to the same conditions as a group i.e. headfixed or not, or advance to Go and No-Go. The training protocol for cages 4-6 is largely invariant. For the last cohort of female mice we did develop software so that each mouse can have individual training parameters that will allow for future use of automated advancement as we have done in another home cage task with an albeit less complex lever task (Silasi et al., 2018).</p><disp-quote content-type="editor-comment"><p>2) It is unclear how the length of individual head-fixations is determined. Can mice exit trials at any time? Further, there are no behavioral measures (other than repeated, &quot;clustered&quot; head-fixation) that indicate whether mice are comfortable in the head-fixation system. A lack of comfort could underlie the poor performance of many of the mice in the detection task.</p></disp-quote><p>There was no mechanism to automatically release mice using force sensing as all trials were short in duration. Trial length was set by the investigator and generally fixed in the range of no more than 45 sec of headfixation. The trial lengths did vary as we added extra time to allow mice to finish a trial once they started.</p><p>From the Discussion section “While we have had success automating headfixing and training, it is clear that some mice are not interested in the system as headfixation is voluntary. […]Furthermore, in our case by combining the task with mesoscale imaging each trial needs to have at least 3 s of data collected before and after the cue given the relatively slow GCAMP kinetics making these trials longer than some motor tasks that were less than 2 s (Silasi et al., 2018) or evaluated reaching trajectories (Bollu et al., 2019).”</p><disp-quote content-type="editor-comment"><p>3) The study pools data from several iterations of the home cage system with different conditions across the cages. Further, cage 3 experienced an unexplained outage. This makes the presentation of data in Figure 3 confusing and difficult to interpret. Clarity would be greatly improved by reporting results for a group of cages with consistent and optimal conditions. At the very least, data should be separated clearly by mouse to make it easier to assess changes in performance. Further, the 23 mice that perform well in training should be presented separately from those that did not progress. Additionally, given that the training periods were of variable length, it is not possible to assess the data (as currently presented) relative to the different training milestones.</p></disp-quote><p>Within Figure 3 we show data from all cages with a legend to indicate cage ID and tag number, we also include a database of all results. We also add a new cohort of female mice and do plot their cage separately in a new supplementary figure, Figure 3—figure supplement 1. We also stress that we include a full SQL database of all events allowing individual mice to be examined. All behavioral data (licking) and functional imaging data is from the 23 mice that performed well and an additional 5 female mice. In the revised group data Figure 9 we now show aggregate as well as individual mouse data and cages data relating body movements to GCAMP signals.</p><disp-quote content-type="editor-comment"><p>4) The inclusion of data from non-head-fixed trials detracts from the manuscript. Given that the system is designed for imaging during behavior (which necessitates head-fixation), only data for head-fixed trials should be presented. There is no comparison of behavioral or imaging data collected with automated (infrequent, short-bout) head-fixation and extended manual head-fixation for comparison.</p></disp-quote><p>We agree that it would be interesting to compare automatically headfixed and extended manual headfixation. However, this was not our intention. Any data regarding the performance in headfixed and non-headfixed cases was included as it might aid someone attempting this work in the future. We should stress that all analysis of mesoscale GCAMP dynamics and task performance is done using fully headfixed trials. We did not use non-headfixed trials in behavioral measures that are presented in figures (such as licking data in Figure 5), other than the comparison of task performance in non-headfixed states.</p><disp-quote content-type="editor-comment"><p>5) Group-level and longitudinal analysis of behavioral data from the detection task (which should have sufficient N), would strengthen the manuscript. The inclusion of limited go/no-go behavioral data detracts from the manuscript, as it does not appear that the current training regimen has been optimized for performance of this task.</p></disp-quote><p>We now add group level analysis in Figures 9 and 10 see these new Figures and the Results section This additional analysis is described below in the response to point 7.</p><p>“Brain-behavior correlation</p><p>Z-scored calcium activity from HL and ALM cortical areas were averaged across trials for each of 13 mice during successful GO trials (Figure 9A; we pooled data from all animals that had at least 20 trials[…] Scrambling brain-behavior pairs across mice resulted in a significant decrease in brain behavior correlation in HL and ALM regions, but not BC (p&lt;0.05, Wilcoxon signed rank test) suggesting tight coupling between the brain task GCaMP and these body parts.”</p><p>“Decoding of trial outcome</p><p>Z-scored ΔF/F0 montages, averaged across mice (454 single trials over 13 mice, for each trial outcome), show different cortical calcium dynamics associated with each Go-trial outcome: correct (code +2), lick late (code -2), or lick early (code -4) (Figure 10A). […] Similarly, running the model using only ALM fluorescence signals (+ALM) yielded significantly higher accuracy than when using only M2 (+M2) or V1 (+V1) signals (Figure 10E, violins, p&lt;0.001, repeated measures ANOVA with post-hoc Bonferroni correction).”</p><disp-quote content-type="editor-comment"><p>6) In section 1, the authors claim that use of a Raspberry Pi camera module is not inferior, but they do not present any data to support this claim.</p></disp-quote><p>We have removed the claim that the Raspberry Pi camera is not inferior and have written this in a more conservative manner.</p><p>From Results “wide field imaging is typically done with larger focal volume (~2-3 mm) to image curved brain surfaces and lower spatial resolution (pixels binned to ~ 32-49 μm) to sum more photons over a larger area (Lim et al., 2012). Thus, the intrinsically large depth of field and reduced resolution provided by the small lenses employed by compact cameras such as the Raspberry Pi Camera Module (Figure 1B, C) are not expected to degrade signals.”</p><disp-quote content-type="editor-comment"><p>7) The mesoscopic imaging presented is sufficient to demonstrate that imaging can be performed in their home cage system. However, that was already established by their previously published manuscript (Murphy et al., 2016). At the very least, group level or longitudinal analysis should be performed to facilitate comparison with results published by other groups with conventional imaging paradigms.</p></disp-quote><p>In the revision we include group analysis of functional imaging data from a total of 13 mice. For simplicity, we analyze group data from a simple go-detection task and were able to create a model that predicts behavioral trial outcome based on brain activity data (Figure 10). This model was used to examine temporal features and regions which exhibited predictive value in the behavioral task. This is new data and is now presented in two additional figures. We have also provided group data on the relationship between body movements and mesoscale brain activity (Figure 9). Here, consistent with other findings, there is a strong correlation between movement and mesoscale activity. Also, consistent with recent data is the finding that individual mice have different strategies for task completion. This work also consistent with findings from the Helmchen lab (Gilad et al., 2018). In total this new group analysis of functional imaging data suggests that our home cage is a well-characterized system that can produce results linking mesoscale.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The authors improved upon a home cage system they reported on previously in Murphy et al., 2016. Docking system was improved for greater mouse participation, with a more effective training period. Additional monitoring of animal's behavioral state was added. Functional data is provided for a go/no-go licking task.</p><p>All materials are provided, along with schematics and acquisition code, and are a benefit to the greater research community. Automated longitudinal population studies are important for furthering our understanding of the neural basis of behavior.</p><p>1) Much of this work has already been published (Murphy et al., 2016). Details provided here are improvements upon the original methods, rather than new findings or techniques.</p></disp-quote><p>We now better describe how this cage is an advance over what was previously published. In addition to having more features, including lick detection, task integration, behavioral camera triggering, and automatic weighing, the current system is much better documented and includes a fully illustrated guide to cage construction (Home cage Construction Guide). We feel this guide is essential for anyone planning to construct one of these systems. We’ve also better documented the open-source Python acquisition software and include an SQL database for tracking mouse parameters which can be done using a live cloud-based format. 2) The home cage is now used with longitudinal GCAMP signals from 13 different animals (See Figures 9 and 10). In this group data, we are able to make several conclusions (described in the response to the subsequent comment) regarding the GCAMP dynamics and behavioral outcome in 2 new added figures.</p><disp-quote content-type="editor-comment"><p>2) Home cage system is described to be optimized for capturing long-term functional changes in cortex, yet the data showed no functional changes after training. Described results did not elucidate benefits of using this system.</p></disp-quote><p>In the revision we include functional group analysis of functional imaging data from a total of 13 different mice, see subsection “Decoding of trial outcome”. For simplicity, we analyze group data from a simple go-detection task and were able to create a model that predicts behavioral trial outcome based on brain activity data. This model was used to examine temporal features and regions which exhibited predictive value in the behavioral task. This new analysis is now presented in two additional figures. We have also provided group data on the relationship between body movements and mesoscale brain activity. Here, consistent with other findings, there is a strong correlation between movement and mesoscale activity. Also, consistent with recent data is the finding that individual mice have different strategies for task completion. This work also consistent with findings from the Helmchen lab (Gilad et al., 2018). In total this new group analysis of functional imaging data suggests that our home cage is a well-characterized system that can produce results linking mesoscale networks to behavior.</p></body></sub-article></article>