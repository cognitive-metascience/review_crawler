<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">37124</article-id><article-id pub-id-type="doi">10.7554/eLife.37124</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Learning recurrent dynamics in spiking networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-109722"><name><surname>Kim</surname><given-names>Christopher M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1322-6207</contrib-id><email>chrismkkim@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-15344"><name><surname>Chow</surname><given-names>Carson C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1463-9553</contrib-id><email>carsonc@niddk.nih.gov</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Laboratory of Biological Modeling, National Institute of Diabetes and Digestive and Kidney Diseases</institution><institution>National Institutes of Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>09</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e37124</elocation-id><history><date date-type="received" iso-8601-date="2018-03-29"><day>29</day><month>03</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2018-09-14"><day>14</day><month>09</month><year>2018</year></date></history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-37124-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.37124.001</object-id><p>Spiking activity of neurons engaged in learning and performing a task show complex spatiotemporal dynamics. While the output of recurrent network models can learn to perform various tasks, the possible range of recurrent dynamics that emerge after learning remains unknown. Here we show that modifying the recurrent connectivity with a recursive least squares algorithm provides sufficient flexibility for synaptic and spiking rate dynamics of spiking networks to produce a wide range of spatiotemporal activity. We apply the training method to learn arbitrary firing patterns, stabilize irregular spiking activity in a network of excitatory and inhibitory neurons respecting Dale’s law, and reproduce the heterogeneous spiking rate patterns of cortical neurons engaged in motor planning and movement. We identify sufficient conditions for successful learning, characterize two types of learning errors, and assess the network capacity. Our findings show that synaptically-coupled recurrent spiking networks possess a vast computational capability that can support the diverse activity patterns in the brain.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>spiking network</kwd><kwd>recurrent dynamics</kwd><kwd>learning</kwd><kwd>universal dynamics</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000062</institution-id><institution>National Institute of Diabetes and Digestive and Kidney Diseases</institution></institution-wrap></funding-source><award-id>Intramural Research Program</award-id><principal-award-recipient><name><surname>Kim</surname><given-names>Christopher M</given-names></name><name><surname>Chow</surname><given-names>Carson C</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Modifying the recurrent connectivity of spiking networks provides sufficient flexibility to generate arbitrarily complex recurrent dynamics, suggesting that individual neurons in a recurrent network have the capability to support near universal dynamics.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neuronal populations exhibit diverse patterns of recurrent activity that can be highly irregular or well-structured when learning or performing a behavioral task (<xref ref-type="bibr" rid="bib15">Churchland and Shenoy, 2007</xref>; <xref ref-type="bibr" rid="bib16">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib26">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Pastalkova et al., 2008</xref>; <xref ref-type="bibr" rid="bib36">Li et al., 2015</xref>). An open questions whether learning-induced synaptic rewiring is sufficient to give rise to the wide range of spiking dynamics that encodes and processes information throughout the brain.</p><p>It has been shown that a network of recurrently connected neuron models can be trained to perform complex motor and cognitive tasks. In this approach, synaptic connections to a set of outputs are trained to generate a desired population-averaged signal, while the activity of individual neurons within the recurrent network emerges in a self-organized way that harnesses chaotic temporally irregular activity of a network of rate-based neurons (<xref ref-type="bibr" rid="bib55">Sompolinsky et al., 1988</xref>) that is made repeatable through direct feedback from the outputs or through training of the recurrent connections (<xref ref-type="bibr" rid="bib39">Maass et al., 2007</xref>; <xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>). The resulting irregular yet stable dynamics provides a rich reservoir from which complex patterns such as motor commands can be extracted by trained output neurons (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib11">Buonomano and Maass, 2009</xref>; <xref ref-type="bibr" rid="bib31">Jaeger and Haas, 2004</xref>), and theoretical studies have shown that the network outputs are able to perform universal computations (<xref ref-type="bibr" rid="bib39">Maass et al., 2007</xref>).</p><p>Here, we explore whether there is even a need for a set of output neurons. Instead, each unit in the recurrent network could be considered to be an output and learn target patterns directly while simultaneously serving as a reservoir. <xref ref-type="bibr" rid="bib33">Laje and Buonomano (2013)</xref> showed that individual rate units in a recurrent network can learn to stabilize innate chaotic trajectories that an untrained network naturally generates. The trained trajectories are then utilized to accomplish timing tasks by summing their activities with trained weights. <xref ref-type="bibr" rid="bib21">DePasquale et al. (2018)</xref> obtained a set of target trajectories from a target network driven externally by the desired network output. They showed that training individual units on such target trajectories and then adjusting the read-out weights yielded better performance than an untrained random recurrent network with a trained feedback loop (i.e. ‘traditional’ FORCE learning). <xref ref-type="bibr" rid="bib50">Rajan et al., 2016</xref> trained a small fraction of synaptic connections in a randomly connected rate network to produce sequential activity derived from cortical neurons engaged in decision making tasks.</p><p>Although these studies demonstrate that units within a rate-based network can learn recurrent dynamics defined by specific forms of target functions, the possible repertoire of the recurrent activity that a recurrent network can learn has not been extensively explored. Moreover, extending this idea to spiking networks, where neurons communicate with time dependent spike induced synapses, poses an additional challenge because it is difficult to coordinate the spiking dynamics of many neurons, especially, if spike times are variable as in a balanced network (<xref ref-type="bibr" rid="bib37">London et al., 2010</xref>). Some success has been achieved by training spiking networks directly with a feedback loop (<xref ref-type="bibr" rid="bib47">Nicola and Clopath, 2017</xref>) or using a rate-based network as an intermediate step (<xref ref-type="bibr" rid="bib20">DePasquale et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Thalmeier et al., 2016</xref>). A different top-down approach is to build networks that emit spikes optimally to correct the discrepancy between the actual and desired network outputs (<xref ref-type="bibr" rid="bib4">Boerlin et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Denève and Machens, 2016</xref>). This optimal coding strategy in a tightly balanced network can be learned with a local plasticity rule (<xref ref-type="bibr" rid="bib6">Brendel et al., 2017</xref>) and is able to generate arbitrary network output at the spike level (<xref ref-type="bibr" rid="bib5">Bourdoukan and Deneve, 2015</xref>; <xref ref-type="bibr" rid="bib19">Denève et al., 2017</xref>).</p><p>We show that a network of spiking neurons is capable of supporting arbitrarily complex coarse-grained recurrent dynamics provided the spatiotemporal patterns of the recurrent activity are diverse, the synaptic dynamics are fast, and the number of neurons in the network is large. We give a theoretical basis for how a network can learn and show various examples, which include stabilizing strong chaotic rate fluctuations in a network of excitatory and inhibitory neurons that respects Dale’s law and constructing a recurrent network that reproduces the spiking rate patterns of a large number of cortical neurons involved in motor planning and movement. Our study suggests that individual neurons in a recurrent network have the capability to support near universal dynamics.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Spiking networks can learn complex recurrent dynamics</title><p>We considered a network of <inline-formula><mml:math id="inf1"><mml:mi>N</mml:mi></mml:math></inline-formula> quadratic integrate-and-fire neurons that are recurrently connected with spike-activated synapses weighted by a connectivity matrix <inline-formula><mml:math id="inf2"><mml:mi>W</mml:mi></mml:math></inline-formula>. We show below that our results do not depend on the spiking mechanism. We focused on two measures of coarse-grained time-dependent neuron activity: (1) the synaptic drive <inline-formula><mml:math id="inf3"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> to neuron <inline-formula><mml:math id="inf4"><mml:mi>i</mml:mi></mml:math></inline-formula> which is given by the <inline-formula><mml:math id="inf5"><mml:mi>W</mml:mi></mml:math></inline-formula>-weighted sum of low-pass filtered incoming spike trains, and (2) the time-averaged spiking rate <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf7"><mml:mi>i</mml:mi></mml:math></inline-formula>. The goal was to find a weight matrix <inline-formula><mml:math id="inf8"><mml:mi>W</mml:mi></mml:math></inline-formula> that can autonomously generate desired recurrent target dynamics when the network of spiking neurons connected by <inline-formula><mml:math id="inf9"><mml:mi>W</mml:mi></mml:math></inline-formula> is stimulated briefly with an external stimulus (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The target dynamics were defined by a set of functions <inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> on a time interval <inline-formula><mml:math id="inf11"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>. Learning of the recurrent connectivity <inline-formula><mml:math id="inf12"><mml:mi>W</mml:mi></mml:math></inline-formula> was considered successful if <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf14"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> evoked by the stimulus matches the target functions <inline-formula><mml:math id="inf15"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> over the time interval <inline-formula><mml:math id="inf16"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> for all neurons <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>.</p><fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.37124.002</object-id><label>Figure 1.</label><caption><title>Synaptic drive and spiking rate of neurons in a recurrent network can learn complex patterns.</title><p>(<bold>a</bold>) Schematic of network training. Blue square represents the external stimulus that elicits the desired response. Black curves represent target output for each neuron. Red arrows represent recurrent connectivity that is trained to produce desired target patterns. (<bold>b</bold>) Synaptic drive of 10 sample neurons before, during and after training. Pre-training is followed by multiple training trials. An external stimulus (blue) is applied prior to training for 100 ms. Synaptic drive (black) is trained to follow the target (red). If the training is successful, the same external stimulus can elicit the desired response. Bottom shows the spike rater of 100 neurons. (<bold>c</bold>) Top, The Pearson correlation between the actual synaptic drive and the target output during training trials. Bottom, The matrix (Fresenius) norm of changes in recurrent connectivity normalized to initial connectivity during training. (<bold>d</bold>) Filtered spike train of 10 neurons before, during and after training. As in (<bold>b</bold>), external stimulus (blue) is applied immediately before training trials. Filtered spike train (black) learns to follow the target spiking rate (red) with large errors during the early trials. Applying the stimulus to a successfully trained network elicits the desired spiking rate patterns in every neuron. (<bold>e</bold>) Top, Same as in (<bold>c</bold>) but measures the correlation between filtered spike trains and target outputs. Bottom, Same as in (<bold>c</bold>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig1-v2"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37124.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Learning arbitrarily complex target patterns in a network of rate-based neurons.</title><p>The network dynamics obey <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The synaptic current <inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to every neuron in the network was trained to follow complex periodic functions <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where the initial phase <inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and frequencies <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> were selected randomly. The elements of initial connectivity matrix <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> were drawn from a Gaussian distribution with mean zero and standard deviation <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> was strong enough to induce chaotic dynamics; Network size <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula>, connection probability between neurons <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>, and time constant <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> ms. External input <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with constant random amplitude was applied to each neuron for 50 ms (blue) and was set to zero elsewhere. (<bold>a</bold>) Before training, the network is in chaotic regime and the synaptic current (black) of individual neurons fluctuates irregularly. (<bold>b</bold>) After learning to follow the target trajectories (red), the synaptic current tracks the target pattern closely in response to the external stimulus.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig1-figsupp1-v2"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37124.004</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Training a network that has no initial connections.</title><p>The coupling strength of the initial recurrent connectivity is zero, and, prior to training, no synaptic or spiking activity appears beyond the first few hundred milliseconds. (<bold>a</bold>) Training synaptic drive patterns using the RLS algorithm. Black curves show the actual synaptic drive of 10 neurons and red curves show the target outputs. Blue shows the 100 ms external stimulus. (<bold>b</bold>) Correlation between synaptic drive and target function (top) and the Frobenius norm of changes in recurrent connectivity normalized to initial connectivity during training (botom). (<bold>c and d</bold>) Same as in (<bold>a</bold>) and (<bold>b</bold>), but spiking rate patterns are trained.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig1-figsupp2-v2"/></fig></fig-group><p>Previous studies have shown that recurrently connected rate units can learn specific forms of activity patterns, such as chaotic trajectories that the initial network could already generate (<xref ref-type="bibr" rid="bib33">Laje and Buonomano, 2013</xref>), trajectories from a target network (<xref ref-type="bibr" rid="bib21">DePasquale et al., 2018</xref>), and sequential activity derived from imaging data (<xref ref-type="bibr" rid="bib50">Rajan et al., 2016</xref>). Our study expanded these results in two ways: first, we trained the recurrent dynamics of spiking networks, and, second, we showed that the repertoire of recurrent dynamics that can be encoded is vast. The primary goal of our paper was to investigate the computational capability of spiking networks to generate arbitrary recurrent dynamics, therefore we neither trained the network outputs (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib58">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Nicola and Clopath, 2017</xref>) nor constrained the target signals to those designed for performing specific computations (<xref ref-type="bibr" rid="bib21">DePasquale et al., 2018</xref>). We focused on training the recurrent activity as in the work of <xref ref-type="bibr" rid="bib33">Laje and Buonomano (2013)</xref> (without the read-outs) and <xref ref-type="bibr" rid="bib50">Rajan et al., 2016</xref>, and considered arbitrary target functions. To train the activity of individual neurons within a spiking network, we extended the Recursive Least Squares (RLS) algorithm developed by Laje and Buonomano in rate-based networks (<xref ref-type="bibr" rid="bib33">Laje and Buonomano, 2013</xref>). The algorithm was based on the FORCE algorithm (<xref ref-type="bibr" rid="bib27">Haykin, 1996</xref>; <xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>), originally developed to train the network outputs by minimizing a quadratic cost function between the activity measure and the target together with a quadratic regularization term (see Materials and methods, 'Training recurrent dynamics'). Example code that trains a network of quadratic integrate-and-fire neurons is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/chrismkkim/SpikeLearning">https://github.com/chrismkkim/SpikeLearning</ext-link> (<xref ref-type="bibr" rid="bib32">Kim and Chow, 2018</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/SpikeLearning">https://github.com/elifesciences-publications/SpikeLearning</ext-link>).</p><p>As a first example, we trained the network to produce synaptic drive patterns that matched a set of sine functions with random frequencies and the spiking rate to match the positive part of the same sine functions. The initial connectivity matrix had connection probability <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> and the coupling strength was drawn from a Normal distribution with mean <inline-formula><mml:math id="inf32"><mml:mn>0</mml:mn></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf33"><mml:mi>σ</mml:mi></mml:math></inline-formula>. Prior to training, the synaptic drive fluctuated irregularly, but as soon as the RLS algorithm was instantiated, the synaptic drives followed the target with small error; rapid changes in <inline-formula><mml:math id="inf34"><mml:mi>W</mml:mi></mml:math></inline-formula> quickly adjusted the recurrent dynamics towards the target (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>) (<xref ref-type="fig" rid="fig1">Figure 1b,c</xref>). As a result, the population spike trains exhibited reproducible patterns across training trials. A brief stimulus preceded each training session to reset the network to a specific state. If the training was successful, the trained response could be elicited whenever the same stimulus was applied regardless of the network state. We were able to train a network of rate-based neurons to learn arbitrarily complex target patterns using the same learning scheme (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>Training the spiking rate was more challenging than training the synaptic drive because small changes in recurrent connectivity did not immediately affect the spiking activity if the effect was below the spike-threshold. Therefore, the spike trains may not follow the desired spiking rate pattern during the early stage of training, and the population spike trains no longer appeared similar across training trials (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). This was also reflected in relatively small changes in recurrent connectivity and the substantially larger number of training runs required to produce desired spiking patterns (<xref ref-type="fig" rid="fig1">Figure 1e</xref>). However, by only applying the training when the total input to a neuron is suprathreshold, the spiking rate could be trained to reproduce the target patterns. The correlation between the actual filtered spike trains and the target spiking rate increased gradually as the training progressed.</p><p>Previous work that trained the network read-out had proposed that the initial recurrent network needed to be at the ‘edge of chaos’ to learn successfully (<xref ref-type="bibr" rid="bib3">Bertschinger and Natschläger, 2004</xref>; <xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib1">Abbott et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Thalmeier et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Nicola and Clopath, 2017</xref>). However, we found that the recurrent connectivity could learn to produce the desired recurrent dynamics regardless of the initial network dynamics and connectivity. Even when the initial network had no synaptic connections, the brief stimulus preceding the training session was sufficient to build a fully functioning recurrent connectivity that captured the target dynamics. The RLS algorithm could grow new synapses or tune existing ones as long as some of the neurons became active after the initial stimulus (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>Learning was not limited to one set of targets; the same network was able to learn multiple sets of targets. We trained the network to follow two independent sets of targets, where each target function was a sine function with random frequency. Every neuron in the network learned both activity patterns after training, and, when stimulated with the appropriate cue, the network recapitulated the specified trained pattern of recurrent dynamics, regardless of initial activity. The synaptic drive and the spiking rate were both able to learn multiple target patterns (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.37124.005</object-id><label>Figure 2.</label><caption><title>Learning multiple target patterns.</title><p>(<bold>a</bold>) The synaptic drive of neurons learns two different target outputs. Blue stimulus evokes the first set of target outputs (red) and the green stimulus evokes the second set of target outputs (red). (<bold>b</bold>) The spiking rate of individual neurons learns two different target outputs.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig2-v2"/></fig><sec id="s2-1-1"><title>Learning arbitrary patterns of activity</title><p>Next, we considered targets generated from various families of functions: complex periodic functions, chaotic trajectories, and Ornstein-Hollenbeck (OU) noise. We randomly selected <inline-formula><mml:math id="inf35"><mml:mi>N</mml:mi></mml:math></inline-formula> different target patterns from one of the families to create a set of heterogeneous targets, and trained the synaptic drive of a network consisting of <inline-formula><mml:math id="inf36"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons to learn the target dynamics. These examples demonstrated that recurrent activity patterns that a spiking network can generate is not limited to specific forms of patterns considered in previous studies (<xref ref-type="bibr" rid="bib33">Laje and Buonomano, 2013</xref>; <xref ref-type="bibr" rid="bib50">Rajan et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">DePasquale et al., 2018</xref>), but can be arbitrary functions. The successful learning suggested that single neurons embedded in a spiking network have the capability to perform universal computations.</p><p>As we will show more rigorously in the next section, we identified two sufficient conditions on the dynamical state and spatiotemporal structure of target dynamics that ensure a wide repertoire of recurrent dynamics can be learned. The first is a ‘quasi-static’ condition that stipulates that the dynamical time scale of target patterns must be slow enough compared to the synaptic time scale and average spiking rate. The second is a ‘heterogeneity’ condition that requires the spatiotemporal structure of target patterns to be diverse enough. The target patterns considered in <xref ref-type="fig" rid="fig3">Figure 3</xref> had slow temporal dynamics in comparison to the synaptic time constant (<inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> ms) and the patterns were selected randomly to promote diverse structure. After training each neuron’s synaptic drive to produce the respective target pattern, the synaptic drive of every neuron in the network followed its target.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.37124.006</object-id><label>Figure 3.</label><caption><title>Quasi-static and heterogeneous patterns can be learned.</title><p>Example target patterns include complex periodic functions (product of sines with random frequencies), chaotic rate units (obtained from a random network of rate units), and OU noise (obtained by low-pass filtering white noise with time constant 100 ms). (<bold>a</bold>) Target patterns (red) overlaid with actual synaptic drive (black) of a trained network. Quasi-static prediction (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) of synaptic drive (blue). (<bold>b</bold>) Spike trains of trained neurons elicited multiple trials, trial-averaged spiking rate calculated by the average number of spikes in 50 ms time bins (black), and predicted spiking rate (blue). (<bold>c</bold>) Performance of trained network as a function of the fraction of randomly selected targets. (<bold>d</bold>) Network response from a trained network after removing all the synaptic connections from 5%, 10% and 40% of randomly selected neurons in the network.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37124.007</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Learning target patterns with low-population spiking rate.</title><p>The synaptic drive of networks consisting of 500 neurons were trained to learn complex periodic functions <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where the initial phase <inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and frequencies <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> were selected randomly from [500 ms, 1000 ms]. (<bold>a</bold>) The amplitude <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, resulting in population spiking rate 2.8 Hz in trained window. (<bold>b</bold>) The amplitude <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, resulting in population spiking rate 1.5 Hz in trained window. (<bold>c</bold>) The amplitude <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, resulting in population spiking rate 0.01 Hz in trained window and learning fails.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig3-figsupp1-v2"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37124.008</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Learning recurrent dynamics with leaky integrate-and-fire and Izhikevich neuron models.</title><p>Synaptic drive of a network of spiking neurons were trained to follow 1000 ms long targets <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> were selected uniformly from the interval [500 ms, 1000 ms]. (<bold>a</bold>) Network consisted of <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> leaky integrate-and-fire neuron models, whose membrane potential obeys <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with a time constant <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>ms; the neuron spikes when <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> exceeds spike threshold <inline-formula><mml:math id="inf51"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> mV then <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is reset to <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>65</mml:mn></mml:mrow></mml:math></inline-formula> mV. Red curves show the target pattern and black curves show the voltage trace and synaptic drive of a trained network. (<bold>b</bold>) Spike rastergram of a trained leaky integrate-and-fire neuron network generating the synaptic drive patterns. (<bold>c</bold>) Network consisted of <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> Izhikevich neurons, whose dynamics are described by two equations <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>5</mml:mn><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>140</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>; the neuron spikes when <inline-formula><mml:math id="inf57"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> exceeds 30 mV, then <inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is reset to <inline-formula><mml:math id="inf59"><mml:mi>c</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is reset to <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula>. Neuron parameters <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mi>d</mml:mi></mml:math></inline-formula> were selected as in the original study (<xref ref-type="bibr" rid="bib29">Izhikevich, 2003</xref>) so that there were equal numbers of regular spiking, intrinsic bursting, chattering, fast spiking and low threshold spiking neurons. Synaptic current <inline-formula><mml:math id="inf64"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is modeled as in <xref ref-type="disp-formula" rid="equ6">Equations 6</xref> for all neuron models with synaptic decay time <inline-formula><mml:math id="inf65"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> ms. Red curves show the target patterns and black curves show the voltage trace and synaptic drive of a trained network. (<bold>d</bold>) Spike rastergram of a trained Izhikevich neuron network showing the trained response of different cell types.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig3-figsupp2-v2"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.37124.009</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Synaptic drive of a network of neurons is trained to learn an identical sine wave while external noise generated independently from OU process is injected to individual neurons.</title><p>The same external noise (gray curves) is applied repeatedly during and after training. (<bold>a</bold>)-(<bold>b</bold>) The amplitude of external noise is varied from (<bold>a</bold>) low, (<bold>b</bold>) medium to (<bold>c</bold>) high. The target sine wave is shown in red and the synaptic drive of neurons are shown in black. The raster plot in (<bold>c</bold>) shows the ensemble of spike trains of a successfully trained network with strong external noise.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig3-figsupp3-v2"/></fig></fig-group><p>To verify the quasi-static condition, we compared the actual to a quasi-static approximation of the spiking rate and synaptic drive. The spiking rates of neurons were approximated using the current-to-rate transfer function with time-dependent synaptic input, and the synaptic drive was approximated by a weighted sum of the presynaptic neurons’ spiking rates. We elicited the trained patterns over multiple trials starting at random initial conditions to calculate the trial-averaged spiking rates. The quasi-static approximations of the synaptic drive and spiking rate closely matched the actual synaptic drive (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) and trial-averaged spiking rates (<xref ref-type="fig" rid="fig3">Figure 3b</xref>).</p><p>To examine how the heterogeneity of target patterns may facilitate learning, we created sets of target patterns where the fraction of randomly generated targets was varied systematically. For non-random targets, we used the same target pattern repeatedly. Networks trained to learn target patterns with strong heterogeneity showed that a network is able to encode target patterns with high accuracy if there is a large fraction of random targets (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). Networks that were trained on too many repeated target patterns failed to learn. Beyond a certain fraction of random patterns, including additional patterns did not improve the performance, suggesting that the set of basis functions was over-complete. We probed the stability of over-complete networks under neuron loss by eliminating all the synaptic connections from a fraction of the neurons. A network was first trained to learn target outputs where all the patterns were selected randomly (i.e. fraction of random targets equals 1) tonsure that the target patterns form a set of redundant basis functions. Then, we elicited the trained patterns after removing a fraction of neurons from the network, which entails eliminating all the synaptic connections from the lost neurons. A trained network with 5% neuron loss was able to generate the trained patterns perfectly, 10% neuron loss resulted in a mild degradation of network response, and trained patterns completely disappeared after 40% neuron loss (<xref ref-type="fig" rid="fig3">Figure 3d</xref>).</p><p>The target dynamics considered in <xref ref-type="fig" rid="fig3">Figure 3</xref> had population spiking rates of 9.1 Hz (periodic), 7.2 Hz (chaotic) and 12.1 Hz (OU) within the training window. To examine how population activity may influence learning, we trained networks to learn target patterns whose average amplitude was reduced gradually across target sets. The networks were able to learn when the population spiking rate of the target dynamics was as low as 1.5 Hz. However, the performance deteriorated as the population spiking rate decreased further (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). To demonstrate that learning does not depend on the spiking mechanism, we trained the synaptic drive of spiking networks using different neuron models. A network of leaky integrate-and-fire neurons, as well as a network of Izhikevich neurons whose neuron parameters were tuned to have five different firing patterns, successfully learned complex synaptic drive patterns (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p></sec><sec id="s2-1-2"><title>Stabilizing rate fluctuations in a network respecting Dale’s law</title><p>A random network with balanced excitation and inhibition is a canonical model for a cortical circuit that produces asynchronous single unit activity (<xref ref-type="bibr" rid="bib55">Sompolinsky et al., 1988</xref>; <xref ref-type="bibr" rid="bib60">van Vreeswijk et al., 1996</xref>; <xref ref-type="bibr" rid="bib52">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib48">Ostojic, 2014</xref>; <xref ref-type="bibr" rid="bib54">Rosenbaum et al., 2017</xref>). The chaotic activity of balanced rate models (<xref ref-type="bibr" rid="bib55">Sompolinsky et al., 1988</xref>) has been harnessed to accomplish complex tasks by including a feedback loop (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>), stabilizing chaotic trajectories (<xref ref-type="bibr" rid="bib33">Laje and Buonomano, 2013</xref>) or introducing low-rank structure to the connectivity matrix (<xref ref-type="bibr" rid="bib41">Mastrogiuseppe and Ostojic, 2017</xref>). Balanced spiking networks have been shown to possess similar capabilities (<xref ref-type="bibr" rid="bib59">Thalmeier et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">DePasquale et al., 2016</xref>; <xref ref-type="bibr" rid="bib1">Abbott et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Nicola and Clopath, 2017</xref>; <xref ref-type="bibr" rid="bib18">Denève and Machens, 2016</xref>), but it is unknown if it is possible to stabilize the heterogeneous fluctuations of the spiking rate in the strong coupling regime (<xref ref-type="bibr" rid="bib48">Ostojic, 2014</xref>). Here, we extended the work of <xref ref-type="bibr" rid="bib33">Laje and Buonomano (2013)</xref> to spiking networks and showed that strongly fluctuating single neuron activities can be turned into dynamic attractors by adjusting the recurrent connectivity.</p><p>We considered a network of randomly connected excitatory and inhibitory neurons that respected Dale’s Law. Prior to training, the synaptic and spiking activity of individual neurons showed large variations across trials because small discrepancies in the initial network state led to rapid divergence of network dynamics. When simulated with two different initial conditions, the synaptic drive to neurons deviated strongly from each other (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), and the spiking activity of single neurons was uncorrelated across trials and the trial-averaged spiking rate had little temporal structure (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). The network activity was also sensitive to small perturbation; the microstate of two identically prepared networks diverged rapidly if one spike was deleted from one of the networks (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). It has been previously questioned as to whether the chaotic nature of an excitatory-inhibitory network could be utilized to perform reliable computations (<xref ref-type="bibr" rid="bib37">London et al., 2010</xref>; <xref ref-type="bibr" rid="bib45">Monteforte and Wolf, 2012</xref>).</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.37124.010</object-id><label>Figure 4.</label><caption><title>Learning innate activity in a network of excitatory and inhibitory neurons that respects Dale’s Law.</title><p>(<bold>a</bold>) Synaptic drive of sample neurons starting at random initial conditions in response to external stimulus prior to training. (<bold>b</bold>) Spike raster of sample neurons evoked by the same stimulus over multiple trials with random initial conditions. (<bold>c</bold>) Single spike perturbation of an untrained network. (<bold>d</bold>)-(<bold>f</bold>) Synaptic drive, multi-trial spiking response and single spike perturbation in a trained network. (<bold>g</bold>) The average phase deviation of theta neurons due to single spike perturbation. (<bold>h</bold>) Left, distribution of eigenvalues of the recurrent connectivity before and after training as a function their absolution values. Right, Eigenvalue spectrum of the recurrent connectivity; gray circle has unit radius. (<bold>i</bold>) The accuracy of quasi-static approximation in untrained networks and the performance of trained networks as a function of coupling strength J and synaptic time constant τ<sub>s</sub>. Color bar shows the Pearson correlation between predicted and actual synaptic drive in untrained networks (left) and innate and actual synaptic drive in trained networks (right).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig4-v2"/></fig><p>As in <xref ref-type="bibr" rid="bib33">Laje and Buonomano (2013)</xref>, we sought to tame the chaotic trajectories of single neuron activities when the coupling strength is strong enough to induce large and irregular spiking rate fluctuations in time and across neurons (<xref ref-type="bibr" rid="bib48">Ostojic, 2014</xref>). We initiated the untrained network with random initial conditions to harvest innate synaptic activity, that is a set of synaptic trajectories that the network already knows how to generate. Then, the recurrent connectivity was trained so that the synaptic drive of every neuron in the network follows the innate pattern when stimulated with an external stimulus. To respect Dale’s Law, the RLS learning rule was modified such that it did not update synaptic connections if there were changes in their signs.</p><p>After training, the synaptic drive to every neuron in the network was able to track the innate trajectories in response to the external stimulus within the trained window and diverged from the target pattern outside the trained window (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). When the trained network was stimulated to evoke the target patterns, the trial-averaged spiking rate developed a temporal structure that was not present in the untrained network (<xref ref-type="fig" rid="fig4">Figure 4e</xref>). To verify the reliability of learned spiking patterns, we simulated the trained network twice with identical initial conditions but deleted one spike <inline-formula><mml:math id="inf66"><mml:mrow><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> ms after evoking the trained response from one of the simulations. Within the trained window, the relative deviation of the microstate was markedly small in comparison to the deviation observed in the untrained network. Outside the trained window, however, two networks diverged rapidly again, which demonstrated that training the recurrent connectivity created an attracting flux tube around what used to be chaotic spike sequences (<xref ref-type="bibr" rid="bib45">Monteforte and Wolf, 2012</xref>) (<xref ref-type="fig" rid="fig4">Figure 4f,g</xref>). Analyzing the eigenvalue spectrum of the recurrent connectivity revealed that the distribution of eigenvalues shifts towards zero and the spectral radius decreased as a result of training, which is consistent with the more stable network dynamics found in trained networks (<xref ref-type="fig" rid="fig4">Figure 4h</xref>).</p><p>To demonstrate that learning the innate trajectories works well when an excitatory-inhibitory network satisfies the quasi-static condition, we scanned the coupling strength <inline-formula><mml:math id="inf67"><mml:mi>J</mml:mi></mml:math></inline-formula> (see Materials and methods, 'Training recurrent dynamics' for the definition) and synaptic time constant <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> over a wide range and evaluated the accuracy of the quasi-static approximation in untrained networks. We find that increasing either <inline-formula><mml:math id="inf69"><mml:mi>J</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> promoted strong fluctuations in spiking rates (<xref ref-type="bibr" rid="bib48">Ostojic, 2014</xref>; <xref ref-type="bibr" rid="bib25">Harish and Hansel, 2015</xref>), hence improving the quasi-static approximation (<xref ref-type="fig" rid="fig4">Figure 4i</xref>). Learning performance was correlated with adherence to the quasi-static approximation, resulting in better performance for strong coupling and long synaptic time constants.</p></sec><sec id="s2-1-3"><title>Generating an ensemble of in vivo spiking patterns</title><p>We next investigated if the training method applied to actual spike recordings of a large number of neurons. In a previous study, a network of rate units was trained to match sequential activity imaged from posterior parietal cortex as a possible mechanism for short-term memory (<xref ref-type="bibr" rid="bib26">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Rajan et al., 2016</xref>). Here, we aimed to construct recurrent spiking networks that captured heterogeneous spiking activity of cortical neurons involved in motor planning and movement (<xref ref-type="bibr" rid="bib15">Churchland and Shenoy, 2007</xref>; <xref ref-type="bibr" rid="bib16">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib36">Li et al., 2015</xref>).</p><p>The in vivo spiking data was obtained from the publicly available data of <xref ref-type="bibr" rid="bib36">Li et al. (2015)</xref>, where they recorded the spike trains of a large number of neurons from the anterior lateral motor cortex of mice engaged in planning and executing directed licking over multiple trials. We compiled the trial-average spiking rate of <inline-formula><mml:math id="inf71"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>227</mml:mn></mml:mrow></mml:math></inline-formula> cortical neurons from their data set (<xref ref-type="bibr" rid="bib35">Li et al., 2014</xref>), and trained a recurrent network model to reproduce the spiking rate patterns of all the <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons autonomously in response to a brief external stimulus. We only trained the recurrent connectivity and did not alter single neuron dynamics or external inputs.</p><p>First, we tested if a recurrent network of size <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was able to generate the spiking rate patterns of the same number of cortical neurons. This network model assumed that the spiking patterns of <inline-formula><mml:math id="inf74"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> cortical neurons could be self-generated within a recurrent network. After training, the spiking rate of neuron models captured the overall trend of the spiking rate, but not the rapid changes that may be pertinent to the short term memory and motor response (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). We hypothesized that the discrepancy may be attributed to other sources of input to the neurons not included in the model, such as recurrent input from other neurons in the local population or input from other areas of the brain, or the neuron dynamics that cannot be captured by our neuron model. We thus sought to improve the performance by adding <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>aux</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> auxiliary neurons to the recurrent network to mimic the spiking activity of unobserved neurons in the local population, and trained the recurrent connectivity of a network of size <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>aux</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). The auxiliary neurons were trained to follow spiking rate patterns obtained from an OU process and provided heterogeneity to the overall population activity patterns. When <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>aux</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, the spiking patterns of neuron models accurately fit that of cortical neurons (<xref ref-type="fig" rid="fig5">Figure 5c</xref>), and the population activity of all <inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> cortical neurons was well captured by the network model (<xref ref-type="fig" rid="fig5">Figure 5d</xref>). The fit to cortical activity improved gradually as a function of the fraction of auxiliary neurons in the network due to increased heterogeneity in the target patterns (<xref ref-type="fig" rid="fig5">Figure 5e</xref>)</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.37124.011</object-id><label>Figure 5.</label><caption><title>Generating in vivo spiking activity in a subnetwork of a recurrent network.</title><p>(<bold>a</bold>) Network schematic showing cortical (black) and auxiliary (white) neuron models trained to follow the spiking rate patterns of cortical neurons and target patterns derived from OU noise, respectively. Multi-trial spike sequences of sample cortical and auxiliary neurons in a successfully trained network. (<bold>b</bold>) Trial-averaged spiking rate of cortical neurons (red) and neuron models (black) when no auxiliary neurons are included. (<bold>c</bold>) Trial-averaged spiking rate of cortical and auxiliary neuron models when <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>aux</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>aux</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>c</bold>) Spiking rate of all the cortical neurons from the data (left) and the recurrent network model (right) trained with <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>aux</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>e</bold>) The fit to cortical dynamics improves as the number of auxiliary neurons increases. (<bold>f</bold>) Random shuffling of synaptic connections between cortical neuron models degrades the fit to cortical data. Error bars show the standard deviation of results from 10 trials.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig5-v2"/></fig><p>To verify that the cortical neurons in the network model were not simply driven by the feed forward inputs from the auxiliary neurons, we randomly shuffled a fraction of recurrent connections between cortical neurons after a successful training. The fit to cortical data deteriorated as the fraction of shuffled synaptic connections between cortical neurons was increased, which confirmed that the recurrent connections between the cortical neurons played a role in generating the spiking patterns (<xref ref-type="fig" rid="fig5">Figure 5f</xref>).</p></sec></sec><sec id="s2-2"><title>Sufficient conditions for learning</title><p>We can quantify the sufficient conditions the target patterns need to satisfy in order to be successfully encoded in a network. The first condition is that the dynamical time scale of both neurons and synapses must be sufficiently fast compared to the target patterns such that targets can be considered constant (quasi-static) on a short time interval. In terms of network dynamics, the quasi-static condition implies that the synaptic and neuron dynamics operate as if in a stationary state even though the stationary values change as the network activity evolves in time. In this quasi-static state, we can use a mean field description of the spiking dynamics to derive a self-consistent equation that captures the time-dependent synaptic and spiking activity of neurons (<xref ref-type="bibr" rid="bib10">Buice and Chow, 2013</xref>; <xref ref-type="bibr" rid="bib48">Ostojic, 2014</xref>; <xref ref-type="bibr" rid="bib7">Brunel, 2000</xref>) (see Materials and methods, 'Mean field description of the quasi-static dynamics'). Under the quasi-static approximation, the synaptic drive satisfies<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and the spiking rate <inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> satisfies<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf82"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is the current-to-rate transfer (i.e. gain) function and <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a constant external input.</p><p>The advantage of operating in a quasi-static state is that both measures of network activity become conducive to learning new patterns. First, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is closed in terms of <inline-formula><mml:math id="inf84"><mml:mi>U</mml:mi></mml:math></inline-formula>, which implies that training the synaptic drive is equivalent to training a rate-based network. Second, the RLS algorithm can efficiently optimize the recurrent connectivity <inline-formula><mml:math id="inf85"><mml:mi>W</mml:mi></mml:math></inline-formula>, thanks to the linearity of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in <inline-formula><mml:math id="inf86"><mml:mi>W</mml:mi></mml:math></inline-formula>, while the synaptic drive closely follows the target patterns as shown in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. The spiking rate also provides a closed description of the network activity, as described in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. However, due to nonlinearity in <inline-formula><mml:math id="inf87"><mml:mi>W</mml:mi></mml:math></inline-formula>, it learns only when the total input to a neuron is supra-threshold, that is the gradient of <inline-formula><mml:math id="inf88"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> must be positive. For this reason, the learning error cannot be controlled as tightly as the synaptic drive and requires additional trials for successful learning as shown in <xref ref-type="fig" rid="fig1">Figure 1d</xref>.</p><p>The second condition requires the target patterns to be sufficiently heterogeneous in time and across neurons. Such complexity allows the ensemble of spiking activity to have a rich spatiotemporal structure to generate the desired activity patterns of every neuron within the network. In the perspective of ‘reservoir computing’ (<xref ref-type="bibr" rid="bib38">Maass et al., 2002</xref>; <xref ref-type="bibr" rid="bib31">Jaeger and Haas, 2004</xref>; <xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>), every neuron in a recurrent network is considered to be a read-out, and, at the same time, it is part of the reservoir that is collectively used to produce desired patterns in single neurons. The heterogeneity condition is equivalent to having a set of complete (or over-complete) basis functions, that is <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> and <inline-formula><mml:math id="inf90"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, to generate the target patterns, that is the left hand side of <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>. The two conditions are not necessarily independent. Heterogeneous targets also foster asynchronous spiking activity that support quasi-static dynamics.</p><p>We can illustrate the necessity of heterogeneous target functions with a simple argument. Successful learning is achieved for the synaptic drive when <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is satisfied. If we discretize time into <inline-formula><mml:math id="inf91"><mml:mi>P</mml:mi></mml:math></inline-formula>‘quasi-static’ bins then we can consider the target <inline-formula><mml:math id="inf92"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> as a <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> matrix that satisfies the system of equations expressed in matrix form as <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>V</mml:mi><mml:mo>≡</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is an <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> matrix. Since the elements of <inline-formula><mml:math id="inf97"><mml:mi>W</mml:mi></mml:math></inline-formula> are the unknowns, it is convenient to consider the transpose of the matrix equation, <inline-formula><mml:math id="inf98"><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Solving for <inline-formula><mml:math id="inf99"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is equivalent to finding <inline-formula><mml:math id="inf100"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="inf101"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf103"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a vector in <inline-formula><mml:math id="inf104"><mml:mi>P</mml:mi></mml:math></inline-formula>-dimensional Euclidean space <inline-formula><mml:math id="inf105"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denoting the <inline-formula><mml:math id="inf106"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> column of <inline-formula><mml:math id="inf107"><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> (the synaptic drive of neuron <inline-formula><mml:math id="inf108"><mml:mi>i</mml:mi></mml:math></inline-formula>) and <inline-formula><mml:math id="inf109"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is an <inline-formula><mml:math id="inf110"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector denoting the <inline-formula><mml:math id="inf111"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> column of <inline-formula><mml:math id="inf112"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> (the incoming synaptic connections to neuron <inline-formula><mml:math id="inf113"><mml:mi>i</mml:mi></mml:math></inline-formula>). We also denote the column vectors of <inline-formula><mml:math id="inf114"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> by <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (the firing rate patterns of neurons induced by the target functions). For each <inline-formula><mml:math id="inf117"><mml:mi>i</mml:mi></mml:math></inline-formula>, the system of equations consists of <inline-formula><mml:math id="inf118"><mml:mi>P</mml:mi></mml:math></inline-formula> equations and <inline-formula><mml:math id="inf119"><mml:mi>N</mml:mi></mml:math></inline-formula> unknowns.</p><p>In general, the system of equations is solvable if all target functions <inline-formula><mml:math id="inf120"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> lie in the subspace spanned by <inline-formula><mml:math id="inf121"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This is equivalent to stating that the target functions can be self-consistently generated by the firing rate patterns induced by the target functions. We define target functions to be sufficiently heterogeneous if <inline-formula><mml:math id="inf122"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is maximal and show that this is a sufficient condition for solutions to exist. Since the span of <inline-formula><mml:math id="inf123"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> encompasses the largest possible subspace in <inline-formula><mml:math id="inf124"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> if <inline-formula><mml:math id="inf125"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is maximal, it is justified as a mathematical definition of sufficiently heterogeneous. In particular, if <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≥</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is maximal, we have <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext>dim</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>span</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which implies that the set of firing rate vectors <inline-formula><mml:math id="inf129"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> fully span <inline-formula><mml:math id="inf130"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, of which the target vectors <inline-formula><mml:math id="inf131"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are elements; in other words, <inline-formula><mml:math id="inf132"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> forms an (over-)complete set of basis functions of <inline-formula><mml:math id="inf133"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. On the other hand, if <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf135"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is maximal, we have <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mtext>dim</mml:mtext><mml:mspace width="thinmathspace"/><mml:mtext>span</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which implies linearly independent <inline-formula><mml:math id="inf137"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can only span an <inline-formula><mml:math id="inf138"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional subspace of <inline-formula><mml:math id="inf139"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, but such subspace still attains the largest possible dimension.</p><p>Now we consider the solvability of <inline-formula><mml:math id="inf140"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf141"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is maximal. For <inline-formula><mml:math id="inf142"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≥</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>, the set of vectors <inline-formula><mml:math id="inf143"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> fully span <inline-formula><mml:math id="inf144"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, or equivalently we can state that there are more unknowns (<inline-formula><mml:math id="inf145"><mml:mi>N</mml:mi></mml:math></inline-formula>) than independent equations (<inline-formula><mml:math id="inf146"><mml:mi>P</mml:mi></mml:math></inline-formula>), in which case the equation can always be satisfied and learning the pattern is possible. If <inline-formula><mml:math id="inf147"><mml:mi>N</mml:mi></mml:math></inline-formula> is strictly larger than <inline-formula><mml:math id="inf148"><mml:mi>P</mml:mi></mml:math></inline-formula> then a regularization term is required for the algorithm to converge to a specific solution out of the many possible solutions, the number of which decreases as <inline-formula><mml:math id="inf149"><mml:mi>P</mml:mi></mml:math></inline-formula> approaches <inline-formula><mml:math id="inf150"><mml:mi>N</mml:mi></mml:math></inline-formula>. For <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>, on the other hand, <inline-formula><mml:math id="inf152"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> spans an <inline-formula><mml:math id="inf153"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional subspace of <inline-formula><mml:math id="inf154"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, or equivalently there will be more equations than unknowns and perfect learning is not possible. However, since <inline-formula><mml:math id="inf155"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is maximal, there is an approximate regression solution of the form <inline-formula><mml:math id="inf156"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where the inverse of <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> exists since the set of vectors <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is linearly independent.</p><p>When <inline-formula><mml:math id="inf159"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is not maximal, successful learning is still possible as long as all <inline-formula><mml:math id="inf160"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> lie close to the subspace spanned by <inline-formula><mml:math id="inf161"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. However, the success depends on the specific choice of target functions, because the dimension of the subspace spanned by <inline-formula><mml:math id="inf162"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is strictly less than <inline-formula><mml:math id="inf163"><mml:mi>P</mml:mi></mml:math></inline-formula>, so whether the rows of <inline-formula><mml:math id="inf164"><mml:mi>U</mml:mi></mml:math></inline-formula> are contained in or close to this subspace is determined by the geometry of the subspace. This shows why increasing pattern heterogeneity, which makes the columns of <inline-formula><mml:math id="inf165"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> more independent and the rank higher, is beneficial for learning. Conversely, as a larger number of neurons is trained on the same target, as considered in <xref ref-type="fig" rid="fig3">Figure 3c</xref>, it becomes increasingly difficult to develop the target pattern <inline-formula><mml:math id="inf166"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>u</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with the limited set of basis functions <inline-formula><mml:math id="inf167"><mml:mrow><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>This argument also shows why learning capability declines as <inline-formula><mml:math id="inf168"><mml:mi>P</mml:mi></mml:math></inline-formula> increases, with a steep decline for <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. If we ascribe a quasi-static bin to some fraction of the pattern correlation time then <inline-formula><mml:math id="inf170"><mml:mi>P</mml:mi></mml:math></inline-formula> will scale with the length of the pattern temporal length. In this way, we can intuitively visualize the temporal storage capacity demonstrated below in Figure 7 through simulations.</p><p>We note that although <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref> describe the dynamical state in which learning works well, merely finding <inline-formula><mml:math id="inf171"><mml:mi>W</mml:mi></mml:math></inline-formula> that satisfies one of the equations does not guarantee that a spiking network with recurrent connectivity <inline-formula><mml:math id="inf172"><mml:mi>W</mml:mi></mml:math></inline-formula> will produce the target dynamics in a stable manner. The recurrent connectivity <inline-formula><mml:math id="inf173"><mml:mi>W</mml:mi></mml:math></inline-formula> needs to be trained iteratively as the network dynamics unfold in time to ensure that the target dynamics is generated in a stable manner (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>). There are three aspects of the training scheme that promote stable dynamics around the target trajectories. First, the stimulus at the onset of the learning window is applied at random times so it only sets the initial network states close to each other but with some random deviations. Training with initial conditions sampled from a small region in the state space forces the trained network to be robust to the choice of initial condition, and the target dynamics can be evoked reliably. Second, various network states around the target trajectories are explored while <inline-formula><mml:math id="inf174"><mml:mi>W</mml:mi></mml:math></inline-formula> is learning the desired dynamics. In-between the time points when <inline-formula><mml:math id="inf175"><mml:mi>W</mml:mi></mml:math></inline-formula> is updated, the network states evolve freely with no constraints and can thus diverge from the desired trajectory. This allows the network to visit different network states in the neighborhood of the target trajectories during training, and the trained network becomes resistant to relatively small perturbations from the target trajectories. Third, the synaptic update rule is designed to reduce the error between the target and the ongoing network activity each time <inline-formula><mml:math id="inf176"><mml:mi>W</mml:mi></mml:math></inline-formula> is updated. Thus, the sequential nature of the training procedure automatically induces stable dynamics by contracting trajectories toward the target throughout the entire path. In sum, robustness to initial conditions and network states around the target trajectories, together with the contractive property of the learning scheme, allow the trained network to generate the target dynamics in a stable manner.</p><sec id="s2-2-1"><title>Characterizing learning error</title><p>Learning errors can be classified into two categories. There are <italic>tracking</italic> errors, which arise because the target is not a solution of the true spiking network dynamics and <italic>sampling</italic> errors, which arise from encoding a continuous function with a finite number of spikes. We note that for a rate network, there would only be a tracking error. We quantified these learning errors as a function of the network and target time scales. The intrinsic time scale of spiking network dynamics was the synaptic decay constant <inline-formula><mml:math id="inf177"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the time scale of target dynamics was the decay constant <inline-formula><mml:math id="inf178"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of OU noise. We used target patterns generated from OU noise since the trajectories have a predetermined time scale and their spatio-temporal patterns are sufficiently heterogeneous.</p><p>We systematically varied <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf180"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from fast AMPA-like (~ 1 ms) to slow NMDA-like synaptic transmission (~ 100 ms) and trained the synaptic drive of networks with synaptic time scale <inline-formula><mml:math id="inf181"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to learn a set of OU trajectories with timescale <inline-formula><mml:math id="inf182"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The parameter scan revealed a learning regime, where the networks successfully encoded the target patterns, and two error-dominant regimes. The tracking error was prevalent when synapses were slow in comparison to target patterns, and the sampling error dominated when the synapses were fast (<xref ref-type="fig" rid="fig6">Figure 6a</xref>).</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.37124.012</object-id><label>Figure 6.</label><caption><title>Sampling and tracking errors.</title><p>Synaptic drive was trained to learn 1 s long trajectories generated from OU noise with decay time <inline-formula><mml:math id="inf183"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. (<bold>a</bold>) Performance of networks of size <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> as a function of synaptic decay time <inline-formula><mml:math id="inf185"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and target decay time <inline-formula><mml:math id="inf186"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. (<bold>b</bold>) Examples of trained networks whose responses show sampling error, tracking error, and successful learning. The target trajectories are identical and <inline-formula><mml:math id="inf187"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms. (<bold>c</bold>) Inverted ‘U’-shaped curve as a function of synaptic decay time. Error bars show the s.d. of five trained networks of size <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>d</bold>) Inverted ‘U’-shaped curve for networks of sizes <inline-formula><mml:math id="inf189"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf190"><mml:mrow><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf191"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms. (<bold>e</bold>) Network performance shown as a function of <inline-formula><mml:math id="inf192"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where the range of <inline-formula><mml:math id="inf193"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is from 30 ms to 500 ms and the range of <inline-formula><mml:math id="inf194"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is from 1ms to 500ms and <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>. (<bold>f</bold>) Network performance shown as a function of <inline-formula><mml:math id="inf196"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mi>N</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> where the range of <inline-formula><mml:math id="inf197"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is from 1 ms to 30 ms, the range of <inline-formula><mml:math id="inf198"><mml:mi>N</mml:mi></mml:math></inline-formula> is from 500 to 1000 and <inline-formula><mml:math id="inf199"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig6-v2"/></fig><p>A network with a synaptic decay time <inline-formula><mml:math id="inf200"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> ms failed to follow rapid changes in the target patterns, but still captured the overall shape, when the target patterns had a faster time scale <inline-formula><mml:math id="inf201"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, Tracking error). This prototypical example showed that the synaptic dynamics were not fast enough to encode the target dynamics in the tracking error regime. With a faster synapse <inline-formula><mml:math id="inf202"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> ms, the synaptic drive was able to learn the identical target trajectories with high accuracy (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, Learning). Note that although the target time scale (<inline-formula><mml:math id="inf203"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms) was significantly slower than the synaptic time scale (<inline-formula><mml:math id="inf204"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> ms), tuning the recurrent synaptic connections was sufficient for the network to generate slow network dynamics using fast synapses. This phenomenon was shown robustly in the learning regime in <xref ref-type="fig" rid="fig4">Figure 4a</xref> where learning occurred successfully for the parameters lying above the diagonal line (<inline-formula><mml:math id="inf205"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). When the synapse was too fast <inline-formula><mml:math id="inf206"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> ms, however, the synaptic drive fluctuated around the target trajectories with high frequency (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, Sampling error). This was a typical network response in the sampling error regime where discrete spikes with narrow width and large amplitude were summed to ‘sample’ the target synaptic activity.</p><p>To better understand how network parameters determined the learning errors, we mathematically analyzed the errors assuming that (1) target dynamics can be encoded if the quasi-static condition holds, and (2) the mean field description of the target dynamics is accurate (see Materials and methods, 'Analysis of learning error'). The learning errors were characterized as a deviation of these assumptions from the actual spiking network dynamics. We found that the tracking errors <inline-formula><mml:math id="inf207"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> were substantial if the quasi-static condition was not valid, that is synapses were not fast enough for spiking networks to encode targets, and the sampling errors <inline-formula><mml:math id="inf208"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> occurred if the mean field description became inaccurate, that is discrete representation of targets in terms of spikes deviated from their continuous representation in terms of spiking rates. The errors were estimated to scale with<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>N</mml:mi></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which implied that tracking error can be controlled as long as synapses are relatively faster than target patterns, and the sampling error can be controlled by either increasing <inline-formula><mml:math id="inf209"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to stretch the width of individual spikes or increasing <inline-formula><mml:math id="inf210"><mml:mi>N</mml:mi></mml:math></inline-formula> to encode the targets with more input spikes. The error estimates revealed the versatility of recurrent spiking networks to encode arbitrary patterns since <inline-formula><mml:math id="inf211"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> can be reduced by tuning <inline-formula><mml:math id="inf212"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to be small enough and <inline-formula><mml:math id="inf213"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> can be reduced by increasing <inline-formula><mml:math id="inf214"><mml:mi>N</mml:mi></mml:math></inline-formula> to be large enough. In particular, target signals substantially slower than the synaptic dynamics (i.e. <inline-formula><mml:math id="inf215"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) can be encoded reliably as long as the network size is large enough to represent the slow signals with filtered spikes that have narrow widths. Such slow dynamics were also investigated in randomly connected recurrent networks when coupling is strong (<xref ref-type="bibr" rid="bib55">Sompolinsky et al., 1988</xref>; <xref ref-type="bibr" rid="bib48">Ostojic, 2014</xref>) and reciprocal connections are over-represented (<xref ref-type="bibr" rid="bib40">Martí et al., 2018</xref>).</p><p>We examined the performance of trained networks to verify if the theoretical results can explain the learning errors. The learning curve, as a function of <inline-formula><mml:math id="inf216"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, had an inverted U-shape when both types of errors were present (<xref ref-type="fig" rid="fig6">Figure 6c, d</xref>). Successful learning occurred in an optimal range of <inline-formula><mml:math id="inf217"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and, consistent with the error analysis, the performance decreased monotonically with <inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on the right branch due to increase in the tracking error while the performance increased monotonically with <inline-formula><mml:math id="inf219"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on the left branch due to decrease in the sampling error. The tracking error was reduced if target patterns were slowed down from <inline-formula><mml:math id="inf220"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> ms to <inline-formula><mml:math id="inf221"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> ms, hence decreased the ratio <inline-formula><mml:math id="inf222"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Then, the learning curve became sigmoidal, and the performance remained high even when <inline-formula><mml:math id="inf223"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was in the slow NMDA regime (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). On the other hand, the sampling error was reduced if the network size was increased from <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf225"><mml:mrow><mml:mn>1500</mml:mn></mml:mrow></mml:math></inline-formula>, which lifted the left branch of the learning curve (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). Note that when two error regimes were well separated, changes in target time scale <inline-formula><mml:math id="inf226"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> did not affect <inline-formula><mml:math id="inf227"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and changes in network size <inline-formula><mml:math id="inf228"><mml:mi>N</mml:mi></mml:math></inline-formula> did not affect <inline-formula><mml:math id="inf229"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, as predicted.</p><p>Finally, we condensed the training results over a wide range of target time scales in the tracking error regime (<xref ref-type="fig" rid="fig6">Figure 6e</xref>), and similarly condensed the training results over different network sizes in the sampling error regime (<xref ref-type="fig" rid="fig6">Figure 6f</xref>) to demonstrate that <inline-formula><mml:math id="inf230"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>N</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> explained the overall performance in the tracking and sampling error regimes, respectively.</p></sec><sec id="s2-2-2"><title>Learning capacity increases with network size</title><p>It has been shown that a recurrent rate network’s capability to encode target patterns deteriorates as a function of the length of time (<xref ref-type="bibr" rid="bib33">Laje and Buonomano, 2013</xref>), but increase in network size can enhance its storage capacity (<xref ref-type="bibr" rid="bib30">Jaeger, 2001</xref>; <xref ref-type="bibr" rid="bib63">White et al., 2004</xref>; <xref ref-type="bibr" rid="bib50">Rajan et al., 2016</xref>). Consistent with these results, we found that the performance of recurrent spiking networks to learn complex trajectories decreased with target length and improved with network size (<xref ref-type="fig" rid="fig7">Figure 7a</xref>).</p><p>To assess the storage capacity of spiking networks, we evaluated the maximal target length that can be encoded in a network as a function of network size. It was necessary to define the target length in terms of its ‘effective length’ to account for the fact that target patterns with the same length may have different effective length due to their temporal structures; for instance, OU noise with short temporal correlation times has more structure to be learned than a constant function. For target trajectories generated from an OU process with decay time <inline-formula><mml:math id="inf232"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we rescaled the target length <inline-formula><mml:math id="inf233"><mml:mi>T</mml:mi></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf234"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and defined the effective length <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The capacity of a network was the maximal <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that can be successfully encoded in a network.</p><p>To estimate the maximal <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we trained networks of fixed size to learn OU trajectories while varying <inline-formula><mml:math id="inf238"><mml:mi>T</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf239"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (each panel in <xref ref-type="fig" rid="fig7">Figure 7b</xref>). Then, for each <inline-formula><mml:math id="inf240"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we found the maximal target length <inline-formula><mml:math id="inf241"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that can be learned successfully, and estimated the maximal <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> by finding a constant <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that best fits the line <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to training results (black lines in <xref ref-type="fig" rid="fig7">Figure 7b</xref>). <xref ref-type="fig" rid="fig7">Figure 7c</xref> shows that the learning capacity <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> increases monotonically with the network size.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.37124.013</object-id><label>Figure 7.</label><caption><title>Capacity as a function of network size.</title><p>(<bold>a</bold>) Performance of trained networks as a function of target length <inline-formula><mml:math id="inf246"><mml:mi>T</mml:mi></mml:math></inline-formula> for networks of size <inline-formula><mml:math id="inf247"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf248"><mml:mrow><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>. Target patterns were generated from OU noise with decay time <inline-formula><mml:math id="inf249"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> ms. (<bold>b</bold>) Networks of fixed sizes trained on a range of target length and correlations. Color bar shows the Pearson correlation between target and actual synaptic drive. The black lines show the function <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was fitted to minimize the least square error between the linear function and maximal target length <inline-formula><mml:math id="inf252"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that can be successfully learned at each <inline-formula><mml:math id="inf253"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. (<bold>c</bold>) Learning capacity <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> shown as a function of network size.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-37124-fig7-v2"/></fig></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our findings show that individual neurons embedded in a recurrent network can learn to produce complex activity by adjusting the recurrent synaptic connections. Most previous research on learning in recurrent neural networks focused on training the network outputs to perform useful computations and subsequently analyzed the recurrent activity in comparison with measured neuron activity (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib58">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib57">Sussillo and Barak, 2013</xref>; <xref ref-type="bibr" rid="bib62">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Chaisangmongkon et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Remington et al., 2018</xref>). In contrast to such output-centric approaches, our study takes a network-centric perspective and directly trains the activity of neurons within a network individually. Several studies have trained a rate-based network model to learn specific forms of target recurrent activity, such as innate chaotic dynamics (<xref ref-type="bibr" rid="bib33">Laje and Buonomano, 2013</xref>), sequential activity (<xref ref-type="bibr" rid="bib50">Rajan et al., 2016</xref>), and trajectories from a target network (<xref ref-type="bibr" rid="bib21">DePasquale et al., 2018</xref>). In this study, we showed that the synaptic drive and spiking rate of a synaptically-coupled spiking network can be trained to follow arbitrary spatiotemporal patterns. The necessary ingredients for learning are that the spike train inputs to a neuron are weakly correlated (i.e. heterogeneous target patterns), the synapses are fast enough (i.e. small tracking error), and the network is large enough (i.e. small sampling error and large capacity). We demonstrated that (1) a network consisting of excitatory and inhibitory neurons can learn to track its strongly fluctuating innate synaptic trajectories, and (2) are current spiking network can learn to reproduce the spiking rate patterns of an ensemble of cortical neurons involved in motor planning and movement.</p><p>Our scheme works because the network quickly enters a quasi-static state where the instantaneous firing rate of a neuron is a fixed function of the inputs (<xref ref-type="fig" rid="fig3">Figure 3a, b</xref>; <xref ref-type="disp-formula" rid="equ1">Equations 1, 2</xref>). Learning fails if the synaptic time scale is slow compared to the time scale of the target, in which case the quasi-static condition is violated and the tracking error becomes large. There is a trade-off between tracking error and sampling noise; fast synapse can decrease the tracking error, but it also increases the sampling noise. Increasing the network size can decrease sampling noise without affecting the tracking error (<xref ref-type="fig" rid="fig6">Figure 6e, f</xref>; <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). Therefore, analysis of learning error and simulations suggest that it is possible to learn arbitrarily complex recurrent dynamics by adjusting the synaptic time scale and network size.</p><p>An important structural property of our network model is that the synaptic inputs are summed linearly, which allows the synaptic activity to be trained using a recursive form of linear regression (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>; <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). Linear summation of synaptic inputs is a standard assumption for many spiking network models (<xref ref-type="bibr" rid="bib60">van Vreeswijk et al., 1996</xref>; <xref ref-type="bibr" rid="bib52">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib7">Brunel, 2000</xref>; <xref ref-type="bibr" rid="bib61">Wang and Buzsáki, 1996</xref>; <xref ref-type="bibr" rid="bib54">Rosenbaum et al., 2017</xref>) and there is physiological evidence that linear summation is prevalent (<xref ref-type="bibr" rid="bib12">Cash and Yuste, 1998</xref>; <xref ref-type="bibr" rid="bib13">Cash and Yuste, 1999</xref>). Training the spiking rate, on the other hand, cannot take full advantage of the linear synapse due to the nonlinear current-to-transfer function (<xref ref-type="fig" rid="fig1">Figure 1d, e</xref>; <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). The network is capable of following a wide repertoire of patterns because even though the network dynamics are highly nonlinear, the system effectively reduces to a linear system for learning. Moreover, learning capacity can be estimated using a simple solvability condition for a linear system. However, nonlinear dendritic processing has been widely observed (<xref ref-type="bibr" rid="bib24">Gasparini and Magee, 2006</xref>; <xref ref-type="bibr" rid="bib46">Nevian et al., 2007</xref>) and may have computational consequences (<xref ref-type="bibr" rid="bib42">Memmesheimer, 2010</xref>; <xref ref-type="bibr" rid="bib43">Memmesheimer and Timme, 2012</xref>; <xref ref-type="bibr" rid="bib59">Thalmeier et al., 2016</xref>). It requires further investigation to find out whether a recurrent network with nonlinear synapses can be trained to learn arbitrary recurrent dynamics.</p><p>We note that our learning scheme does not train precise spike times; it either trains the spiking rate or the synaptic drive. The stimulus at the onset of the learning window attempts to set the network to a specific state, but due to the variability of the initial conditions the network states can only be set approximately close to each other across trials. Because of this discrepancy in network states at the onset, the spike times are not aligned precisely across trials. Hence, our learning scheme supports rate coding as opposed to spike coding. However, spike trains that have temporally irregular structure across neurons actually enhance the rate coding scheme by providing sufficient computational complexity to encode the target dynamics (Results, 'Sufficient conditions for learning'). In fact, all neurons in the network can be trained to follow the same target patterns as long as there is sufficient heterogeneity, for example noisy external input, and the neuron time constant is fast enough (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). We also note that the same learning scheme can also be used to train the recurrent dynamics of rate-based networks (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). In fact, the learning is more efficient in a rate network since there is no sampling error to avoid.</p><p>The RLS algorithm, as demonstrated in this and other studies (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>; <xref ref-type="bibr" rid="bib58">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib33">Laje and Buonomano, 2013</xref>; <xref ref-type="bibr" rid="bib50">Rajan et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">DePasquale et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Wang et al., 2018</xref>), successfully generates desired outputs in a stable manner because the synaptic update rule contracts the network activity towards the target output, and the synaptic connections are adjusted while the network explores various states around the target trajectories. It would be interesting to examine more rigorously how such an iterative learning scheme turns a set of arbitrary functions into dynamic attractors to which the network dynamics converge transiently. Recent studies investigated how stable dynamics emerge when the read-outs of a rate-based network are trained to learn fixed points or continuous values (<xref ref-type="bibr" rid="bib53">Rivkind and Barak, 2017</xref>; <xref ref-type="bibr" rid="bib2">Beer and Barak, 2018</xref>). In addition, previous studies have investigated the mathematical relationship between the patterns of stored fixed points and the recurrent connectivity in simple network models (<xref ref-type="bibr" rid="bib17">Curto et al., 2013</xref>; <xref ref-type="bibr" rid="bib9">Brunel, 2016</xref>).</p><p>Although our results demonstrated that recurrent spiking networks have the capability to generate a wide range of repertoire of recurrent dynamics, it is unlikely that a biological network is using this particular learning scheme. The learning rule derived from recursive least squares algorithm is very effective but is nonlocal in time, that is it uses the activity of all presynaptic neurons within the train time window to update synaptic weights. Moreover, each neuron in the network is assigned with a target signal and the synaptic connections are updated at a fast time scale as the error function is computed in a supervised manner. It would be of interest to find out whether more biologically plausible learning schemes, such as reward-based learning (<xref ref-type="bibr" rid="bib23">Fiete and Seung, 2006</xref>; <xref ref-type="bibr" rid="bib28">Hoerzer et al., 2014</xref>; <xref ref-type="bibr" rid="bib44">Miconi, 2017</xref>) can lead to similar performance.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Network of spiking neurons</title><p>We considered a network of <inline-formula><mml:math id="inf255"><mml:mi>N</mml:mi></mml:math></inline-formula> randomly and sparsely connected quadratic integrate-and-fire neurons given by<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf256"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a dimensionless variable representing membrane potential, <inline-formula><mml:math id="inf257"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is an applied input, <inline-formula><mml:math id="inf258"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is the total synaptic drive the neuron receives from other neurons in the recurrent network, and <inline-formula><mml:math id="inf259"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> ms is a neuron time constant. The threshold to spiking is zero input. For negative total input, the neuron is at rest and for positive input, <inline-formula><mml:math id="inf260"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> will go to infinity or ‘blow up’ in finite time from any initial condition. The neuron is considered to spike at <inline-formula><mml:math id="inf261"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> whereupon it is reset to <inline-formula><mml:math id="inf262"><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib22">Ermentrout, 1996</xref>; <xref ref-type="bibr" rid="bib34">Latham et al., 2000</xref>).</p><p>To simulate the dynamics of quadratic integrate-and-fire neurons, we used its phase representation, that is theta neuron model, that can be derived by a simple change of variables, <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>tan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>; its dynamics are governed by<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where a spike is emitted when <inline-formula><mml:math id="inf264"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula>. The synaptic drive to a neuron obeys<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the spike train neuron <inline-formula><mml:math id="inf266"><mml:mi>j</mml:mi></mml:math></inline-formula> generates up to time <inline-formula><mml:math id="inf267"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf268"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a synaptic time constant.</p><p>The recurrent connectivity <inline-formula><mml:math id="inf269"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> describes the synaptic coupling from neuron <inline-formula><mml:math id="inf270"><mml:mi>j</mml:mi></mml:math></inline-formula> to neuron <inline-formula><mml:math id="inf271"><mml:mi>i</mml:mi></mml:math></inline-formula>. It can be any real matrix but in many of the simulations we use a random matrix with connection probability <inline-formula><mml:math id="inf272"><mml:mi>p</mml:mi></mml:math></inline-formula>, and the coupling strength of non-zero elements is modeled differently for different figures.</p></sec><sec id="s4-2"><title>Training recurrent dynamics</title><p>To train the synaptic and spiking rate dynamics of individual neurons, it is more convenient to divide the synaptic drive <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> into two parts; one that isolates the spike train of single neuron and computes its synaptic filtering<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and the other that combines all the presynaptic neurons’ spiking activity and computes the synaptic drive<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:malignmark/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The synaptic drive <inline-formula><mml:math id="inf273"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the filtered spike train <inline-formula><mml:math id="inf274"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are two measures of spiking activity that have been trained in this study. Note that <xref ref-type="disp-formula" rid="equ7 equ8">Equations 7 and 8</xref> generate synaptic dynamics that are equivalent to <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>.</p><sec id="s4-2-1"><title>Training procedure</title><p>We select <inline-formula><mml:math id="inf275"><mml:mi>N</mml:mi></mml:math></inline-formula> target trajectories <inline-formula><mml:math id="inf276"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of length <inline-formula><mml:math id="inf277"><mml:mi>T</mml:mi></mml:math></inline-formula> ms for a recurrent network consisting of <inline-formula><mml:math id="inf278"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons. We train either the synaptic drive or spiking rate of individual neuron <inline-formula><mml:math id="inf279"><mml:mi>i</mml:mi></mml:math></inline-formula> to follow the target <inline-formula><mml:math id="inf280"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> over time interval <inline-formula><mml:math id="inf281"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. External stimulus <inline-formula><mml:math id="inf283"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with amplitude sampled uniformly from <inline-formula><mml:math id="inf284"><mml:mrow><mml:mo>[</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> is applied to neuron <inline-formula><mml:math id="inf285"><mml:mi>i</mml:mi></mml:math></inline-formula> for all <inline-formula><mml:math id="inf286"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> for 100 ms immediately preceding the training to situate the network at a specific state. During training, the recurrent connectivity <inline-formula><mml:math id="inf287"><mml:mi>W</mml:mi></mml:math></inline-formula> is updated every <inline-formula><mml:math id="inf288"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> ms using a learning rule described below in order to steer the network dynamics toward the target dynamics. The training is repeated multiple times until changes in the recurrent connectivity stabilize.</p></sec><sec id="s4-2-2"><title>Training synaptic drive</title><p>Recent studies extended the RLS learning (also known as FORCE methods) developed in rate networks (<xref ref-type="bibr" rid="bib56">Sussillo and Abbott, 2009</xref>) either directly (<xref ref-type="bibr" rid="bib47">Nicola and Clopath, 2017</xref>) or indirectly using rate networks as an intermediate step (<xref ref-type="bibr" rid="bib20">DePasquale et al., 2016</xref>; <xref ref-type="bibr" rid="bib1">Abbott et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Thalmeier et al., 2016</xref>) to train the output of spiking networks. Our learning rule uses the RLS learning but is different from previous studies in that (a) it trains the activity of individual neurons within a spiking network and (b) neurons are trained directly by adjusting the recurrent synaptic connections without using any intermediate networks. We modified the learning rule developed by Laje and Buonomano in a network of rate units (<xref ref-type="bibr" rid="bib33">Laje and Buonomano, 2013</xref>) and also provided mathematical derivation of the learning rules for both the synaptic drive and spiking rates (see Materials and methods, 'Derivation of synaptic learning rules' for details).</p><p>When learning the synaptic drive patterns, the objective is to find recurrent connectivity <inline-formula><mml:math id="inf289"><mml:mi>W</mml:mi></mml:math></inline-formula> that minimizes the cost function<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>which measures the mean-square error between the targets and the synaptic drive over the time interval <inline-formula><mml:math id="inf290"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> plus a quadratic regularization term. To derive the learning rule, we use <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> to express <inline-formula><mml:math id="inf291"><mml:mi>u</mml:mi></mml:math></inline-formula> as a function of <inline-formula><mml:math id="inf292"><mml:mi>W</mml:mi></mml:math></inline-formula>, view the synaptic connections <inline-formula><mml:math id="inf293"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to neuron <inline-formula><mml:math id="inf294"><mml:mi>i</mml:mi></mml:math></inline-formula> to be the read-out weights that determine the synaptic drive <inline-formula><mml:math id="inf295"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and apply the learning rule to the row vectors of <inline-formula><mml:math id="inf296"><mml:mi>W</mml:mi></mml:math></inline-formula>. To keep the recurrent connectivity sparse, learning occurs only on synaptic connections that are non-zero prior to training.</p><p>Let <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> be the reduced row vector of <inline-formula><mml:math id="inf298"><mml:mrow><mml:mi>W</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> consisting of elements that have non-zero connections to neuron <inline-formula><mml:math id="inf299"><mml:mi>i</mml:mi></mml:math></inline-formula> prior to training. Similarly, let <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> be a (column) vector of filtered spikes of presynaptic neurons that have non-zero connections to neuron <inline-formula><mml:math id="inf301"><mml:mi>i</mml:mi></mml:math></inline-formula>. The synaptic update to neuron <inline-formula><mml:math id="inf302"><mml:mi>i</mml:mi></mml:math></inline-formula> is<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where the error term is<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>and the inverse of the correlation matrix of filtered spike trains is<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Finally, <inline-formula><mml:math id="inf303"><mml:mrow><mml:mi>W</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is obtained by concatenating the row vectors <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-2-3"><title>Training spiking rate</title><p>To train the spiking rate of neurons, we approximate the spike train <inline-formula><mml:math id="inf305"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf306"><mml:mi>i</mml:mi></mml:math></inline-formula> with its spiking rate <inline-formula><mml:math id="inf307"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf308"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is the current-to-rate transfer function of theta neuron model. For constant input,<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msqrt><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:msqrt><mml:mtext> </mml:mtext><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mtext>where </mml:mtext><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and for noisy input<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>π</mml:mi></mml:mfrac><mml:msqrt><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf309"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is a good approximation of <inline-formula><mml:math id="inf310"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and has a smooth transition around <inline-formula><mml:math id="inf311"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we used <inline-formula><mml:math id="inf312"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>≡</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf313"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib8">Brunel and Latham, 2003</xref>). The objective is to find recurrent connectivity <inline-formula><mml:math id="inf314"><mml:mi>W</mml:mi></mml:math></inline-formula> that minimizes the cost function<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>If we define <inline-formula><mml:math id="inf315"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf316"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as before, we can derive the following synaptic update to neuron <inline-formula><mml:math id="inf317"><mml:mi>i</mml:mi></mml:math></inline-formula><disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where the error term is<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>and<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>(see Materials and methods, 'Derivation of synaptic learning rules' for details). Note that the nonlinear effects of the transfer function is included in<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>which scales the spiking activity of neuron <inline-formula><mml:math id="inf318"><mml:mi>i</mml:mi></mml:math></inline-formula> by its gain function <inline-formula><mml:math id="inf319"><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>.</p><p>As before, <inline-formula><mml:math id="inf320"><mml:mrow><mml:mi>W</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is obtained by concatenating the row vectors <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></sec><sec id="s4-3"><title>Simulation parameters</title><sec id="s4-3-1"><title><xref ref-type="fig" rid="fig1">Figure 1</xref></title><p>A network of <inline-formula><mml:math id="inf322"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> neurons was connected randomly with probability <inline-formula><mml:math id="inf323"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> and the coupling strength was drawn from a Normal distribution with mean 0 and standard deviation <inline-formula><mml:math id="inf324"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf325"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>. In addition, the average of all non-zero synaptic connections to a neuron was subtracted from the connections to the neuron such that the summed coupling strength was precisely zero. Networks with balanced excitatory and inhibitory connections produced highly fluctuating synaptic and spiking activity in all neurons. The synaptic decay time was <inline-formula><mml:math id="inf326"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> ms.</p><p>The target functions for the synaptic drive (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) were sine waves <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where the amplitude <inline-formula><mml:math id="inf328"><mml:mi>A</mml:mi></mml:math></inline-formula>, initial phase <inline-formula><mml:math id="inf329"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and period <inline-formula><mml:math id="inf330"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> were sampled uniformly from <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1.5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>300</mml:mn><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. We generated <inline-formula><mml:math id="inf334"><mml:mi>N</mml:mi></mml:math></inline-formula> distinct target functions of length <inline-formula><mml:math id="inf335"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> ms. The target functions for the spiking rate (<xref ref-type="fig" rid="fig1">Figure 1d</xref>) were <inline-formula><mml:math id="inf336"><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf337"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> were the same synaptic drive patterns that have been generated.</p><p>Immediately before each training loop, every neuron was stimulated for 50 ms with constant external stimulus that had random amplitude sampled from <inline-formula><mml:math id="inf338"><mml:mrow><mml:mo>[</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>. The same external stimulus was used across training loops. The recurrent connectivity was updated every <inline-formula><mml:math id="inf339"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> ms during training using the learning rule derived from RLS algorithm and the learning rate was <inline-formula><mml:math id="inf340"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. After training, the network was stimulated with the external stimulus to evoke the trained patterns. The performance was measured by calculating the average Pearson correlation between target functions and the evoked network response.</p></sec><sec id="s4-3-2"><title><xref ref-type="fig" rid="fig2">Figure 2</xref></title><p>The initial network and target functions were generated as in <xref ref-type="fig" rid="fig1">Figure 1</xref> using the same parameters, but now the target functions consisted of two sets of <inline-formula><mml:math id="inf341"><mml:mi>N</mml:mi></mml:math></inline-formula> sine waves. To learn two sets of target patterns, the training loops alternated between two patterns, and immediately before each training loop, every neuron was stimulated for 50 ms with constant external stimuli that had random amplitudes, using a different stimulus for each pattern. Each target pattern was trained for 100 loops (i.e. total 200 training loops), synaptic update was every <inline-formula><mml:math id="inf342"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> ms, and the learning rate was <inline-formula><mml:math id="inf343"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>. To evoke one of the target patterns after training, the network was stimulated with the external stimulus that was used to train that target pattern.</p><p>The network consisted of <inline-formula><mml:math id="inf344"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> neurons. The initial connectivity was sparsely connected with connection probability <inline-formula><mml:math id="inf345"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> and coupling strength was sampled from a Normal distribution with mean <inline-formula><mml:math id="inf346"><mml:mn>0</mml:mn></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf348"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The synaptic decay time was <inline-formula><mml:math id="inf349"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> ms.</p><p>We considered three families of target functions with length <inline-formula><mml:math id="inf350"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> ms. The complex periodic functions were defined as a product of two sine waves <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf352"><mml:mi>A</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf353"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf354"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf355"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> were sampled randomly from intervals <inline-formula><mml:math id="inf356"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1.5</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf357"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mtext> ms</mml:mtext><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf358"><mml:mrow><mml:mo>[</mml:mo><mml:mn>500</mml:mn><mml:mtext> ms</mml:mtext><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mtext> ms</mml:mtext><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf359"><mml:mrow><mml:mo>[</mml:mo><mml:mn>100</mml:mn><mml:mtext> ms</mml:mtext><mml:mo>,</mml:mo><mml:mn>500</mml:mn><mml:mtext> ms</mml:mtext><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>, respectively. The chaotic rate activity was generated from a network of <inline-formula><mml:math id="inf360"><mml:mi>N</mml:mi></mml:math></inline-formula> randomly connected rate units, <inline-formula><mml:math id="inf361"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf362"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> ms, <inline-formula><mml:math id="inf363"><mml:mrow><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf364"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is non-zero with probability <inline-formula><mml:math id="inf365"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> and is drawn from Gaussian distribution with mean zero and standard deviation <inline-formula><mml:math id="inf366"><mml:mrow><mml:mi>g</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf367"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>. The Ornstein-Ulenbeck process was obtained by simulating, <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>ξ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf369"><mml:mi>N</mml:mi></mml:math></inline-formula> times with random initial conditions and different realizations of the white noise <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> satisfying <inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>ξ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>ξ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The decay time constant was <inline-formula><mml:math id="inf373"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> ms, and the amplitude of target function was determined by <inline-formula><mml:math id="inf374"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The recurrent connectivity was updated every <inline-formula><mml:math id="inf375"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> ms during training, the learning rate was <inline-formula><mml:math id="inf376"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and the training loop was repeated 30 times.</p></sec><sec id="s4-3-3"><title><xref ref-type="fig" rid="fig4">Figure 4</xref></title><p>A balanced network had two populations where the excitatory population consisted of <inline-formula><mml:math id="inf377"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> neurons and the inhibitory population consisted of <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons with ratio <inline-formula><mml:math id="inf379"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> and network size <inline-formula><mml:math id="inf380"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>. Each neuron received <inline-formula><mml:math id="inf381"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> excitatory connections with strength <inline-formula><mml:math id="inf382"><mml:mi>J</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> inhibitory connections with strength <inline-formula><mml:math id="inf384"><mml:mrow><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:math></inline-formula> from randomly selected excitatory and inhibitory neurons. The connection probability was set to <inline-formula><mml:math id="inf385"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> to have sparse connectivity. The relative strength of inhibition to excitation <inline-formula><mml:math id="inf386"><mml:mi>g</mml:mi></mml:math></inline-formula> was set to <inline-formula><mml:math id="inf387"><mml:mn>5</mml:mn></mml:math></inline-formula> so that the network was inhibition dominant (<xref ref-type="bibr" rid="bib7">Brunel, 2000</xref>). In <xref ref-type="fig" rid="fig4">Figure 4a–h</xref>, the initial coupling strength <inline-formula><mml:math id="inf388"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> and synaptic decay time <inline-formula><mml:math id="inf389"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula> ms were adjusted to be large enough, so that the synaptic drive and spiking rate of individual neurons fluctuated strongly and slowly prior to training.</p><p>After running the initial network that started at random initial conditions for 3 s, we recorded the synaptic drive of all neurons for 2 s to harvest target trajectories that are innate to the balanced network. Then, the synaptic drive was trained to learn the innate trajectories, where synaptic update occurred every <inline-formula><mml:math id="inf390"><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> ms, learning rate was <inline-formula><mml:math id="inf391"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and training loop was repeated 40 times. To respect Dale’s Law while training the network, we did not modify the synaptic connections if the synaptic update reversed the sign of original connections, either from excitatory to inhibitory or from inhibitory to excitatory. Moreover, the synaptic connections that attempted to change their signs were excluded in subsequent trainings. In bf <xref ref-type="fig" rid="fig4">Figure 4h</xref>, the initial and trained connectivity matrices were normalized by a factor <inline-formula><mml:math id="inf392"><mml:mrow><mml:msqrt><mml:mrow><mml:mo>[</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mi>J</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mi>J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>]</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> so that the spectral radius of the initial connectivity matrix is approximately 1, then we plotted the eigenvalue spectrum of the normalized matrices.</p><p>In <xref ref-type="fig" rid="fig4">Figure 4i</xref>, the coupling strength <inline-formula><mml:math id="inf393"><mml:mi>J</mml:mi></mml:math></inline-formula> was scanned from 1 to 6 in increments of 0.25, and the synaptic decay time <inline-formula><mml:math id="inf394"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was scanned from 5 ms to 100 ms in increments of 5 ms. To measure the accuracy of quasi-static approximation in untrained networks, we simulated the network dynamics for each pair of <inline-formula><mml:math id="inf395"><mml:mi>J</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf396"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then calculated the average Person correlation between the predicted synaptic drive (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) and the actual synaptic drive. To measure the performance of trained networks, we repeated the training 10 times using different initial network configurations and innate trajectories, and calculated the Pearson correlation between the innate trajectories and the evoked network response for all 10 trainings. The heat map shows the best performance out of 10 trainings for each pair, <inline-formula><mml:math id="inf397"><mml:mi>J</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf398"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-3-4"><title><xref ref-type="fig" rid="fig5">Figure 5</xref></title><p>The initial connectivity was sparsely connected with connection probability <inline-formula><mml:math id="inf399"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> and the coupling strength was sampled from a Normal distribution with mean <inline-formula><mml:math id="inf400"><mml:mn>0</mml:mn></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf401"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf402"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The synaptic decay time was <inline-formula><mml:math id="inf403"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> ms. There were in total <inline-formula><mml:math id="inf404"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons in the network model, of which <inline-formula><mml:math id="inf405"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> neurons, called cortical neurons, were trained to learn the spiking rate patterns of cortical neurons, and <inline-formula><mml:math id="inf406"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>aux</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> neurons, called auxiliary neurons, were trained to learn trajectories generated from OU process.</p><p>We used the trial-averaged spiking rates of neurons recorded in the anterior lateral motor cortex of mice engaged in motor planning and movement that lasted 4600 ms (<xref ref-type="bibr" rid="bib36">Li et al., 2015</xref>). The data was available from the website CRCNS.ORG (<xref ref-type="bibr" rid="bib35">Li et al., 2014</xref>). We selected <inline-formula><mml:math id="inf407"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>227</mml:mn></mml:mrow></mml:math></inline-formula> neurons from the data set, whose average spiking rate during the behavioral task was greater than <inline-formula><mml:math id="inf408"><mml:mn>5</mml:mn></mml:math></inline-formula> Hz. Each cortical neuron in the network model was trained to learn the spiking rate pattern of one of the real cortical neurons.</p><p>To generate target rate functions for the auxiliary neurons, we simulated an OU process, <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>ξ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf410"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>800</mml:mn></mml:mrow></mml:math></inline-formula> ms and <inline-formula><mml:math id="inf411"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, then converted into spiking rate <inline-formula><mml:math id="inf412"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and low-pass filtered with decay time <inline-formula><mml:math id="inf413"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to make it smooth. Each auxiliary neuron was trained on 4600 ms-long target rate function that was generated with a random initial condition.</p></sec><sec id="s4-3-5"><title><xref ref-type="fig" rid="fig6">Figure 6</xref> and <xref ref-type="fig" rid="fig7">7</xref></title><p>Networks consisting of <inline-formula><mml:math id="inf414"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> neurons with no initial connections and synaptic decay time <inline-formula><mml:math id="inf415"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> were trained to learn OU process with decay time <inline-formula><mml:math id="inf416"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and length <inline-formula><mml:math id="inf417"><mml:mi>T</mml:mi></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig6">Figure 6</xref>, target length was fixed to <inline-formula><mml:math id="inf418"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> ms while the time constants <inline-formula><mml:math id="inf419"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf420"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> were varied systematically from <inline-formula><mml:math id="inf421"><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> ms to <inline-formula><mml:math id="inf422"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> ms in log-scale. The trainings were repeated five times for each pair of <inline-formula><mml:math id="inf423"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf424"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to find the average performance. In <xref ref-type="fig" rid="fig7">Figure 7</xref>, the synaptic decay time was fixed to <inline-formula><mml:math id="inf425"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> ms and <inline-formula><mml:math id="inf426"><mml:mi>T</mml:mi></mml:math></inline-formula> was scanned from 250 ms to 5000 ms in increments of <inline-formula><mml:math id="inf427"><mml:mrow><mml:mn>250</mml:mn></mml:mrow></mml:math></inline-formula> ms, <inline-formula><mml:math id="inf428"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was scanned from 25 ms to 500 ms in increments of 25 ms, and <inline-formula><mml:math id="inf429"><mml:mi>N</mml:mi></mml:math></inline-formula> was scanned from 500 to 1000 in increments of 50.</p><p>To ensure that the network connectivity after training is sparse, synaptic learning occurred only on connections that were randomly selected with probability <inline-formula><mml:math id="inf430"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> prior to training. Recurrent connectivity was updated every <inline-formula><mml:math id="inf431"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> ms during training, learning rate was <inline-formula><mml:math id="inf432"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and training loop was repeated 30 times. The average Pearson correlation between the target functions and the evoked synaptic activity was calculated to measure the network performance after training.</p></sec></sec><sec id="s4-4"><title>Derivation of synaptic learning rules</title><p>Here, we derive the synaptic update rules for the synaptic drive and spiking rate trainings, <xref ref-type="disp-formula" rid="equ10 equ16">Equations 10 and 16</xref>. We use RLS algorithm (<xref ref-type="bibr" rid="bib27">Haykin, 1996</xref>) to learn target functions <inline-formula><mml:math id="inf433"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> defined on a time interval <inline-formula><mml:math id="inf434"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>, and the synaptic update occurs at evenly spaced time points, <inline-formula><mml:math id="inf435"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mn>...</mml:mn><mml:mo>≤</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>In the following derivation, super-script <inline-formula><mml:math id="inf436"><mml:mi>k</mml:mi></mml:math></inline-formula> on a variable <inline-formula><mml:math id="inf437"><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> implies that <inline-formula><mml:math id="inf438"><mml:mi>X</mml:mi></mml:math></inline-formula> is evaluated at <inline-formula><mml:math id="inf439"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and the sub-script <inline-formula><mml:math id="inf440"><mml:mi>i</mml:mi></mml:math></inline-formula> implies that <inline-formula><mml:math id="inf441"><mml:mi>X</mml:mi></mml:math></inline-formula> pertains to neuron <inline-formula><mml:math id="inf442"><mml:mi>i</mml:mi></mml:math></inline-formula>.</p><sec id="s4-4-1"><title>Training synaptic drive</title><p>The cost function measures the discrepancy between the target functions <inline-formula><mml:math id="inf443"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and the synaptic drive <inline-formula><mml:math id="inf444"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf445"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> at discrete time points <inline-formula><mml:math id="inf446"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi>W</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The Recursive Least Squares (RLS) algorithm solves the problem iteratively by finding a solution <inline-formula><mml:math id="inf447"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> to <xref ref-type="disp-formula" rid="equ20">Equation 20</xref> at <inline-formula><mml:math id="inf448"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and updating the solution at next time step <inline-formula><mml:math id="inf449"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We do not directly find the entire matrix <inline-formula><mml:math id="inf450"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, but find each row of <inline-formula><mml:math id="inf451"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, that is synaptic connections to each neuron <inline-formula><mml:math id="inf452"><mml:mi>i</mml:mi></mml:math></inline-formula> that minimize the discrepancy between <inline-formula><mml:math id="inf453"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf454"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then simply combine them to obtain <inline-formula><mml:math id="inf455"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>To find the <inline-formula><mml:math id="inf456"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> row of <inline-formula><mml:math id="inf457"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, we denote it by <inline-formula><mml:math id="inf458"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and rewrite the cost function for neuron <inline-formula><mml:math id="inf459"><mml:mi>i</mml:mi></mml:math></inline-formula> that evaluates the discrepancy between <inline-formula><mml:math id="inf460"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf461"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> on a time interval <inline-formula><mml:math id="inf462"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Calculating the gradient and setting it to 0, we obtain<disp-formula id="equ22"><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We express the equation concisely as follows.<disp-formula id="equ23"><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">q</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ24"><label>(22)</label><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">q</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To find <inline-formula><mml:math id="inf463"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> iteratively, we rewrite <xref ref-type="disp-formula" rid="equ24">Equation 22</xref> up to <inline-formula><mml:math id="inf464"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ25"><label>(23)</label><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">q</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and subtract <xref ref-type="disp-formula" rid="equ25 equ26">Equations 23 and 24</xref> to obtain<disp-formula id="equ26"><label>(24)</label><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The update rule for <inline-formula><mml:math id="inf465"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is then given by<disp-formula id="equ27"><label>(25)</label><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where the error term is<disp-formula id="equ28"><label>(26)</label><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The matrix inverse <inline-formula><mml:math id="inf466"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> can be computed iteratively<disp-formula id="equ29"><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>using the matrix identity<disp-formula id="equ30"><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-4-2"><title>Training spiking rate</title><p>To train the spiking rate of neurons, we approximate the spike train <inline-formula><mml:math id="inf467"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf468"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with its spiking rate <inline-formula><mml:math id="inf469"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf470"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the current-to-rate transfer function of theta neuron model. For constant input,<disp-formula id="equ31"><label>(27)</label><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msqrt><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mtext>where</mml:mtext><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and for noisy input<disp-formula id="equ32"><label>(28)</label><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>π</mml:mi></mml:mfrac><mml:msqrt><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf471"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is a good approximation of <inline-formula><mml:math id="inf472"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and has a smooth transition around <inline-formula><mml:math id="inf473"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we used <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>≡</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib8">Brunel and Latham, 2003</xref>).</p><p>If the synaptic update occurs at discrete time points, <inline-formula><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the objective is to find recurrent connectivity <inline-formula><mml:math id="inf477"><mml:mi>W</mml:mi></mml:math></inline-formula> that minimizes the cost function<disp-formula id="equ33"><label>(29)</label><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi>W</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As in training the synaptic drive, we optimize the following cost function to train each row of <inline-formula><mml:math id="inf478"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> that evaluates the discrepancy between the spiking rate of neuron <inline-formula><mml:math id="inf479"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the target spiking rate <inline-formula><mml:math id="inf480"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> over a time interval <inline-formula><mml:math id="inf481"><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ34"><label>(30)</label><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Calculating the gradient and setting it to zero, we obtain<disp-formula id="equ35"><label>(31)</label><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where<disp-formula id="equ36"><label>(32)</label><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></disp-formula>is the vector of filtered spike trains scaled by the gain of neuron <inline-formula><mml:math id="inf482"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note that when evaluating <inline-formula><mml:math id="inf483"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ36">Equation 32</xref>, we use the approximation <inline-formula><mml:math id="inf484"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>≈</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> to avoid introducing nonlinear functions of <inline-formula><mml:math id="inf485"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>To find an update rule for <inline-formula><mml:math id="inf486"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, we rewrite <xref ref-type="disp-formula" rid="equ35">Equation 31</xref> up to <inline-formula><mml:math id="inf487"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ37"><label>(33)</label><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and subtract <xref ref-type="disp-formula" rid="equ35 equ37">Equations 31 and 33</xref> and obtain<disp-formula id="equ38"><label>(34)</label><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ39"><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf488"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is updated by small increment, we can approximate the first line in <xref ref-type="disp-formula" rid="equ38">Equation 34</xref>,<disp-formula id="equ40"><label>(35)</label><mml:math id="m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></disp-formula>where we use the approximation <inline-formula><mml:math id="inf489"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as before to evaluate the derivative <inline-formula><mml:math id="inf490"><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>. Substituting <xref ref-type="disp-formula" rid="equ40">Equation 35</xref> to <xref ref-type="disp-formula" rid="equ38">Equation 34</xref>, we obtain the update rule<disp-formula id="equ41"><label>(36)</label><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where the error is<disp-formula id="equ42"><label>(37)</label><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and the correlation matrix of the normalized spiking activity is<disp-formula id="equ43"><label>(38)</label><mml:math id="m43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As shown above, the matrix inverse <inline-formula><mml:math id="inf491"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> can be computed iteratively,<disp-formula id="equ44"><mml:math id="m44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec></sec><sec id="s4-5"><title>Mean field description of the quasi-static dynamics</title><p>We say that a network is in a quasi-static state if the synaptic drive to a neuron changes sufficiently slower than the dynamical time scale of neurons and synapses. Here, we use a formalism developed by <xref ref-type="bibr" rid="bib10">Buice and Chow (2013)</xref> and derive <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>, which provide a mean field description of the synaptic and spiking rate dynamics of neurons in the quasi-static state.</p><p>First, we recast single neuron dynamic <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> in terms of the empirical distribution of neuron’s phase <inline-formula><mml:math id="inf492"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Since the number of neurons in the network is conserved, we can write the Klimontovich equation for the phase distribution<disp-formula id="equ45"><label>(39)</label><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf493"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The synaptic drive <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> can be written in the form<disp-formula id="equ46"><label>(40)</label><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>since <inline-formula><mml:math id="inf494"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf495"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for a theta neuron model. <xref ref-type="disp-formula" rid="equ45">Equation 39</xref>, together with <xref ref-type="disp-formula" rid="equ46">Equation 40</xref>, fully describes the network dynamics.</p><p>Next, to obtain a mean field description of the spiking dynamics, we take the ensemble average prepared with different initial conditions and ignore the contribution of higher order moments resulting from nonlinear terms <inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then we obtain the mean field equation<disp-formula id="equ47"><label>(41)</label><mml:math id="m47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ48"><label>(42)</label><mml:math id="m48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf497"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf498"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We note that the mean field <xref ref-type="disp-formula" rid="equ47 equ48">Equations 41 and 42</xref> provide a good description of the trained network dynamics because <inline-formula><mml:math id="inf499"><mml:mi>W</mml:mi></mml:math></inline-formula> learns over repeatedly training trials and starting at random initial conditions, to minimize the error between target trajectories and actual neuron activity.</p><p>Now, we assume that the temporal dynamics of synaptic drive and neuron phase can be suppressed in the quasi-static state,<disp-formula id="equ49"><label>(43)</label><mml:math id="m49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>ρ</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Substituting <xref ref-type="disp-formula" rid="equ49">Equation 43</xref> to <xref ref-type="disp-formula" rid="equ47">Equation 41</xref>, but allowing <inline-formula><mml:math id="inf500"><mml:mrow><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> to be time-dependent, we obtain the quasi-static solution of phase density<disp-formula id="equ50"><label>(44)</label><mml:math id="m50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msqrt><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:msqrt><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ51"><mml:math id="m51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ52"><label>(45)</label><mml:math id="m52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:msqrt><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>π</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>the current-to-rate transfer function of a theta neuron model. Substituting <xref ref-type="disp-formula" rid="equ49">Equation 43</xref> and <xref ref-type="disp-formula" rid="equ52">Equation 45</xref> to <xref ref-type="disp-formula" rid="equ48">Equation 42</xref>, we obtain a quasi-static solution of the synaptic drive<disp-formula id="equ53"><label>(46)</label><mml:math id="m53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>If we define the spiking rate of a neuron as <inline-formula><mml:math id="inf501"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, we immediately obtain<disp-formula id="equ54"><label>(47)</label><mml:math id="m54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s4-6"><title>Analysis of learning error</title><p>In this section, we identify and analyze two types of learning errors, assuming that for sufficiently heterogeneous targets, (1) the learning rule finds a recurrent connectivity <inline-formula><mml:math id="inf502"><mml:mi>W</mml:mi></mml:math></inline-formula> that can generate target patterns if the quasi-static condition holds, and (2) the mean field description of the spiking network dynamics is accurate due to the error function and repeated training trials. These assumptions imply that <xref ref-type="disp-formula" rid="equ53 equ54">Equations 46 and 47</xref> hold for the target patterns <inline-formula><mml:math id="inf503"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the trained <inline-formula><mml:math id="inf504"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We show that learning errors arise when our assumptions become inaccurate, hence the network dynamics described by <xref ref-type="disp-formula" rid="equ53 equ54">Equations 46 and 47</xref> deviate from the actual spiking network dynamics. As we will see, tracking error is prevalent if the target is not an exact solution of the mean field dynamics (i.e. quasi-static approximation fails), and the sampling error dominates if the discrete spikes do not accurately represent continuous targets (i.e. mean field approximation fails).</p><p>Suppose we are trying to learn a target <inline-formula><mml:math id="inf505"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> which obeys an Ornstein-Ulenbeck process<disp-formula id="equ55"><label>(48)</label><mml:math id="m55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>on a time interval <inline-formula><mml:math id="inf506"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf507"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are independent white noise with zero mean and variance <inline-formula><mml:math id="inf508"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The time constant <inline-formula><mml:math id="inf509"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> determines the temporal correlation of a target trajectory. In order for perfect training, the target dynamics (<xref ref-type="disp-formula" rid="equ55">Equation 48</xref>) needs to be compatible with the network dynamics (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>); in other words, there must exist a recurrent connectivity <inline-formula><mml:math id="inf510"><mml:mi>W</mml:mi></mml:math></inline-formula> such that the following equation<disp-formula id="equ56"><label>(49)</label><mml:math id="m56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>obtained by substituting the solution of <xref ref-type="disp-formula" rid="equ55">Equation 48</xref> into <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> must hold for <inline-formula><mml:math id="inf511"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf512"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> maps the synaptic drive <inline-formula><mml:math id="inf513"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to the entire spike train <inline-formula><mml:math id="inf514"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>It is very difficult to find <inline-formula><mml:math id="inf515"><mml:mi>W</mml:mi></mml:math></inline-formula> that may solve <xref ref-type="disp-formula" rid="equ56">Equation 49</xref> exactly since it requires fully understanding the solution space of a high-dimensional system of nonlinear ordinary differential equations. Instead, we assume that the target patterns are quasi-static and the learning rule finds a recurrent connectivity <inline-formula><mml:math id="inf516"><mml:mi>W</mml:mi></mml:math></inline-formula> that satisfies<disp-formula id="equ57"><label>(50)</label><mml:math id="m57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We then substitute <xref ref-type="disp-formula" rid="equ57">Equation 50</xref> to <xref ref-type="disp-formula" rid="equ56">Equation 49</xref> to estimate how the quasi-static mean field dynamics deviate from the actual spiking network dynamics. A straightforward calculation shows that<disp-formula id="equ58"><label>(51)</label><mml:math id="m58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula>where we define the tracking and sampling errors as<disp-formula id="equ59"><label>(52)</label><mml:math id="m59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>and<disp-formula id="equ60"><label>(53)</label><mml:math id="m60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>on the time interval <inline-formula><mml:math id="inf517"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><sec id="s4-6-1"><title>Tracking error</title><p>From its definition, <inline-formula><mml:math id="inf518"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> captures the deviation of the quasi-static solution (<xref ref-type="disp-formula" rid="equ57">Equation 50</xref>) from the exact solution of the mean field description obtained when <inline-formula><mml:math id="inf519"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf520"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> becomes large if the quasi-static condition (<xref ref-type="disp-formula" rid="equ49">Equation 43</xref>) fails and, in such network state, the synaptic dynamic is not able to ‘track’ the target patterns, thus learning is obstructed. In the following, we estimate <inline-formula><mml:math id="inf521"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in terms of two time scales <inline-formula><mml:math id="inf522"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf523"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>First, we take the Fourier transform of <xref ref-type="disp-formula" rid="equ59">Equation 52</xref> and obtain<disp-formula id="equ61"><label>(54)</label><mml:math id="m61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>i</mml:mtext><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>ω</mml:mi><mml:mo>⋅</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Next, normalize <inline-formula><mml:math id="inf524"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf525"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to estimate the tracking error for target patterns with different amplitudes, then compute the power of normalized tracking error.<disp-formula id="equ62"><label>(55)</label><mml:math id="m62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">Ω</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>track</mml:mtext></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf526"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the cut-off frequency of the power spectrum of a Gaussian process, <inline-formula><mml:math id="inf527"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>4</mml:mn><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>ω</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the tracking error scales with <inline-formula><mml:math id="inf528"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-6-2"><title>Sampling error</title><p><inline-formula><mml:math id="inf529"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> captures how the actual representation of target patterns in terms of spikes deviates from their continuous representation in terms of rate functions. In the following, we estimate <inline-formula><mml:math id="inf530"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in terms of <inline-formula><mml:math id="inf531"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf532"><mml:mi>N</mml:mi></mml:math></inline-formula> under the assumption that the continuous representation provides an accurate description of the target patterns.</p><p>We low-pass filtered <inline-formula><mml:math id="inf533"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to estimate the sampling error since the synaptic drive (i.e. the target variable in this estimate) is a <inline-formula><mml:math id="inf534"><mml:mi>W</mml:mi></mml:math></inline-formula> weighted sum of filtered spikes with width that scales with <inline-formula><mml:math id="inf535"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. If the spike trains of neurons are uncorrelated (i.e. cross product terms are negligible),<disp-formula id="equ63"><label>(56)</label><mml:math id="m63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow><mml:mrow><mml:mtext>filtered</mml:mtext></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf536"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the filtered spike train and <inline-formula><mml:math id="inf537"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the empirical estimate of mean spiking rate on a short time interval.</p><p>First, we calculate the fluctuation of filtered spike trains under the assumption that a neuron generates spikes sparsely, hence the filtered spikes are non-overlapping. Let <inline-formula><mml:math id="inf538"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> be a spike train of neuron <inline-formula><mml:math id="inf539"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and the filtered spike train <inline-formula><mml:math id="inf540"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then, the rate fluctuation of neuron <inline-formula><mml:math id="inf541"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is<disp-formula id="equ64"><mml:math id="m64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(57)</mml:mtext></mml:mtd><mml:mtd><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(58)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(59)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf542"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is summed over the average number of spikes, <inline-formula><mml:math id="inf543"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, generated in the time interval of length <inline-formula><mml:math id="inf544"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Next, to estimate the effect of network size on the sampling error, we examined <xref ref-type="disp-formula" rid="equ57">Equation 50</xref> and observed that <inline-formula><mml:math id="inf545"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This follows from that, for pre-determined target patterns, <inline-formula><mml:math id="inf546"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> regardless of the network size, hence <inline-formula><mml:math id="inf547"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> must scale with <inline-formula><mml:math id="inf548"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in order for both sides of the equation to be compatible. If the network is dense, that is the number of synaptic connections to a neuron is <inline-formula><mml:math id="inf549"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on average, then the sampling error scales as follows.<disp-formula id="equ65"><label>(60)</label><mml:math id="m65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mtext>sample</mml:mtext></mml:mrow><mml:mrow><mml:mtext>filtered</mml:mtext></mml:mrow></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mtext> </mml:mtext><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported [in part] by the Intramural Research Program of the NIH, The National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Writing—review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.37124.014</object-id><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-37124-transrepform-v2.pdf"/></supplementary-material><p>Example computer code that trains recurrent spiking networks is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/chrismkkim/SpikeLearning">https://github.com/chrismkkim/SpikeLearning</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/SpikeLearning">https://github.com/elifesciences-publications/SpikeLearning</ext-link>)</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>DePasquale</surname> <given-names>B</given-names></name><name><surname>Memmesheimer</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Building functional networks of spiking model neurons</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>350</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nn.4241</pub-id><pub-id pub-id-type="pmid">26906501</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Beer</surname> <given-names>C</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dynamics of dynamics: following the formation of a line attractor</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1805.09603">https://arxiv.org/abs/1805.09603</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertschinger</surname> <given-names>N</given-names></name><name><surname>Natschläger</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Real-time computation at the edge of chaos in recurrent neural networks</article-title><source>Neural Computation</source><volume>16</volume><fpage>1413</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1162/089976604323057443</pub-id><pub-id pub-id-type="pmid">15165396</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boerlin</surname> <given-names>M</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Denève</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1003258</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003258</pub-id><pub-id pub-id-type="pmid">24244113</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bourdoukan</surname> <given-names>R</given-names></name><name><surname>Deneve</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Enforcing balance allows local supervised learning in spiking recurrent networks</article-title><conf-name>Advances in Neural Information Processing SystemsAdvances in Neural Information Processing Systems 28 (NIPS 2015)</conf-name><fpage>982</fpage><lpage>990</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brendel</surname> <given-names>W</given-names></name><name><surname>Bourdoukan</surname> <given-names>R</given-names></name><name><surname>Vertechi</surname> <given-names>P</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Denéve</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning to represent signals spike by spike</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1703.03777">https://arxiv.org/abs/1703.03777</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title><source>Journal of Computational Neuroscience</source><volume>8</volume><fpage>183</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1023/A:1008925309027</pub-id><pub-id pub-id-type="pmid">10809012</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname> <given-names>N</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Firing rate of the noisy quadratic integrate-and-fire neuron</article-title><source>Neural Computation</source><volume>15</volume><fpage>2281</fpage><lpage>2306</lpage><pub-id pub-id-type="doi">10.1162/089976603322362365</pub-id><pub-id pub-id-type="pmid">14511522</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Is cortical connectivity optimized for storing information?</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>749</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1038/nn.4286</pub-id><pub-id pub-id-type="pmid">27065365</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buice</surname> <given-names>MA</given-names></name><name><surname>Chow</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic finite size effects in spiking neural networks</article-title><source>PLoS Computational Biology</source><volume>9</volume><elocation-id>e1002872</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002872</pub-id><pub-id pub-id-type="pmid">23359258</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname> <given-names>DV</given-names></name><name><surname>Maass</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>113</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nrn2558</pub-id><pub-id pub-id-type="pmid">19145235</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cash</surname> <given-names>S</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Input summation by cultured pyramidal neurons is linear and Position-Independent</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>10</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-01-00010.1998</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cash</surname> <given-names>S</given-names></name><name><surname>Yuste</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Linear summation of excitatory inputs by CA1 pyramidal neurons</article-title><source>Neuron</source><volume>22</volume><fpage>383</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(00)81098-3</pub-id><pub-id pub-id-type="pmid">10069343</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaisangmongkon</surname> <given-names>W</given-names></name><name><surname>Swaminathan</surname> <given-names>SK</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Computing by robust transience: how the Fronto-Parietal network performs sequential, Category-Based decisions</article-title><source>Neuron</source><volume>93</volume><fpage>1504</fpage><lpage>1517</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.002</pub-id><pub-id pub-id-type="pmid">28334612</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal complexity and heterogeneity of single-neuron activity in premotor and motor cortex</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>4235</fpage><lpage>4257</lpage><pub-id pub-id-type="doi">10.1152/jn.00095.2007</pub-id><pub-id pub-id-type="pmid">17376854</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Foster</surname> <given-names>JD</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Mark</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id><pub-id pub-id-type="pmid">22722855</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curto</surname> <given-names>C</given-names></name><name><surname>Degeratu</surname> <given-names>A</given-names></name><name><surname>Itskov</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Encoding binary neural codes in networks of threshold-linear neurons</article-title><source>Neural Computation</source><volume>25</volume><fpage>2858</fpage><lpage>2903</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00504</pub-id><pub-id pub-id-type="pmid">23895048</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denève</surname> <given-names>S</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Efficient codes and balanced networks</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>375</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1038/nn.4243</pub-id><pub-id pub-id-type="pmid">26906504</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denève</surname> <given-names>S</given-names></name><name><surname>Alemi</surname> <given-names>A</given-names></name><name><surname>Bourdoukan</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The brain as an efficient and robust adaptive learner</article-title><source>Neuron</source><volume>94</volume><fpage>969</fpage><lpage>977</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.016</pub-id><pub-id pub-id-type="pmid">28595053</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>DePasquale</surname> <given-names>B</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using Firing-Rate dynamics to train recurrent networks of spiking model neurons</article-title><source>Arxiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1601.07620">https://arxiv.org/abs/1601.07620</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DePasquale</surname> <given-names>B</given-names></name><name><surname>Cueva</surname> <given-names>CJ</given-names></name><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Escola</surname> <given-names>GS</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>full-FORCE: a target-based method for training recurrent networks</article-title><source>PLoS One</source><volume>13</volume><elocation-id>e0191527</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0191527</pub-id><pub-id pub-id-type="pmid">29415041</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ermentrout</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Type I membranes, phase resetting curves, and synchrony</article-title><source>Neural Computation</source><volume>8</volume><fpage>979</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.5.979</pub-id><pub-id pub-id-type="pmid">8697231</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname> <given-names>IR</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Gradient learning in spiking neural networks by dynamic perturbation of conductances</article-title><source>Physical Review Letters</source><volume>97</volume><elocation-id>e048104</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.97.048104</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gasparini</surname> <given-names>S</given-names></name><name><surname>Magee</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>State-dependent dendritic computation in hippocampal CA1 pyramidal neurons</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>2088</fpage><lpage>2100</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4428-05.2006</pub-id><pub-id pub-id-type="pmid">16481442</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harish</surname> <given-names>O</given-names></name><name><surname>Hansel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Asynchronous rate Chaos in spiking neuronal circuits</article-title><source>PLoS Computational Biology</source><volume>11</volume><elocation-id>e1004266</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004266</pub-id><pub-id pub-id-type="pmid">26230679</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haykin</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Adaptive Filter Theory</source><edition>third Ed</edition><publisher-loc>Upper Saddle River, NJ, USA</publisher-loc><publisher-name>Prentice-Hall, Inc</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoerzer</surname> <given-names>GM</given-names></name><name><surname>Legenstein</surname> <given-names>R</given-names></name><name><surname>Maass</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning</article-title><source>Cerebral cortex</source><volume>24</volume><fpage>677</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs348</pub-id><pub-id pub-id-type="pmid">23146969</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izhikevich</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Simple model of spiking neurons</article-title><source>IEEE Transactions on Neural Networks</source><volume>14</volume><fpage>1569</fpage><lpage>1572</lpage><pub-id pub-id-type="doi">10.1109/TNN.2003.820440</pub-id><pub-id pub-id-type="pmid">18244602</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jaeger</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Short Term Memory in Echo State Networks</source><volume>5</volume><publisher-name>GMD-German National Research Institute for Computer Science</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname> <given-names>H</given-names></name><name><surname>Haas</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication</article-title><source>Science</source><volume>304</volume><fpage>78</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1126/science.1091277</pub-id><pub-id pub-id-type="pmid">15064413</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>C</given-names></name><name><surname>Chow</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title><italic>Spike learning</italic></data-title><source>Github</source><version designator="693e790">693e790</version><ext-link ext-link-type="uri" xlink:href="https://github.com/chrismkkim/SpikeLearning">https://github.com/chrismkkim/SpikeLearning</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laje</surname> <given-names>R</given-names></name><name><surname>Buonomano</surname> <given-names>DV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Robust timing and motor patterns by taming Chaos in recurrent neural networks</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>925</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nn.3405</pub-id><pub-id pub-id-type="pmid">23708144</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Richmond</surname> <given-names>BJ</given-names></name><name><surname>Nelson</surname> <given-names>PG</given-names></name><name><surname>Nirenberg</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Intrinsic dynamics in neuronal networks. I. Theory</article-title><source>Journal of Neurophysiology</source><volume>83</volume><fpage>808</fpage><lpage>827</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.83.2.808</pub-id><pub-id pub-id-type="pmid">10669496</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>N</given-names></name><name><surname>Chen</surname> <given-names>T-W</given-names></name><name><surname>Guo</surname> <given-names>ZV</given-names></name><name><surname>Gerfen</surname> <given-names>CR</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><data-title>Extracellular recordings from anterior lateral motor cortex (alm) neurons of adult mice performing a tactile decision behavior</data-title><source>Collaborative Research in Computational Neuroscience - Data Sharing</source><ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/motor-cortex/alm-1">https://crcns.org/data-sets/motor-cortex/alm-1</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>N</given-names></name><name><surname>Chen</surname> <given-names>TW</given-names></name><name><surname>Guo</surname> <given-names>ZV</given-names></name><name><surname>Gerfen</surname> <given-names>CR</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A motor cortex circuit for motor planning and movement</article-title><source>Nature</source><volume>519</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature14178</pub-id><pub-id pub-id-type="pmid">25731172</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>London</surname> <given-names>M</given-names></name><name><surname>Roth</surname> <given-names>A</given-names></name><name><surname>Beeren</surname> <given-names>L</given-names></name><name><surname>Häusser</surname> <given-names>M</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Sensitivity to perturbations in vivo implies high noise and suggests rate coding in cortex</article-title><source>Nature</source><volume>466</volume><fpage>123</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1038/nature09086</pub-id><pub-id pub-id-type="pmid">20596024</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname> <given-names>W</given-names></name><name><surname>Natschläger</surname> <given-names>T</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural Computation</source><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname> <given-names>W</given-names></name><name><surname>Joshi</surname> <given-names>P</given-names></name><name><surname>Sontag</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Computational aspects of feedback in neural circuits</article-title><source>PLoS Computational Biology</source><volume>3</volume><elocation-id>e165</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0020165</pub-id><pub-id pub-id-type="pmid">17238280</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martí</surname> <given-names>D</given-names></name><name><surname>Brunel</surname> <given-names>N</given-names></name><name><surname>Ostojic</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Correlations between synapses in pairs of neurons slow down dynamics in randomly connected neural networks</article-title><source>Physical Review E</source><volume>97</volume><elocation-id>062314</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.97.062314</pub-id><pub-id pub-id-type="pmid">30011528</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname> <given-names>F</given-names></name><name><surname>Ostojic</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Linking connectivity, dynamics and computations in recurrent neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1711.09672">https://arxiv.org/abs/1711.09672</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Memmesheimer</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Quantitative prediction of intermittent high-frequency oscillations in neural networks with supralinear dendritic interactions</article-title><source>PNAS</source><volume>107</volume><fpage>11092</fpage><lpage>11097</lpage><pub-id pub-id-type="doi">10.1073/pnas.0909615107</pub-id><pub-id pub-id-type="pmid">20511534</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Memmesheimer</surname> <given-names>RM</given-names></name><name><surname>Timme</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Non-additive coupling enables propagation of synchronous spiking activity in purely random networks</article-title><source>PLoS Computational Biology</source><volume>8</volume><elocation-id>e1002384</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002384</pub-id><pub-id pub-id-type="pmid">22532791</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miconi</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks</article-title><source>eLife</source><volume>6</volume><elocation-id>e20899</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.20899</pub-id><pub-id pub-id-type="pmid">28230528</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monteforte</surname> <given-names>M</given-names></name><name><surname>Wolf</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Dynamic flux tubes form reservoirs of stability in neuronal circuits</article-title><source>Physical Review X</source><volume>2</volume><elocation-id>041007</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.2.041007</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nevian</surname> <given-names>T</given-names></name><name><surname>Larkum</surname> <given-names>ME</given-names></name><name><surname>Polsky</surname> <given-names>A</given-names></name><name><surname>Schiller</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Properties of basal dendrites of layer 5 pyramidal neurons: a direct patch-clamp recording study</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>206</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1038/nn1826</pub-id><pub-id pub-id-type="pmid">17206140</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicola</surname> <given-names>W</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Supervised learning in spiking neural networks with FORCE training</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>2208</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01827-3</pub-id><pub-id pub-id-type="pmid">29263361</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostojic</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>594</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1038/nn.3658</pub-id><pub-id pub-id-type="pmid">24561997</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pastalkova</surname> <given-names>E</given-names></name><name><surname>Itskov</surname> <given-names>V</given-names></name><name><surname>Amarasingham</surname> <given-names>A</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Internally generated cell assembly sequences in the rat hippocampus</article-title><source>Science</source><volume>321</volume><fpage>1322</fpage><lpage>1327</lpage><pub-id pub-id-type="doi">10.1126/science.1159775</pub-id><pub-id pub-id-type="pmid">18772431</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Recurrent network models of sequence generation and memory</article-title><source>Neuron</source><volume>90</volume><fpage>128</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.009</pub-id><pub-id pub-id-type="pmid">26971945</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remington</surname> <given-names>ED</given-names></name><name><surname>Narain</surname> <given-names>D</given-names></name><name><surname>Hosseini</surname> <given-names>EA</given-names></name><name><surname>Jazayeri</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible sensorimotor computations through rapid reconfiguration of cortical dynamics</article-title><source>Neuron</source><volume>98</volume><fpage>1005</fpage><lpage>1019</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.020</pub-id><pub-id pub-id-type="pmid">29879384</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname> <given-names>A</given-names></name><name><surname>de la Rocha</surname> <given-names>J</given-names></name><name><surname>Bartho</surname> <given-names>P</given-names></name><name><surname>Hollender</surname> <given-names>L</given-names></name><name><surname>Parga</surname> <given-names>N</given-names></name><name><surname>Reyes</surname> <given-names>A</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The asynchronous state in cortical circuits</article-title><source>Science</source><volume>327</volume><fpage>587</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1126/science.1179850</pub-id><pub-id pub-id-type="pmid">20110507</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivkind</surname> <given-names>A</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Local dynamics in trained recurrent neural networks</article-title><source>Physical Review Letters</source><volume>118</volume><elocation-id>258101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.118.258101</pub-id><pub-id pub-id-type="pmid">28696758</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenbaum</surname> <given-names>R</given-names></name><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name><name><surname>Rubin</surname> <given-names>JE</given-names></name><name><surname>Doiron</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The spatial structure of correlated neuronal variability</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>107</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1038/nn.4433</pub-id><pub-id pub-id-type="pmid">27798630</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Crisanti</surname> <given-names>A</given-names></name><name><surname>Sommers</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Chaos in random neural networks</article-title><source>Physical Review Letters</source><volume>61</volume><fpage>259</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.61.259</pub-id><pub-id pub-id-type="pmid">10039285</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural computation</source><volume>25</volume><pub-id pub-id-type="doi">10.1162/NECO_a_00409</pub-id><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thalmeier</surname> <given-names>D</given-names></name><name><surname>Uhlmann</surname> <given-names>M</given-names></name><name><surname>Kappen</surname> <given-names>HJ</given-names></name><name><surname>Memmesheimer</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Learning Universal Computations with Spikes</article-title><source>PLoS computational biology</source><volume>12</volume><elocation-id>e1004895</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004895</pub-id><pub-id pub-id-type="pmid">27309381</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vreeswijk</surname> <given-names>C</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Vreeswijk</surname> <given-names>Cvan</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><volume>274</volume><fpage>1724</fpage><lpage>1726</lpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1724</pub-id><pub-id pub-id-type="pmid">8939866</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Gamma oscillation by synaptic inhibition in a hippocampal interneuronal network model</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>6402</fpage><lpage>6413</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-20-06402.1996</pub-id><pub-id pub-id-type="pmid">8815919</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>J</given-names></name><name><surname>Narain</surname> <given-names>D</given-names></name><name><surname>Hosseini</surname> <given-names>EA</given-names></name><name><surname>Jazayeri</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible timing by temporal scaling of cortical responses</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>102</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0028-6</pub-id><pub-id pub-id-type="pmid">29203897</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname> <given-names>OL</given-names></name><name><surname>Lee</surname> <given-names>DD</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Short-term memory in orthogonal neural networks</article-title><source>Physical Review Letters</source><volume>92</volume><elocation-id>148102</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.92.148102</pub-id><pub-id pub-id-type="pmid">15089576</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.37124.016</article-id><title-group><article-title>Decision letter</article-title></title-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Learning recurrent dynamics in spiking networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by Timothy Behrens as the Senior Editor and two reviewers, one of whom is the Reviewing Editor. One of the reviewers, Brian DePasquale, has agreed to reveal his identity.</p><p>The Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The focus of most (maybe all?) research on RNNs has been on output: how do you train recurrent networks to produce a low dimensional (compared to the dimensionality of the network) target function? This paper asks a different question: how do you train recurrent networks so that neurons within the network produce their own target function? Here the target can be either firing rate or synaptic drive. What's interesting about this paper is that the authors give the conditions under which training is possible. These rules provide insight into the limits of performance of spiking RNNs, something that has, I believe, been lacking in the field.</p><p>Essential revisions:</p><p>Reviewers A and B's full reviews, along with the subsequent discussion are included below. You should focus on making A happy (which is pretty easy, since he/she had only minor comments). However, when you revise the paper, you probably should keep B's review in mind – his/her interpretation of your paper may be shared by others.</p><p><italic>Reviewer A comments:</italic> </p><p>The focus of most (maybe all?) research on RNNs has been on output: how do you train recurrent networks to produce a low dimensional (compared to the dimensionality of the network) target function? This paper asks a different question: how do you train recurrent networks so that neurons within the network produce their own target function? Here the target can be either firing rate or synaptic drive.</p><p>What's interesting about this paper is that the authors give the conditions under which training is possible:</p><p>- The target functions have to be sufficiently diverse..</p><p>- A sufficiently large number of neurons have to be associated with targets.</p><p>- The synaptic time constant can't be too long compared to the timescale of the target functions.</p><p>- the synaptic time constant can't be too short compared to 1/N (N is the number of neurons in the network).</p><p>- The training period can't be too long compared to N.</p><p>These rules provide insight into the limits of performance of spiking RNNs, something that has, I believe, been lacking in the field. [Minor comments not shown.]</p><p><italic>Reviewer B comments:</italic> </p><p>The authors present an interesting study examining the capability of recurrent networks of QIF neurons for learning various types of signals. They apply a RLS learning rule so that the network produces various periodic functions, filtered noise, the activity of chaotic networks, and experimental data. They present evidence to support their contention that two key conditions need be met for learning: (1) including a sufficiently rich set of target functions; (2) appropriately matched synaptic time-scale relative to the target dynamics. Additionally, a mean field analysis was performed and shown to be consistent with dynamics after learning.</p><p>While a handful of curious findings are presented, for the following two reasons I do not believe the work, as presented, is suitable for publication in this journal.</p><p>First, the examples, learning conditions and supporting experiments do not constitute a significant advance over previous work. While I cannot refer to prior work that has studied the ability to produce the specific signals presented, the authors do not properly motivate the importance of these particular signals. Prior publications that train RNNs (spiking too) typically focus on constructing networks that can perform tasks, where the motivation for the signals they construct is obvious: they use signals necessary for performing computations. For three of the examples (rate chaos, OU noise and spiking chaos) it's not clear what computation these networks might perform after learning (in fact, the primary motivation for stabilizing rate chaos, e.g. in FORCE learning, was so that the network COULD produce a computation after learning). In the case of the work of Laje and Buonomano (which the authors claim to extend to spiking networks), they sought to stabilize chaotic rate trajectories as a model for understanding timing tasks; here the authors do not present any use for these dynamical patterns, just that they can be learned.</p><p>Second, the presented work is simultaneously too technical (and therefore too specific) and too vague to be of significant interest to the general neuroscience community. The mean field analysis is interesting and stands out when compared to other published works, which have not extensively studied how analytic solutions compare to simulations in trained networks. Nevertheless, this is a minor, technical point, not likely to be interesting to most neuroscientists (unless the authors can explain why this is interesting).</p><p>While the question of universal computation is of general interest, the presented results are not sufficiently rigorous to match the claims. While they claim that previous work has been impressive but not sufficient to showcase the full richness of what can be expressed by spiking RNNs, the authors claim to learn &quot;arbitrary&quot; and &quot;near universal&quot; dynamics but only show examples (as previous work has) from a rather small set of potential dynamics. If this paper were to be published, the authors would need to clarify what they mean by this to accurately reflect the presented results or include a proof of the universality they are claiming to show. Another question of general interest is what constitutes a &quot;sufficient basis&quot; for learning but here again the authors only provide a few examples that are not shown to extend beyond the direct application studied. This approach is inferior to previous methods, since no prescription is offered for establishing sufficiency other than generating additional random signals; in previous work where specific computations were examined, the only signals required were those necessary for performing the computation and these could be &quot;bootstrapped&quot; by exploiting knowledge about the train-ability of firing rate networks. The authors offer a bold claim (identifying the sufficiency of a rich basis) with pretty weak support (just keep adding random signals).</p><p>I conclude by highlighting the most interesting and promising result: training with neural data. That the training required auxiliary dynamics is interesting and provides a prediction of a set of dynamical patterns that should exist in the brain in order for the network to perform the task it did. Expounding on these results would yield a very interesting finding, albeit beyond the scope of a simple revision to the present work.</p><p><italic>A's response to B's comments (during discussion phase):</italic> </p><p>My main feeling is that they are looking at a very different problem than anybody else: they're training every single neuron (or a large fraction of neurons) to produce target functions, with different target functions (but similar timescales) for each neuron. This is, in my opinion, completely unrealistic – it's much more realistic to train networks to produce relatively low dimensional output patterns. But what I thought was interesting was that the paper gave bounds on performance in general in terms of timescales of the network and the target output of each neuron. If you can't produce target functions with a particular timescale when you can train each neuron individually, I would think that you certainly can't train the network as a whole to produce target functions with that timescale. Which is a point they might have emphasized more, but that would be easy to fix. But maybe I'm interested simply because it was something I always wondered about.</p><p>With this view, your first objection, which is that they didn't consider interesting functions, becomes, I think, moot. Your second objection is that the work is too technical and too vague. I agree the mean field analysis is very technical, and will be followed by only a handful of people. But it doesn't seem to be an integral part of the paper. And I think the rest is pretty easy to follow. Not sure what you meant by too vague – could you expand?</p><p>You also say that their results are not sufficiently rigorous. However, if they can make the arguments rigorous, would you be satisfied, at least about the rigorousness (but not necessarily about the first two points)?</p><p><italic>B's response to A:</italic> </p><p>I disagree with you that they are looking at a different problem than anybody else. However, none of this works addresses the conditions for learning, I agree with you there.</p><p>I agree with you that understanding the limits of learning in RNNs is interesting and lacking, but to me, their results related to that point only account for a fraction of the full paper. As written, that point is embedded within a range of other findings that are unmotivated (e.g. the various other signals they train networks to perform that do not support their results about the limits of learning), incomplete (using data to train networks, as I wrote in my review) and that do not ultimately support the point about learnability. However, if they were to remove these other results, the point on learnability alone would not constitute a full paper.</p><p>In my view, they introduce two main conditions for learning: that the synaptic timescale should roughly match the targets and that the targets should be &quot;suffciently rich&quot;. The second condition highlights my concerns about the vagueness and rigorness of the results. They illustrate the need for sufficient richness by removing targets from some of the training sets they used and show that these network cannot learn (Figure 3). While this is anecdotally interesting, I do not believe that illustrating that this is true in a handful of cases is sufficient to say anything general about the conditions necessary for learning. At best, this is good enough to make some pretty hand-waving arguments about the need for an overcomplete basis, which is precisely what they do in Section 2. Again, it's not that I don't find this interesting, I just don't seem much substance there.</p><p>Their first condition for learning is definitely much more rigorously addressed (the simulations and calculations of Figure 6 are nice) but I find this neither surprising or groundbreaking (you can't learn slow signals without slow synapses, or fast signals without fast synapses....) I think this point could be stated much more simply without the need for the various training targets they used (chaotic rate models and stochastic noise) which again, strengthens my resolve that much of what they do feels unmotivated. Even though Figure 6 being nicely done, 1 figure is not enough for a paper.</p><p>Finally, I think the tasks they are learning are perfectly &quot;interesting&quot;, just unmotivated.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.37124.017</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>We would like to thank the reviewers for their valuable comments. We have made the code for learning recurrent spiking dynamics available on-line at https://github.com/chrismkkim/SpikeLearning</p><p>We will use following summary as reference points when responding to the reviewers’ comments:</p><p>S1) As Reviewer A pointed out, most research on reservoir computing in RNNs has focused on training the network output to perform a specific computation [Sussillo and Abbott, 2009; Laje and Buonomano, 2013; Rajan, Harvey, and Tank, 2016; DePasquale et al., 2018]. The implicit idea is that the RNN’s job is to provide a reservoir of trajectories that can be combined linearly with trainable weights to produce a desired output. In contrast, we ask the question of whether the output neurons are necessary at all. What if the RNN could be trained such that the output and reservoir were one and the same? And can this be done in a spiking network? None of these questions had answers before our work.</p><p>S2) In order for the reservoir computing scheme to work the RNN needs to be rich enough and repeatable, which is nontrivial to attain [Sussillo and Abbott, 2009] achieves this by stabilizing an inherently chaotic network with feedback or training, [Laje and Buonomano, 2013] trains the weights of the RNN to stabilize specific chaotic trajectories, [Rajan, Harvey, and Tank, 2016] used sequential activity derived from experimental data, and [DePasquale et al., 2018] used network activity extracted from a “target network”.</p><p>S3) We identified two conditions under which learning is successful for any arbitrary signal: (1) the synaptic time scales must be fast enough and the network size must be large enough and (2) the set of target functions must be sufficiently heterogeneous.</p><p>Response to essential revisions:</p><p>Reviewer B commented that the set of target functions a network learns (e.g. OU noise) was not motivated for performing computations. We respectfully disagree. As stated in (S1), our goal was to show that an RNN could produce arbitrary patterns such that there would be no need to distinguish between the RNN and the output neurons. The question we address is specifically the computational capability of an RNN in terms of the patterns it can produce. As stated in (S2), previous studies considered only specific forms of target functions for the RNN, hence it remains unknown whether a spiking network can learn “arbitrary” functions. Laje and Buonomano even stated (see their Online Methods) that “choosing innate trajectories from another network did not produce effective training”, alluding that recurrent networks may not be able to learn arbitrary functions. Our examples show that this is not the case and that individual neurons may be able to perform universal computations. The target functions considered in Figure 3 (periodic functions, rate chaos, OU noise) provide a few examples of random functions that a network can learn. We felt that this provided a generic selection of arbitrary functions, and also showed that the network is able to reproduce firing patterns of an animal performing a task. We revised the introduction of the original manuscript and added additional explanation in the first paragraph of discussion to make it clear that our goal was to investigate the computational capability of RNNs to encode recurrent spiking dynamics.</p><p>We also wish to correct Reviewer B’s interpretation of the first condition for learning (small sampling and tracking errors) since his statement “you can’t learn slow signals without slow synapses, or fast signals without fast synapses” is not correct and we apologize for not making this point more clear in our original text. It is possible to learn slow signals without slow synapses. As stated in (S3), the first condition requires that synapses to be fast enough (i.e. small tracking error,etrack~ts/tc) and network size to be large enough (i.e. small sampling error,esample~1/tsN), then the network can learn any signals, in particular the slow ones. As long as the synapse is faster than the target (i.e. their time scales need not be comparable), and there are plenty of neurons in the network to produce continuous target signals, forming appropriate recurrent connections can compensate for the short synaptic time scale and allows the network to generate slow signals. This is the main conclusion of the error analysis and is stated immediately after Equation 3. Our simulation results also support the error analysis: in Figure 6A, the parameter regime above the diagonal line (i.e. slow signal and fast synapses, <italic>t<sub>c</sub> &gt; t<sub>s</sub></italic>) learns successfully, and similarly learning is successful if <italic>t<sub>s</sub>/t<sub>c</sub> &lt;</italic> 1 in Figure 6E. Such slow dynamics, consistent with our results, have been studied in random rate networks [Sompolinsky, Crisanti and Sommers, 1988], random spiking networks [Ostojic, 2014] and random rate networks when reciprocal connections are over-represented [Mart´ı, Brunel and Ostojic, 2018]. The example shown in Figure 6B also demonstrates that networks with fast synapses (<italic>τ<sub>s</sub> </italic>= 30ms) can learn slow signals (<italic>τ<sub>c</sub> </italic>= 100ms). We pointed out these facts in the third and fourth paragraphs of Results section. What may have led to the confusion is that for a fixed network size, synapses cannot be too fast or too slow, as there is a trade off between tracking and sampling error (See inverted “U”-curves in Figure 6C,D). However, as we also show, the sampling error can be overcome with more neurons so synapses can be arbitrarily fast as long as the network can be arbitrarily large (Figure 6D).</p><p>The second condition for learning is that the set of target functions need to be sufficiently heterogeneous. Reviewer B criticized that our definition of the second condition is vague and lacks rigor since we only provided anecdotal examples in Figure 3. However, we actually discussed the mathematical basis for the second condition in subsection “Learning capacity increases with network size” in the original manuscript. We now realize that our original treatment was not convincing and have strived to improve it so that it better articulates our argument. Here we present the main ideas of the mathematical argument (See the revised manuscript for details).</p><p>The network dynamics under the quasi-static condition can be expressed in a matrix form <italic>U</italic> = <italic>WV</italic> where <italic>U</italic> and <italic>V</italic> ≡ <italic>φ(U</italic> + <italic>I</italic>) are <italic>N</italic> × <italic>P</italic> matrices. Here, the <italic>i</italic><sup>th</sup> row of <italic>U</italic> and <italic>V</italic> is the time discretized trajectory of neuron <italic>i</italic>’s synaptic drive and spiking rate, respectively. The question of successful learning is reduced to analyzing the solution set of <italic>W</italic> to this system of equations. In principle, learning is possible when the rows of <italic>U</italic> are spanned by the rows of <italic>V,</italic> i.e. target functions can be self-consistently generated by firing rate patterns induced by the targets. We define target functions to be sufficiently heterogeneous if the firing rate matrix <italic>V</italic> is full-rank because under such condition that the image of <italic>V</italic> encompasses the largest possible space.</p><p>We show that rank(<italic>V</italic>) being maximal is sufficient for an exact or approximate solution <italic>W</italic> to exist. When <italic>N</italic> ≥ <italic>P</italic>, the maximal rank(<italic>V</italic>) implies that the rows of <italic>V</italic> span the entire Euclidean space R<italic><sup>P</sup></italic>, of which the target functions (i.e. rows of <italic>U</italic>) are elements. Equivalently, we can say that the number of unknowns (<italic>N</italic>) exceeds or equals the number of independent equations (<italic>P</italic>). Therefore, multiple perfect solutions are possible, and the regularization term is required for the learning scheme to converge to one solution. When <italic>N &lt; P</italic>, the maximal rank(<italic>V</italic>) implies that the linearly independent rows of <italic>V</italic> span an <italic>N</italic>-dimensional subspace of R<italic><sup>P</sup></italic>. The number of unknowns (<italic>N</italic>) is less than the number of equations (<italic>P</italic>), so perfect learning is not possible and we can only find an approximate regression solution <italic>W</italic> = <italic>UV<sup>T</sup>(VV<sup>T</sup></italic>)<sup>−1</sup> where the inverse of <italic>VV<sup>T</sup> </italic>exists because rank(<italic>V</italic>) is maximal. If rank(<italic>V</italic>) is not maximal, a solution can still exists if rows of <italic>U</italic> are close to the subspace spanned by <italic>V.</italic> However, in this case the success depend on the specific choice of target functions because the dimension of the subspace spanned by <italic>V</italic> is strictly less than <italic>P</italic>, so whether the rows of <italic>U</italic> are contained in or close to this subspace is determined by the geometry of the subspace. This shows why increasing pattern heterogeneity, which makes the rows of <italic>V</italic> independent and the rank higher, is beneficial for learning. Conversely, if a large number of neurons is trained on the same target, it becomes increasingly difficult to develop the target patterns with limited basis functions. Given that this mathematical explanation is easily missed, we discussed the underlying mathematics in detail after introducing the second condition for learning in the fourth paragraph of Results section in the revised manuscript.</p></body></sub-article></article>