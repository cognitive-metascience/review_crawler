<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">24763</article-id><article-id pub-id-type="doi">10.7554/eLife.24763</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Contributions of local speech encoding and functional connectivity to audio-visual speech perception</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-49137"><name><surname>Giordano</surname><given-names>Bruno L</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7002-0486</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-32067"><name><surname>Ince</surname><given-names>Robin A A</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8427-0507</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-21934"><name><surname>Gross</surname><given-names>Joachim</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-32069"><name><surname>Schyns</surname><given-names>Philippe G</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="other" rid="par-6"/><xref ref-type="other" rid="par-7"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-40025"><name><surname>Panzeri</surname><given-names>Stefano</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1700-8909</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-8"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-5124"><name><surname>Kayser</surname><given-names>Christoph</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7362-5704</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-9"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Institut de Neurosciences de la Timone UMR 7289, Aix Marseille Université – Centre National de la Recherche Scientifique</institution>, <addr-line><named-content content-type="city">Marseille</named-content></addr-line>, <country>France</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Institute of Neuroscience and Psychology</institution>, <institution>University of Glasgow</institution>, <addr-line><named-content content-type="city">Glasgow</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Neural Computation Laboratory, Center for Neuroscience and Cognitive Systems</institution>, <institution>Istituto Italiano di Tecnologia</institution>, <addr-line><named-content content-type="city">Rovereto</named-content></addr-line>, <country>Italy</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schroeder</surname><given-names>Charles E</given-names></name><role>Reviewing editor</role><aff><institution>Columbia University College of Physicians and Surgeons</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><email>Bruno.Giordano@glasgow.ac.uk</email> (BLG);</corresp><corresp id="cor2"><email>christoph.kayser@glasgow.ac.uk</email> (CK)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>07</day><month>06</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>6</volume><elocation-id>e24763</elocation-id><history><date date-type="received"><day>31</day><month>12</month><year>2016</year></date><date date-type="accepted"><day>07</day><month>05</month><year>2017</year></date></history><permissions><copyright-statement>© 2017, Giordano et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Giordano et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-24763-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.24763.001</object-id><p>Seeing a speaker’s face enhances speech intelligibility in adverse environments. We investigated the underlying network mechanisms by quantifying local speech representations and directed connectivity in MEG data obtained while human participants listened to speech of varying acoustic SNR and visual context. During high acoustic SNR speech encoding by temporally entrained brain activity was strong in temporal and inferior frontal cortex, while during low SNR strong entrainment emerged in premotor and superior frontal cortex. These changes in local encoding were accompanied by changes in directed connectivity along the ventral stream and the auditory-premotor axis. Importantly, the behavioral benefit arising from seeing the speaker’s face was not predicted by changes in local encoding but rather by enhanced functional connectivity between temporal and inferior frontal cortex. Our results demonstrate a role of auditory-frontal interactions in visual speech representations and suggest that functional connectivity along the ventral pathway facilitates speech comprehension in multisensory environments.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.001">http://dx.doi.org/10.7554/eLife.24763.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.24763.002</object-id><title>eLife digest</title><p>When listening to someone in a noisy environment, such as a cocktail party, we can understand the speaker more easily if we can also see his or her face. Movements of the lips and tongue convey additional information that helps the listener’s brain separate out syllables, words and sentences. However, exactly where in the brain this effect occurs and how it works remain unclear.</p><p>To find out, Giordano et al. scanned the brains of healthy volunteers as they watched clips of people speaking. The clarity of the speech varied between clips. Furthermore, in some of the clips the lip movements of the speaker corresponded to the speech in question, whereas in others the lip movements were nonsense babble. As expected, the volunteers performed better on a word recognition task when the speech was clear and when the lips movements agreed with the spoken dialogue.</p><p>Watching the video clips stimulated rhythmic activity in multiple regions of the volunteers’ brains, including areas that process sound and areas that plan movements. Speech is itself rhythmic, and the volunteers’ brain activity synchronized with the rhythms of the speech they were listening to. Seeing the speaker’s face increased this degree of synchrony. However, it also made it easier for sound-processing regions within the listeners’ brains to transfer information to one other. Notably, only the latter effect predicted improved performance on the word recognition task. This suggests that seeing a person’s face makes it easier to understand his or her speech by boosting communication between brain regions, rather than through effects on individual areas.</p><p>Further work is required to determine where and how the brain encodes lip movements and speech sounds. The next challenge will be to identify where these two sets of information interact, and how the brain merges them together to generate the impression of specific words.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.002">http://dx.doi.org/10.7554/eLife.24763.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>auditory system</kwd><kwd>magnetoencephalography</kwd><kwd>audio-visual speech entrainment</kwd><kwd>directed functional connectivity</kwd><kwd>inferior frontal gyrus</kwd><kwd>premotor cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/L027534/1</award-id><principal-award-recipient><name><surname>Kayser</surname><given-names>Christoph</given-names></name><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><award-group id="par-9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ERC-2014-CoG grant No 646657</award-id><principal-award-recipient><name><surname>Kayser</surname><given-names>Christoph</given-names></name></principal-award-recipient></award-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/M009742/1</award-id><principal-award-recipient><name><surname>Giordano</surname><given-names>Bruno L</given-names></name><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Joint Senior Investigator Grant No 098433</award-id><principal-award-recipient><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Senior Investigator Grant 107802/Z/15/Z</award-id><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Senior Investigator Award, UK; 107802</award-id><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>Multidisciplinary University Research Initiative USA/UK; 172046-01</award-id><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name></principal-award-recipient></award-group><award-group id="par-7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000005</institution-id><institution>U.S. Department of Defense</institution></institution-wrap></funding-source><award-id>Multidisciplinary University Research Initiative USA/UK; 172046-01</award-id><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name></principal-award-recipient></award-group><award-group id="par-8"><funding-source><institution-wrap><institution>Autonomous Province of Trento</institution></institution-wrap></funding-source><award-id>Grandi Progetti 2012 Characterizing and Improving Brain Mechanisms of Attention-ATTEND</award-id><principal-award-recipient><name><surname>Panzeri</surname><given-names>Stefano</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Seeing a speaker's face aids comprehension by facilitating functional connectivity between the temporal and frontal lobes.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>When communicating in challenging acoustic environments we profit tremendously from visual cues arising from the speakers face. Movements of the lips, tongue or the eyes convey significant information that can boost speech intelligibility and facilitate the attentive tracking of individual speakers (<xref ref-type="bibr" rid="bib89">Ross et al., 2007</xref>; <xref ref-type="bibr" rid="bib99">Sumby and Pollack, 1954</xref>). This multisensory benefit is strongest for continuous speech, where visual signals provide temporal markers to segment words or syllables, or provide linguistic cues (<xref ref-type="bibr" rid="bib40">Grant and Seitz, 1998</xref>). Previous work has identified the synchronization of brain rhythms between interlocutors as a potential neural mechanism underlying the visual enhancement of intelligibility (<xref ref-type="bibr" rid="bib44">Hasson et al., 2012</xref>; <xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>; <xref ref-type="bibr" rid="bib80">Peelle and Sommers, 2015</xref>; <xref ref-type="bibr" rid="bib81">Pickering and Garrod, 2013</xref>; <xref ref-type="bibr" rid="bib93">Schroeder et al., 2008</xref>). Both acoustic and visual speech signals exhibit pseudo-rhythmic temporal structures at prosodic and syllabic rates (<xref ref-type="bibr" rid="bib24">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib96">Schwartz and Savariaux, 2014</xref>). These regular features can entrain rhythmic activity in the observer’s brain and facilitate perception by aligning neural excitability with acoustic or visual speech features (<xref ref-type="bibr" rid="bib38">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib68">Mesgarani and Chang, 2012</xref>; <xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>; <xref ref-type="bibr" rid="bib79">Peelle and Davis, 2012</xref>; <xref ref-type="bibr" rid="bib94">Schroeder and Lakatos, 2009</xref>; <xref ref-type="bibr" rid="bib93">Schroeder et al., 2008</xref>; <xref ref-type="bibr" rid="bib104">van Wassenhove, 2013</xref>; <xref ref-type="bibr" rid="bib113">Zion Golumbic et al., 2013a</xref>). While this model predicts the visual enhancement of speech encoding in challenging multisensory environments, the network organization of multisensory speech encoding remains unclear.</p><p>Previous work has implicated many brain regions in the visual enhancement of speech, including superior temporal (<xref ref-type="bibr" rid="bib9">Beauchamp et al., 2004</xref>; <xref ref-type="bibr" rid="bib71">Nath and Beauchamp, 2011</xref>; <xref ref-type="bibr" rid="bib88">Riedel et al., 2015</xref>; <xref ref-type="bibr" rid="bib102">van Atteveldt et al., 2004</xref>), premotor and inferior frontal cortices (<xref ref-type="bibr" rid="bib5">Arnal et al., 2009</xref>; <xref ref-type="bibr" rid="bib32">Evans and Davis, 2015</xref>; <xref ref-type="bibr" rid="bib46">Hasson et al., 2007b</xref>; <xref ref-type="bibr" rid="bib63">Lee and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib67">Meister et al., 2007</xref>; <xref ref-type="bibr" rid="bib97">Skipper et al., 2009</xref>; <xref ref-type="bibr" rid="bib111">Wright et al., 2003</xref>). Furthermore, some studies have shown that the visual facilitation of speech encoding may even commence in early auditory cortices (<xref ref-type="bibr" rid="bib12">Besle et al., 2008</xref>; <xref ref-type="bibr" rid="bib23">Chandrasekaran et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Ghazanfar et al., 2005</xref>; <xref ref-type="bibr" rid="bib56">Kayser et al., 2010</xref>; <xref ref-type="bibr" rid="bib62">Lakatos et al., 2009</xref>; <xref ref-type="bibr" rid="bib113">Zion Golumbic et al., 2013a</xref>). However, it remains to be understood whether visual context shapes the encoding of speech differentially within distinct regions of the auditory pathways, or whether the visual facilitation observed within auditory regions is simply fed forward to upstream areas, perhaps without further modification. Hence, it is still unclear whether the enhancement of speech-to-brain entrainment is a general mechanism that mediates visual benefits at multiple stages along the auditory pathways.</p><p>Many previous studies on this question were limited by conceptual shortcomings: first, many have focused on generic brain activations rather than directly mapping the task-relevant sensory representations (activation mapping vs. information mapping [<xref ref-type="bibr" rid="bib61">Kriegeskorte et al., 2006</xref>]), and hence have not quantified multisensory influences on those neural representations shaping behavioral performance. Those who did focused largely on auditory cortical activity (<xref ref-type="bibr" rid="bib114">Zion Golumbic et al., 2013b</xref>) or did not perform source analysis of the underlying brain activity (<xref ref-type="bibr" rid="bib28">Crosse et al., 2015</xref>). Second, while many studies have correlated speech-induced local brain activity with behavioral performance, few studies have quantified directed connectivity along the auditory pathways to ask whether perceptual benefits are better explained by changes in local encoding or by changes in functional connectivity (but see [<xref ref-type="bibr" rid="bib3">Alho et al., 2014</xref>]). And third, many studies have neglected the continuous predictive structure of speech by focusing on isolated words or syllables (but see [<xref ref-type="bibr" rid="bib28">Crosse et al., 2015</xref>]). However, this structure may play a central role for mediating the visual benefits (<xref ref-type="bibr" rid="bib11">Bernstein et al., 2004</xref>; <xref ref-type="bibr" rid="bib38">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib93">Schroeder et al., 2008</xref>). Importantly, given that the predictive visual context interacts with acoustic signal quality to increase perceptual benefits in adverse environments (<xref ref-type="bibr" rid="bib19">Callan et al., 2014</xref>; <xref ref-type="bibr" rid="bib89">Ross et al., 2007</xref>; <xref ref-type="bibr" rid="bib95">Schwartz et al., 2004</xref>; <xref ref-type="bibr" rid="bib99">Sumby and Pollack, 1954</xref>), one needs to manipulate both factors to fully address this question. Fourth, most studies focused on either the encoding of acoustic speech signals in a multisensory context, or quantified brain activity induced by visual speech, but little is known about the dependencies of neural representations of the acoustic and visual components of realistic speech (but see [<xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>]). Overcoming these problems, we here capitalize on the statistical and conceptual power offered by naturalistic continuous speech to study the network mechanisms that underlie the visual facilitation of speech perception.</p><p>Using source localized MEG activity we systematically investigated how local representations of acoustic and visual speech signals and task-relevant directed functional connectivity along the auditory pathways change with visual context and acoustic signal quality. Specifically, we extracted neural signatures of acoustically-driven speech representations by quantifying the mutual information (MI) between the MEG signal and the acoustic speech envelope. Similarly, we extracted neural signatures of visually-driven speech representations by quantifying the MI between lip movements and the MEG signal. Furthermore, we quantified directed causal connectivity between nodes in the speech network using time-lagged mutual information between MEG source signals. Using linear modelling we then asked how each of these signatures (acoustic and visual speech encoding; connectivity) are affected by contextual information about the speakers face, by the acoustic signal to noise ratio, and by their interaction. In addition, we used measures of information theoretic redundancy to test whether the local representations of acoustic speech are directly related to the temporal dynamics of lip movements or rather reflect visual contextual information more indirectly. And finally, we asked how local speech encoding and network connectivity relate to behavioral performance.</p><p>Our results describe multiple and functionally distinct representations of acoustic and visual speech in the brain. These are differentially affected by acoustic SNR and visual context, and are not trivially explained by a simple superposition of representations of the acoustic speech and lip movement information. However, none of these local speech representations was predictive of the degree of visual enhancement of speech comprehension. Rather, this behavioral benefit was predicted only by changes in directed functional connectivity.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Participants (n = 19) were presented with continuous speech that varied in acoustic quality (signal to noise ratio, SNR) and the informativeness of the speaker’s face. The visual context could be either informative (VI), showing the face producing the acoustic speech, or uninformative (VN), showing the same face producing nonsense babble (<xref ref-type="fig" rid="fig1">Figure 1A,B</xref>). We measured brain-wide activity using MEG while participants listened to eight six-minute texts and performed a delayed word recognition task. Behavioral performance was better during high SNR and an informative visual context (<xref ref-type="fig" rid="fig2">Figure 2</xref>): a repeated measures ANOVA revealed a significant effect of SNR (F(3,54) = 36.22, p&lt;0.001, Huynh-Feldt corrected, η<sup>2</sup><sub>p</sub> = 0.67), and of visual context (F(1,18) = 18.95, p&lt;0.001, η<sup>2</sup><sub>p</sub> = 51), as well as a significant interaction (F(3,54) = 4.34, p=0.008, η<sup>2</sup><sub>p</sub> = 0.19). This interaction arose from a significant visual enhancement (VI vs VN) for SNRs of 4 and 8 dB (paired T(18) ≥ 3.00, Bonferroni corrected p≤0.032; p&gt;0.95 for other SNRs).<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.003</object-id><label>Figure 1.</label><caption><title>Experimental paradigm and analysis.</title><p>(<bold>A</bold>) Stimuli consisted of 8 continuous 6 min long audio-visual speech samples. For each condition we extracted the acoustic speech envelope as well as the temporal trajectory of the lip contour (video frames, top right: magnification of lip opening and contour). (<bold>B</bold>) The experimental design comprised eight conditions, defined by the factorial combination of 4 levels of speech to background signal to noise ratio (SNR = 2, 4, 6, and 8 dB) and two levels of visual informativeness (VI: Visual context Informative: video showing the narrator in synch with speech; VN: Visual context Not informative: video showing the narrator producing babble speech). Experimental conditions lasted 1 (SNR) or 3 (VIVN) minutes, and were presented in pseudo-randomized order. (<bold>C</bold>) Analyses were carried out on band-pass filtered speech envelope and MEG signals. The MEG data were source-projected onto a grey-matter grid. One analysis quantified speech entrainment, i.e. the mutual information (MI) between the MEG data and the acoustic speech envelope (speech MI), as well as between the MEG and the lip contour (lip MI), and the extent to which these were modulated by the experimental conditions. A second analysis quantified directed functional connectivity (DI) between seeds and the extent to which this was modulated by the experimental conditions. A final analysis assessed the correlation of either MI or DI with word-recognition performance. Relevant variables in deposited data (doi:10.5061/dryad.j4567): SE_speech; LE_lip.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.003">http://dx.doi.org/10.7554/eLife.24763.003</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig1-v2"/></fig><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.004</object-id><label>Figure 2.</label><caption><title>Behavioral performance.</title><p>Word recognition performance for each of the experimental conditions (mean ± SEM across participants n = 19). Deposited data: BEHAV_perf.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.004">http://dx.doi.org/10.7554/eLife.24763.004</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig2-v2"/></fig></p><p>To study the neural mechanisms underlying this behavioral benefit we analyzed source-projected MEG data using information theoretic tools to quantify the fidelity of local neural representations of the acoustic speech envelope (speech MI), local representations of the visual lip movement (lip MI), as well as the directed causal connectivity between relevant regions (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). For both, local encoding and connectivity, we (1) modelled the extent to which they were modulated by the experimental conditions, and we (2) asked whether they correlated with behavioral performance across conditions and with the visual benefit across SNRs (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><sec id="s2-1"><title>Widespread speech-to-brain entrainment at multiple time scales</title><p>Speech-to-brain entrainment was quantified by the mutual information (speech MI) between the MEG time course and the acoustic speech envelope (not the speech + noise mixture) in individual frequency bands (<xref ref-type="bibr" rid="bib42">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib57">Kayser et al., 2015</xref>). At the group-level we observed widespread significant speech MI in all considered bands from 0.25 to 48 Hz (FWE = 0.05), except between 18–24 Hz (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). Consistent with previous results (<xref ref-type="bibr" rid="bib42">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Ng et al., 2013</xref>; <xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>) speech MI was higher at low frequencies and strongest below 4 Hz (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). This time scale is typically associated with syllabic boundaries or prosodic stress (<xref ref-type="bibr" rid="bib38">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib41">Greenberg et al., 2003</xref>). Indeed, the average syllabic rate was 212 syllables per minute in the present material, corresponding to about 3.5 Hz. Across frequencies, significant speech MI was strongest in bilateral auditory cortex and was more extended within the right hemisphere (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A and C</xref>). Indeed, peak significant MI values were significantly higher in the right compared to the left hemisphere at frequencies below 12 Hz (paired t-tests; T(18) ≥ 3.1, p≤0.043 Bonferroni corrected), and did not differ at higher frequencies (T(18) ≤ 2.78, p≥0.09). This lateralization of speech-to-brain entrainment at frequencies below 12 Hz is consistent with previous reports (<xref ref-type="bibr" rid="bib42">Gross et al., 2013</xref>). Importantly, we observed significant speech-to-brain entrainment not only within temporal cortices but across multiple regions in the occipital, frontal and parietal lobes, consistent with the notion that speech information is represented also within motor and frontal regions (<xref ref-type="bibr" rid="bib16">Bornkessel-Schlesewsky et al., 2015</xref>; <xref ref-type="bibr" rid="bib31">Du et al., 2014</xref>; <xref ref-type="bibr" rid="bib97">Skipper et al., 2009</xref>).</p></sec><sec id="s2-2"><title>Speech entrainment is modulated by SNR within and beyond auditory cortex</title><p>To determine the regions where acoustic signal quality and visual context affect the encoding of acoustic speech we modelled the condition-specific speech MI values based on effects of acoustic signal quality (SNR), visual informativeness (VIVN), and their interaction (SNRxVIVN). Random-effects significance was tested using a permutation procedure and cluster enhancement, correcting for multiple comparisons along all relevant dimensions. Effects of experimental factors emerged in multiple regions at frequencies below 4 Hz (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Increasing the acoustic signal quality (SNR; <xref ref-type="fig" rid="fig3">Figure 3A</xref>) resulted in stronger speech MI in the right auditory cortex (1–4 Hz; local peak T statistic = 4.46 in posterior superior temporal gyrus; pSTG-R; <xref ref-type="table" rid="tbl1">Table 1</xref>), right parietal cortex (local peak T = 3.94 in supramarginal gyrus; SMG-R), and right dorso-ventral frontal cortex (IFGop-R; global peak T = 5.06). We also observed significant positive SNR effects within the right temporo-parietal and occipital cortex at 12–18 Hz (local peak right lingual gyrus, T = 5.12). However, inspection of the participant-specific data suggested that this effect was not reliable (for only 58% of participants showed a speech MI increase with SNR, as opposed to a minimum of 84% for the other SNR effects), possibly because the comparatively lower power of speech envelope fluctuations at higher frequencies (c.f. <xref ref-type="fig" rid="fig1">Figure 1A</xref>); hence this effect is not discussed further.<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.005</object-id><label>Figure 3.</label><caption><title>Modulation of speech-to-brain entrainment by acoustic SNR and visual informativeness.</title><p>Changes in speech MI with the experimental factors were quantified using a GLM for the condition-specific speech MI based on the effects of SNR (<bold>A</bold>), visual informativeness VIVN (<bold>B</bold>), and their interaction (SNRxVIVN) (<bold>C</bold>). The figures display the cortical-surface projection onto the Freesurfer template (proximity = 10 mm) of the group-level significant statistics for each GLM effect (FWE = 0.05). Graphs show the average speech MI values for each condition (mean ± SEM), for local and global (red asterisk) of the T maps. Lines indicate the across-participant average regression model and numbers indicate the group-average standardized regression coefficient for SNR in the VI and VN conditions (&gt;/ &lt; 0.0 = positive/negative, rounded to 0). (<bold>D</bold>) T maps illustrating the opposite SNR effects within voxels with significant SNRxVIVN effects. MI graphs for the peaks of these maps are shown in (<bold>C</bold>) (IFGor-R and SFG-R = global T peaks for SNR effects in VI and VN, respectively). (<bold>E</bold>) Location of global and local seeds of GLM T maps, used for the analysis of directed connectivity. See also <xref ref-type="table" rid="tbl1">Tables 1</xref> and <xref ref-type="table" rid="tbl2">2</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s2">2</xref>. Deposited data: SE_meg; SE_speech; SE_miS.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.005">http://dx.doi.org/10.7554/eLife.24763.005</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig3-v2"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24763.006</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Entrainment of rhythmic MEG activity to the speech envelope and lip movements.</title><p>(<bold>A</bold>) Projection of significant speech MI maps, which quantify the entrainment of MEG source activity to the speech envelope, onto the Freesurfer template (FWE = 0.05; proximity = 10 mm; surface-projected significant MI maps rescaled within volume from minimum significant MI to the 99.5th percentile of the surface projection). (<bold>B</bold>) Projection of significant lip MI maps. (<bold>C</bold>) Peak speech / lip MI values in the two hemispheres as a function of frequency (mean ± SEM). Deposited data: SE_meg; SE_speech; SE_miS; LE_meg; LE_lip; LE_miL.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.006">http://dx.doi.org/10.7554/eLife.24763.006</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig3-figsupp1-v2"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24763.007</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Information theoretic decomposition of speech entrainment.</title><p>The figure shows condition-specific information terms for the relevant ROIs / bands (c.f. <xref ref-type="fig" rid="fig3">Figure 3E</xref>). (<bold>A</bold>) Speech MI. (<bold>B</bold>) Conditional mutual information, CMI(MEG;Speech), factoring out common influences between speech and lip signals. (<bold>C</bold>) Information theoretic redundancy between speech and lip MI. error-bars = ± SEM. See also <xref ref-type="table" rid="tbl2">Table 2</xref>. Deposited data: ID_meg; ID_speech; ID_lip; ID_infoterms.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.007">http://dx.doi.org/10.7554/eLife.24763.007</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig3-figsupp2-v2"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24763.008</object-id><label>Figure 3—figure supplement 3.</label><caption><title>Condition-changes in the amplitude of oscillatory activity.</title><p>Difference in the time-averaged Hilbert amplitude of the MEG signal between the VI and VN conditions for each ROI, as a function of frequency. Error bars bracket the 99% parametric confidence interval for the participant averaged difference. Thick bars highlight significant differences (FWE = 0.05 across ROIs and frequencies). a.u. = arbitrary units. Deposited data: SE_meg; AMP_amp.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.008">http://dx.doi.org/10.7554/eLife.24763.008</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig3-figsupp3-v2"/></fig></fig-group><table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.009</object-id><label>Table 1.</label><caption><p>Condition effects on speech MI. The table lists global and local peaks in the GLM T-maps. Anatomical labels and Brodmann areas are based on the AAL and Talairach atlases. β = standardized regression coefficient; SEM = standard error of the participant average. ROI-contralat. = T test for a significant difference of GLM betas between the respective ROI and its contralateral grid voxel.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.009">http://dx.doi.org/10.7554/eLife.24763.009</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Anatomical label</th><th>Brodmann area</th><th colspan="3">MNI coordinates</th><th>GLM effect</th><th>Frequency Band</th><th>T(18)</th><th>β(SEM)</th><th valign="top">T(18) ROI-contralat.</th></tr></thead><tbody><tr><td>HG-R</td><td>42</td><td>63</td><td>−21</td><td>11</td><td>VIVN</td><td>0.25–1 Hz</td><td>4.75<sup>*</sup></td><td>0.39 (0.06)</td><td valign="top">2.00</td></tr><tr><td valign="top">pSTG-R</td><td valign="top">22</td><td valign="top">48</td><td valign="top">−30</td><td valign="top">8</td><td valign="top">SNR</td><td valign="top">1–4 Hz</td><td valign="top">4.46<sup>*</sup></td><td valign="top">0.48 (0.08)</td><td valign="top">2.36</td></tr><tr><td>SMG-R</td><td>40</td><td>57</td><td>−30</td><td>38</td><td>SNR</td><td>1–4 Hz</td><td>3.94<sup>*</sup></td><td>0.29 (0.09)</td><td valign="top">0.22</td></tr><tr><td>PMC-L</td><td>6</td><td>−54</td><td>0</td><td>32</td><td>VIVN</td><td>1–4 Hz</td><td>3.81<sup>*</sup></td><td>0.27 (0.06)</td><td valign="top">−0.65</td></tr><tr><td>IFGt-R</td><td>46</td><td>42</td><td>33</td><td>2</td><td>SNRxVIVN</td><td>0.25–1 Hz</td><td>3.62<sup>*</sup></td><td>0.29 (0.07)</td><td valign="top">1.48</td></tr><tr><td>IFGop-R</td><td>47</td><td>51</td><td>18</td><td>2</td><td>SNR</td><td>1–4 Hz</td><td>5.06<sup>*</sup></td><td>0.36 (0.08)</td><td valign="top">6.03<sup>*</sup></td></tr><tr><td>IFGor-R</td><td>47</td><td>30</td><td>26</td><td>−16</td><td>SNR in VI</td><td>0.25–1 Hz</td><td>5.07<sup>*</sup></td><td>0.44 (0.08)</td><td valign="top">1.92</td></tr><tr><td>SFG-R</td><td>6</td><td>12</td><td>30</td><td>58</td><td>SNR in VN</td><td>0.25–1 Hz</td><td>−3.55<sup>*</sup></td><td>−0.41 (0.09)</td><td valign="top">−2.21</td></tr><tr><td>VC-R</td><td>17/18</td><td>18</td><td>−102</td><td>-4</td><td>VIVN</td><td>1–4 Hz</td><td>6.01<sup>*</sup></td><td>0.45 (0.06)</td><td valign="top">1.84</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><p>*denotes significant effects (FWE = 0.05 corrected for multiple comparisons). Relevant variables in deposited data (doi:10.5061/dryad.j4567): SE_meg; SE_speech; SE_miS.</p></fn></table-wrap-foot></table-wrap></p></sec><sec id="s2-3"><title>Visual context reveals distinct strategies for handling speech in noise in premotor, superior and inferior frontal cortex</title><p>Contrasting informative and not-informative visual contexts revealed stronger speech MI when seeing the speakers face (VI) at frequencies below 4 Hz in both hemispheres (<xref ref-type="fig" rid="fig3">Figure 3B</xref>): the right temporo-parietal cortex (0.25–1 Hz; HG; T = 4.75; <xref ref-type="table" rid="tbl1">Table 1</xref>), bilateral occipital cortex (1–4 Hz; global T peak in right visual cortex VC-R;=6.01) and left premotor cortex (1–4 Hz; PMC-L; local T peak = 3.81). Interestingly, the condition-specific pattern of MI for VC-R was characterized by an increase in speech MI with decreasing SNR during the VI condition, pointing to a stronger visual enhancement during more adverse listening conditions. The same effect was seen in premotor cortex (PMC-L).</p><p>Since visual benefits for perception emerge mostly when acoustic signals are degraded (<xref ref-type="fig" rid="fig2">Figure 2</xref>) (<xref ref-type="bibr" rid="bib89">Ross et al., 2007</xref>; <xref ref-type="bibr" rid="bib99">Sumby and Pollack, 1954</xref>), the interaction of acoustic and visual factors provides a crucial test for detecting non-trivial audio-visual interactions. We found significant interactions in the 0.25–1 Hz band in the right dorso-ventral frontal lobe, which peaked in the pars triangularis (IFGt-R; T = 3.62; <xref ref-type="fig" rid="fig3">Figure 3C</xref>; <xref ref-type="table" rid="tbl1">Table 1</xref>). Importantly, investigating the SNR effect in the frontal cortex voxels revealed two distinct strategies for handling speech in noise dependent on visual context (<xref ref-type="fig" rid="fig3">Figure 3D</xref>): During VI speech MI increased with SNR in ventral frontal cortex (peak T for SNR in pars orbitalis; IFGor-R; T = 5.07), while in dorsal frontal cortex speech MI was strongest at low SNRs during VN (peak T in superior frontal gyrus; SFG-R; T = −3.55). This demonstrates distinct functional roles of ventral and dorsal prefrontal regions in speech encoding and reveals a unique role of superior frontal cortex for enhancing speech representations in a poorly informative context, such as the absence of visual information in conjunction with poor acoustic signals. For further analysis we focused on these regions and frequency bands revealed by the GLM effects (<xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p></sec><sec id="s2-4"><title>Condition effects are hemisphere-dominant but not strictly lateralized</title><p>Our results reveal significantly stronger entrainment at low frequencies (c.f. <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) and a prevalence of condition effects on speech MI in the right hemisphere (c.f. <xref ref-type="fig" rid="fig3">Figure 3</xref>). We directly tested whether these condition effects were significantly lateralized by comparing the respective GLM effects between corresponding ROIs across hemispheres (<xref ref-type="table" rid="tbl1">Table 1</xref>). This revealed that only the 1–4 Hz SNR effect in IFGop-R was significantly lateralized (T(18) = 6.03; FWE = 0.05 corrected across ROIs), while all other GLM effects did not differ significantly between hemispheres.</p></sec><sec id="s2-5"><title>Noise invariant dynamic representations of lip movements</title><p>To complement the above analysis of speech-to-brain entrainment we also systematically analyzed the entrainment of brain activity to lip movements (lip MI). This allowed us to address whether the enhancement of the encoding of acoustic speech during an informative visual context arises from a co-representation of acoustic and visual speech information in the same regions or not. As expected based on previous work, the acoustic speech envelope and the trajectory of lip movements for the present material were temporally coherent, in particular in the delta and theta bands (<xref ref-type="fig" rid="fig1">Figure 1A</xref>)(<xref ref-type="bibr" rid="bib24">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>; <xref ref-type="bibr" rid="bib96">Schwartz and Savariaux, 2014</xref>).</p><p>Lip-to-brain entrainment was quantified for the visual informative condition only, across the same frequency bands as considered for the speech MI (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). This revealed wide-spread significant lip MI in frequency bands below 8 Hz, with the strongest lip entrainment occurring in occipital cortex (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). Peak lip MI values were larger in the right hemisphere, in particular for the 4–8 Hz band (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>), but this effect was not significant after correction for multiple comparisons (T(18) ≤ 2.53, p≥0.06). We then asked whether in any regions with significant lip MI the encoding of lip information changed with SNR. No significant SNR effects were found (FWE = 0.05, corrected across voxels and 0–12 Hz frequency bands), demonstrating that the encoding of lip signals is invariant across acoustic conditions. We also directly compared speech MI and lip MI within the ROIs highlighted by the condition effects on speech MI (c.f. <xref ref-type="fig" rid="fig3">Figure 3E</xref>). In most ROIs speech MI was significantly stronger than lip MI (<xref ref-type="table" rid="tbl2">Table 2</xref>; T(18) HG-R, pSTG-R, IFGop-R and PMC-L ≥3.58; FWE = 0.05 corrected across ROIs), while lip MI was significantly stronger in VC-R (T(18) = −3.35; FWE = 0.05).<table-wrap id="tbl2" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.010</object-id><label>Table 2.</label><caption><p>Analysis of the contribution of audio-visual signals in shaping entrainment. For each region / effect of interest (c.f. <xref ref-type="table" rid="tbl1">Table 1</xref>) the table lists the comparison of condition-averaged speech and lip MI (positive = greater speech MI); the condition effects (GLM) on the conditional mutual information (CMI) between the MEG signal and the speech envelope, while partialling out effects of lip signals; and the condition-averaged information theoretic redundancy between speech and lip MI.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.010">http://dx.doi.org/10.7554/eLife.24763.010</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2">Speech vs. lip MI</th><th colspan="2">Speech-Lip redundancy</th><th colspan="3">Speech CMI</th></tr><tr><th>Label</th><th>T(18)</th><th>Avg(SEM)</th><th>T(18)</th><th>Avg(SEM)</th><th>Effect</th><th>T(18)</th><th>β(SEM)</th></tr></thead><tbody><tr><td>HG-R</td><td>4.27*</td><td>28.16 (6.59)</td><td>0.73</td><td>0.33 (0.44)</td><td>VIVN</td><td>4.37*</td><td>0.35 (0.06)</td></tr><tr><td>pSTG-R</td><td>3.90*</td><td>5.42 (1.39)</td><td>0.49</td><td>0.19 (0.38)</td><td valign="top">SNR</td><td>4.66*</td><td>0.49 (0.08)</td></tr><tr><td>SMG-R</td><td>2.95</td><td>1.32 (0.45)</td><td>1.10</td><td>0.51 (0.47)</td><td valign="top">SNR</td><td>4.10*</td><td>0.29 (0.09)</td></tr><tr><td>PMC-L</td><td>3.58*</td><td>1.06 (0.30)</td><td>3.83*</td><td>2.42 (0.63)</td><td valign="top">VIVN</td><td>3.47*</td><td>0.24 (0.06)</td></tr><tr><td>IFGt-R</td><td>1.21</td><td>0.87 (0.72)</td><td>2.29</td><td>1.75 (0.77)</td><td valign="top">SNRxVIVN</td><td>4.07*</td><td>0.31 (0.07)</td></tr><tr><td>IFGopR</td><td>3.68*</td><td>1.50 (0.41)</td><td>4.69*</td><td>1.56 (0.33)</td><td valign="top">SNR</td><td>4.70*</td><td>0.35 (0.07)</td></tr><tr><td>SFG-R</td><td>0.88</td><td>0.61 (0.70)</td><td>4.13*</td><td>2.37 (0.57)</td><td valign="top">SNR in VN</td><td>−3.62*</td><td>−0.43 (0.09)</td></tr><tr><td>VC-R</td><td>−3.35*</td><td>−2.19 (0.65)</td><td>2.37</td><td>0.68 (0.29)</td><td valign="top">VIVN</td><td>5.77*</td><td>0.45 (0.06)</td></tr></tbody></table><table-wrap-foot><fn id="tblfn2"><p>*denotes significant effects (FWE = 0.05 corrected for multiple comparisons). Deposited data: ID_meg; ID_speech; ID_lip; ID_infoterms.</p></fn></table-wrap-foot></table-wrap></p></sec><sec id="s2-6"><title>Speech entrainment does not reflect trivial entrainment to lip dynamics</title><p>Given that only the speech and not the lip representation were affected by SNR the above results suggest that both acoustic and visual speech signals are represented independently in rhythmically entrained brain activity. To address the interrelation between the representations of acoustic and visual speech signals more directly, we asked whether the condition effects on speech MI result from genuine changes in the encoding of the acoustic speech envelope, or whether they result from a superposition of local representations of the acoustic and the visual speech signals. Given that visual and acoustic speech are temporally coherent and offer temporally redundant information, it could be that the enhancement of speech MI during the VI condition simply results from a superposition of local representations of the visual and acoustic signals arising within the same brain region. Alternatively, it could be that the speech-to-brain entrainment reflects a representation of the acoustic speech signal that is informed by visual contextual information, but which is not a one to one reflection of the dynamics of lip movements. We performed two analyses to address this.</p><p>First, we calculated the conditional mutual information between the MEG signal and the acoustic speech envelop while partialling out the temporal dynamics common to lip movements and the speech envelope. If the condition effects on speech MI reflect changes within genuine acoustic representations, they should persist when removing direct influences of lip movements. Indeed, we found that all of the condition effects reported in <xref ref-type="fig" rid="fig3">Figure 3</xref> persisted when computed based on conditional MI (absolute T(18) ≥ 3.47; compare <xref ref-type="table" rid="tbl2">Table 2</xref> for CMI with <xref ref-type="table" rid="tbl1">Table 1</xref> for MI; ROI-specific MI and CMI values are shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A,B</xref>).</p><p>Second, we computed the information-theoretic redundancy between the local speech and lip representations. Independent representations of each speech signal would result in small redundancy values, while a common representation of lip and acoustic speech signals would reflect in a redundant representation. Across SNRs we found that these representations were significantly redundant in the ventral and dorsal frontal cortex (T(18) ≥ 3.83, for SFG-R, IFGop-R, IFGt-Rand PMC-L) but not in the temporal lobe or early auditory and visual cortices (FWE = 0.05 corrected across ROIs; <xref ref-type="table" rid="tbl2">Table 2</xref>; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C</xref>). However, the actual redundancy values were rather small (condition-averaged values all below 3%). All in all, this suggests that the local representations of the acoustic speech envelope in sensory regions are informed by visual evidence but in large do not represent the same information that is provided by the dynamics of lip movements. This in particular also holds for the acoustic speech MI in visual cortex. The stronger redundancy in association cortex (IFG, SFG, PMC) suggests that these regions feature co-representations of acoustic speech and lip movements.</p></sec><sec id="s2-7"><title>Directed causal connectivity within the speech network</title><p>The diversity of the patterns of speech entrainment in temporal, premotor and inferior frontal regions across conditions shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> could arise from the individual encoding properties of each region, or from changes in functional connectivity between regions with conditions. To directly test this, we quantified the directed causal connectivity between these regions of interest. To this end we used Directed Information (DI), also known as Transfer Entropy, an information theoretic measure of Wiener-Granger causality (<xref ref-type="bibr" rid="bib65">Massey, 1990</xref>; <xref ref-type="bibr" rid="bib92">Schreiber, 2000</xref>). We took advantage of previous work that made this measure statistically robust when applied to neural data (<xref ref-type="bibr" rid="bib13">Besserve et al., 2015</xref>; <xref ref-type="bibr" rid="bib52">Ince et al., 2017</xref>).</p><p>We observed significant condition-averaged DI between multiple nodes of the speech network (FWE = 0.05; <xref ref-type="fig" rid="fig4">Figure 4A</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). This included among others the feed-forward pathways of the ventral and dorsal auditory streams, such as from auditory cortex (HG-R) and superior temporal regions (pSTG-R) to premotor (PMC-L) and to inferior frontal regions (IFGt-R, IFGop-R), from right parietal cortex (SMG-R) to premotor cortex (PMC-L), as well as feed-back connections from premotor and inferior frontal regions to temporal regions. In addition, we also observed significant connectivity between frontal (SFG-R) and visual cortex (VC).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.011</object-id><label>Figure 4.</label><caption><title>Directed causal connectivity within the speech-entrained network.</title><p>Directed connectivity between seeds of interest (c.f. <xref ref-type="fig" rid="fig3">Figure 3E</xref>) was quantified using Directed Information (DI). (<bold>A</bold>) Maximum significant condition-average DI across lags (FWE = 0.05 across lags; white = no significant DI). (<bold>B</bold>) Significant condition effects (GLM for SNR, VIVN or their interaction) on DI (FWE = 0.05 across speech/brain lags and seed/target pairs). Bar graphs display condition-specific DI values for each significant GLM effect along with the across-participants average regression model (lines). Numbers indicate the group-average standardized betas for SNR in the VI and VN conditions, averaged across lags associated with a significant GLM effect (&gt;/ &lt; 0.0 = positive/negative, rounded to 0). Error-bars = ± SEM. See also <xref ref-type="table" rid="tbl3">Table 3</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. Deposited data: DI_meg; DI_speech; DI_di; DI_brainlag; DI_speechlag.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.011">http://dx.doi.org/10.7554/eLife.24763.011</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig4-v2"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.24763.012</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Directed functional connectivity within the speech-entrained network.</title><p>(<bold>A</bold>) Significant condition-averaged directed information (DI) values between all seed-target pairs as a function of the speech (τ<sub>Speech</sub>) and brain lags (τ<sub>Brain</sub>). (<bold>B</bold>) Group-level statistical maps for the GLM effects on DI of acoustic signal quality (SNR), visual informativeness (VIVN) and their interaction. Deposited data: DI_meg; DI_speech; DI_di; DI_brainlag; DI_speechlag.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.012">http://dx.doi.org/10.7554/eLife.24763.012</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig4-figsupp1-v2"/></fig></fig-group></p><p>We then asked whether and where connectivity changed with experimental conditions (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="table" rid="tbl3">Table 3</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). Within the right ventral stream feed-forward connectivity from the temporal lobe (HG-R, pSTG-R) to frontal cortex (IFGt-R, IFGop-R) was enhanced during high acoustic SNR (FWE = 0.05; T(18) ≥ 3.1). More interestingly, this connectivity was further enhanced in the presence of an informative visual context (pSTG-R → IFGt-R, VIVN effect, T = 4.57), demonstrating a direct influence of visual context on the propagation of information along the ventral stream. Interactions of acoustic and visual context on connectivity were also found from auditory (HG-R) to premotor cortex (PMC-L, negative interaction; T = −3.01). Here connectivity increased with increasing SNR in the absence of visual information and increased with decreasing SNR during an informative context, suggesting that visual information changes the qualitative nature of auditory-motor interactions. An opposite interaction was observed between the frontal lobe and visual cortex (SFG-R → VC-R, T = 3.69). Finally, feed-back connectivity along the ventral pathway was significantly stronger during high SNRs (IFGt-R → pSTG-R; T = 4.56).<table-wrap id="tbl3" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.013</object-id><label>Table 3.</label><caption><p>Analysis of directed connectivity (DI). The table lists connections with significant condition-averaged DI, and condition effects on DI. SEM = standard error of participant average; β = standardized regression coefficients. T(18) = maximum T statistic within significance mask. All reported effects are significant (FWE = 0.05 corrected for multiple comparisons). Deposited data: DI_meg; DI_speech; DI_di; DI_brainlag; DI_speechlag.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.013">http://dx.doi.org/10.7554/eLife.24763.013</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th>DI</th><th colspan="3">Condition effects (GLM)</th></tr><tr><th>Seed</th><th>Target</th><th>T(18)</th><th>Effect</th><th>T(18)</th><th>β(SEM)</th></tr></thead><tbody><tr><td>HG-R</td><td>PMC-L</td><td>3.38</td><td>SNRxVIVN</td><td>−3.01</td><td>−0.24 (0.08)</td></tr><tr><td>HG-R</td><td>IFGt-R</td><td>3.03</td><td>SNR</td><td>3.32</td><td>0.31 (0.09)</td></tr><tr><td>HG-R</td><td>IFGopR</td><td>4.54</td><td>SNR</td><td>3.19</td><td>0.26 (0.07)</td></tr><tr><td rowspan="2">pSTG-R</td><td rowspan="2">IFGt-R</td><td rowspan="2">3.39</td><td>SNR</td><td>3.91</td><td>0.32 (0.09)</td></tr><tr><td>VIVN</td><td>4.57</td><td>0.23 (0.05)</td></tr><tr><td>pSTG-R</td><td>IFGopR</td><td>4.12</td><td>SNR</td><td>3.31</td><td>0.28 (0.08)</td></tr><tr><td>IFGt-R</td><td>IFGopR</td><td>3.76</td><td>VIVN</td><td>3.56</td><td>0.21 (0.06)</td></tr><tr><td>IFGopR</td><td>pSTG-R</td><td>4.16</td><td>SNR</td><td>4.65</td><td>0.31 (0.09)</td></tr><tr><td>SFG-R</td><td>VC-R</td><td>4.40</td><td>SNRxVIVN</td><td>3.69</td><td>0.28 (0.08)</td></tr></tbody></table></table-wrap></p></sec><sec id="s2-8"><title>Does speech entrainment or connectivity shape behavioral performance?</title><p>We performed two analyses to test whether and where changes in the local representation of speech information or directed connectivity (DI) contribute to explaining the multisensory behavioral benefits (c.f. <xref ref-type="fig" rid="fig2">Figure 2</xref>). Given the main focus on the visual enhancement of perception we implemented this analysis only for speech and not for lip MI. First, we asked where speech-MI and DI relates to performance changes across all experimental conditions (incl. changes in SNR). This revealed a significant correlation between condition-specific word-recognition performance and the strength of speech MI in pSTG-R and IFGt-R (r ≥ 0.28; FWE = 0.05; <xref ref-type="table" rid="tbl4">Table 4</xref> and <xref ref-type="fig" rid="fig5">Figure 5A</xref>), suggesting that stronger entrainment in the ventral stream facilitates comprehension. This hypothesis was further corroborated by a significant correlation of connectivity along the ventral stream with behavioral performance, both in feed-forward (HG-R → IFGt-R/IFGop-R; pSTG-R → IFGt-R/IFGop-R; r ≥ 0.24, <xref ref-type="table" rid="tbl4">Table 4</xref>) and feed-back directions (IFGop-R → pSTG-R; r = 0.37). The enhanced quality of speech perception during favorable listening conditions hence results from enhanced speech encoding and the supporting network connections along the temporal-frontal axis.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.014</object-id><label>Figure 5.</label><caption><title>Neuro-behavioral correlations.</title><p>(<bold>A</bold>) Correlations between behavioral performance and condition-specific speech MI (perform. (<bold>r</bold>), and correlations between the visual enhancement of performance and the visual enhancement in MI (vis. enhanc. (<bold>r</bold>). (<bold>B</bold>) Same for DI. Only those ROIs or connections exhibiting significant correlations are shown. error-bars = ± SEM. See also <xref ref-type="table" rid="tbl2">Tables 2</xref>–<xref ref-type="table" rid="tbl3">3</xref>. Deposited data: BEHAV_perf; SE_meg; DI_meg; SE_miS; DI_di; NBC_miS; NBC_di.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.014">http://dx.doi.org/10.7554/eLife.24763.014</ext-link></p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-24763-fig5-v2"/></fig><table-wrap id="tbl4" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.015</object-id><label>Table 4.</label><caption><p>Association of behavioral performance with speech entrainment and connectivity. Performance: T statistic and average of participant-specific correlation (SEM) between behavioral performance and speech MI / DI. Visual enhancement: correlation between SNR-specific behavioral benefit (VI-VN) and the respective difference in speech-MI or DI.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.015">http://dx.doi.org/10.7554/eLife.24763.015</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="6" valign="top">Speech MI</th></tr><tr><th valign="top"/><th colspan="2" valign="top">Performance</th><th colspan="3" valign="top">Visual enhancement</th></tr><tr><th valign="top"/><th valign="top">T(18)</th><th valign="top">r(SEM)</th><th valign="top">T(18)</th><th colspan="2" valign="top">r(SEM)</th></tr></thead><tbody><tr><td valign="top">HG-R</td><td valign="top">1.27</td><td valign="top">0.13(0.10)</td><td valign="top">0.21</td><td colspan="2" valign="top">0.04(0.15)</td></tr><tr><td valign="top">pSTG-R</td><td valign="top">3.43 *</td><td valign="top">0.30(0.09)</td><td valign="top">0.53</td><td colspan="2" valign="top">0.07(0.11)</td></tr><tr><td valign="top">SMG-R</td><td valign="top">2.35</td><td valign="top">0.23(0.09)</td><td valign="top">-0.39</td><td colspan="2" valign="top">-0.07(0.14)</td></tr><tr><td valign="top">PMC-L</td><td valign="top">0.47</td><td valign="top">0.04(0.08)</td><td valign="top">0.13</td><td colspan="2" valign="top">0.03(0.16)</td></tr><tr><td valign="top">IFGt-R</td><td valign="top">3.09 *</td><td valign="top">0.28(0.09)</td><td valign="top">1.25</td><td colspan="2" valign="top">0.29(0.18)</td></tr><tr><td valign="top">IFGopR</td><td valign="top">2.38</td><td valign="top">0.24(0.09)</td><td valign="top">-0.25</td><td colspan="2" valign="top">-0.05(0.17)</td></tr><tr><td valign="top">SFG-R</td><td valign="top">-0.47</td><td valign="top">-0.04(0.08)</td><td valign="top">1.61</td><td colspan="2" valign="top">0.35(0.17)</td></tr><tr><td valign="top">VC-R</td><td valign="top">1.55</td><td valign="top">0.18(0.10)</td><td valign="top">-0.82</td><td colspan="2" valign="top">-0.14(0.14)</td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th colspan="6" valign="top">Directed connectivity</th></tr><tr><th valign="top"/><th valign="top"/><th colspan="2" valign="top">Performance</th><th colspan="2" valign="top">Visual enhancement</th></tr><tr><th valign="top">Seed</th><th valign="top">Target</th><th valign="top">T(18)</th><th valign="top">r(SEM)</th><th valign="top">T(18)</th><th valign="top">r(SEM)</th></tr></thead><tbody><tr><td valign="top">HG-R</td><td valign="top">PMC-L</td><td valign="top">0.90</td><td valign="top">0.06(0.06)</td><td valign="top">-0.07</td><td valign="top">-0.01(0.14)</td></tr><tr><td valign="top">HG-R</td><td valign="top">IFGt-R</td><td valign="top">4.83 *</td><td valign="top">0.31(0.07)</td><td valign="top">2.55 *</td><td valign="top">0.28(0.11)</td></tr><tr><td valign="top">HG-R</td><td valign="top">IFGopR</td><td valign="top">3.19 *</td><td valign="top">0.24(0.07)</td><td valign="top">1.86</td><td valign="top">0.31(0.17)</td></tr><tr><td valign="top">pSTG-R</td><td valign="top">IFGt-R</td><td valign="top">4.28 *</td><td valign="top">0.27(0.06)</td><td valign="top">1.28</td><td valign="top">0.16(0.12)</td></tr><tr><td valign="top">pSTG-R</td><td valign="top">IFGopR</td><td valign="top">3.59 *</td><td valign="top">0.29(0.08)</td><td valign="top">1.82</td><td valign="top">0.32(0.17)</td></tr><tr><td valign="top">IFGt-R</td><td valign="top">IFGopR</td><td valign="top">1.11</td><td valign="top">0.08(0.07)</td><td valign="top">2.27</td><td valign="top">0.33(0.14)</td></tr><tr><td valign="top">IFGopR</td><td valign="top">pSTG-R</td><td valign="top">4.51 *</td><td valign="top">0.37(0.08)</td><td valign="top">2.55 *</td><td valign="top">0.37(0.15)</td></tr><tr><td valign="top">SFG-R</td><td valign="top">VC-R</td><td valign="top">-0.04</td><td valign="top">0.00(0.08)</td><td valign="top">0.90</td><td valign="top">0.17(0.18)</td></tr></tbody></table><table-wrap-foot><fn id="tblfn3"><p>*denotes significant effects (FWE = 0.05 corrected for multiple comparisons). Deposited data: BEHAV_perf; SE_meg; DI_meg; SE_miS; DI_di; NBC_miS; NBC_di.</p></fn></table-wrap-foot></table-wrap></p><p>Second, we asked whether and where the improvement in behavioral performance with an informative visual context (VI-VN) correlates with an enhancement in speech encoding or connectivity. This revealed no significant correlations between the visual enhancement of local speech MI and perceptual benefits (all T values &lt; FWE = 0.05 threshold; <xref ref-type="table" rid="tbl4">Table 4</xref>). However, changes in both feed-forward (HG-R → IFGt-R; r = 0.28; <xref ref-type="fig" rid="fig5">Figure 5B</xref>) and feed-back connections (IFGop-R → pSTG-R; r = 0.37) along the ventral stream were significantly correlated with the multisensory perceptual benefit (FWE = 0.05).</p></sec><sec id="s2-9"><title>Changes in speech entrainment are not a result of changes in the amplitude of brain activity</title><p>We verified that the reported condition effects on speech MI are not simply a by-product of changes in the overall oscillatory activity. To this end we calculated the condition averaged Hilbert amplitude for each ROI and performed a GLM analysis for condition effects as for speech entrainment (FWE = 0.05 with correction across ROIs and frequency bands; <xref ref-type="table" rid="tbl5">Table 5</xref>; <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). This revealed a reduction of oscillatory activity during the visual informative condition in the occipital cortex across many bands (VC-R, 4–48 Hz), in the inferior frontal cortex (IFG-R and IFGop-R, 24–48 Hz), and in the pSTG-R at 4–8 Hz and 18–24 Hz. No significant effects of SNR or SNRxVIVN interactions were found (FWE = 0.05). Importantly, none of these VIVN effects overlapped with the significant changes in speech MI (0.25–4 Hz) and only the reduction in pSTG-R power overlapped with condition effects in connectivity. All in all this suggests that the reported changes in speech encoding and functional connectivity are not systematically related to changes in the strength of oscillatory activity withy acoustic SNR or visual context.<table-wrap id="tbl5" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.016</object-id><label>Table 5.</label><caption><p>Changes in band-limited source signal amplitude with experimental conditions. The table lists GLM T-statistics, participant averaged standardized regression coefficients (and SEM) for significant VIVN effects on MEG source amplitude (FWE = 0.05 corrected across ROIs and frequency bands).. Effects of SNR and SNRxVIVN interactions were also tested but not significant Deposited data: SE_meg; AMP_amp.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.016">http://dx.doi.org/10.7554/eLife.24763.016</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>ROI</th><th>Band</th><th>T(18)</th><th>β(SEM)</th></tr></thead><tbody><tr><td>pSTG-R</td><td>4–8 Hz</td><td>−3.66</td><td valign="bottom">−0.38 (0.09)</td></tr><tr><td>pSTG-R</td><td>18–24 Hz</td><td>−4.11</td><td valign="bottom">−0.40 (0.08)</td></tr><tr><td>IFGt-R</td><td>24–36 Hz</td><td>−3.91</td><td valign="bottom">−0.40 (0.06)</td></tr><tr><td>IFGt-R</td><td>30–48 Hz</td><td>−4.49</td><td valign="bottom">−0.39 (0.08)</td></tr><tr><td>IFGop-R</td><td>24–36 Hz</td><td>−4.44</td><td valign="bottom">−0.40 (0.07)</td></tr><tr><td>IFGop-R</td><td>30–48 Hz</td><td>−4.14</td><td valign="bottom">−0.41 (0.07)</td></tr><tr><td>VC-R</td><td>4–8 Hz</td><td valign="bottom">−3.70</td><td valign="bottom">−0.55 (0.08)</td></tr><tr><td>VC-R</td><td>8–12 Hz</td><td valign="bottom">−4.53</td><td valign="bottom">−0.70 (0.05)</td></tr><tr><td>VC-R</td><td>12–18 Hz</td><td valign="bottom">−5.20</td><td valign="bottom">−0.70 (0.05)</td></tr><tr><td>VC-R</td><td>18–24 Hz</td><td valign="bottom">−5.57</td><td valign="bottom">−0.66 (0.06)</td></tr><tr><td>VC-R</td><td>24–36 Hz</td><td valign="bottom">−5.57</td><td valign="bottom">−0.55 (0.08)</td></tr><tr><td>VC-R</td><td>30–48 Hz</td><td>−4.54</td><td>−0.46 (0.10)</td></tr></tbody></table></table-wrap></p></sec><sec id="s2-10"><title>Changes in directed connectivity do not reflect changes in phase-amplitude coupling</title><p>Cross-frequency coupling between the phase and amplitudes of different rhythmic brain signals has been implicated in mediating neural computations and communication (<xref ref-type="bibr" rid="bib22">Canolty and Knight, 2010</xref>). We asked whether the above results on functional connectivity are systematically related to specific patterns of phase-amplitude coupling (PAC). We first searched for significant condition-average PAC between each pair of ROIs across a wide range of frequency combinations. This revealed significant PAC within VC-R, within pSTG-R and within SMG-R, as well as significant coupling of the 18–24 Hz VC-R power with the 0.25–1 Hz IFGop-R phase (FWE = 0.05; see <xref ref-type="table" rid="tbl6">Table 6</xref>). However, we found no significant changes in PAC with experimental conditions, suggesting that the changes in functional connectivity described above are not systematically related to specific patterns of cross-frequency coupling.<table-wrap id="tbl6" position="float"><object-id pub-id-type="doi">10.7554/eLife.24763.017</object-id><label>Table 6.</label><caption><p>Analysis of phase-amplitude coupling (PAC). The table lists the significant condition-averaged PAC values for all pairs or ROIs and frequency bands (FWE = 0.05 corrected across pairs of phase and power frequencies). SEM = standard error of participant average. None of these changed significantly with conditions (no GLM effects at FWE = 0.05). Deposited data: SE_meg.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.24763.017">http://dx.doi.org/10.7554/eLife.24763.017</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Phase ROI (band)</th><th valign="top">Power ROI (band)</th><th valign="top">T(18)</th><th valign="top">Pac(SEM)</th></tr></thead><tbody><tr><td>pSTG-R (1–4 Hz)</td><td valign="top">pSTG-R (8–12 Hz)</td><td valign="top">3.26</td><td valign="top">0.22 (0.07)</td></tr><tr><td>SMG-R (4–8 Hz)</td><td valign="top">SMG-R (30–48 Hz)</td><td valign="top">3.58</td><td valign="top">0.27 (0.07)</td></tr><tr><td>IFGop-R (0.25–1 Hz)</td><td valign="top">VC-R (18–24 Hz)</td><td valign="top">3.08</td><td valign="top">0.22 (0.07)</td></tr><tr><td>VC-R (4–8 Hz)</td><td valign="top">VC-R (8–12 Hz)</td><td valign="top">3.06</td><td valign="top">0.35 (0.11)</td></tr><tr><td>VC-R (1–4 Hz)</td><td valign="top">VC-R (12–18 Hz)</td><td valign="top">3.44</td><td valign="top">0.48 (0.13)</td></tr><tr><td>VC-R (4–8 Hz)</td><td valign="top">VC-R (24–36 Hz)</td><td valign="top">3.76</td><td valign="top">0.26 (0.07)</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present study provides a comprehensive picture of how acoustic signal quality and visual context interact to shape the encoding of acoustic and visual speech information and the directed functional connectivity along speech-sensitive cortex. Our results reveal a dominance of feed-forward pathways from auditory regions to inferior frontal cortex under favorable conditions, such as during high acoustic SNR. We also demonstrate the visual enhancement of acoustic speech encoding in auditory cortex, as well as non-trivial interactions of acoustic quality and visual context in premotor and in superior and inferior frontal regions. Furthermore, our results reveal the superposition of acoustic and visual speech signals (lip movements) in association regions and the dominance of visual speech representations in visual cortex. These patterns of local encoding were accompanied by changes in directed connectivity along the ventral pathway and from auditory to premotor cortex. Yet, the behavioral benefit arising from seeing the speaker’s face was not related to any region-specific visual enhancement of acoustic speech encoding. Rather, changes in directed functional connectivity along the ventral stream were predictive of the multisensory behavioral benefit.</p><sec id="s3-1"><title>Entrained auditory and visual speech representations in temporal, parietal and frontal lobes</title><p>We observed functionally distinct patterns of speech-to-brain entrainment along the auditory pathways. Previous studies on speech entrainment have largely focused on the auditory cortex, where entrainment to the speech envelope is strongest (<xref ref-type="bibr" rid="bib30">Ding and Simon, 2013</xref>; <xref ref-type="bibr" rid="bib42">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib59">Keitel et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Mesgarani and Chang, 2012</xref>; <xref ref-type="bibr" rid="bib113">Zion Golumbic et al., 2013a</xref>), and only few studies have systematically compared speech entrainment along auditory pathways (<xref ref-type="bibr" rid="bib114">Zion Golumbic et al., 2013b</xref>). This was in part due to the difficulty to separate distinct processes reflecting entrainment when contrasting only few experimental conditions (e.g. forward and reversed speech [<xref ref-type="bibr" rid="bib29">Ding and Simon, 2012</xref>; <xref ref-type="bibr" rid="bib42">Gross et al., 2013</xref>]), or based on the difficulty to separate contributions from visual (i.e. lip movements) and acoustic speech signals (<xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>). Based on the susceptibility to changes in acoustic signal quality and visual context, the systematic use of region-specific temporal lags between stimulus and brain response, and the systematic analysis of both acoustic and visual speech signals, we here establish entrainment as a ubiquitous mechanism reflecting distinct acoustic and visual speech representations along auditory pathways.</p><p>Entrainment to the acoustic speech envelope was reduced with decreasing acoustic SNR in temporal, parietal and ventral prefrontal cortex, directly reflecting the reduction in behavioral performance in challenging environments. In contrast, entrainment was enhanced during low SNR in superior frontal and premotor cortex. While there is strong support for a role of frontal and premotor regions in speech processing (<xref ref-type="bibr" rid="bib31">Du et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Evans and Davis, 2015</xref>; <xref ref-type="bibr" rid="bib47">Heim et al., 2008</xref>; <xref ref-type="bibr" rid="bib67">Meister et al., 2007</xref>; <xref ref-type="bibr" rid="bib70">Morillon et al., 2015</xref>; <xref ref-type="bibr" rid="bib87">Rauschecker and Scott, 2009</xref>; <xref ref-type="bibr" rid="bib97">Skipper et al., 2009</xref>; <xref ref-type="bibr" rid="bib108">Wild et al., 2012</xref>), most evidence comes from stimulus-evoked activity rather than signatures of neural speech encoding. We directly demonstrate the specific enhancement of frontal (PMC, SFG) speech representations during challenging conditions. This enhancement is not directly inherited from the temporal lobe, as temporal regions exhibited either no visual facilitation (pSTG) or visual facilitation without an interaction with SNR (HG).</p><p>We also observed significant entrainment to the temporal trajectory of lip movements in visual cortex, the temporal lobe and frontal cortex (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This confirms a previous study, which has specifically focused on the temporal coherence between brain activity and lip movements (<xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>). Importantly, by comparing the local encoding of both the acoustic and visual speech information, and conditioning out the visual signal from the speech MI, we found that sensory cortices and the temporal lobe provide largely independent representations of the acoustic and visual speech signals. Indeed, the information theoretic redundancy between acoustic and visual representations was small and was significant only in association regions (SFG, IFG, PMC). This suggests that early sensory cortices contain largely independent representations of acoustic and visual speech information, while association regions provide a superposition of auditory and visual speech representations. However, the condition effects on the acoustic representation in any of the analyzed regions did not disappear when factoring out the representation of lip movements, suggesting that these auditory and visual representations are differentially influenced by sensory context. These findings extend previous studies by demonstrating the co-existence of visual and auditory speech representations along auditory pathways, but also reiterate the role of PMC as one candidate region that directly links neural representations of lip movements with perception (<xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>).</p></sec><sec id="s3-2"><title>Multisensory enhancement of speech encoding in the frontal lobe</title><p>Visual information from the speakers’ face provides multiple cues that enhance intelligibility. In support of a behavioral multisensory benefit we found stronger entrainment to the speech envelope during an informative visual context in multiple bilateral regions. First, we replicated the visual enhancement of auditory cortical representations (HG) (<xref ref-type="bibr" rid="bib12">Besle et al., 2008</xref>; <xref ref-type="bibr" rid="bib56">Kayser et al., 2010</xref>; <xref ref-type="bibr" rid="bib113">Zion Golumbic et al., 2013a</xref>). Second, visual enhancement of an acoustic speech representation was also visible in early visual areas, as suggested by prior studies (<xref ref-type="bibr" rid="bib71">Nath and Beauchamp, 2011</xref>; <xref ref-type="bibr" rid="bib90">Schepers et al., 2015</xref>). Importantly, our information theoretic analysis suggests that this representation of acoustic speech is distinct from the visual representation of lip dynamics, which co-exists in the same region. The visual enhancement of acoustic speech encoding in visual cortex was strongest when SNR was low, unlike the encoding of lip movements, which was not affected by acoustic SNR. Hence this effect is most likely explained by top-down signals providing acoustic feedback to visual cortices (<xref ref-type="bibr" rid="bib105">Vetter et al., 2014</xref>). Third, speech representations in ventral prefrontal cortex were selectively involved during highly reliable multisensory conditions and were reduced in the absence of the speakers face. These findings are in line with suggestions that the IFG facilitates comprehension (<xref ref-type="bibr" rid="bib3">Alho et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Evans and Davis, 2015</xref>; <xref ref-type="bibr" rid="bib46">Hasson et al., 2007b</xref>; <xref ref-type="bibr" rid="bib48">Hickok and Poeppel, 2007</xref>) and implements multisensory processes (<xref ref-type="bibr" rid="bib19">Callan et al., 2014</xref>, <xref ref-type="bibr" rid="bib21">2003</xref>; <xref ref-type="bibr" rid="bib63">Lee and Noppeney, 2011</xref>), possibly by providing amodal phonological, syntactic and semantic processes (<xref ref-type="bibr" rid="bib27">Clos et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Ferstl et al., 2008</xref>; <xref ref-type="bibr" rid="bib66">McGettigan et al., 2012</xref>). Previous studies often reported enhanced IFG response amplitudes under challenging conditions (<xref ref-type="bibr" rid="bib43">Guediche et al., 2014</xref>). In contrast, by quantifying the fidelity of speech representations, we here show that speech encoding is generally better during favorable SNRs. This discrepancy is not necessarily surprising, if one assumes that IFG representations are derived from those in the temporal lobe, which are also more reliable during high SNRs. Noteworthy, however, we found that speech representations within ventral IFG are selectively stronger during an informative visual context, even when discounting direct co-representations of lip movements. We thereby directly confirm the hypothesis that IFG speech encoding is enhanced by visual context.</p><p>Furthermore, we demonstrate the visual enhancement of speech representations in premotor regions, which could implement the mapping of audio-visual speech features onto articulatory representations (<xref ref-type="bibr" rid="bib67">Meister et al., 2007</xref>; <xref ref-type="bibr" rid="bib70">Morillon et al., 2015</xref>; <xref ref-type="bibr" rid="bib69">Morís Fernández et al., 2015</xref>; <xref ref-type="bibr" rid="bib97">Skipper et al., 2009</xref>; <xref ref-type="bibr" rid="bib109">Wilson et al., 2004</xref>). We show that that this enhancement is inversely related to acoustic signal quality. While this observation is in agreement with the notion that perceptual benefits are strongest under adverse conditions (<xref ref-type="bibr" rid="bib89">Ross et al., 2007</xref>; <xref ref-type="bibr" rid="bib99">Sumby and Pollack, 1954</xref>), there was no significant correlation between the visual enhancement of premotor encoding and behavioral performance. Our results thereby deviate from previous work that has suggested a driving role of premotor regions in shaping intelligibility (<xref ref-type="bibr" rid="bib3">Alho et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Osnes et al., 2011</xref>). Rather, we support a modulatory influence of auditory-motor interactions (<xref ref-type="bibr" rid="bib3">Alho et al., 2014</xref>; <xref ref-type="bibr" rid="bib20">Callan et al., 2004</xref>; <xref ref-type="bibr" rid="bib48">Hickok and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib60">Krieger-Redwood et al., 2013</xref>; <xref ref-type="bibr" rid="bib70">Morillon et al., 2015</xref>). In another study we recently quantified dynamic representations of lip movements, calculated when discounting influences of the acoustic speech, and reported that left premotor activity was significantly predictive of behavioral performance (<xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>). One explanation for this discrepancy may be the presence of a memory component in the present behavioral task, which may engage other brain regions (e.g. IFG) more than other tasks. Another explanation could be that premotor regions contain, besides an acoustic speech representation described here, complementary information about visual speech that is not directly available in the acoustic speech contour, and is either genuinely visual or correlated with more complex acoustic properties of speech. Further work is required to disentangle the multisensory nature of speech encoding in premotor cortex.</p><p>Finally, our results highlight an interesting role of the superior frontal gyrus, where entrainment was strongest when sensory information was most impoverished (low SNR, visual not informative) or when the speakers face was combined with clear speech (high SNR, visual informative). Superior frontal cortex has been implied in high level inference processes underlying comprehension, sentence level integration or the exchange with memory (<xref ref-type="bibr" rid="bib33">Ferstl et al., 2008</xref>; <xref ref-type="bibr" rid="bib45">Hasson et al., 2007a</xref>; <xref ref-type="bibr" rid="bib112">Yarkoni et al., 2008</xref>) and is sometimes considered part of the broader semantic network (<xref ref-type="bibr" rid="bib15">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="bib39">Gow and Olson, 2016</xref>; <xref ref-type="bibr" rid="bib86">Price, 2012</xref>). Our data show that the SFG plays a critical role for speech encoding under challenging conditions, possibly by mediating sentence-level processes during low SNRs or the comparison of visual prosody with acoustic inputs in multisensory contexts.</p></sec><sec id="s3-3"><title>Multisensory behavioral benefits arise from distributed network mechanisms</title><p>To understand whether the condition-specific patterns of local speech representations emerge within each region, or whether they are possibly established by network interactions, we investigated the directed functional connectivity between regions of interest. While many studies have assessed the connectivity between auditory regions (e.g. [<xref ref-type="bibr" rid="bib1">Abrams et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Chu et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Fonteneau et al., 2015</xref>; <xref ref-type="bibr" rid="bib77">Park et al., 2015</xref>]), few have quantified the behavioral relevance of these connections (<xref ref-type="bibr" rid="bib3">Alho et al., 2014</xref>).</p><p>We observed significant intra-hemispheric connectivity between right temporal, parietal and frontal regions, in line with the transmission of speech information from the temporal lobe along the auditory pathways (<xref ref-type="bibr" rid="bib16">Bornkessel-Schlesewsky et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Hickok, 2012</xref>; <xref ref-type="bibr" rid="bib83">Poeppel, 2014</xref>). Supporting the idea that acoustic representations are progressively transformed along these pathways we found that the condition-specific patterns of functional connectivity differed systematically along the ventral and dorsal streams. While connectivity along the ventral stream was predictive of behavioral performance and strongest during favorable listening conditions, the inter-hemispheric connectivity to left premotor cortex was strongest during adverse multisensory conditions, i.e. when seeing the speakers face at low SNR. Interestingly, this pattern of functional connectivity matches the pattern of speech entrainment in PMC, reiterating the selective and distinctive contribution of premotor regions in speech encoding during multisensory conditions (<xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>). Our results therefore suggest that premotor representations are informed by auditory regions (HG, pSTG), rather than being driven by the frontal lobe, an interpretation that is supported by previous work (<xref ref-type="bibr" rid="bib3">Alho et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Gow and Olson, 2016</xref>; <xref ref-type="bibr" rid="bib75">Osnes et al., 2011</xref>).</p><p>We also observed a non-trivial pattern of connectivity between the SFG and visual cortex. Here the condition-specific pattern of connectivity was similar to the pattern of entrainment in the SFG, suggesting that high-level inference processes or sentence-level integration of information in the SFG contribute to the feed-back transmission of predictive information to visual cortex (<xref ref-type="bibr" rid="bib105">Vetter et al., 2014</xref>). For example, the increase of connectivity with decreasing SNR during the visual non-informative condition could serve to minimize the influence of visual speech information when this is in apparent conflict with the acoustic information in challenging environments (<xref ref-type="bibr" rid="bib69">Morís Fernández et al., 2015</xref>).</p><p>Across conditions behavioral performance was supported both by an enhancement of speech representations along the ventral pathway as well as enhanced functional connectivity. This enhanced functional connectivity emerged both along feed-forward and feed-back directions between temporal and inferior frontal regions, and was strongest (in effect size) along the feed-back route. This underlines the hypothesis that recurrent processing, rather than a simple feed-forward sweep, is central to speech intelligibility (<xref ref-type="bibr" rid="bib16">Bornkessel-Schlesewsky et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Hickok, 2012</xref>; <xref ref-type="bibr" rid="bib83">Poeppel, 2014</xref>). Central to the scope of the present study, however, we found that no single region-specific effect could explain the visual behavioral benefit. Rather, the benefit arising from seeing the speakers face was significantly correlated with the enhancement of recurrent functional connectivity along the ventral stream (HG → IFG → pSTG). Our results hence point to a distributed origin of the visual enhancement of speech intelligibility. As previously proposed (<xref ref-type="bibr" rid="bib12">Besle et al., 2008</xref>; <xref ref-type="bibr" rid="bib36">Ghazanfar et al., 2005</xref>; <xref ref-type="bibr" rid="bib37">Ghazanfar and Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bib56">Kayser et al., 2010</xref>; <xref ref-type="bibr" rid="bib113">Zion Golumbic et al., 2013a</xref>) this visual enhancement involves early auditory regions, but as we show here, also relies on the recurrent transformation of speech representations between temporal and frontal regions.</p></sec><sec id="s3-4"><title>A lack of evidence for lateralized representations</title><p>While the effects of experimental conditions on speech MI dominated in the right hemisphere we found little evidence that these effects were indeed significantly stronger in one hemisphere. Indeed, only the SNR effect in IFGop was significantly lateralized, while all other effects were comparable between hemispheres. Hence care needs to be taken when interpreting our results as evidence for a lateralization of speech encoding. At the same time we note that a potential right dominance of speech entrainment is in agreement with the hypothesis that right temporal regions extract acoustic information predominantly on the syllabic and prosodic time scales (<xref ref-type="bibr" rid="bib38">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib82">Poeppel, 2003</xref>). Further, several studies have shown that the right hemisphere becomes particularly involved in the representation of connected speech (<xref ref-type="bibr" rid="bib2">Alexandrou et al., 2017</xref>; <xref ref-type="bibr" rid="bib17">Bourguignon et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Fonteneau et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Horowitz-Kraus et al., 2015</xref>), and one previous study directly demonstrated the prevalence of speech-to-brain entrainment in delta and theta bands in the right hemisphere during continuous listening (<xref ref-type="bibr" rid="bib42">Gross et al., 2013</xref>). This makes it little surprising that the right hemisphere becomes strongly involved in representing continuous multisensory speech. Furthermore, we a bias towards the right hemisphere may in part also be a by-product of the use of entrainment as a n index to characterize speech encoding, given that the signal power of acoustic and visual speech is highest at low frequencies (c.f. <xref ref-type="fig" rid="fig1">Figure 1</xref>), and given that the right hemisphere supposedly has a preference for speech information at long time scales (<xref ref-type="bibr" rid="bib38">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib82">Poeppel, 2003</xref>).</p></sec><sec id="s3-5"><title>The mechanistic underpinnings of audio-visual speech encoding</title><p>Speech perception relies on mechanisms related to predictive coding, in order to fill in acoustically masked signals and to exploit temporal regularities and cross-modal redundancies to predict when to expect what type of syllable or phoneme (<xref ref-type="bibr" rid="bib24">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib80">Peelle and Sommers, 2015</xref>; <xref ref-type="bibr" rid="bib100">Tavano and Scharinger, 2015</xref>). Predictions modulate auditory evoked responses in an area specific manner, involve both the ventral and dorsal pathways (<xref ref-type="bibr" rid="bib55">Kandylaki et al., 2016</xref>; <xref ref-type="bibr" rid="bib98">Sohoglu and Chait, 2016</xref>), and affect both feedforward and feedback connections (<xref ref-type="bibr" rid="bib8">Auksztulewicz and Friston, 2016</xref>; <xref ref-type="bibr" rid="bib25">Chennu et al., 2016</xref>). While an informative visual context facilitates the correction of predictions about expected speech using incoming multisensory evidence, we can only speculate about a direct link between the reported effects and predictive processes. Previous studies have implied delta band activity and the dorsal auditory stream in mediating temporal predictions (<xref ref-type="bibr" rid="bib4">Arnal and Giraud, 2012</xref>; <xref ref-type="bibr" rid="bib6">Arnal et al., 2011</xref>; <xref ref-type="bibr" rid="bib55">Kandylaki et al., 2016</xref>). Hence, the changes in delta speech entrainment across conditions seen here may well reflect changes related to the prevision of temporal predictions.</p><p>Several computational candidate mechanisms have been proposed for how multisensory information could be integrated at the level of neural populations (<xref ref-type="bibr" rid="bib73">Ohshiro et al., 2011</xref>; <xref ref-type="bibr" rid="bib85">Pouget et al., 2002</xref>; <xref ref-type="bibr" rid="bib103">van Atteveldt et al., 2014</xref>). The focus on rhythmic activity in the present study lends itself to suggest a key role of the phase resetting of oscillatory process, as proposed previously (<xref ref-type="bibr" rid="bib93">Schroeder et al., 2008</xref>; <xref ref-type="bibr" rid="bib101">Thorne and Debener, 2014</xref>; <xref ref-type="bibr" rid="bib103">van Atteveldt et al., 2014</xref>). However, given the indirect nature of the neuroimaging signals the present study can’t rule in or out the involvement of specific neural processes.</p></sec><sec id="s3-6"><title>Conclusion</title><p>Our results provide a network view on the dynamic speech representations in multisensory environments. While premotor and superior frontal regions are specifically engaged in the most challenging environments, the visual enhancement of comprehension at intermediate SNRs seems to be mediated by interactions within the core speech network along the ventral pathway. Such a distributed neural origin of multisensory benefits is in line with the notion of a hierarchical organization of multisensory processing, and the idea that comprehension is shaped by network connectivity more than the engagement of particular brain regions.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>Nineteen right handed healthy adults (10 females; age from 18 to 37) participated in this study. Subject sample size was based on previous MEG/EEG studies that contrasted speech MI derived from rhythmic brain activity between conditions (19 and 22 participants in [<xref ref-type="bibr" rid="bib42">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>], respectively). All participants were tested for normal hearing, were briefed about the nature and goal of this study, and received financial compensation for their participation. The study was conducted in accordance with the Declaration of Helsinki and was approved by the local ethics committee (College of Science and Engineering, University of Glasgow). Written informed consent was obtained from all participants.</p><sec id="s4-1"><title>Stimulus material</title><p>The stimulus material consisted of audio-visual recordings based on text transcripts taken from publicly available TED talks also used in a previous study (<xref ref-type="bibr" rid="bib57">Kayser et al., 2015</xref>) (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; see also [<xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>]). Acoustic (44.1 kHz sampling rate) and video recordings (25 Hz frame rate, 1920 by 1080 pixels) were obtained while a trained male native English speaker narrated these texts. The root mean square (RMS) intensity of each audio recording was normalized using 6 s sliding windows to ensure a constant average intensity. Across the eight texts the average speech rate was 160 words (range 138–177) per minute, and the syllabic rate was 212 syllables (range 192–226) per minute.</p></sec><sec id="s4-2"><title>Experimental design and stimulus presentation</title><p>We presented each of the eight texts as continuous 6 min sample, while manipulating the acoustic quality and the visual relevance in a block design within each text (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The visual relevance was manipulated by either presenting the video matching the respective speech (visual informative, VI) or presenting a 3 s babble sequence that was repeated continuously (visual not informative, VN), and which started and ended with the mouth closed to avoid transients. The signal to noise ratio (SNR) of the acoustic speech was manipulated by presenting the speech on background cacophony of natural sounds and scaling the relative intensity of the speech while keeping the intensity of the background fixed. We used relative SNR values of +8, +6, +4 and +2 dB RMS intensity levels. The acoustic background consisted of a cacophony of naturalistic sounds, created by randomly superimposing various naturalistic sounds from a larger database (using about 40 sounds at each moment in time) (<xref ref-type="bibr" rid="bib58">Kayser et al., 2016</xref>). This resulted in a total of 8 conditions (four SNR levels; visual informative or irrelevant) that were introduced in a block design (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The SNR changed from minute to minute in a pseudo-random manner (12 one minute blocks per SNR level). Visual relevance was manipulated within 3 min sub-blocks. Texts were presented with self-paced pauses. The stimulus presentation was controlled using the Psychophysics toolbox in Matlab (<xref ref-type="bibr" rid="bib18">Brainard, 1997</xref>). Acoustic stimuli were presented using an Etymotic ER-30 tubephone (tube length = 4 m) at 44.1 kHz sampling rate and an average intensity of 65 dB RMS level, calibrated separately for each ear. Visual stimuli were presented in grey-scale and projected onto a translucent screen at 1280 × 720 pixels at 25 fps covering a field of view of 25 × 19 degrees.</p><p>Subjects performed a delayed comprehension tasks after each block, whereby they had to indicate whether a specific word (noun) was mentioned in the previous text (six words per text) or not (six words per text) in a two alternative forced choice task. The words chosen from the presented text were randomly selected and covered all eight conditions. The average performance across all trials was 73 ± 2% correct (mean and SEM across subjects), showing that subjects indeed paid attention to the stimulus. Behavioral performance for the words contained in the presented text was averaged within each condition, and analyzed using a repeated measures ANOVA, with SNR and VIVN as within-subject factors. By experimental design, the false alarm rate, i.e. the number of mistaken recognitions of words that were not part of the stimulus, was constant across experimental conditions. As a consequence, condition-specific d’ measures of word recall were strongly correlated with condition-specific word-recall performance (mean correlation and SEM across subjects = 0.97 ± 0.06; T(18) for significant group-average Fisher-Z transformed correlation = 32.57, p&lt;0.001).</p></sec><sec id="s4-3"><title>Pre-processing of speech envelope and lip movements</title><p>We extracted the envelope of the speech signal (not the speech plus background mixture) by computing the wide-band envelope at 150 Hz temporal resolution as in previous work (<xref ref-type="bibr" rid="bib24">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib57">Kayser et al., 2015</xref>). The speech signal was filtered (fourth order Butterworth filter; forward and reverse) into six frequency bands (100 Hz - 4 kHz) spaced to cover equal widths on the cochlear map. The wide-band envelope was defined as the average of the Hilbert envelopes of these band-limited signals (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The temporal trajectory of the lip contour was extracted by first identifying the lips based on their hue and then detecting the area of mouth-opening between the lips (<xref ref-type="bibr" rid="bib78">Park et al., 2016</xref>). For each video frame, the mouth aperture was subsequently estimated as the area covered by an ellipsoid fit to the detected lip contours, which was then resampled to 150 Hz for further analysis (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We estimated the coherence between the speech envelope and lip contour using spectral analysis (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p></sec><sec id="s4-4"><title>MEG data collection</title><p>MEG recordings were acquired with a 248-magnetometers whole-head MEG system (MAGNES 3600 WH, 4-D Neuroimaging) at a sampling rate of 1017.25 Hz. Participants were seated upright. The position of five coils, marking fiducial landmarks on the head of the participants, was acquired at the beginning and at the end of each block. Across blocks, and participants, the maximum change in their position was 3.6 mm, on average (STD = 1.2 mm).</p></sec><sec id="s4-5"><title>MEG pre-processing</title><p>Analyses were carried out in Matlab using the Fieldtrip toolbox (<xref ref-type="bibr" rid="bib74">Oostenveld et al., 2011</xref>), SPM12, and code for the computation of information-theoretic measures (<xref ref-type="bibr" rid="bib52">Ince et al., 2017</xref>). Block-specific data were pre-processed separately. Infrequent SQUID jumps (observed in 1.5% of the channels, on average) were repaired using piecewise cubic polynomial interpolation. Environmental magnetic noise was removed using regression based on principal components of reference channels. Both the MEG and reference data were filtered using a forward-reverse 70 Hz FIR low-pass (−40 dB at 72.5 Hz); a 0.2 Hz elliptic high-pass (−40 dB at 0.1 Hz); and a 50 Hz FIR notch filter (−40 dB at 50 ± 1 Hz). Across participants and blocks, 7 MEG channels were discarded as they exhibited a frequency spectrum deviating consistently from the median spectrum (shared variance &lt;25%). For analysis signals were resampled to 150 Hz and once more high-pass filtered at 0.2 Hz (forward-reverse elliptic filter). ECG and EOG artefacts were subsequently removed using ICA in fieldtrip (runica, 40 principal components), and were identified based on the time course and topography of IC components (<xref ref-type="bibr" rid="bib50">Hipp and Siegel, 2013</xref>).</p></sec><sec id="s4-6"><title>Structural data and source localization</title><p>High resolution anatomical MRI scans were acquired for each participant (voxel size = 1 mm<sup>3</sup>) and co-registered to the MEG data using a semi-automated procedure. Anatomicals were segmented into grey and white matter and cerebro-spinal fluid (<xref ref-type="bibr" rid="bib7">Ashburner and Friston, 2005</xref>). The parameters for the affine registration of the anatomical to the MNI template were estimated, and used to normalize the grey matter probability maps of each individual to the MNI space. A group MNI source-projection grid with a resolution of 3 mm was prepared including only voxels associated with a group-average grey-matter probability of at least 0.25. The projection grid excluded various subcortical structures, identified using the AAL atlas (e.g., vermis, caudate, putamen and the cerebellum). Leadfields were computed based on a single shell conductor model. Time-domain projections were obtained on a block-by-block basis using LCMV spatial filters (regularization = 5%). A different LCMV filter was used for each frequency band by computing the sensor covariance for the band-pass filtered sensor signals. Further analyses focused on the maximum-variance orientation of each dipole.</p></sec><sec id="s4-7"><title>Analysis of speech and lip to brain entrainment</title><p>Motivated by previous work (<xref ref-type="bibr" rid="bib42">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Ng et al., 2013</xref>), we considered eight partly overlapping frequency bands (0.25–1 Hz, 1–4 Hz, 4–8 Hz, 8–12 Hz, 12–18 Hz, 18–24 Hz, 24–36 Hz, and 30–48 Hz), and isolated these from the full-spectrum MEG signals, the speech envelope and the lip trajectory in each band using a forward-reverse fourth order Butterworth filter (magnitude of frequency response at band limits = −6 dB). Entrainment was quantified using the mutual information (MI) between the filtered MEG and speech envelope or lip time courses:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="2em"/><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="1em"/><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The MI was calculated using a bin-less approach based on statistical copulas, which provides greater sensitivity than methods based on binned signals (<xref ref-type="bibr" rid="bib52">Ince et al., 2017</xref>).</p><p>To quantify the entrainment of brain activity to the speech envelope / lip movement we first determined the optimal time lag between MEG signals and the stimulus for individual bands and source voxels using a permutation-based RFX estimate. Lag estimates were obtained based on a quadratic fit, excluding lags with insignificant MI (permutation-based FDR = 0.01). Voxels without an estimate were assigned the median estimate within the same frequency band, and volumetric maps of the optimal lags were smoothed with a Gaussian (FWHM = 10 mm). Speech / lip MI were then estimated for each band and voxel using the optimal lag. The significance of group-level MI values was assessed within a permutation-based RFX framework that relied on MI values corrected for bias at the single-subject level, and on cluster mass enhancement of the test statistics corrected for multiple comparisons at the second level (<xref ref-type="bibr" rid="bib64">Maris and Oostenveld, 2007</xref>). At the single-subject level, null distributions were obtained by shuffling the assignment of stimulus and MEG, independently for each participant, that is, by permuting the six speech segments within each of the eight experimental conditions (using the same permutation across bands). Participant-specific bias-corrected MI values were then defined as the actual MI minus the median MI across all 720 possible null permutations. Group-level RFX testing relied on T-statistics for the null-hypothesis that the participant-averaged bias-corrected MI was significantly larger than zero. To this end we generated 10,000 samples of the group-averaged MI from the participant-specific null distributions, used cluster-mass enhancement across voxels and frequencies (cluster-forming threshold T(18) = 2.1) to extract the maximum cluster T across frequency bands and voxels, and considered as significant a cluster-enhanced T statistic higher than the 95th percentile of the permutation distribution (corresponding to FWE = 0.05). Significant speech MI was determined across all conditions, whereas significant lip MI was derived only for the VI condition.</p><p>To determine whether and where speech / lip entrainment was modulated by the experimental factors we used a permutation-based RFX GLM framework (<xref ref-type="bibr" rid="bib110">Winkler et al., 2014</xref>). For each participant individually we considered the condition-specific bias-corrected MI averaged across repetitions and estimated the coefficients of a GLM for predicting MI based on SNR (2, 4, 6, 8 dB), VIVN (1 = Visual Informative; −1 = Visual Not informative), and their interaction; for lip MI we only considered the SNR effect in the VI condition. We computed a group-level T-statistic for assessing the hypothesis that the across-participant average GLM coefficient was significantly different than zero, using cluster-mass enhancement across voxels and frequencies. Permutation testing relied on the Freedman-Lane procedure (<xref ref-type="bibr" rid="bib35">Freedman and Lane, 1983</xref>). Independently for each participant and GLM effect, we estimated the parameters of a reduced GLM that includes all of the effects but the one to be tested and extracted the residuals of the prediction. We then permuted the condition-specific residuals and extracted the GLM coefficient for the effect of interest estimated for these reshuffled residuals. We obtained a permutation T statistic for the group-average GLM coefficient of interest using the max-statistics. We considered as significant T values whose absolute value was higher than the 95<sup>th</sup> percentile of the absolute value of 10,000 permutation samples, correcting for multiple comparisons across voxels / bands (FWE = 0.05). We only considered significant GLM effects in conjunction with a significant condition-average entrainment.</p></sec><sec id="s4-8"><title>Analysis of directed functional connectivity</title><p>To quantify directed functional connectivity we relied on the concept of Wiener-Granger causality and its information theoretic implementation known as Transfer Entropy or directed information (DI) (<xref ref-type="bibr" rid="bib65">Massey, 1990</xref>; <xref ref-type="bibr" rid="bib92">Schreiber, 2000</xref>; <xref ref-type="bibr" rid="bib106">Vicente et al., 2011</xref>; <xref ref-type="bibr" rid="bib107">Wibral et al., 2011</xref>). Directed information in its original formulation (<xref ref-type="bibr" rid="bib65">Massey, 1990</xref>) (termed DI<sup>*</sup> here) quantifies causal connectivity by measuring the degree to which the past of a seed predicts the future of a target signal, conditional on the past of the target, defined at a specific lag (τ<sub>Brain</sub>):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo>τ</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>While DI<sup>*</sup> provides a measure of the overall directed influence from seed to target, it can be susceptible to statistical biases arising from limited sampling, common inputs or signal auto-correlations (<xref ref-type="bibr" rid="bib13">Besserve et al., 2015</xref>, <xref ref-type="bibr" rid="bib14">2010</xref>; <xref ref-type="bibr" rid="bib52">Ince et al., 2017</xref>; <xref ref-type="bibr" rid="bib76">Panzeri et al., 2007</xref>). We regularized and made this measure more conservative by subtracting out values of DI computed at fixed values of speech envelope. This subtraction removes terms – including the statistical biases described above – that cannot possibly carry speech information (because they are computed at fixed speech envelope). This results in an estimate that is more robust and more directly related to changes in the sensory input than classical transfer entropy (the same measure was termed directed feature information in [<xref ref-type="bibr" rid="bib52">Ince et al., 2017</xref>, <xref ref-type="bibr" rid="bib54">Ince et al., 2015</xref>]). DI was defined here as<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where DI<sup>*</sup>|Speech denotes the DI<sup>*</sup> conditioned on the speech envelope. Positive values of DI indicate directed functional connectivity between seed and target at a specific brain (τ<sub>Brain</sub>) and speech lag (τ<sub>Speech</sub>). The actual DI values were furthermore Z-scored against random effects for added robustness, which facilitates statistical comparisons between conditions across subjects (<xref ref-type="bibr" rid="bib13">Besserve et al., 2015</xref>). To this end DI, as estimated for each participant and connection from <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, was Z-scored against the distribution of DI values obtained from condition-shuffled estimates (using the same randomization procedure as for MI). DI was computed for speech lags between 0 and 500 ms and brain lags between 0 and 250 ms, at steps of one sample (1/150 Hz). We estimated DI on the frequency range of 0.25–8 Hz (forward-reverse fourth order Butterworth filter), which spans all the frequencies relevant for the condition effects on speech MI (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The use of a single frequency band for the connectivity analysis greatly reduced the computational burden and statistical testing compared to the use of multiple bands, while the use of a larger bandwidth here also allowed for greater robustness of underlying estimators (<xref ref-type="bibr" rid="bib14">Besserve et al., 2010</xref>). Furthermore, we computed DI by considering the bivariate MEG response defined by the band-passed source signal and its first-order difference, as this offers additional statistical robustness (<xref ref-type="bibr" rid="bib52">Ince et al., 2017</xref>, <xref ref-type="bibr" rid="bib53">2016</xref>). Seeds for the DI analysis were the global and local peaks of the GLM-T maps quantifying the SNR, VIVN and SNRxVIVN modulation of entrainment, and the SFG-R voxel characterized by the peak negative effect of SNR in the visual informative condition, for a total of 8 seeds (<xref ref-type="table" rid="tbl1">Table 1</xref> and <xref ref-type="fig" rid="fig3">Figure 3E</xref>). To test for the significance of condition-average DI we used the same permutation-based RFX approach as for speech MI, testing the hypothesis that bias-corrected DI &gt; 0. We used 2D cluster-mass enhancement of the T statistics within speech/brain lag dimensions correcting for multiple comparisons across speech and brain lags (FWE = 0.05). To test for significant DI effects with experimental conditions we relied on the same GLM strategy as for MI effects, again with the same differences pertaining to cluster enhancement and comparison correction (FWE = 0.05 across lags and seed/target pairs). We only considered DI modulations in conjunction with a significant condition-average DI.</p></sec><sec id="s4-9"><title>Neuro-behavioral correlations</title><p>We used a permutation-based RFX approach to assess (1) whether an increase in condition-specific speech-MI or DI was associated with an increase in behavioral performance, and (2) whether the visual enhancement (VI-VN) of speech MI or DI was associated with stronger behavioral gains. We focused on the eight regions used as seeds for the DI analysis (c.f. <xref ref-type="fig" rid="fig3">Figure 3E</xref>). For speech MI we initially tested whether the participant-average Fisher Z-transformed correlation between condition-specific performance and speech-MI was significantly larger than zero. Uncorrected p-values were computed using the percentile method, where FWE = 0.05 p-values corrected across regions were computed using maximum statistics. We subsequently tested the positive correlation between SNR-specific visual gains (VI-VN) in speech-MI and behavioral performance using the same approach, but considered only those regions characterized by a significant condition-specific MI/performance association. For DI, we focused on those lags characterized by a significant SNR, VIVN, or SNRxVIVN DI modulation. Significance testing proceeded as for speech MI, except that Z-transformed correlations were computed independently for each lag and then averaged across lags (FWE = 0.05 corrected across all seed/target pairs).</p></sec><sec id="s4-10"><title>Analysis of the lateralization of entrainment and entrainment modulation effects</title><p>We tested for a significant lateralization of the GLM effects on speech MI reported in <xref ref-type="fig" rid="fig3">Figure 3</xref>. To this end we extracted participant specific GLM betas for each effect in the respective ROI and band. We then extracted the same GLM coefficient for the contralateral voxel and computed the between-hemispheric difference. This was tested for significance using a two-sided RFX test based on a sign-permutation of the across-participant T value (10,000 permutations), with maximum-statistic multiple comparison correction across ROIs (FWE = 0.05; <xref ref-type="table" rid="tbl1">Table 1</xref>).</p></sec><sec id="s4-11"><title>Decomposition of audio-visual information</title><p>To test whether the condition modulation of speech MI could be attributed to a co-representation of visual lip information in the same ROI we calculated the conditional information between the MEG and the speech envelope, factoring out the encoding of temporal dynamics common to the speech and lip signals. With MI_speech&amp;lip defined as MI(MEG;speech,lip), the CMI was defined as follows<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where the first term on the right-hand side denotes the information carried by the local MEG signal about both the acoustic and visual speech, and the second term the MI about only the visual speech. The respective CMI values were then tested for significant condition effects (<xref ref-type="table" rid="tbl2">Table 2</xref>).</p><p>To further test whether the local representations of acoustic and visual speech in each ROI were independent or possibly strongly redundant (hence capturing the same aspect of sensory information), we computed a measure of normalized information theoretic redundancy during the VI condition as follows (<xref ref-type="bibr" rid="bib10">Belitski et al., 2010</xref>; <xref ref-type="bibr" rid="bib84">Pola et al., 2003</xref>; <xref ref-type="bibr" rid="bib91">et al., 2003</xref>):<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi mathvariant="normal">&amp;</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This expresses redundancy as percentage of the total information that there would be in its absence of any redundancy. For these analysis both speech and lip signals were extracted at their respective optimal lag for each ROI/band and a common segment to each stimulus and the MEG activity was used for the calculation (segment duration = 60 s – 320 ms). Statistical tests contrasting condition-averaged information terms relied on the same RFX permutation framework and correction across all relevant dimensions as in all other analyses (FWE = 0.05). We compared condition-averaged MI_speech with MI_lip values using a two-sided test, contrasted condition-averaged redundancy values with their statistical bias (null-distribution), and tested for condition effects (GLM) on the CMI values.</p></sec><sec id="s4-12"><title>Analysis of condition effects on MEG signal amplitude</title><p>The amplitude within specific bands was defined as the absolute value of the instantaneous Hilbert-transformed band-pass MEG signal beamformed to each of the ROIs (c.f. <xref ref-type="fig" rid="fig3">Figure 3E</xref>). For each participant and experimental condition, we averaged the amplitude of the MEG time courses across time and repetitions of the same condition. Significance testing of condition changes in amplitude relied on the same RFX permutation-based approach as for the other modulation analyses, with maximum statistic multiple comparisons correction across ROIs and frequency bands (FWE = 0.05).</p></sec><sec id="s4-13"><title>Analysis of phase-amplitude couplings</title><p>We computed a measure of phase-amplitude coupling (PAC) between the oscillatory activity in different bands and regions. PAC was defined as<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>∗</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>θ</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>where A<sub>FH</sub> and θ<sub>FL</sub> denote the instantaneous Hilbert amplitude and phase angle of the high- and low-frequency MEG pass-band signal, respectively, and N is the number of time samples of the pass-band MEG signal in a specific condition. Low-frequency phase was extracted for the 0.25–1, 1–4, and 4–8 Hz bands. High-frequency amplitude was extracted for the 8–12, 12–18, 18–24, 24–36 and 30–48 Hz bands. We tested for both a significant condition-average PAC and for a significant modulation of PAC with conditions. Significance testing relied on the same RFX permutation-based approach as for the other modulation analyses, with maximum statistic correction for multiple comparisons across pairs of phase/power frequency pairs for the significance of condition averaged PAC, and also across pairs of phase/power ROIs for the GLM modulation (FWE = 0.05).</p></sec><sec id="s4-14"><title>Data sharing</title><p>The data analyzed for the ROI results presented in <xref ref-type="fig" rid="fig2">Figures 2</xref>–<xref ref-type="fig" rid="fig5">5</xref>,in the figure supplements and in <xref ref-type="table" rid="tbl1">Tables 1</xref>–<xref ref-type="table" rid="tbl5">5</xref>, as well as the speech and lip time courses analyzed in <xref ref-type="fig" rid="fig1">Figure 1</xref>, have been deposited on Dryad (doi:10.5061/dryad.j4567).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Hyojin Park for sharing audio-visual materials used to prepare the stimuli in this study.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>BLG, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>RAAI, Resources, Software, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>JG, Funding acquisition, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>PGS, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>SP, Conceptualization, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>CK, Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study was conducted in accordance with the Declaration of Helsinki and was approved by the local ethics committee (College of Science and Engineering, University of Glasgow). Written informed consent was obtained from all participants.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><sec id="s7" sec-type="datasets"><title>Major datasets</title><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" id="data-ro1" source-id="http://dx.doi.org/10.5061/dryad.j4567" source-id-type="uri"><collab>Giordano BL</collab><x>,</x> <collab>Ince RAA</collab><x>,</x> <collab>Gross J</collab><x>,</x> <collab>Schyns PG</collab><x>,</x> <collab>Panzeri S</collab><x>,</x> <collab>Kayser C</collab><x>,</x> <year>2017</year> <x>,</x><source>Data from: Contributions of local speech encoding and functional connectivity to audio-visual speech perception</source><x>,</x> <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.j4567">http://dx.doi.org/10.5061/dryad.j4567</ext-link><x>,</x> <comment>Available at Dryad Digital Repository under a CC0 Public Domain Dedication</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrams</surname><given-names>DA</given-names></name><name><surname>Ryali</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Balaban</surname><given-names>E</given-names></name><name><surname>Levitin</surname><given-names>DJ</given-names></name><name><surname>Menon</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multivariate activation and connectivity patterns discriminate speech intelligibility in Wernicke's, Broca's, and Geschwind's areas</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>1703</fpage><lpage>1714</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs165</pub-id><pub-id pub-id-type="pmid">22693339</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexandrou</surname><given-names>AM</given-names></name><name><surname>Saarinen</surname><given-names>T</given-names></name><name><surname>Mäkelä</surname><given-names>S</given-names></name><name><surname>Kujala</surname><given-names>J</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The right hemisphere is highlighted in connected natural speech production and perception</article-title><source>NeuroImage</source><volume>152</volume><fpage>628</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.006</pub-id><pub-id pub-id-type="pmid">28268122</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alho</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>FH</given-names></name><name><surname>Sato</surname><given-names>M</given-names></name><name><surname>Tiitinen</surname><given-names>H</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Jääskeläinen</surname><given-names>IP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Enhanced neural synchrony between left auditory and premotor cortex is associated with successful phonetic categorization</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>394</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00394</pub-id><pub-id pub-id-type="pmid">24834062</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and sensory predictions</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.05.003</pub-id><pub-id pub-id-type="pmid">22682813</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Kell</surname><given-names>CA</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dual neural routing of visual facilitation in speech processing</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>13445</fpage><lpage>13453</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3194-09.2009</pub-id><pub-id pub-id-type="pmid">19864557</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transitions in neural oscillations reflect prediction errors generated in audiovisual speech</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>797</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1038/nn.2810</pub-id><pub-id pub-id-type="pmid">21552273</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>J</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Unified segmentation</article-title><source>NeuroImage</source><volume>26</volume><fpage>839</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.02.018</pub-id><pub-id pub-id-type="pmid">15955494</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Repetition suppression and its contextual determinants in predictive coding</article-title><source>Cortex</source><volume>80</volume><fpage>125</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.11.024</pub-id><pub-id pub-id-type="pmid">26861557</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Argall</surname><given-names>BD</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Duyn</surname><given-names>JH</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Unraveling multisensory integration: patchy organization within human STS multisensory cortex</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1190</fpage><lpage>1192</lpage><pub-id pub-id-type="doi">10.1038/nn1333</pub-id><pub-id pub-id-type="pmid">15475952</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belitski</surname><given-names>A</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Magri</surname><given-names>C</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Sensory information in local field potentials and spikes from visual and auditory cortices: time scales and frequency bands</article-title><source>Journal of Computational Neuroscience</source><volume>29</volume><fpage>533</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1007/s10827-010-0230-y</pub-id><pub-id pub-id-type="pmid">20232128</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Auer</surname><given-names>ET</given-names></name><name><surname>Takayanagi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Auditory speech detection in noise enhanced by lipreading</article-title><source>Speech Communication</source><volume>44</volume><fpage>5</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.specom.2004.10.011</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besle</surname><given-names>J</given-names></name><name><surname>Fischer</surname><given-names>C</given-names></name><name><surname>Bidet-Caulet</surname><given-names>A</given-names></name><name><surname>Lecaignard</surname><given-names>F</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Giard</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual activation and audiovisual interactions in the auditory cortex during speech perception: intracranial recordings in humans</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>14301</fpage><lpage>14310</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2875-08.2008</pub-id><pub-id pub-id-type="pmid">19109511</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besserve</surname><given-names>M</given-names></name><name><surname>Lowe</surname><given-names>SC</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Shifts of Gamma phase across primary visual cortical sites reflect Dynamic Stimulus-Modulated Information transfer</article-title><source>PLoS Biology</source><volume>13</volume><elocation-id>e1002257</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002257</pub-id><pub-id pub-id-type="pmid">26394205</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besserve</surname><given-names>M</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Causal relationships between frequency bands of extracellular signals in visual cortex revealed by an information theoretic analysis</article-title><source>Journal of Computational Neuroscience</source><volume>29</volume><fpage>547</fpage><lpage>566</lpage><pub-id pub-id-type="doi">10.1007/s10827-010-0236-5</pub-id><pub-id pub-id-type="pmid">20396940</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name><name><surname>Graves</surname><given-names>WW</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Where is the semantic system? A critical review and meta-analysis of 120 functional neuroimaging studies</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>2767</fpage><lpage>2796</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp055</pub-id><pub-id pub-id-type="pmid">19329570</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornkessel-Schlesewsky</surname><given-names>I</given-names></name><name><surname>Schlesewsky</surname><given-names>M</given-names></name><name><surname>Small</surname><given-names>SL</given-names></name><name><surname>Rauschecker</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neurobiological roots of language in primate audition: common computational properties</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>142</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.12.008</pub-id><pub-id pub-id-type="pmid">25600585</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourguignon</surname><given-names>M</given-names></name><name><surname>De Tiège</surname><given-names>X</given-names></name><name><surname>de Beeck</surname><given-names>MO</given-names></name><name><surname>Ligot</surname><given-names>N</given-names></name><name><surname>Paquier</surname><given-names>P</given-names></name><name><surname>Van Bogaert</surname><given-names>P</given-names></name><name><surname>Goldman</surname><given-names>S</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Jousmäki</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The pace of prosodic phrasing couples the listener's cortex to the reader's voice</article-title><source>Human Brain Mapping</source><volume>34</volume><fpage>314</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1002/hbm.21442</pub-id><pub-id pub-id-type="pmid">22392861</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname><given-names>DE</given-names></name><name><surname>Jones</surname><given-names>JA</given-names></name><name><surname>Callan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multisensory and modality specific processing of visual speech in different regions of the premotor cortex</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>389</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00389</pub-id><pub-id pub-id-type="pmid">24860526</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname><given-names>DE</given-names></name><name><surname>Jones</surname><given-names>JA</given-names></name><name><surname>Callan</surname><given-names>AM</given-names></name><name><surname>Akahane-Yamada</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Phonetic perceptual identification by native- and second-language speakers differentially activates brain regions involved with acoustic phonetic processing and those involved with articulatory-auditory/orosensory internal models</article-title><source>NeuroImage</source><volume>22</volume><fpage>1182</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.006</pub-id><pub-id pub-id-type="pmid">15219590</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname><given-names>DE</given-names></name><name><surname>Jones</surname><given-names>JA</given-names></name><name><surname>Munhall</surname><given-names>K</given-names></name><name><surname>Callan</surname><given-names>AM</given-names></name><name><surname>Kroos</surname><given-names>C</given-names></name><name><surname>Vatikiotis-Bateson</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neural processes underlying perceptual enhancement by visual speech gestures</article-title><source>NeuroReport</source><volume>14</volume><fpage>2213</fpage><lpage>2218</lpage><pub-id pub-id-type="doi">10.1097/00001756-200312020-00016</pub-id><pub-id pub-id-type="pmid">14625450</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canolty</surname><given-names>RT</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The functional role of cross-frequency coupling</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>506</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.09.001</pub-id><pub-id pub-id-type="pmid">20932795</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Lemus</surname><given-names>L</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic faces speed up the onset of auditory cortical spiking responses during vocal detection</article-title><source>PNAS</source><volume>110</volume><fpage>E4668</fpage><lpage>E4677</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312518110</pub-id><pub-id pub-id-type="pmid">24218574</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Trubanova</surname><given-names>A</given-names></name><name><surname>Stillittano</surname><given-names>S</given-names></name><name><surname>Caplier</surname><given-names>A</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The natural statistics of audiovisual speech</article-title><source>PLoS Computational Biology</source><volume>5</volume><elocation-id>e1000436</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id><pub-id pub-id-type="pmid">19609344</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chennu</surname><given-names>S</given-names></name><name><surname>Noreika</surname><given-names>V</given-names></name><name><surname>Gueorguiev</surname><given-names>D</given-names></name><name><surname>Shtyrov</surname><given-names>Y</given-names></name><name><surname>Bekinschtein</surname><given-names>TA</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Silent expectations: dynamic Causal modeling of cortical prediction and attention to sounds that Weren't</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>8305</fpage><lpage>8316</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1125-16.2016</pub-id><pub-id pub-id-type="pmid">27511005</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chu</surname><given-names>YH</given-names></name><name><surname>Lin</surname><given-names>FH</given-names></name><name><surname>Chou</surname><given-names>YJ</given-names></name><name><surname>Tsai</surname><given-names>KW</given-names></name><name><surname>Kuo</surname><given-names>WJ</given-names></name><name><surname>Jääskeläinen</surname><given-names>IP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Effective cerebral connectivity during silent speech reading revealed by functional magnetic resonance imaging</article-title><source>PLoS One</source><volume>8</volume><elocation-id>e80265</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0080265</pub-id><pub-id pub-id-type="pmid">24278266</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clos</surname><given-names>M</given-names></name><name><surname>Langner</surname><given-names>R</given-names></name><name><surname>Meyer</surname><given-names>M</given-names></name><name><surname>Oechslin</surname><given-names>MS</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Effects of prior information on decoding degraded speech: an fMRI study</article-title><source>Human Brain Mapping</source><volume>35</volume><fpage>61</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1002/hbm.22151</pub-id><pub-id pub-id-type="pmid">22936472</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Butler</surname><given-names>JS</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Congruent visual speech enhances cortical entrainment to continuous auditory speech in Noise-Free conditions</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>14195</fpage><lpage>14204</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1829-15.2015</pub-id><pub-id pub-id-type="pmid">26490860</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural coding of continuous speech in auditory cortex during monaural and dichotic listening</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>78</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1152/jn.00297.2011</pub-id><pub-id pub-id-type="pmid">21975452</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Adaptive temporal encoding leads to a background-insensitive cortical representation of speech</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>5728</fpage><lpage>5735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5297-12.2013</pub-id><pub-id pub-id-type="pmid">23536086</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Buchsbaum</surname><given-names>BR</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name><name><surname>Alain</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Noise differentially impacts phoneme representations in the auditory and speech motor systems</article-title><source>PNAS</source><volume>111</volume><fpage>7126</fpage><lpage>7131</lpage><pub-id pub-id-type="doi">10.1073/pnas.1318738111</pub-id><pub-id pub-id-type="pmid">24778251</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>S</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hierarchical organization of auditory and motor representations in speech perception: evidence from searchlight similarity analysis</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>4772</fpage><lpage>4788</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv136</pub-id><pub-id pub-id-type="pmid">26157026</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferstl</surname><given-names>EC</given-names></name><name><surname>Neumann</surname><given-names>J</given-names></name><name><surname>Bogler</surname><given-names>C</given-names></name><name><surname>von Cramon</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The extended language network: a meta-analysis of neuroimaging studies on text comprehension</article-title><source>Human Brain Mapping</source><volume>29</volume><fpage>581</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1002/hbm.20422</pub-id><pub-id pub-id-type="pmid">17557297</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonteneau</surname><given-names>E</given-names></name><name><surname>Bozic</surname><given-names>M</given-names></name><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Brain network connectivity during language comprehension: interacting linguistic and perceptual subsystems</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3962</fpage><lpage>3976</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu283</pub-id><pub-id pub-id-type="pmid">25452574</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>D</given-names></name><name><surname>Lane</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>A nonstochastic interpretation of reported significance levels</article-title><source>Journal of Business &amp; Economic Statistics</source><volume>1</volume><fpage>292</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1080/07350015.1983.10509354</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Maier</surname><given-names>JX</given-names></name><name><surname>Hoffman</surname><given-names>KL</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multisensory integration of dynamic faces and voices in rhesus monkey auditory cortex</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>5004</fpage><lpage>5012</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0799-05.2005</pub-id><pub-id pub-id-type="pmid">15901781</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Is neocortex essentially multisensory?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>278</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id><pub-id pub-id-type="pmid">16713325</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gow</surname><given-names>DW</given-names></name><name><surname>Olson</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Sentential influences on acoustic-phonetic processing: a granger causality analysis of multimodal imaging data</article-title><source>Language, Cognition and Neuroscience</source><volume>31</volume><fpage>841</fpage><lpage>855</lpage><pub-id pub-id-type="doi">10.1080/23273798.2015.1029498</pub-id><pub-id pub-id-type="pmid">27595118</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Seitz</surname><given-names>PF</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Measures of auditory-visual integration in nonsense syllables and sentences</article-title><source>The Journal of the Acoustical Society of America</source><volume>104</volume><fpage>2438</fpage><lpage>2450</lpage><pub-id pub-id-type="doi">10.1121/1.423751</pub-id><pub-id pub-id-type="pmid">10491705</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>S</given-names></name><name><surname>Carvey</surname><given-names>H</given-names></name><name><surname>Hitchcock</surname><given-names>L</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Temporal properties of spontaneous speech—a syllable-centric perspective</article-title><source>Journal of Phonetics</source><volume>31</volume><fpage>465</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/j.wocn.2003.09.005</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Hoogenboom</surname><given-names>N</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Schyns</surname><given-names>P</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title><source>PLoS Biology</source><volume>11</volume><elocation-id>e1001752</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001752</pub-id><pub-id pub-id-type="pmid">24391472</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guediche</surname><given-names>S</given-names></name><name><surname>Blumstein</surname><given-names>SE</given-names></name><name><surname>Fiez</surname><given-names>JA</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Speech perception under adverse conditions: insights from behavioral, computational, and neuroscience research</article-title><source>Frontiers in Systems Neuroscience</source><volume>7</volume><elocation-id>126</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00126</pub-id><pub-id pub-id-type="pmid">24427119</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Galantucci</surname><given-names>B</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name><name><surname>Keysers</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Brain-to-brain coupling: a mechanism for creating and sharing a social world</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>114</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.12.007</pub-id><pub-id pub-id-type="pmid">22221820</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nusbaum</surname><given-names>HC</given-names></name><name><surname>Small</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2007">2007a</year><article-title>Brain networks subserving the extraction of sentence information and its encoding to memory</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2899</fpage><lpage>2913</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm016</pub-id><pub-id pub-id-type="pmid">17372276</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Skipper</surname><given-names>JI</given-names></name><name><surname>Nusbaum</surname><given-names>HC</given-names></name><name><surname>Small</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2007">2007b</year><article-title>Abstract coding of audiovisual speech: beyond sensory representation</article-title><source>Neuron</source><volume>56</volume><fpage>1116</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.09.037</pub-id><pub-id pub-id-type="pmid">18093531</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heim</surname><given-names>S</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Specialisation in Broca's region for semantic, phonological, and syntactic fluency?</article-title><source>NeuroImage</source><volume>40</volume><fpage>1362</fpage><lpage>1368</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.01.009</pub-id><pub-id pub-id-type="pmid">18296070</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Computational neuroanatomy of speech production</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>135</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1038/nrn3158</pub-id><pub-id pub-id-type="pmid">22218206</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hipp</surname><given-names>JF</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociating neuronal gamma-band activity from cranial and ocular muscle activity in EEG</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>338</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00338</pub-id><pub-id pub-id-type="pmid">23847508</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horowitz-Kraus</surname><given-names>T</given-names></name><name><surname>Grainger</surname><given-names>M</given-names></name><name><surname>DiFrancesco</surname><given-names>M</given-names></name><name><surname>Vannest</surname><given-names>J</given-names></name><name><surname>Holland</surname><given-names>SK</given-names></name><collab>CMIND Authorship Consortium</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Right is not always wrong: dti and fMRI evidence for the reliance of reading comprehension on language-comprehension networks in the right hemisphere</article-title><source>Brain Imaging and Behavior</source><volume>9</volume><fpage>19</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1007/s11682-014-9341-9</pub-id><pub-id pub-id-type="pmid">25515348</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian Copula</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>1541</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1002/hbm.23471</pub-id><pub-id pub-id-type="pmid">27860095</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Jaworska</surname><given-names>K</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>van Rijsbergen</surname><given-names>NJ</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The deceptively simple N170 reflects Network Information Processing Mechanisms Involving Visual Feature Coding and transfer across hemispheres</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>4123</fpage><lpage>4135</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw196</pub-id><pub-id pub-id-type="pmid">27550865</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>van Rijsbergen</surname><given-names>NJ</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Tracing the flow of perceptual features in an algorithmic brain network</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>17681</elocation-id><pub-id pub-id-type="doi">10.1038/srep17681</pub-id><pub-id pub-id-type="pmid">26635299</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kandylaki</surname><given-names>KD</given-names></name><name><surname>Nagels</surname><given-names>A</given-names></name><name><surname>Tune</surname><given-names>S</given-names></name><name><surname>Kircher</surname><given-names>T</given-names></name><name><surname>Wiese</surname><given-names>R</given-names></name><name><surname>Schlesewsky</surname><given-names>M</given-names></name><name><surname>Bornkessel-Schlesewsky</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Predicting 'When' in discourse engages the human dorsal auditory stream: An fMRI study using naturalistic stories</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>12180</fpage><lpage>12191</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4100-15.2016</pub-id><pub-id pub-id-type="pmid">27903727</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual enhancement of the information representation in auditory cortex</article-title><source>Current Biology</source><volume>20</volume><fpage>19</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.10.068</pub-id><pub-id pub-id-type="pmid">20036538</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>SJ</given-names></name><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Irregular Speech Rate dissociates Auditory Cortical Entrainment, evoked responses, and Frontal Alpha</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>14691</fpage><lpage>14701</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2243-15.2015</pub-id><pub-id pub-id-type="pmid">26538641</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>SJ</given-names></name><name><surname>McNair</surname><given-names>SW</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prestimulus influences on auditory perception from sensory representations and decision processes</article-title><source>PNAS</source><volume>113</volume><fpage>4842</fpage><lpage>4847</lpage><pub-id pub-id-type="doi">10.1073/pnas.1524087113</pub-id><pub-id pub-id-type="pmid">27071110</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks</article-title><source>NeuroImage</source><volume>147</volume><fpage>32</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.062</pub-id><pub-id pub-id-type="pmid">27903440</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krieger-Redwood</surname><given-names>K</given-names></name><name><surname>Gaskell</surname><given-names>MG</given-names></name><name><surname>Lindsay</surname><given-names>S</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The selective role of premotor cortex in speech perception: a contribution to phoneme judgements but not speech comprehension</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>2179</fpage><lpage>2188</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00463</pub-id><pub-id pub-id-type="pmid">23937689</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Information-based functional brain mapping</article-title><source>PNAS</source><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>O'Connell</surname><given-names>MN</given-names></name><name><surname>Barczak</surname><given-names>A</given-names></name><name><surname>Mills</surname><given-names>A</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The leading sense: supramodal control of neurophysiological context by attention</article-title><source>Neuron</source><volume>64</volume><fpage>419</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.10.014</pub-id><pub-id pub-id-type="pmid">19914189</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Physical and perceptual factors shape the neural mechanisms that integrate audiovisual signals in speech comprehension</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>11338</fpage><lpage>11350</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6510-10.2011</pub-id><pub-id pub-id-type="pmid">21813693</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Massey</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1990">1990</year><chapter-title>Causality, Feedback and Directed Information</chapter-title><source>Proc Int Symp Inf Theory Applic (ISITA-90)</source><fpage>303</fpage><lpage>305</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGettigan</surname><given-names>C</given-names></name><name><surname>Faulkner</surname><given-names>A</given-names></name><name><surname>Altarelli</surname><given-names>I</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Baverstock</surname><given-names>H</given-names></name><name><surname>Scott</surname><given-names>SK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Speech comprehension aided by multiple modalities: behavioural and neural interactions</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>762</fpage><lpage>776</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.01.010</pub-id><pub-id pub-id-type="pmid">22266262</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname><given-names>IG</given-names></name><name><surname>Wilson</surname><given-names>SM</given-names></name><name><surname>Deblieck</surname><given-names>C</given-names></name><name><surname>Wu</surname><given-names>AD</given-names></name><name><surname>Iacoboni</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The essential role of premotor cortex in speech perception</article-title><source>Current Biology</source><volume>17</volume><fpage>1692</fpage><lpage>1696</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.08.064</pub-id><pub-id pub-id-type="pmid">17900904</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title><source>Nature</source><volume>485</volume><fpage>233</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature11020</pub-id><pub-id pub-id-type="pmid">22522927</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morís Fernández</surname><given-names>L</given-names></name><name><surname>Visser</surname><given-names>M</given-names></name><name><surname>Ventura-Campos</surname><given-names>N</given-names></name><name><surname>Ávila</surname><given-names>C</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Top-down attention regulates the neural expression of audiovisual integration</article-title><source>NeuroImage</source><volume>119</volume><fpage>272</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.052</pub-id><pub-id pub-id-type="pmid">26119022</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Hackett</surname><given-names>TA</given-names></name><name><surname>Kajikawa</surname><given-names>Y</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predictive motor control of sensory dynamics in auditory active sensing</article-title><source>Current Opinion in Neurobiology</source><volume>31</volume><fpage>230</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.12.005</pub-id><pub-id pub-id-type="pmid">25594376</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>AR</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Dynamic changes in superior temporal sulcus connectivity during perception of noisy audiovisual speech</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1704</fpage><lpage>1714</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4853-10.2011</pub-id><pub-id pub-id-type="pmid">21289179</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>BS</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>EEG phase patterns reflect the selectivity of neural firing</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>389</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs031</pub-id><pub-id pub-id-type="pmid">22345353</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohshiro</surname><given-names>T</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A normalization model of multisensory integration</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>775</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1038/nn.2815</pub-id><pub-id pub-id-type="pmid">21552274</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osnes</surname><given-names>B</given-names></name><name><surname>Hugdahl</surname><given-names>K</given-names></name><name><surname>Specht</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Effective connectivity analysis demonstrates involvement of premotor cortex during speech perception</article-title><source>NeuroImage</source><volume>54</volume><fpage>2437</fpage><lpage>2445</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.078</pub-id><pub-id pub-id-type="pmid">20932914</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Senatore</surname><given-names>R</given-names></name><name><surname>Montemurro</surname><given-names>MA</given-names></name><name><surname>Petersen</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Correcting for the sampling Bias problem in spike train information measures</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>1064</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.1152/jn.00559.2007</pub-id><pub-id pub-id-type="pmid">17615128</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title><source>Current Biology</source><volume>25</volume><fpage>1649</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.049</pub-id><pub-id pub-id-type="pmid">26028433</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Lip movements entrain the observers' low-frequency brain oscillations to facilitate speech intelligibility</article-title><source>eLife</source><volume>5</volume><elocation-id>e14521</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14521</pub-id><pub-id pub-id-type="pmid">27146891</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural oscillations carry speech rhythm through to comprehension</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>320</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00320</pub-id><pub-id pub-id-type="pmid">22973251</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Sommers</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prediction and constraint in audiovisual speech perception</article-title><source>Cortex</source><volume>68</volume><fpage>169</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.006</pub-id><pub-id pub-id-type="pmid">25890390</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pickering</surname><given-names>MJ</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>An integrated theory of language production and comprehension</article-title><source>Behavioral and Brain Sciences</source><volume>36</volume><fpage>329</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12001495</pub-id><pub-id pub-id-type="pmid">23789620</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The analysis of speech in different temporal integration windows: cerebral lateralization as ‘asymmetric sampling in time’</article-title><source>Speech Communication</source><volume>41</volume><fpage>245</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00107-3</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neuroanatomic and neurophysiological infrastructure for speech and language</article-title><source>Current Opinion in Neurobiology</source><volume>28</volume><fpage>142</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.07.005</pub-id><pub-id pub-id-type="pmid">25064048</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pola</surname><given-names>G</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>An exact method to quantify the information transmitted by different mechanisms of correlational coding</article-title><source>Network: Computation in Neural Systems</source><volume>14</volume><fpage>35</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1088/0954-898X/14/1/303</pub-id><pub-id pub-id-type="pmid">12613551</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A computational perspective on the neural basis of multisensory spatial representations</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>741</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1038/nrn914</pub-id><pub-id pub-id-type="pmid">12209122</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A review and synthesis of the first 20 years of PET and fMRI studies of heard speech, spoken language and reading</article-title><source>NeuroImage</source><volume>62</volume><fpage>816</fpage><lpage>847</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.062</pub-id><pub-id pub-id-type="pmid">22584224</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>JP</given-names></name><name><surname>Scott</surname><given-names>SK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>718</fpage><lpage>724</lpage><pub-id pub-id-type="doi">10.1038/nn.2331</pub-id><pub-id pub-id-type="pmid">19471271</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riedel</surname><given-names>P</given-names></name><name><surname>Ragert</surname><given-names>P</given-names></name><name><surname>Schelinski</surname><given-names>S</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name><name><surname>von Kriegstein</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual face-movement sensitive cortex is relevant for auditory-only speech recognition</article-title><source>Cortex</source><volume>68</volume><fpage>86</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2014.11.016</pub-id><pub-id pub-id-type="pmid">25650106</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>LA</given-names></name><name><surname>Saint-Amour</surname><given-names>D</given-names></name><name><surname>Leavitt</surname><given-names>VM</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Do you see what I am saying? exploring visual enhancement of speech comprehension in noisy environments</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>1147</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl024</pub-id><pub-id pub-id-type="pmid">16785256</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schepers</surname><given-names>IM</given-names></name><name><surname>Yoshor</surname><given-names>D</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Electrocorticography reveals enhanced Visual Cortex responses to visual speech</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>4103</fpage><lpage>4110</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu127</pub-id><pub-id pub-id-type="pmid">24904069</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Synergy, redundancy, and independence in population codes</article-title><source>Journal of Neuroscience</source><volume>23</volume><fpage>11539</fpage><lpage>11553</lpage><pub-id pub-id-type="pmid">14684857</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiber</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Measuring information transfer</article-title><source>Physical Review Letters</source><volume>85</volume><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.85.461</pub-id><pub-id pub-id-type="pmid">10991308</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Kajikawa</surname><given-names>Y</given-names></name><name><surname>Partan</surname><given-names>S</given-names></name><name><surname>Puce</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neuronal oscillations and visual amplification of speech</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>106</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.01.002</pub-id><pub-id pub-id-type="pmid">18280772</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Low-frequency neuronal oscillations as instruments of sensory selection</article-title><source>Trends in Neurosciences</source><volume>32</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.09.012</pub-id><pub-id pub-id-type="pmid">19012975</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>JL</given-names></name><name><surname>Berthommier</surname><given-names>F</given-names></name><name><surname>Savariaux</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Seeing to hear better: evidence for early audio-visual interactions in speech identification</article-title><source>Cognition</source><volume>93</volume><fpage>B69</fpage><lpage>B78</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2004.01.006</pub-id><pub-id pub-id-type="pmid">15147940</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>JL</given-names></name><name><surname>Savariaux</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>No, there is no 150 ms lead of visual speech on auditory speech, but a range of audiovisual asynchronies varying from small audio lead to large audio lag</article-title><source>PLoS Computational Biology</source><volume>10</volume><elocation-id>e1003743</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003743</pub-id><pub-id pub-id-type="pmid">25079216</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>JI</given-names></name><name><surname>Goldin-Meadow</surname><given-names>S</given-names></name><name><surname>Nusbaum</surname><given-names>HC</given-names></name><name><surname>Small</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gestures orchestrate brain networks for language understanding</article-title><source>Current Biology</source><volume>19</volume><fpage>661</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.02.051</pub-id><pub-id pub-id-type="pmid">19327997</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Detecting and representing predictable structure during auditory scene analysis</article-title><source>eLife</source><volume>5</volume><elocation-id>e19113</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19113</pub-id><pub-id pub-id-type="pmid">27602577</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sumby</surname><given-names>WH</given-names></name><name><surname>Pollack</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Visual contribution to speech intelligibility in noise</article-title><source>The Journal of the Acoustical Society of America</source><volume>26</volume><fpage>212</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1121/1.1907309</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavano</surname><given-names>A</given-names></name><name><surname>Scharinger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prediction in speech and language processing</article-title><source>Cortex</source><volume>68</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.05.001</pub-id><pub-id pub-id-type="pmid">26048658</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorne</surname><given-names>JD</given-names></name><name><surname>Debener</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Look now and hear what's coming: on the functional role of cross-modal phase reset</article-title><source>Hearing Research</source><volume>307</volume><fpage>144</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.07.002</pub-id><pub-id pub-id-type="pmid">23856236</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Atteveldt</surname><given-names>N</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Blomert</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Integration of letters and speech sounds in the human brain</article-title><source>Neuron</source><volume>43</volume><fpage>271</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.06.025</pub-id><pub-id pub-id-type="pmid">15260962</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Atteveldt</surname><given-names>N</given-names></name><name><surname>Murray</surname><given-names>MM</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multisensory integration: flexible use of general operations</article-title><source>Neuron</source><volume>81</volume><fpage>1240</fpage><lpage>1253</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.02.044</pub-id><pub-id pub-id-type="pmid">24656248</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Wassenhove</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Speech through ears and eyes: interfacing the senses with the supramodal brain</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>388</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00388</pub-id><pub-id pub-id-type="pmid">23874309</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vetter</surname><given-names>P</given-names></name><name><surname>Smith</surname><given-names>FW</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding sound and imagery content in early visual cortex</article-title><source>Current Biology</source><volume>24</volume><fpage>1256</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.04.020</pub-id><pub-id pub-id-type="pmid">24856208</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vicente</surname><given-names>R</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Lindner</surname><given-names>M</given-names></name><name><surname>Pipa</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transfer entropy--a model-free measure of effective connectivity for the neurosciences</article-title><source>Journal of Computational Neuroscience</source><volume>30</volume><fpage>45</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1007/s10827-010-0262-3</pub-id><pub-id pub-id-type="pmid">20706781</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Rahm</surname><given-names>B</given-names></name><name><surname>Rieder</surname><given-names>M</given-names></name><name><surname>Lindner</surname><given-names>M</given-names></name><name><surname>Vicente</surname><given-names>R</given-names></name><name><surname>Kaiser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transfer entropy in magnetoencephalographic data: quantifying information flow in cortical and cerebellar networks</article-title><source>Progress in Biophysics and Molecular Biology</source><volume>105</volume><fpage>80</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.pbiomolbio.2010.11.006</pub-id><pub-id pub-id-type="pmid">21115029</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wild</surname><given-names>CJ</given-names></name><name><surname>Yusuf</surname><given-names>A</given-names></name><name><surname>Wilson</surname><given-names>DE</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effortful listening: the processing of degraded speech depends critically on attention</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>14010</fpage><lpage>14021</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1528-12.2012</pub-id><pub-id pub-id-type="pmid">23035108</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>SM</given-names></name><name><surname>Saygin</surname><given-names>AP</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Iacoboni</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Listening to speech activates motor areas involved in speech production</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>701</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/nn1263</pub-id><pub-id pub-id-type="pmid">15184903</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>AM</given-names></name><name><surname>Ridgway</surname><given-names>GR</given-names></name><name><surname>Webster</surname><given-names>MA</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Permutation inference for the general linear model</article-title><source>NeuroImage</source><volume>92</volume><fpage>381</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.01.060</pub-id><pub-id pub-id-type="pmid">24530839</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>TM</given-names></name><name><surname>Pelphrey</surname><given-names>KA</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>McKeown</surname><given-names>MJ</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Polysensory interactions along lateral temporal regions evoked by audiovisual speech</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1034</fpage><lpage>1043</lpage><pub-id pub-id-type="doi">10.1093/cercor/13.10.1034</pub-id><pub-id pub-id-type="pmid">12967920</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Speer</surname><given-names>NK</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural substrates of narrative comprehension and memory</article-title><source>NeuroImage</source><volume>41</volume><fpage>1408</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.062</pub-id><pub-id pub-id-type="pmid">18499478</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname><given-names>E</given-names></name><name><surname>Cogan</surname><given-names>GB</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Visual input enhances selective speech envelope tracking in auditory cortex at a &quot;cocktail party&quot;</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>1417</fpage><lpage>1426</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3675-12.2013</pub-id><pub-id pub-id-type="pmid">23345218</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname><given-names>EM</given-names></name><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Schevon</surname><given-names>CA</given-names></name><name><surname>McKhann</surname><given-names>GM</given-names></name><name><surname>Goodman</surname><given-names>RR</given-names></name><name><surname>Emerson</surname><given-names>R</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Mechanisms underlying selective neuronal tracking of attended speech at a &quot;cocktail party&quot;</article-title><source>Neuron</source><volume>77</volume><fpage>980</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id><pub-id pub-id-type="pmid">23473326</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.24763.020</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schroeder</surname><given-names>Charles E</given-names></name><role>Reviewing editor</role><aff><institution>Columbia University College of Physicians and Surgeons</institution>, <country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Contributions of local speech encoding and functional connectivity to audio-visual speech integration&quot; for consideration by <italic>eLife</italic>. Your article has been favorably evaluated by Richard Ivry (Senior Editor) and three reviewers, one of whom is a member of our Board of Reviewing Editors.. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This study used MEG along with perceptual measures to address the question of how SNR and the informativeness of visual inputs combine to enhance both information encoding and the network interactions of brain areas processing speech cues. The authors highlight several results including 1) predictable feedforward auditory activation during high SNR, 2) visually mediated facilitation of auditory information representation in both auditory and premotor cortex, 3) an interaction of SNR and visual informativeness in in several frontal regions and 4) strong associated patterns of feedforward and feedback connectivity between auditory and premotor cortices. The behavioral benefits of viewing the speaker's face seemed best associated with the connectivity changes.</p><p>Essential revisions:</p><p>The reviewers identified a number of concerns and suggestions that the authors should explicitly address:</p><p>1) To disambiguate power and functional connectivity, and for other reasons (below), it would be very helpful if the authors would carefully detail the distribution of MEG power (by frequency) for each of the experimental conditions, but particularly for the increasing auditory SNR conditions. An additional comment below on this.</p><p>2) In general, the analysis lumps frequencies together a bit more than is ideal, the only division discussed at present is between the.25-1.0 Hz and the 1.0-4.0 Hz bands. Following from the power analysis asked for above, are there effects in other obvious bands (e.g., 4-7, etc.), and coupling interactions between bands? Do cross frequency interactions play a part in cross regional interactions?</p><p>3) The Discussion brings up the issue of predictive coding (citing the Arnal and Bastos studies). If the predictive coding account is to be invoked, it also makes sense to explore some of its predictions; e.g., the prediction error is the main component of the feedforward signal.</p><p>4) It would be helpful if the authors might be able to better link their findings to mechanisms of multimodal enhancement, e.g., phase reset vs. divisive normalization (van Atteveldt, 2015).</p><p>5) It's not clear what you mean by integration or enhancement. Normally, if you thought two different signals were being integrated, you would measure them each separately and then show that something non-linear happens when you combine them. MI is particularly well suited for this kind of analysis. You don't really show that here. For instance, it could be that in the low VI condition, subjects are simply not looking at the visual stimuli (there's no eye tracking as far as I can tell, so you can't say). This would make your comparison an AV vs. A comparison as opposed to a faulty integration interpretation. Furthermore, in the high VI case, it is unclear if two channels of sensory data are being integrated or if the brain is trading one channel for another. The easiest way forward I think, is to remove integration and keep enhancement.</p><p>6) It is concerning that you find such minimal (essentially non-existent) results in the LH. This is even true of the visual cortex! This may undermine the strength of the manuscript and make it a bit unclear what is actually happening. While you rightfully point out that your results may be expected given the frequency range of interest (&lt;=4 Hz) and its relation to prosodic/syllabic information, I think this then requires a particular class of interpretation that I don't think is present in the current manuscript. Note that while you are correct to point out that various speech models (e.g. Hickok and Poeppel) suggest a rightward bias towards analyses on longer timescales, the models do indicate that this information is processed in both hemispheres, a result not demonstrated in the present manuscript. At the very least, I would reframe your interpretation to focus on what exactly you think the RH is doing here and why the LH is not doing it. If you think that audiovisual information is being integrated on longer timescales only, then say so. If you think that your measure only measures longer timescales and that this is why you only see RH effects, then say that.</p><p>7) The directed connectivity analysis is very interesting, but hard to interpret. It seems that in general, directed connectivity increases as a function of stimulus SNR (but again, this may be a by-product of neural SNR) and there is generally an effect of visual information, but not in all cases. The most interesting results are the interaction results (HGR-&gt;PMCL and SFGR-&gt;VCR) which show opposite effects. I think these results warrant more of an explanation especially given that they differ from the other results.</p><p>8) In your previous work (Park et al. 2016), you recognized that the visual information from AV speech is strongly correlated with the speech envelope. You rightfully incorporate this in your analysis for that manuscript, but not this one. One can make the argument that the purpose of that work was different (role of motor cortex vs. audio-visual integration), but I think then that you need to explain clearly what you have in mind when you say integration. What is exactly is being integrated and how? A clearer idea may also help the previous RH vs. LH issues listed above.</p><p>9) Related to this point it would also be helpful if this paper's findings were more explicitly separated from those of the authors' paper last year in <italic>eLife</italic> (Park et al).</p><p>10) It would be helpful if a supplementary analysis were done for each of the behavioral results that uses d' instead of probability of correct answers.</p><p>11) Although the main conclusions about the SNR effects are convincing, the conclusions related to the multisensory effects are not. It is not ruled out that the multisensory effects could be purely caused by lip reading effects. Recently, a series of papers by Edmund Lalor's group showed that a talking face alone can lead to neural entrainment to the (not presented) speech envelope. Previous work by Luo et al. showed a similar effect.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.24763.021</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions:</italic> </p><p><italic>The reviewers identified a number of concerns and suggestions that the authors should explicitly address:</italic> </p><p><italic>1) To disambiguate power and functional connectivity, and for other reasons (below), it would be very helpful if the authors would carefully detail the distribution of MEG power (by frequency) for each of the experimental conditions, but particularly for the increasing auditory SNR conditions. An additional comment below on this.</italic> </p><p>To address this point we have systematically analyzed the power of oscillatory activity and potential changes of this across the experimental conditions. We implemented this analysis for each of the frequency bands investigated in the speech entrainment analysis and focused on the ROIs used in the functional connectivity analysis, i.e., those voxels that exhibited interesting condition effects in speech entrainment. This analysis is detailed in the Methods (section ‘Analysis of condition effects on MEG amplitude’). The respective results are presented in a separate section in the Results (section ‘Changes in speech entrainment are not a result of changes in oscillatory amplitude’), <xref ref-type="table" rid="tbl5">Table 5</xref>and <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>. They reveal a suppression of power during the VI condition in several ROIs and bands, but no significant SNR or SNRxVIVN effects on power. Importantly, none of these changes in the strength of oscillatory activity overlaps spectrally and anatomically with the condition effects on speech entrainment reported in our main results (<xref ref-type="fig" rid="fig3">Figure 3</xref>); and only the change in functional connectivity between pSTG and IFG (<xref ref-type="fig" rid="fig4">Figure 4</xref>) overlaps with a change in pSTG‐R power in the 4‐8 Hz range. All in all, these results suggest that the reported visual enhancement of speech encoding and connectivity are unlikely to be systematically linked to changes in the strength of oscillatory activity, as we now conclude in the respective section of the Results.</p><p><italic>2) In general, the analysis lumps frequencies together a bit more than is ideal, the only division discussed at present is between the.25-1.0 Hz and the 1.0-4.0 Hz bands. Following from the power analysis asked for above, are there effects in other obvious bands (e.g., 4-7, etc.), and coupling interactions between bands? Do cross frequency interactions play a part in cross regional interactions?</italic> </p><p>The focus on the lowest frequency bands (0.25‐1 and 1‐4Hz) in the Results and Discussion is a genuine finding, not a selection bias in our approach. As outlined in the Methods, we followed a data driven approach and quantified speech entrainment and condition effects across a wide range of frequency bands, from 0.25Hz to 48Hz. The width of these bands increased with frequency, as is common practice in filtering and wavelet approaches. By Heisenberg’s theorem, using more narrow bands will necessarily compromise temporal resolution, which however is important to quantify reliably the temporal similarity between speech and brain signals (Gross JNeurosciMethods’ 14). Here the specific choice of bands and bandwidths was driven by our experience in analyzing band‐limited field potentials and neuroimaging signals (Kayser et al. Neuron ‘09, Belitski JCompNeurosci’ 10; Ng et al. CerCor’ 12; Keitel et al. Neuroimage ‘16), which maximizes sensitivity without compromising frequency resolution, and ensures computational tractability, as closer frequencies tend to be highly correlated and increase computational burden without providing any additional gain in knowledge.</p><p>To address the second part of the question we have implemented a systematic analysis of cross‐frequency interactions as characterized by phase‐amplitude coupling (PAC). We focused on PAC based on the phase in low frequencies (up to 8Hz) and the power in higher bands (above 8 Hz). The relevant details are presented in the Methods (section ‘Analysis of phase‐amplitude coupling’) and the findings are presented in the Results (section ‘Changes in directed connectivity do not reflect changes in phase‐amplitude coupling’) and <xref ref-type="table" rid="tbl6">Table 6</xref>. While we observed significant PAC across a number of bands and ROIs, we did not find significant changes in PAC with the experimental conditions, suggesting that the condition‐ changes in functional connectivity are not systematically related to specific patterns of cross‐ frequency coupling, as we now conclude in the Results.</p><p><italic>3) The Discussion brings up the issue of predictive coding (citing the Arnal and Bastos studies). If the predictive coding account is to be invoked, it also makes sense to explore some of its predictions; e.g., the prediction error is the main component of the feedforward signal.</italic> </p><p>We agree that the mentioning of predictive coding in the previous submission was rather unspecific and hypothetical. Unfortunately the current experimental design does not allow a direct test of such models and their predictions, as no experimentally quantifiable prediction error or manipulation of such was included. However, given the prevailing notion that lip reading holds predictive signals for acoustic speech (Chandrasekaran PlosComputBiol’ 09; Bernstein SpeechComm ‘04), we expanded the Discussion to provide a more in depth reflection of this issue, as well as to better link the present results to previous work on predictive coding along the auditory pathways (extended first paragraph in the section ‘The mechanistic underpinnings of audio‐visual speech encoding’ of the Discussion).</p><p><italic>4) It would be helpful if the authors might be able to better link their findings to mechanisms of multimodal enhancement, e.g., phase reset vs. divisive normalization (van Atteveldt, 2015).</italic> </p><p>We have added a section discussing potential mechanisms to the Discussion. However, similar to previous studies (e.g. van Attefeldt Neuron 2014) we have to remain vague, as our data don’t directly speak on the underlying neural mechanisms (last paragraph of the Discussion section ‘The mechanistic underpinnings of audio‐visual speech encoding’).</p><p><italic>5) It's not clear what you mean by integration or enhancement. Normally, if you thought two different signals were being integrated, you would measure them each separately and then show that something non-linear happens when you combine them. MI is particularly well suited for this kind of analysis. You don't really show that here. For instance, it could be that in the low VI condition, subjects are simply not looking at the visual stimuli (there's no eye tracking as far as I can tell, so you can't say). This would make your comparison an AV vs. A comparison as opposed to a faulty integration interpretation. Furthermore, in the high VI case, it is unclear if two channels of sensory data are being integrated or if the brain is trading one channel for another. The easiest way forward I think, is to remove integration and keep enhancement.</italic> </p><p>We agree that it is important to define these terms well, in light of the sometimes discrepant use of the term integration (Stein EuroJNeurosci ‘10). First, in the revised manuscript we now avoid the term integration, as our study does not provide direct evidence for the integration of the same speech feature across auditory and visual domains, as would be required to make a direct claim pertaining to multisensory feature integration. As a consequence, we have also modified the title of this submission.</p><p>Second, to enhance the impact of this study along these lines and to address this and other comments (points 8, 11 below), we have added several additional analyses. We have analyzed the encoding of lip movement signals and we have used conditional mutual information and information theoretic redundancy to quantify speech encoding while discounting influences of lip movements. These newly added results are presented in new sections of the Results (‘Noise invariant dynamic representations of lip movements’ and ‘Speech entrainment does not reflect trivial entrainment to lip signals’), <xref ref-type="table" rid="tbl2">Table 2</xref>, and a new <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>. They demonstrate that the entrainment of brain activity to the acoustic speech envelope is largely reflecting a visually‐informed but genuine acoustic representation. Our findings hence speak in favor of an interpretation in which the reported local effects reflect changes within an acoustically driven speech representation that is influenced by visual and acoustic context, rather than a bimodal audio‐visual representation of acoustic and visual speech information within the respective ROIs. Yet, given the lack of clear evidence for the integration of the same feature or linguistic information within each ROI in the present data, we refrain from interpreting our results in the light of multisensory integration and hence avoid this term.</p><p>We have also revised the Discussion to more clearly interpret our results in the context of the newly added analyses (in section ‘Entrained auditory and visual speech representations in temporal, parietal and frontal lobes’).</p><p><italic>6) It is concerning that you find such minimal (essentially non-existent) results in the LH. This is even true of the visual cortex! This may undermine the strength of the manuscript and make it a bit unclear what is actually happening. While you rightfully point out that your results may be expected given the frequency range of interest (&lt;=4 Hz) and its relation to prosodic/syllabic information, I think this then requires a particular class of interpretation that I don't think is present in the current manuscript. Note that while you are correct to point out that various speech models (e.g. Hickok and Poeppel) suggest a rightward bias towards analyses on longer timescales, the models do indicate that this information is processed in both hemispheres, a result not demonstrated in the present manuscript. At the very least, I would reframe your interpretation to focus on what exactly you think the RH is doing here and why the LH is not doing it. If you think that audiovisual information is being integrated on longer timescales only, then say so. If you think that your measure only measures longer timescales and that this is why you only see RH effects, then say that.</italic> </p><p>The comment identifies an important issue that still poses controversies in the current literature. The data presented in the previous submission exhibited significantly stronger speech entrainment in the right vs. the left hemisphere (revised <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), and we had reported significant condition effects mostly within the right hemisphere. However, this had left it unclear whether there was indeed a significant difference between hemispheres in terms of the strength of the condition effects (as expressed by the GLM betas). We have performed an additional analysis to directly test this, which is now presented in the Results (section ‘Condition effects are hemisphere‐dominant but not strictly lateralized’). This revealed that only one of the reported speech MI effects in <xref ref-type="fig" rid="fig3">Figure 3</xref> is significantly lateralized (the SNR effect in IFGop‐R); presented in new <xref ref-type="table" rid="tbl2">Table 2</xref>. This suggests that care should be taken when interpreting the dominance of significant condition effects in the RH as evidence for a significant lateralization.</p><p>Furthermore, we would like to note that the apparent lateralization of speech related activity as reported in the literature potentially depends on a large number of factors, incl. the nature of the recorded brain signal (fMRI, MEG frequency bands), the type of the stimulus material (single words, full sentences), and the specific statistical contrast used (activation vs. rest, or a specific experimental effect such as the encoding of word identity). For example, several studies have specifically implied the right hemisphere in the encoding of connected speech (Alexandrou Neuroimage’ 17; Horowitz‐Kraus ‘15), or demonstrated a stronger involvement of low‐frequency activity or low‐frequency speech‐entrainment in the right hemisphere (Giraud Neuron’ 07; Gross et al. PlosBiol’ 13). Given the previous literature it is not surprising to us to observe a dominance of the right hemisphere in the present study, which uses continuous speech.</p><p>Finally, we agree with the reviewers that the observed dominance of the RH could possibly result from the use of speech‐to‐brain entrainment as an index of speech encoding in the present study. It is possible that this measure naturally emphasizes longer time scales (e.g. below 12Hz) given their dominance on signal power (c.f. <xref ref-type="fig" rid="fig1">Figure 1A</xref>, speech envelope power plot). Given the possibility that the RH is particularly involved in processing speech information on these time scales, the methodological approach used here may contribute to the observed RH dominance.</p><p>We revised the Discussion to include these considerations and the new results (new section ‘A lack of evidence for lateralized representations’).</p><p><italic>7) The directed connectivity analysis is very interesting, but hard to interpret. It seems that in general, directed connectivity increases as a function of stimulus SNR (but again, this may be a by-product of neural SNR) and there is generally an effect of visual information, but not in all cases. The most interesting results are the interaction results (HGR-&gt;PMCL and SFGR-&gt;VCR) which show opposite effects. I think these results warrant more of an explanation especially given that they differ from the other results.</italic></p><p>In the Discussion we emphasize the positive main effects of SNR and VIVN, in particular as these relate to the behavioral data and hence are the most important and straightforward to interpret. However, we do agree that these mentioned (negative) interactions are interesting and warrant explicit consideration. We have extended the Discussion to elaborate on these in more detail.</p><p><italic>8) In your previous work (Park et al. 2016), you recognized that the visual information from AV speech is strongly correlated with the speech envelope. You rightfully incorporate this in your analysis for that manuscript, but not this one. One can make the argument that the purpose of that work was different (role of motor cortex vs. audio-visual integration), but I think then that you need to explain clearly what you have in mind when you say integration. What is exactly is being integrated and how? A clearer idea may also help the previous RH vs. LH issues listed above.</italic> </p><p>This is an important point, which we have addressed by substantial additional data analysis (as also mentioned under point 5 above). We have revised the Methods to provide all the relevant details (section ‘Decomposition of audio‐visual information’).</p><p>First, we characterized the temporal dynamics of lip movements, as in our previous study (Park et al., 2016), and systematically quantified the lip‐to‐brain entrainment in the same way as the speech‐to‐brain entrainment. The Results (section ‘Noise invariant dynamic representations of lip movements’) and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, and <xref ref-type="table" rid="tbl2">Table 2</xref>, reveal the wide‐spread entrainment to dynamic lip signals in visual cortex and the temporal lobe. However, we did not find significant changes in the lip encoding across SNRs. Furthermore, we performed a direct comparison of the relative strength of acoustic and lip MI within the regions of interest (<xref ref-type="table" rid="tbl2">Table 2</xref>). This revealed stronger encoding of acoustic speech across many ROIs and bands, but a dominance of lip representations in visual cortex. Consequently, the revised manuscript now provides a comparative analysis of neural entrainment to acoustic (speech envelope) and visual (lip movements) speech signals at the same time scales, and how each of these is affected by acoustic SNR.</p><p>Second, we now include two analyses of the information theoretic dependency between acoustic and visual speech representations (section ‘Speech entrainment does not reflect trivial entrainment to lip dynamics). The results (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, <xref ref-type="table" rid="tbl2">Table 2</xref>)demonstrate that the entrainment of brain activity to the acoustic speech envelope within the considered ROIs is not trivially explained by an overlap of separate representations of the acoustic and visual speech signals, but rather reflects a visually informed, but genuinely acoustic representation. Importantly, all of the condition effects on speech entrainment reported in <xref ref-type="fig" rid="fig3">Figure 3</xref> persisted when factoring out direct influences of lip movements using conditional mutual information (<xref ref-type="table" rid="tbl2">Table 2</xref>). Furthermore, the redundancy between representations of acoustic and visual speech was small, and only significant in association cortices.</p><p>These results have provided several important new insights: First, early sensory regions seem to contain largely genuine representations of acoustic speech, while association regions contain overlapping representations of acoustic speech and lip movements. Second, the representation of acoustic speech in visual cortex is not a trivial consequence of the local representation of lip movements information (which we describe by the additional analysis), but rather seems to reflect an independent representation of acoustically informed speech. These insights have been included in the revised Discussion (subsection “Multisensory enhancement of speech encoding in the frontal lobe”).</p><p><italic>9) Related to this point it would also be helpful if this paper's findings were more explicitly separated from those of the authors' paper last year in eLife (Park et al).</italic> </p><p>We now better differentiate the current and this previous study, which has become even more important given that the revised manuscript in part replicates some of the findings reported in Park et al. 2016 (subsections “Entrained auditory and visual speech representations in temporal, parietal and frontal lobes” and “Multisensory behavioral benefits arise from distributed network mechanisms”).</p><p><italic>10) It would be helpful if a supplementary analysis were done for each of the behavioral results that uses d' instead of probability of correct answers.</italic> </p><p>By design of our experimental paradigm the specific words that were part of the presented story (the signal in detection theory) were associated with specific experimental conditions; however, the words that were not part of the story (the noise) were not. As a consequence, whereas hit rate varied across conditions, false alarm rate was bound to be constant across conditions. Under these circumstances, condition‐specific d’ is strongly correlated with condition‐specific performance. Specifically, the group‐average correlation between condition‐specific performance and d’ was 0.97 (SEM = 0.06) and highly significant (T(18) = 32.57, p &lt; 10<sup>-6</sup>). As a result, it would have made little difference to compute the neuro‐behavioral correlations with d’ rather than the probability correct. We have added this information to the Methods (section ‘Experimental design and stimulus presentation’).</p><p><italic>11) Although the main conclusions about the SNR effects are convincing, the conclusions related to the multisensory effects are not. It is not ruled out that the multisensory effects could be purely caused by lip reading effects. Recently, a series of papers by Edmund Lalor's group showed that a talking face alone can lead to neural entrainment to the (not presented) speech envelope. Previous work by Luo et al. showed a similar effect.</italic> </p><p>This relates to point 8 above, which we have addressed with substantial additional analysis as outlined there.</p></body></sub-article></article>