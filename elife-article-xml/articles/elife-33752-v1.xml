<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">33752</article-id><article-id pub-id-type="doi">10.7554/eLife.33752</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A neural-level model of spatial memory and imagery</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-39187"><name><surname>Bicanski</surname><given-names>Andrej</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3356-1034</contrib-id><email>a.bicanski@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-14649"><name><surname>Burgess</surname><given-names>Neil</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0646-6584</contrib-id><email>n.burgess@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Institute of Cognitive Neuroscience</institution><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Colgin</surname><given-names>Laura</given-names></name><role>Reviewing Editor</role><aff><institution>The University of Texas at Austin, Center for Learning and Memory</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>04</day><month>09</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>7</volume><elocation-id>e33752</elocation-id><history><date date-type="received" iso-8601-date="2017-11-28"><day>28</day><month>11</month><year>2017</year></date><date date-type="accepted" iso-8601-date="2018-07-29"><day>29</day><month>07</month><year>2018</year></date></history><permissions><copyright-statement>© 2018, Bicanski et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Bicanski et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-33752-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.33752.001</object-id><p>We present a model of how neural representations of egocentric spatial experiences in parietal cortex interface with viewpoint-independent representations in medial temporal areas, via retrosplenial cortex, to enable many key aspects of spatial cognition. This account shows how previously reported neural responses (place, head-direction and grid cells, allocentric boundary- and object-vector cells, gain-field neurons) can map onto higher cognitive function in a modular way, and predicts new cell types (egocentric and head-direction-modulated boundary- and object-vector cells). The model predicts how these neural populations should interact across multiple brain regions to support spatial memory, scene construction, novelty-detection, ‘trace cells’, and mental navigation. Simulated behavior and firing rate maps are compared to experimental data, for example showing how object-vector cells allow items to be remembered within a contextual representation based on environmental boundaries, and how grid cells could update the viewpoint in imagery during planning and short-cutting by driving sequential place cell activity.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational model</kwd><kwd>episodic memory</kwd><kwd>spatially selective cells</kwd><kwd>spatial cognition</kwd><kwd>scene construction</kwd><kwd>trace cells</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>NEUROMEM</award-id><principal-award-recipient><name><surname>Bicanski</surname><given-names>Andrej</given-names></name><name><surname>Burgess</surname><given-names>Neil</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Human Brain Project SGA1</institution></institution-wrap></funding-source><award-id>720270</award-id><principal-award-recipient><name><surname>Bicanski</surname><given-names>Andrej</given-names></name><name><surname>Burgess</surname><given-names>Neil</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>SpaceCog</award-id><principal-award-recipient><name><surname>Bicanski</surname><given-names>Andrej</given-names></name><name><surname>Burgess</surname><given-names>Neil</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Human Brain Project SGA2</institution></institution-wrap></funding-source><award-id>785907</award-id><principal-award-recipient><name><surname>Bicanski</surname><given-names>Andrej</given-names></name><name><surname>Burgess</surname><given-names>Neil</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Burgess</surname><given-names>Neil</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The BB model explains spatial cognition in terms of interactions between specific neuronal populations, providing a common computational framework for the human neuropsychological and in vivo animal electrophysiological literatures.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The ability to reconstruct perceptual experiences into imagery constitutes one of the hallmarks of human cognition, from the ability to imagine past episodes (Tulving 1985) to planning future scenarios (<xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>). Intriguingly, this ability (also known as ‘scene construction’ and ‘episodic future thinking’) appears to depend on the hippocampal system (<xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>; <xref ref-type="bibr" rid="bib64">Hassabis et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">Buckner, 2010</xref>), in which direct (spatial) correlates of the activities of single neurons have long been identified in rodents (<xref ref-type="bibr" rid="bib106">O'Keefe and Nadel, 1978</xref>; <xref ref-type="bibr" rid="bib139">Taube et al., 1990a</xref>; <xref ref-type="bibr" rid="bib60">Hafting et al., 2005</xref>) and more recently in humans (<xref ref-type="bibr" rid="bib45">Ekstrom et al., 2003</xref>; <xref ref-type="bibr" rid="bib75">Jacobs et al., 2010</xref>). The rich catalog of behavioral, neuropsychological and functional imaging findings on one side, and the vast literature of electrophysiological research on the other (see e.g. <xref ref-type="bibr" rid="bib23">Burgess et al., 2002</xref>), promises to allow an explanation of higher cognitive functions such as spatial memory and imagery directly in terms of the interactions of neural populations in specific brain areas. However, while attaining this type of understanding is a major aim of cognitive neuroscience, it cannot usually be captured by a few simple equations because of the number and complexity of the systems involved. Here, we show how neural activity could give rise to spatial cognition, using simulations of multiple brain areas whose predictions can be directly compared to experimental data at neuronal, systems and behavioral levels.</p><p>Extending the Byrne, Becker and Burgess model of spatial memory and imagery of empty environments (<xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>), we propose a large-scale systems-level model of the interaction between Papez’ circuit, parietal, retrosplenial, and medial temporal areas. The model relates the neural response properties of well-known cells types in multiple brain regions to cognitive phenomena such as memory for the spatial context of encountered objects and mental navigation within familiar environments. In brief, egocentric (i.e. body-centered) representations of the local sensory environment, corresponding to a specific point of view, are transformed into viewpoint-independent (allocentric or world-centered) representations for long-term storage in the medial temporal lobes (MTL). The reverse process allows reconstruction of viewpoint-dependent egocentric representations from stored allocentric representations, supporting imagery and recollection.</p><p>Neural populations in the medial temporal lobe (MTL) are modeled after cell types reported in rodent electrophysiology studies. These include place cells (PCs), which fire when an animal traverses a specific location within the environment (<xref ref-type="bibr" rid="bib105">O'Keefe and Dostrovsky, 1971</xref>); head direction cells (HDCs), which fire according to the animal’s head direction relative to the external environment, irrespective of location (<xref ref-type="bibr" rid="bib141">Taube and Ranck, 1990</xref>; <xref ref-type="bibr" rid="bib139">Taube et al., 1990a</xref>; <xref ref-type="bibr" rid="bib140">Taube et al., 1990b</xref>); boundary vector cells (<xref ref-type="bibr" rid="bib87">Lever et al., 2009</xref>); henceforth BVCs), which fire in response to the presence of a boundary at a specific combination of distance and allocentric direction (i.e. North, East, West, South, irrespective of an agent’s orientation); and grid cells (GCs), which exhibit multiple, regularly spaced firing fields (<xref ref-type="bibr" rid="bib60">Hafting et al., 2005</xref>). Evidence for the presence of these cell types in human and non-human primates is mounting steadily (<xref ref-type="bibr" rid="bib120">Robertson et al., 1999</xref>; <xref ref-type="bibr" rid="bib45">Ekstrom et al., 2003</xref>; <xref ref-type="bibr" rid="bib75">Jacobs et al., 2010</xref>; <xref ref-type="bibr" rid="bib37">Doeller et al., 2010</xref>; <xref ref-type="bibr" rid="bib14">Bellmund et al., 2016</xref>; <xref ref-type="bibr" rid="bib72">Horner et al., 2016</xref>; <xref ref-type="bibr" rid="bib98">Nadasdy et al., 2017</xref>).</p><p>The egocentric representation supporting imagery has been suggested to reside in medial parietal cortex (e.g. the precuneus; <xref ref-type="bibr" rid="bib49">Fletcher et al., 1996</xref>; <xref ref-type="bibr" rid="bib81">Knauff et al., 2000</xref>; <xref ref-type="bibr" rid="bib50">Formisano et al., 2002</xref>; <xref ref-type="bibr" rid="bib121">Sack et al., 2002</xref>; <xref ref-type="bibr" rid="bib156">Wallentin et al. (2006)</xref>; <xref ref-type="bibr" rid="bib68">Hebscher et al., 2018</xref>). In the model, it is referred to as the ‘parietal window’ (PW). Its neurons code for the presence of scene elements (boundaries, landmarks, objects) in peri-personal space (ahead, left, right) and correspond to a representation along the dorsal visual stream (the ‘where’ pathway; <xref ref-type="bibr" rid="bib149">Ungerleider, 1982</xref>; <xref ref-type="bibr" rid="bib93">Mishkin et al., 1983</xref>). The parietal window boundary coding (PWb) cells are egocentric analogues of BVCs (<xref ref-type="bibr" rid="bib12">Barry et al., 2006</xref>; <xref ref-type="bibr" rid="bib87">Lever et al., 2009</xref>), consistent with evidence that parietal areas support egocentric spatial processing (<xref ref-type="bibr" rid="bib17">Bisiach and Luzzatti, 1978</xref>; <xref ref-type="bibr" rid="bib99">Nitz, 2009</xref>; <xref ref-type="bibr" rid="bib124">Save and Poucet, 2009</xref>; <xref ref-type="bibr" rid="bib160">Wilber et al., 2014</xref>).</p><p>The transformation between egocentric (parietal) and allocentric (MTL) reference frames is performed by a gain-field circuit in retrosplenial cortex (<xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>; <xref ref-type="bibr" rid="bib160">Wilber et al., 2014</xref>; <xref ref-type="bibr" rid="bib4">Alexander and Nitz, 2015</xref>; <xref ref-type="bibr" rid="bib15">Bicanski and Burgess, 2016</xref>), analogous to gain-field neurons found in posterior parietal cortex (<xref ref-type="bibr" rid="bib130">Snyder et al., 1998</xref>; <xref ref-type="bibr" rid="bib122">Salinas and Abbott, 1995</xref>; <xref ref-type="bibr" rid="bib116">Pouget and Sejnowski, 1997</xref>; <xref ref-type="bibr" rid="bib115">Pouget et al., 2002</xref>) or parieto-occipital areas (<xref ref-type="bibr" rid="bib53">Galletti et al., 1995</xref>). Head-direction provides the gain-modulation in the transformation circuit, producing directionally modulated boundary vector cells which connect egocentric and allocentric boundary coding neurons. That this transformation between egocentric directions (left, right, ahead) and environmentally-referenced directions (nominally North, South, East, West) requires input from the head-direction cells found along Papez’s circuit (<xref ref-type="bibr" rid="bib139">Taube et al., 1990a</xref>; <xref ref-type="bibr" rid="bib140">Taube et al., 1990b</xref>) is consistent with its involvement in episodic memory (e.g. <xref ref-type="bibr" rid="bib2">Aggleton and Brown, 1999</xref>; <xref ref-type="bibr" rid="bib34">Delay and Brion, 1969</xref>).</p><p>During perception the egocentric parietal window representation is based on (highly processed) sensory inputs. That is, it is driven in a bottom-up manner, and the transformation circuit maps the egocentric PWb representation to allocentric BVCs. When the transformation circuit acts in reverse (top-down mode), it reconstructs the parietal representation from BVCs which are co-active with other medial temporal cell populations, forming the substrate of viewpoint-independent (i.e. allocentric) memory. This yields an orientation-specific (egocentric) parietal representation (a specific point of view) and constitutes the model’s account of (spatial) imagery and explicit recall of spatial configurations of known spaces (<xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>). <xref ref-type="fig" rid="fig1">Figure 1</xref> depicts a simplified schematic of the model.</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.002</object-id><label>Figure 1.</label><caption><title>Simplified model schematic.</title><p>(<bold>A</bold>) Processed sensory inputs reach parietal areas and support an egocentric representation of the local environment (in a head-centered frame of reference). Retrosplenial cortex uses current head or gaze direction to perform the transformation from egocentric to allocentric coding. At a given location, environmental layout is represented as an allocentric code by activity in a set of BVCs, the place cells (PCs) corresponding to the location, and perirhinal neurons representing boundary identities (in a familiar environment, all these representations are associated via Hebbian learning to form an attractor network). Black arrows indicate the flow of information during perception and memory encoding (bottom-up). Dotted arrows indicate the reverse flow of information, reconstructing the parietal representation from view-point invariant memory (imagery, top-down). (<bold>B</bold>) Illustration of the egocentric (left panel) and allocentric frame of reference (right panel), where the vector <italic>s</italic> indicates South (an arbitrary reference direction) and the angle <italic>a</italic> is coded for by head direction cells, which modulate the transformation circuit. This allows BVCs and PCs to code for location within a given environmental layout irrespective of the agent’s head direction (HD). The place field (PF, black circle) of an example PC is shown together with possible BVC inputs driving the PC (broad grey arrows).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig1-v1"/></fig><p>To account for the presence of objects within the environment, we propose allocentric object vector cells (OVCs) analogous to BVCs, and show how object-locations can be embedded into spatial memory, supported by visuo-spatial attention. Importantly, the proposed object-coding populations in the MTL map onto recently discovered neuronal populations (<xref ref-type="bibr" rid="bib35">Deshmukh and Knierim, 2013</xref>; <xref ref-type="bibr" rid="bib73">Hoydal et al., 2017</xref>). We also predict a population of egocentric object-coding cells in the parietal window (PWo cells: egocentric analogues to OVCs), as well as directionally modulated boundary and object coding neurons (in the transformation circuit). Finally, we include a grid cell population to account for mental navigation and planning, which drives sequential place cell firing reminiscent of hippocampal ‘replay’ (<xref ref-type="bibr" rid="bib162">Wilson and McNaughton, 1994</xref>; <xref ref-type="bibr" rid="bib51">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib36">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="bib78">Karlsson and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib31">Carr et al., 2011</xref>) and preplay (<xref ref-type="bibr" rid="bib39">Dragoi and Tonegawa, 2011</xref>; <xref ref-type="bibr" rid="bib167">Ólafsdóttir et al., 2015</xref>). We refer to this model as the BB-model.</p></sec><sec id="s2" sec-type="methods"><title>Methods</title><p>Here, we describe the neural populations of the BB-model and how they interact in detail. Technical details of the implementation, equations, and parameter values can be found in the Appendix.</p><sec id="s2-1"><title>Receptive field topology and visualization of data</title><p>We visualize the firing properties of individual spatially selective neurons as firing rate maps that reflect the activity of a neuron averaged over time spent in each location. We also show population activity by arranging all neurons belonging to one population according to the relative locations of their receptive fields (see <xref ref-type="fig" rid="fig2">Figure 2A–C</xref>), plotting a snapshot of their momentary firing rates. In the case of boundary-selective neurons such a population snapshot will yield an outline of the current sensory environment (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Naturally, these neurons may not be physically organized in the same way, and these plots should not be confused with the firing rate maps of individual neurons (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Hence, population snapshots (heat maps) and firing rate maps (Matlab ‘jet’ colormap) are shown in distinct color-codes (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.003</object-id><label>Figure 2.</label><caption><title>Receptive field topology and visualization of neural activity.</title><p>(<bold>A1</bold>) Illustration of the distribution of receptive field centers (RFs) of place cells (PCs), which tile the environment. (<bold>A2</bold>) Receptive fields of boundary responsive neurons, be they allocentric (BVCs) or egocentric (PWb neurons), are distributed on a polar grid, with individual receptive fields centered on each delineated polygon. Two example receptive fields (calculated according to <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>) are overlaid (bright colors) on the polar grids for illustration. Note that each receptive field covers multiple polygons, that is neighboring receptive fields overlap. The polar grids of receptive fields tile space around the agent (red arrow head at center of plots), that is they are anchored to the agent and move with it (for both BVCs and PWb neurons). In addition, for PWb neurons the polar grid of receptive fields also rotates with the agent (i.e. their tuning is egocentric). (<bold>B1</bold>) As the agent (black arrowhead) moves through an environment, place cells (B2) track its location. (<bold>B2</bold>) Snapshot of the population activity of all place cells arranged according to the topology of their firing fields (see A1). (<bold>C1,2</bold>) Snapshots of the population activity for BVCs and boundary selective PW neurons (PWb), respectively. Cells are again distributed according to the topology of their receptive fields (see A2), that is each cell is placed at the location occupied by the centre of its receptive field in peri-personal space (ahead is shown as up for PW neurons; North is shown as up for BVCs). See Section on the transformation circuit, <xref ref-type="video" rid="video1">Video 1</xref>, and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for the mapping between PW and BVCs patterns via the transformation circuit. (<bold>D1,2</bold>) Unlike snapshots of population activity, firing rate maps show the activity of individual neurons averaged over a whole trial in which the agent explores the environment, here for a place cell (D1) and for a boundary vector cell with a receptive field due East (D2, tuning distance roughly 85 cm).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig2-v1"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.33752.004</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Caption: Illustration of single cell coding in the retrosplenial transformation circuit.</title><p>Shaded areas indicate the Parietal Window (PWb), the transformation circuit, head direction modulation, and boundary vector cells (BVCs). Example cells are represented as stylized firing rate maps in a simple square environment. Firing related to the North, East, South and West walls is depicted in four colors (blue, yellow, purple, red, respectively). Only 4 PWb cells, 16 transformation circuit neurons, 4 BVCs and four head-direction modulations (North, East, South, West) are shown for simplicity. PWb neurons have egocentric receptive fields (RFs, dashed ovals, shown left of and within each square) that are attached to the agent (black triangle). The RFs respond to boundaries at a specific distance and egocentric direction (ahead, left, right, behind). As the agent moves around the environment, any boundary can fall into the egocentric RF, depending on the agent’s orientation (four example positions and orientations shown), resulting in a firing rate map with firing related to all four boundaries (i.e. the blue, yellow, purple, and red bands, each conditional on the agent facing in a different direction). Considering the PWb cell with the RF ahead of the agent (top left, green star): due to the HD modulation a different RSC cell is receptive to input from that PWb neuron depending on the agent‘s current orientation. For example when the agent is facing East the 2nd row of the RSC transformation circuit is receptive to inputs from the PWb, and the first PWb neuron projects to the second cell in that row (green arrow). That RSC cell in turn projects to a BVC with a RF to the East (downward light grey arrow). The Eastward BVC also gets inputs from the other 3 PWb cells when the agent faces in the other three directions, via the other RSC neurons in the second column (connectivity not shown, but indicated by matching symbols: hexagon, triangle, square). Thus, this BVC can fire whenever the agent is near to the East wall, irrespective of the agent’s orientation. In top-down mode (imagery), PWb cells are driven by different BVCs depending on the facing direction of the animal. The PWb cell with the RF ahead of the agent (top left, green star) recieves connections from all transformation circuit neurons shown with star symbols: conveying input from the Eastward BVC when facing East, the Northward BVC when facing North etc. In this way, it is driven to fire whenever there is a boundary ahead of the agent. All connections between PWb and BVC cells and transformation circuit neurons are bidirectional, to enable both bottom-up and top-down operation.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig2-figsupp1-v1"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video1.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.005</object-id><label>Video 1.</label><caption><title>Surface plots (heat maps) visualize theneural activity of populations of cells.</title><p>The video shows a visualization of the simulated neural activity in the retrosplenial transformation circuit as a simulated agent moves in a simple, familiar environment (See Figure 2-figure supplement 1 for further details). Individual sublayers of the transformation circuit are shown in a circular arrangement around the head direction ring. Head direction cells track the agent's heading and confer a gain modulation on the retrosplenial sublayers. The transformation circuit then drives boundary vector cells (see main text). Surface plots (heat maps) visualize the neural activity of populations of cells. Individual cells correspond to pixels/polygons on the heat maps (compare to figures). Cells are arranged according to the distribution of their receptive fields; however, this arrangement does not necessarily reflect anatomical relations. Bright colors indicate strong firing. Abbreviations: PWb, Parietal Window, egocentric boundary representations (ahead is up); HDCs, Head Direction Cells; TR, Retrosplenial transformation sublayers; BVCs, Boundary Vector Cells (North is up); egoc. agent view, egocentric field of view of the agentwithin the environment, purple outlines denote visibleboundary segments which correspond to sensoryinputs to the PWb (ahead is up); alloc. agent position, allocentric position of the agent in the environment (North is up).</p></caption></media></sec><sec id="s2-2"><title>The parietal window</title><p>Perceived and imagined egocentric sensory experience is represented in the ‘parietal window’ (PW), which consists of two neural populations - one coding for extended boundaries (‘PWb neurons’), and one for discrete objects (‘PWo neurons’). The receptive fields of both populations lie in peri-personal space, that is are tuned to distances and directions ahead, left or right of the agent, tile the ground around the agent, and rotate together with the agent (<xref ref-type="fig" rid="fig2">Figure 2A2</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>). Reciprocal connections to and from the retrosplenial transformation circuit (RSC/TR, see below) allow the parietal window representations to be transformed into allocentric (orientation-independent) representations (i.e. boundary and object vector cells) in the MTL and vice versa. Intriguingly, cells that encode an egocentric representation of boundary locations (akin to parietal window neurons in the present model) have recently been described (<xref ref-type="bibr" rid="bib70">Hinman et al., 2017</xref>).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.006</object-id><label>Figure 3.</label><caption><title>The agent model and population snapshots for object representations.</title><p>(<bold>A</bold>) Top panel: The egocentric field of view of the agent (black arrow head). Purple boundaries fall into the forward-facing 180 degree field of view and provide bottom-up drive to the parietal window (PWb; not shown, but see <xref ref-type="fig" rid="fig2">Figure 2C2</xref>). The environment contains two discrete objects (green circles). Bottom panel: Allocentric positions of the agent (black triangle) and objects (green circles). (<bold>B</bold>) Object-related parietal window (PWo) activity (top panel) and OVC activity (bottom panel) due to object 2, South-East of the agent, at time T1. (<bold>C</bold>) PWo activity (top panel) and OVC activity (bottom panel) due to object 1, North-East of the agent, at time T2. A heuristically implemented attention model ensures that only one object at a time drives the parietal window (PWo). (<bold>D</bold>) Illustration of the encoding of an object encountered in a familiar environment. Dashed connections are learned (as Hebbian weight updates) between active cells. Solid lines indicate connections learned in the training phase, representing the spatial context. Note that place cells (PCs) anchor the object representation to the spatial context.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig3-v1"/></fig></sec><sec id="s2-3"><title>The agent model and perceptual drive</title><p>An agent model supplies perceptual information, driving the parietal window in a bottom-up manner. The virtual agent moves along trajectories in simple 2D environments (<xref ref-type="fig" rid="fig2">Figure 2B1</xref>). Turning motions of the agent act on the head direction network to shift the activity packet in the head direction ring attractor. Egocentric distances to environmental boundaries in a 180-degree field of view in front of the agent are used to drive the corresponding parietal window (PWb) neurons. The retrosplenial circuit (section &quot;The Head Direction Attractor Network and the Transformation Circuit&quot;) transforms this parietal window activity into BVC activity, which in turn drives PC activity in the pattern-completing MTL network (<xref ref-type="bibr" rid="bib103">O'Keefe and Burgess, 1996</xref>; <xref ref-type="bibr" rid="bib62">Hartley et al., 2000</xref>). Thus, simplified perceptual drive conveyed to the MTL allows the model to self-localize in the environment based purely on sensory inputs.</p></sec><sec id="s2-4"><title>The medial temporal lobe network</title><sec id="s2-4-1"><title>Spatial context</title><p>The medial temporal lobe (MTL) network for spatial context is comprised of three interconnected neural populations: the PCs and BVCs code for the position of the agent relative to a given boundary configuration, and perirhinal neurons code for the identity (e.g. texture, color etc) of boundaries (PRb neurons). Identity has to be signaled by cells separate from BVCs because the latter respond to any boundary at a given distance and direction.</p></sec><sec id="s2-4-2"><title>Discrete objects</title><p>The allocentric object code is comprised of two populations of neurons. First, similarly to extended boundaries, the identity of discrete objects must be coded for by perirhinal neurons (PRo neurons). Second, we hypothesize an allocentric representation of object location, termed object vector cells (OVCs), analogous to BVCs (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), with receptive fields at a fixed distance in an allocentric direction.</p><p>Interestingly, cells which respond to the presence of small objects and resemble OVCs have recently been identified in the rodent literature (<xref ref-type="bibr" rid="bib35">Deshmukh and Knierim, 2013</xref>; <xref ref-type="bibr" rid="bib73">Hoydal et al., 2017</xref>), and could reside in the hippocampus proper or one synapse away. Although we treat them separately, BVCs and OVCs could in theory start out as one population in which individual cells specialize to respond only to specific types of object with experience (e.g. to small objects in the case of OVCs; see <xref ref-type="bibr" rid="bib11">Barry and Burgess, 2007</xref>).</p></sec><sec id="s2-4-3"><title>The role of perirhinal neurons</title><p>OVCs, like BVCs and parietal window (PWo and PWb) neurons signal geometric relations between object or boundary locations and the agent, but not the identity of the object or boundary. OVCs and BVCs fire for any object or boundary occupying their receptive fields. Conversely, an object’s or boundary’s identity is indicated, irrespective of its location, by perirhinal neurons. They lie at the apex of the ventral visual stream (the ‘what’ pathway; <xref ref-type="bibr" rid="bib149">Ungerleider, 1982</xref>; <xref ref-type="bibr" rid="bib93">Mishkin et al., 1983</xref>; <xref ref-type="bibr" rid="bib56">Goodale and Milner, 1992</xref>; <xref ref-type="bibr" rid="bib33">Davachi, 2006</xref>; <xref ref-type="bibr" rid="bib150">Valyear et al., 2006</xref>) and encode the identities or sensory characteristics of boundaries and objects, driven by a visual recognition process which is not explicitly modeled. Only in concert with perirhinal identity neurons does the object or boundary code uniquely represent a specific object or boundary at a specific direction and distance from the agent.</p></sec><sec id="s2-4-4"><title>Connections among medial temporal lobe populations</title><p>BVCs and OVCs have reciprocal connections to the transformation circuit, allowing them to be driven by perceptual inputs (‘bottom up’), or to project their representations to the parietal window (‘top down’).</p><p>For simulations of the agent in a familiar environment, the connectivity among the medial temporal lobe populations which comprise the spatial context (PCs, BVCs, PRb neurons) is learned in a training phase, resulting in an attractor network, such that mutual excitatory connections between neurons ensure pattern completion. Hence, partial activity in a set of PCs, BVCs, and/or PRb neurons - will re-activate a complete, previously learned representation of spatial context in these populations. OVCs and PRo neurons are initially disconnected from the populations that represent the spatial context. The simulated agent can then explore the environment and encode objects into memory along the way.</p></sec></sec><sec id="s2-5"><title>The head direction attractor network and the transformation circuit</title><p>Head direction cells (HDCs) are arranged in a simple ring-attractor circuit (<xref ref-type="bibr" rid="bib129">Skaggs et al., 1995</xref>; <xref ref-type="bibr" rid="bib166">Zhang, 1996</xref>). Current head direction, encoded by activity in this attractor circuit, is updated by angular velocity information as the agent explores the environment. The head direction signal enables the egocentric-allocentric transformation carried out by retrosplenial cortex.</p><p>Because of their identical topology, the PWb/BVC population pair and the PWo/OVC population pair can each make use of the same transformation circuit. For simplicity we illustrate its function via BVCs and their PWb counterparts. The retrosplenial transformation circuit (RSC/TR) consists of 20 sublayers. Each sublayer is a copy of the BVC population, with firing within each sublayer also tuned to a specific head-direction (directions are evenly spaced in the [0360] degree range). That is, individual cells in the transformation circuit are directionally modulated boundary vector cells, and connect egocentric (parietal) PWb neurons and allocentric BVCs (in the MTL) in a mutually consistent way. All connections are reciprocal. For example, a BVC with a receptive field to the East is mapped onto a PWb neuron with a receptive field to the right of the agent when facing North, but is mapped onto a PWb neuron with a receptive field to the left of the agent when facing South. Similarly, a PWb neuron with a receptive field ahead of the agent is mapped onto a BVC with a receptive field to the West when facing West but is mapped onto a BVC with a receptive field to the North when facing North. <xref ref-type="fig" rid="fig2">Figure 2C</xref> depicts population snapshots that are mapped onto each other by the transformation circuit (also see <xref ref-type="video" rid="video1">Video 1</xref>), while <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> illustrates the connections and firing rate maps at the single cell level. We hypothesize that the egocentric-allocentric transformation circuit is set up during development (see Appendix for the setup of the circuit).</p></sec><sec id="s2-6"><title>Bottom-up vs top-down modes of operation</title><p>During perception, the egocentric parietal window representation is based on sensory inputs (‘bottom-up’ mode). The PW representations thus determine MTL activity via the transformation circuit. ‘Running the transformation in reverse’ (‘top-down’ mode), that is reconstructing parietal window activity based on BVCs/OVCs, is the BB-models account of visuo-spatial imagery. To implement the switch between modes of operation, we assume that the balance between bottom-up and top-down connections is subject to neuromodulation (see e.g. <xref ref-type="bibr" rid="bib67">Hasselmo, 2006</xref>); Appendix, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and following). For example, connections from the parietal window (PWb and PWo) populations to the transformation circuit and thence onto BVCs/OVCs are at full strength in bottom-up mode, but down-regulated to 5% of their maximum value in top-down mode. Conversely, connections from BVCs/OVCs to the transformation circuit and onwards to the parietal window are down-regulated during bottom-up perception (5% of their maximum value) and reach full strength only during imagery (top-down reconstruction).</p></sec><sec id="s2-7"><title>Embedding object-representations into a spatial context: attention and encoding</title><p>Unlike boundaries, which are hard-coded in the simulations (corresponding to the agent moving in a familiar environment), object representations are learned on the fly (simulating the ability to remember objects found in new locations in the environment).</p><p>As noted above (section &quot;The Role of Perirhinal Neurons&quot;), to uniquely characterize the egocentric perceptual state of encountering an object within an environment requires the co-activation of perirhinal (PRo) neurons (signaling identity) and the corresponding parietal window (PWo) (signaling location in peripersonal space). Moreover, maximal co-firing of only one PRo neuron with one PWo neuron (or OVC, in allocentric terms) at a given location is required for an unambiguous association (<xref ref-type="fig" rid="fig3">Figure 3A–C</xref>). If multiple conjunctions of object location and identity are concurrently represented then it is impossible to associate each object identity uniquely with one location - that is, object-location binding would be ambiguous. To ensure a unique representation, we allow the agent to direct attention to each visible object in sequence (compare <xref ref-type="fig" rid="fig3">Figure 3B and C</xref>; for a review of attentional mechanisms see <xref ref-type="bibr" rid="bib154">VanRullen, 2013</xref>). This leads to a specific set of PWo, OVC and PRo neurons, corresponding to a single object at a given location, being co-active for a short period while connections between MTL neurons develop (including those with PCs, see <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Then, attention is redirected and a different set of PWo, OVC and PRo neurons becomes co-active. We set a fixed length for an attentional cycle (600 time units). However, we do not model the mechanistic origins of attention. Attention is supplied as a simple rhythmic modulation of perceptual activity in the parietal window.</p><p>To encode objects in their spatial context the connections between OVCs, PRo neurons and currently active PCs are strengthened. By linking OVCs and PRo neurons to PCs, the object code is explicitly attached to the spatial context because the same PCs are reciprocally connected to the BVCs that represent the geometric properties of the environment (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). A connection between PRo neurons and HDCs is also strengthened to allow recall to re-instantiate the head direction at encoding during imagery (see Simulation 1.0 below).</p><p>Finally, if multiple objects are present in a scene we do not by default encode all perceivable objects equally strongly into memory. We trigger encoding of an object when it reaches a threshold level of ‘salience’. In general, ‘salience’ could reflect many factors; here, we simulate relatively few objects and assume that salience becomes maximal at a given proximity, and prevent any further learning thereafter.</p></sec><sec id="s2-8"><title>Imagery and the role of grid cells</title><p>Grid cells (GCs; Hafting et al. 2005) are thought to interface self-motion information with place cells (PCs) to enable vector navigation (<xref ref-type="bibr" rid="bib82">Kubie and Fenton, 2012</xref>; <xref ref-type="bibr" rid="bib47">Erdem and Hasselmo, 2012</xref>; <xref ref-type="bibr" rid="bib27">Bush et al., 2015</xref>; <xref ref-type="bibr" rid="bib136">Stemmler et al., 2015</xref>), shortcutting, and mental navigation (<xref ref-type="bibr" rid="bib14">Bellmund et al., 2016</xref>); Horner et al. 2016). Importantly, both self-motion inputs (via GCs) and sensory inputs (e.g. mediated via BVCs and OVCs) converge onto PCs and both types of inputs may be weighted according to their reliability (<xref ref-type="bibr" rid="bib48">Evans et al., 2016</xref>). GCs could thus support PC activity when sensory inputs are unreliable or absent. Here, GC inputs can drive PC firing during imagined navigation (see Section Novelty Detection (Simulations 1.3, 1.4)), whereas perceived scene elements, mediated via BVC and OVCs, provide the main input to PCs during unimpaired perception.</p><p>We include a GC module in the BB-model that, driven by heuristically implemented mock-motor-efference signals (self-motion signals with suppressed motor output), can update the spatial memory network in the absence of sensory inputs. The GC input allows the model to perform mental navigation (imagined movement through a known environment). By virtue of connections from GCs to PCs, the GCs can shift an activity bump smoothly along the sheet of PCs. Pattern completion in the medial temporal lobe network then updates the BVC representation according to the shifted PC representation. BVCs in turn update the parietal window representation (top-down), smoothly shifting the egocentric field of view in imagery (i.e. updating the parietal window representations) during imagined movement. Thus, self-motion related updating (sometimes referred to as ‘path integration’) and mental navigation share the same mechanism (<xref ref-type="bibr" rid="bib143">Tcheang et al., 2011</xref>).</p><p>Connection weights between GCs and PCs are calculated as a simple Hebbian association between PC firing at a given coordinate (according to the mapping shown in <xref ref-type="fig" rid="fig2">Figure 2A,B</xref>) and pre-calculated firing rate maps of GCs (7 modules with 100 cells each, see Appendix for details).</p></sec><sec id="s2-9"><title>Model summary</title><p>An agent employing a simple model of attention alongside dedicated object-related neural populations in perirhinal, parietal and parahippocampal (BVCs and OVCs) cortices allow the encoding of scene representations (i.e. objects in a spatial context) into memory. Transforming egocentric representations via the retrosplenial transformation circuit yields viewpoint-independent (allocentric) representations in the medial temporal lobe, while reconstructing the parietal window representation (which is driven by sensory inputs during perception) from memory is the model’s account of recall as an act of visuo-spatial imagery. Grid cells allow for mental navigation. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the complete schematic of the BB-model, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for details of the RSC transformation circuit.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.007</object-id><label>Figure 4.</label><caption><title>The BB-model.</title><p>‘Bottom-up’ mode of operation: Egocentric representations of extended boundaries (PWb) and discrete objects (PWo) are instantiated in the parietal window (PWb/o) based on inputs from the agent model while it explores a simple 2D environment. Attention sequentially modulates object-related PW activity to allow for unambiguous neural representations of an object at a given location. The angular velocity of the agent drives the translation of an activity packet in the head direction ring attractor network. Retrosplenial cortex (RSC) carries out the transformation from egocentric representations in the PW to allocentric representations in the MTL (driving BVCs and OVCs). The transformation circuit consists of 20 sublayers, each maximally modulated by a specific head direction while the remaining circuit is inhibited (Inh). In the medial temporal lobe network, perirhinal neurons (PRb/o) code for the identity of an object or extended boundary. PCs, BVCs and perirhinal neurons are reciprocally connected in an attractor network. Following encoding after object encounters, PCs are also reciprocally connected to OVCs and PRo neurons. ‘Top-down’ mode of operation: Activity in a subset of PCs, BVCs, and/or perirhinal neurons spreads to the rest of the MTL network (pattern completion) by virtue of intrinsic connectivity. With perceptual inputs to the PW disengaged (i.e. during recollection), the transformation circuit reconstructs parietal window (PWb/o) activity based on the current BVC and OVC activity. Updating PCs via entorhinal cortex (EC) GC inputs allows for a shift of viewpoint in imagery.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig4-v1"/></fig></sec><sec id="s2-10"><title>Quantification</title><p>To obtain a measure of successful recall or of novelty detection (i.e. mismatch between the perceived and remembered scenes), we correlate the population vectors of the model’s neural populations between recall (the reconstruction in imagery) and encoding. These correlations are compared to correlations between recall and randomly sampled times as the agent navigates the environment in bottom-up mode. This measure of mismatch could potentially be compared to experimental measures of overlap between neuronal populations (e.g. <xref ref-type="bibr" rid="bib59">Guzowski et al., 1999</xref>) in animals, or ‘representational similarity’ measures in fMRI, e.g. <xref ref-type="bibr" rid="bib119">Ritchey et al., 2013</xref>).</p></sec></sec><sec id="s3"><title>Simulations</title><p>In this section, we explore the capabilities of the BB-model in simulations and derive predictions for future research. Each simulation is accompanied by a Figure, a supplementary video visualizing the time course of activity patterns of neural populations, and a brief discussion. In Section Discussion, we offer a more general discussion of the model.</p><sec id="s3-1"><title>Encoding of objects in spatial memory and recall (Simulation 1.0)</title><p>We let the agent model explore the square environment depicted in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. However, the spatial context now contains an isolated object (<xref ref-type="fig" rid="fig5">Figure 5</xref>). During exploration, parietal window (PWb) neurons activate BVCs via the retrosplenial transformation circuit (RSC/TR), which in turn drive place cell activity. Similarly, when the object is present PWo neurons are activated, which drive OVCs via the transformation circuit. At the same time, object/boundary identity is signalled by perirhinal neurons (PRb/o). When the agent comes within a certain distance (here 55 cm) of an object, the following connection weights are changed to form Hebbian associations: PRo neurons are associated with PCs, HDCs, and OVCs; OVCs are associated with PCs and PRo neurons (also see <xref ref-type="fig" rid="fig3">Figure 3D</xref>); PCs are already connected to BVCs (in a familiar context). The weight change is calculated as the outer product of population vectors of the corresponding neuronal populations (yielding the Hebbian update), normalized, and added to the given weight matrix.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.008</object-id><label>Figure 5.</label><caption><p>(<bold>A</bold>) Bottom-up mode of operation. Population snapshots at the moment of encoding during an encounter with a single object in a familiar spatial context. Left to right: PWb/o populations driven by sensory input project to the head-direction-modulated retrosplenial transformation circuit (RSC/TR, omitted for clarity, see <xref ref-type="video" rid="video1">Video 1</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>); The transformation circuit projects its output to BVCs and OVCs; BVCs and PRb neurons constitute the main drive to PCs; perirhinal (PRb/o) neurons are driven externally, representing object recognition in the ventral visual stream. At the moment of encoding, reciprocal connections between PCs and OVCs, OVCs and PRo neurons, PCs and PRo neurons, and PRo neurons and current head direction are learned (see <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Right-most panels show the agent in the environment and the PC population snapshot representing current allocentric agent position. (<bold>B</bold>) Top-down mode of operation, after the agent has moved away from the object (black triangle, right-most panel). Current is injected into a PRo neuron (bottom right of panel), modelling a cue to remember the encounter with that object. This drives PCs associated to the PRo neuron at encoding (dashed orange connections show all associations learned at encoding). The connection weights switch globally from bottom-up to top-down (connections previously at 5% of their maximum value now at 100% and vice versa; orange arrows). PCs become the main drive to OVCs, BVCs and PRb neurons. BVC and OVC representations are transformed to their parietal window counterparts, thus reconstructing parietal representations (PWb/PWo) similar to those at the time of encoding (compare left-most panels in A and B). That is, the agent has reconstructed a point of view embodied by parietal window activity corresponding to the location of encoding (red triangle, right-most panel). Heat maps show population firing rates frozen in time (black: zero firing; white: maximal firing).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig5-v1"/></fig><p>After the agent has finished its assigned trajectory, we test object-location memory via object-cued recall. That is, modeling some external trigger to remember a given object (e.g. ‘Where did I leave my keys?'), current is injected into the PRo neuron coding for the identity of the object to-be recalled. By virtue of learned connections, the PRo neuron drives the PCs which were active at encoding. Pattern completion in the MTL recovers the complete spatial context by driving activity in BVCs and PRb neurons. The connections from PRo neurons to head direction cells (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) ensure a modulation of the transformation circuit such that allocentric BVC and OVC activity will be transformed to yield the parietal representation (i.e. a point of view) similar to the one at the time of encoding. That is, object-cued recall corresponds to a full reconstruction of the scene when the object was encoded. <xref ref-type="fig" rid="fig5">Figure 5</xref> depicts the encoding (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) and recall phases (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) of simulation 1.0. <xref ref-type="video" rid="video2">Video 2</xref> shows the entire trial. To facilitate matching simulation numbers and figures to videos, <xref ref-type="table" rid="table1">Table 1</xref> lists all simulations and relates them to their corresponding figures and videos.</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.009</object-id><label>Table 1.</label><caption><title>List of simulations, their content, corresponding Figures and videos</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Simulation no.</th><th valign="top">Content</th><th valign="top">Related figures</th><th valign="top">Video no.</th></tr></thead><tbody><tr><td valign="top"> 0</td><td valign="top">Activity in the transformation circuit</td><td valign="top"><xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref></td><td valign="top">1</td></tr><tr><td valign="top"> 1.0</td><td valign="top">Object-cued recall</td><td valign="top"><xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>,8A</td><td valign="top">2</td></tr><tr><td valign="top"> 1.0n1</td><td valign="top">Object-cued recall with neuron loss</td><td valign="top"><xref ref-type="fig" rid="fig8">Figure 8B</xref></td><td valign="top">3</td></tr><tr><td valign="top"> 1.0n2</td><td valign="top">Object-cued recall with firing rate noise</td><td valign="top"><xref ref-type="fig" rid="fig8">Figure 8C</xref></td><td valign="top">4</td></tr><tr><td valign="top"> 1.1</td><td valign="top">Papez’ circuit Lesion (anterograde amnesia)</td><td valign="top"><xref ref-type="fig" rid="fig7">Figure 7A</xref></td><td valign="top">5</td></tr><tr><td valign="top"> 1.2</td><td valign="top">Papez’ circuit Lesion (retrograde amnesia)</td><td valign="top"><xref ref-type="fig" rid="fig7">Figure 7B</xref></td><td valign="top">6</td></tr><tr><td valign="top"> 1.3</td><td valign="top">Object novelty (intact hippocampus)</td><td valign="top"><xref ref-type="fig" rid="fig9">Figure 9A</xref></td><td valign="top">7</td></tr><tr><td valign="top"> 1.4</td><td valign="top">Object novelty (lesioned hippocampus)</td><td valign="top"><xref ref-type="fig" rid="fig9">Figure 9B</xref></td><td valign="top">8</td></tr><tr><td valign="top"> 2.1</td><td valign="top">Boundary trace responses</td><td valign="top"><xref ref-type="fig" rid="fig10">Figure 10A,B,C</xref></td><td valign="top">9</td></tr><tr><td valign="top"> 2.2</td><td valign="top">Object trace responses</td><td valign="top"><xref ref-type="fig" rid="fig10">Figure 10D</xref></td><td valign="top">10</td></tr><tr><td valign="top"> 3.0</td><td valign="top">Inspection of scene elements in imagery</td><td valign="top"><xref ref-type="fig" rid="fig11">Figure 11</xref></td><td valign="top">11</td></tr><tr><td valign="top"> 4.0</td><td valign="top">Mental Navigation</td><td valign="top"><xref ref-type="fig" rid="fig12">Figure 12</xref></td><td valign="top">12</td></tr><tr><td valign="top"> 5.0</td><td valign="top">Planning and short-cutting</td><td valign="top"><xref ref-type="fig" rid="fig13">Figure 13</xref></td><td valign="top">13</td></tr></tbody></table></table-wrap><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video2.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.010</object-id><label>Video 2.</label><caption><title>This video shows a visualization of the simulated neural activity as the agent moves in a familiar environment and encounters a novel object.</title><p>The agent approaches the object and encodes it into long-term memory. Upon navigating past the object the agent initiates recall, reinstating patterns of neural activity similar to the patterns present during the original object encounter. Recall is identified with the re-construction of the original scene in visuo-spatial imagery (see main text). Please see caption of Video 1 for abbreviations. </p></caption></media><p>Recollection in the BB-model results in visuo-spatial imagery of a coherent scene from a single viewpoint and direction, that is it implements a process of scene construction (<xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>; <xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>; <xref ref-type="bibr" rid="bib64">Hassabis et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">Buckner, 2010</xref>) at the neuronal level. A mental image is re-constructed in the parietal window reminiscent of the perceptual activity present at encoding. Note that during imagery BVCs (and hence PWb neurons, <xref ref-type="fig" rid="fig5">Figure 5B</xref>) all around the agent are reactivated by place cells, because the environment is familiar (the agent having experienced multiple points of view at each location during the training phase). We do not simulate selective attention for boundaries (i.e. PWb neurons), although see <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>.</p><p>Similar tasks in humans appear to engage the full network, including Papez’ circuit, where head direction cells are found (for review see <xref ref-type="bibr" rid="bib142">Taube, 2007</xref>); retrosplenial cortex (where we hypothesize the transformation circuit to be located) (<xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib84">Lambrey et al., 2012</xref>; <xref ref-type="bibr" rid="bib8">Auger and Maguire, 2013</xref>; <xref ref-type="bibr" rid="bib46">Epstein and Vass, 2014</xref>; <xref ref-type="bibr" rid="bib89">Marchette et al., 2014</xref>; <xref ref-type="bibr" rid="bib127">Shine et al., 2016</xref>); medial parietal areas (<xref ref-type="bibr" rid="bib49">Fletcher et al., 1996</xref>; <xref ref-type="bibr" rid="bib68">Hebscher et al., 2018</xref>); parahippocampus and hippocampus (<xref ref-type="bibr" rid="bib64">Hassabis et al., 2007</xref>; <xref ref-type="bibr" rid="bib1">Addis et al., 2007</xref>; <xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>; <xref ref-type="bibr" rid="bib16">Bird et al., 2010</xref>); and possibly the entorhinal cortex (<xref ref-type="bibr" rid="bib7">Atance and O'Neill, 2001</xref>; <xref ref-type="bibr" rid="bib14">Bellmund et al., 2016</xref>; Horner et al. 2016; also see Simulation 4.0).</p><p>At the neuronal level, a key component of the BB-model are the object vector cells (OVCs) which code for the location of objects in peri-personal space. In Figure 5 the cells are organized according to the topology of their receptive fields in space, with the agent at the center (also compare to <xref ref-type="fig" rid="fig2">Figure 2A2</xref>). However, in rodent experiments individual spatially selective cells (like PCs or GCs) are normally visualized as time-integrated firing rate maps. We ran a separate simulation with three objects in the environment to examine firing rate maps of individual cells. OVCs show firing fields at a fixed allocentric distance and angle from objects (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The BB-model predicts that OVC-like responses should be found as close as one synapse away from the hippocampus and were introduced as a parsimonious object code, analogous to BVCs and exploiting the existing transformation circuit. However, these rate maps show a striking resemblance to similar data from cells recently reported in the hippocampus of rodents (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, compare to <xref ref-type="bibr" rid="bib35">Deshmukh and Knierim, 2013</xref>). While <xref ref-type="bibr" rid="bib35">Deshmukh and Knierim (2013)</xref> found these cells in the hippocampus, the object selectivity of these hippocampal neurons may have been inherited from other areas, such as lateral entorhinal cortex (<xref ref-type="bibr" rid="bib146">Tsao et al., 2013</xref>), parahippocampal cortex (due to their similarities to BVCs) or medial entorhinal cortex (<xref ref-type="bibr" rid="bib131">Solstad et al., 2008</xref>; <xref ref-type="bibr" rid="bib73">Hoydal et al., 2017</xref>).</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.011</object-id><label>Figure 6.</label><caption><title>Firing fields of object vector cells.</title><p>(<bold>A</bold>) Firing rate maps for representative object vector cells (OVCs), firing for objects with a fixed allocentric location and direction relative to the agent. Object locations superimposed as green circles. Note that the objects have different identities, which would be captured by perirhinal neurons, not OVCs. Compare to <xref ref-type="fig" rid="fig4">Figure 4</xref> in <xref ref-type="bibr" rid="bib35">Deshmukh and Knierim, 2013</xref>. White lines point from objects to firing fields. Red dotted line added for comparison with B. (<bold>B</bold>) Distribution of the objects in the arena and an illustration of a possible agent trajectory.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig6-v1"/></fig><p>Anatomical connections between the potential loci of BVCs/OVCs and retrosplenial cortex (the suggested location of the egocentric-allocentric transformation circuit) exist. BVCs have been found in the subicular complex (<xref ref-type="bibr" rid="bib87">Lever et al., 2009</xref>), and the related border cells and OVCs in medial entorhinal cortex (<xref ref-type="bibr" rid="bib131">Solstad et al., 2008</xref>; <xref ref-type="bibr" rid="bib73">Hoydal et al., 2017</xref>). Both areas receive projections from retrosplenial cortex (<xref ref-type="bibr" rid="bib77">Jones and Witter, 2007</xref>), and project back to it (<xref ref-type="bibr" rid="bib164">Wyss and Van Groen, 1992</xref>).</p></sec><sec id="s3-2"><title>Papez’ circuit lesions induce amnesia (Simulations 1.1, 1.2)</title><p><xref ref-type="fig" rid="fig5">Figure 5</xref> depicts the model performing encoding and object-cued recall. However, the model also allows simulation of some of the classic pathologies of long-term memory. Lesions along Papez’ circuit have long been known to induce amnesia (<xref ref-type="bibr" rid="bib34">Delay and Brion, 1969</xref>; <xref ref-type="bibr" rid="bib135">Squire and Slater, 1978</xref>; <xref ref-type="bibr" rid="bib134">1989</xref>; <xref ref-type="bibr" rid="bib111">Parker and Gaffan, 1997</xref>; <xref ref-type="bibr" rid="bib3">Aggleton et al., 2016</xref>). Thus, lesions to the fornix and mammilary bodies severely impact recollection, although recognition can be less affected (<xref ref-type="bibr" rid="bib147">Tsivilis et al., 2008</xref>). In the context of spatial representations, Papez’ circuit is notable for containing head direction cells (as well as many other cell types not in the model). That is, the mammillary bodies (more specifically the lateral mammillary nucleus, LMN), anterior dorsal thalamus, retrosplenial cortex, parts of the subicular complex and medial entorhinal cortex all contain head direction cells (<xref ref-type="bibr" rid="bib142">Taube, 2007</xref>; <xref ref-type="bibr" rid="bib123">Sargolini et al., 2006</xref>). Thus, lesioning Papez’ circuit removes (at least) the head direction signal from our model, and is modeled by setting the input from head direction cells to the retrosplenial transformation circuit (RSC/TR) to zero.</p><p>In the bottom-up mode of operation (perception), the lesion removes drive to the transformation circuit and consequently to the boundary vector cells and object vector cells. That is, the perceived location of an object (present in the egocentric parietal representation) cannot elicit activity in the MTL and thus cannot be encoded into memory (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Some residual MTL activity reflects input from perirhinal neurons representing the identity of perceived familiar boundaries (i.e. recognition mediated by perirhinal cells is spared). In the top-down mode of operation (recall) there are two effects: (i) Since no new elements can be encoded into memory, post-lesion events cannot be recalled (anterograde amnesia; Simulation 1.1, <xref ref-type="fig" rid="fig7">Figure 7A</xref>, Video 5); and (ii) For pre-existing memories (e.g. of an object encountered prior to the lesion), place cells (and thus the remaining MTL populations) can be driven via learned connections from perirhinal neurons (e.g. when cued with the object identity; Simulation 1.2, <xref ref-type="fig" rid="fig7">Figure 7B</xref>, Video 6), but no meaningful egocentric representation can be instantiated in parietal areas, preventing episodic recollection/imagery. Equating the absence of parietal activity with the inability to recollect is strongly suggested by the fact that visuo-spatial imagery in humans relies on access to an egocentric representation (as in hemispatial representational neglect; <xref ref-type="bibr" rid="bib17">Bisiach and Luzzatti, 1978</xref>). Simulations 1.1 and 1.2 show that the egocentric neural correlates of objects and boundaries present in the visual field persist in the parietal window only while the agent perceives them (they could also be held in working memory, which is not modelled here). Note that perirhinal cells and upstream ventral visual stream inputs are spared, so that an agent could still report the identity of the object.</p><fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.012</object-id><label>Figure 7.</label><caption><title>Papez’ circuit lesions.</title><p>(<bold>A</bold>) In the bottom-up mode of operation (perception), a lesion to the head direction circuit removes drive to the transformation circuit and consequently to the boundary vector cells (BVCs) and object vector cells (OVCs). A perceived object (present in the egocentric parietal representation, PWo) cannot elicit activity in the MTL and thus cannot be encoded into long-term memory, causing anterograde amnesia. Place cells fire at random locations, driven by perirhinal neurons. (<bold>B</bold>) For memories of an object encountered before the lesion, place cells can be cued by perirhinal neurons, and pattern completion recruits associated OVC, BVCs and perirhinal neurons, but no meaningful representation can be instantiated in parietal areas, preventing episodic recollection/imagery (retrograde amnesia for hippocampus-dependent memories).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig7-v1"/></fig></sec><sec id="s3-3"><title>Quantification, robustness to noise and neuron loss</title><p><xref ref-type="fig" rid="fig8">Figure 8A</xref> shows correlations between population vectors of neural patterns during imagery/recall and those during encoding for Simulation 1.0 (Object-cued recall; <xref ref-type="fig" rid="fig5">Figure 5</xref>). OVCs and PCs exhibit correlation values close to one, indicating faithful reproduction of patterns. BVC correlations are somewhat diminished because recall reactivates all boundaries fully, compared to a field of view of 180 degrees during perception with limited reactivation of cells representing boundaries outside the field of view. PW neurons show correlations below one because at recall reinstatement in parietal areas requires the egocentric-allocentric transformation (i.e. OVC signals passed through retrosplenial cells), which blurs the pattern compared to perceptual instatement in the parietal window (i.e. imagined representations are not as precise as those generated by perception).</p><fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.013</object-id><label>Figure 8.</label><caption><title>Correlation of neural population vectors between recall/imagery and encoding.</title><p>(<bold>A</bold>) In the intact model, OVCs and place cells exhibit correlation values close to one, indicating faithful reproduction of patterns. (<bold>B</bold>) Random neuron loss (20% of cells in all populations except for the head direction ring). (<bold>C</bold>) The effect of firing rate noise. Noise is also applied to all 20 retrosplenial transformation circuit sublayers (as is neuron loss; correlations not shown for clarity). Firing rate noise is implemented as excursions from the momentary firing rate as determined by the regular inputs to a given cell (up to peak firing rate). The amplitudes of perturbations are normally distributed (mean 20%, standard deviation 5%) and applied multiplicatively at each time step). White bars show the correlation between the neural patterns at encoding vs recall (RvE), while black bars show the average correlation between the neural patterns at recall vs pattern sampled at random times/locations (here every 100 ms; RvRP). Each bar is averaged over 20 separate instances of the same simulation (with newly drawn random numbers). Error bars indicate standard deviation across simulations.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig8-v1"/></fig><p>To test the model’s robustness with regard to firing rate noise and neuron loss, we perform two sets of simulations (modifications of Simulation 1.0, object-cued recall). In the first set we randomly chose cells in equal proportions in all model areas (except HDCs) to be permanently deactivated and assess recall into visuo-spatial imagery. Up to 20% of the place cells, grid cells, OVCs, BVCs, parietal and retrosplenial neurons were deactivated. Head direction cells were excluded because of the very low number simulated (see below). Although we do not attempt to model any specific neurological condition, this type of simulation could serve as a starting point for models of diffuse damage, as might occur in anoxia, Alzheimer’s disease or aging. The average correlations between the population vectors at encoding versus recall are shown in <xref ref-type="fig" rid="fig8">Figure 8B</xref>.</p><p>The ability to maintain a stable attractor state among place cells and head direction cells is critical to the functioning of the model, while damage in the remaining (feed-forward) model components manifests in gradual degradation in the ability to represent the locations of objects and boundaries (see accompanying <xref ref-type="video" rid="video3">Video 3</xref>). For example, if certain parts of the parietal window suffer from neuron loss, the reconstruction in imagery is impaired only at the locations in peri-personal space encoded by the missing neurons (indeed, this can model representational neglect; <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>), see also <xref ref-type="bibr" rid="bib116">Pouget and Sejnowski, 1997</xref>). The place cell population was more robust to silencing than the head-direction population (containing only 100 neurons), simply because greater numbers of neurons were simulated, giving greater redundancy. As long as a stable attractor state is present, the model can still encode and recall meaningful representations, giving highly correlated perceived and recalled patterns (<xref ref-type="fig" rid="fig8">Figure 8B</xref>).</p><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video3.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.014</object-id><label>Video 3.</label><caption><title>This video shows the same scenario as <xref ref-type="video" rid="video2">Video 2</xref> (object-cued recall), however, with 20% randomly chosen lesioned cells per area.</title><p>The agent moves in a familiar environment and encounters a novel object. The agent approaches the object and encodes it into long-term memory. Upon navigating past the object, the agent initiates recall, reinstating patterns of neural activity similar to the patterns present during the original object encounter. Please see caption of Video 1 for abbreviations.</p></caption></media><p>The model is also robust to adding firing rate noise (up to 20% of peak firing rate) to all cells. Correlations between patterns at encoding and recall remain similar to the noise-free case, see <xref ref-type="fig" rid="fig8">Figure 8C</xref>. <xref ref-type="video" rid="video3">Videos 3</xref> and <xref ref-type="video" rid="video4">4</xref> show an instance from the neuron-loss and firing rate noise simulations respectively.</p><media id="video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video4.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.015</object-id><label>Video 4.</label><caption><title>This video shows the same scenario as <xref ref-type="video" rid="video2">Video 2</xref> (object-cued recall), however, with firing rate noise applied to all neurons (max. 20% of peak rate).</title><p>The agent moves in a familiar environment and encounters a novel object. The agent approaches the object and encodes it into long-term memory. Upon navigating past the object the agent initiates recall, reinstating patterns of neural activity similar to the patterns present during the original object encounter. Please see caption of Video 1 for abbreviations. </p></caption></media><media id="video5" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video5.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.016</object-id><label>Video 5.</label><caption><title>This video shows a visualization of the simulated neural activity as the agent encounters an object and subsequently tries to engage recall similar to Simulation 1.0 (<xref ref-type="video" rid="video2">Video 2</xref>).</title><p>However, a lesion to the head direction system (head direction cells are found along Papez' circuit) precludes the agent from laying down new memories, because the transformation circuit cannot drive the medial temporal lobe. That is the transformation circuit cannot instantiate OVC/BVC representations derived from sensory input for subsequent encoding, leading to anterograde amnesia in the model agent (see main text). Please see caption of Video 1 for abbreviations. </p></caption></media><media id="video6" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video6.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.017</object-id><label>Video 6.</label><caption><title>This video shows a visualization of the simulated neural activity as the agent moves through an empty environment and tries to engage recall of a previously present object.</title><p>A lesion to the head direction system (head direction cells are found along Papez' circuit) has been implemented similar to Simulation 1.1 (<xref ref-type="video" rid="video5">Video 5</xref>). The agent is supplied with the connection weights learned in Simulation 1.0 (<xref ref-type="video" rid="video2">Video 2</xref>), where it has successfully memorized a scene with an object. That is, the agent has acquired a memory before the lesion. However, even though cueing with the object re-activates the correct medial temporal representations, due to the lesion no reinstatement in the parietal window cannot occur, leading to retrograde amnesia for hippocampus-dependent memories in the model agent. Note, it is hypothesized that a cognitive agent only has conscious access to the egocentric parietal representation, as suggested by hemispatial representational negelct (<xref ref-type="bibr" rid="bib17">Bisiach and Luzzatti, 1978</xref>) (see main text). Please see caption of Video 1 for abbreviations. </p></caption></media></sec><sec id="s3-4"><title>Novelty detection (Simulations 1.3, 1.4)</title><p>In the model, hippocampal place cells bind all scene elements together. The locations of these scene elements relative to the agent are encoded in the firing of boundary vector cells (BVCs) and object vector cells (OVCs). Rats show a spontaneous preference for exploring novel/altered stimuli compared to familiar/unchanged ones. We simulate one of these experiments (<xref ref-type="bibr" rid="bib97">Mumby et al., 2002</xref>), in which rats preferentially explore one of two objects that has been shifted to a new location within a given environment, a behavior impaired by hippocampal lesions. In Simulations 1.3 and 1.4, the agent experiences an environment containing two objects, one of which is later moved. We define a mismatch signal as the difference in firing of object vector cells during encoding versus recall (modelled as imagery, at the encoding location), and assume that the relative amounts of exploration would be proportional to the mismatch signal.</p><p>With an intact hippocampus (<xref ref-type="fig" rid="fig9">Figure 9</xref>; <xref ref-type="video" rid="video7">Video 7</xref>), the moved object generates a significant novelty signal, due to the mismatch between recalled (top-down) OVC firing and perceptual (bottom-up) OVC firing at the encoding location. That detection of a change in position requires the hippocampus is consistent with place cells binding the relative location of an object (via object vector cells) to perirhinal neurons signalling the identity of an object.</p><fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.018</object-id><label>Figure 9.</label><caption><title>Detection of moved objects via OVC firing mismatch.</title><p>(<bold>A</bold>) Two objects are encoded from a given location (left). After encoding, object one is moved further North. When the agent returns to the encoding location, the perceived position of object one differs from that at encoding (blue line, middle panel). When the agent initiates recall (right) the perceived location of object 1 (green filled circle) and its imagined location (end point of blue line) differ. (<bold>B</bold>) PC activity is the same in all three circumstances, that is PC activity alone is insufficient to tell which object has moved. (<bold>C-D</bold>) The perceived location as represented by OVCs during perception (C; objects 1 and 2 sampled sequentially at times T1, T2) and during recall (D; objects 1 and 2 sampled sequentially at times T3, T4). Blue circle in panel D indicates the previously perceived position of object 1. Inset bar graphs show the concurrent activity of perirhinal cells (PRo). (<bold>E</bold>) The mismatch in OVC firing results in near zero correlation between encoding and recall patterns for object 1 (black bar), while object 2 (white bar) exhibits a strong correlation, so that object one would be preferentially explored. Note, the correlation for object two is less than 1 because of the residual OVC activity of the other object (secondary peaks in both panels in D, driven by learned PC-to-OVC connections). (<bold>F</bold>) A hippocampal lesion removes PC population activity, so that OVC activity is not anchored to the agent’s location at encoding. (<bold>G-H</bold>) An incidental match between learned and recalled OVC patterns can occur for either object at specific locations (red arrow heads in second panel in G), but otherwise mismatch is signaled for both objects equally and neither object receives preferential exploration.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig9-v1"/></fig><media id="video7" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video7.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.019</object-id><label>Video 7.</label><caption><title>This video shows a visualization of the simulated neural activity in a reproduction of the object novelty paradigm of <xref ref-type="bibr" rid="bib97">Mumby et al., 2002</xref>; detecting that one of two objects has been moved).</title><p>The agent is faced with two objects and encodes them (sequentially) into memory. Following some behavior one of the two objects is moved. Note, in real experiments the animal is removed for this manipulation. In simulation, this is unnecessary. Once the agent has returned to location of encoding, it is faced with the manipulated object array. The agent then initiates recall for objects one and two in sequence. The patterns of OVC re-activation can be compared to the corresponding patterns during perception (population vectors correlated, see main text). For the moved object, the comparison signals a change (near zero correlation). That object would hence be preferentially explored by the agent, and the next movement target for the agent is set accordingly (see main text). Please see caption of Video 1 for abbreviations. </p></caption></media><p>Hippocampal lesions are implemented by setting the firing rates of hippocampal neurons to zero. A hippocampal lesion (<xref ref-type="fig" rid="fig9">Figure 9</xref>; <xref ref-type="video" rid="video8">Video 8</xref>) precludes the generation of a meaningful novelty signal because the agent is incapable of generating a coherent point of view for recollection, and the appropriate BVC configuration cannot be activated by the now missing hippocampal input. Connections between object vector cells and perirhinal neurons (see <xref ref-type="fig" rid="fig3">Figure 3D</xref>) can still form during encoding in the lesioned agent. Thus some OVC activity is present during recall due to these connections. However, this activity is not location specific. Without the reference frame of place cells and thence BVC activity this residual OVC activity during recall can be generated anywhere (see <xref ref-type="fig" rid="fig9">Figure 9F–H</xref>). It only tells the agent that it has seen this object at a given distance and direction, but not where in the environment it was seen. Hence, the mismatch signal is equal for both objects, and exploration time would be split roughly evenly between them. However, if the agent happens to be at the same distance and direction from the objects as at encoding, then perceptual OVC activity will match the recalled OVC activity (<xref ref-type="fig" rid="fig9">Figure 9G,H</xref>), which might correspond to the ability of focal hippocampal amnesics to detect the familiarity of an arrangement of objects if tested from the same viewpoint as encoding (<xref ref-type="bibr" rid="bib80">King et al., 2002</xref>; but see also <xref ref-type="bibr" rid="bib128">Shrager et al., 2007</xref>).</p><media id="video8" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video8.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.020</object-id><label>Video 8.</label><caption><title>should be compared to <xref ref-type="video" rid="video7">Video 7</xref>.</title><p>It shows a reproduction of the object novelty paradigm of <xref ref-type="bibr" rid="bib97">Mumby et al., 2002</xref>; detecting that one of two objects has been moved). The agent is faced with two objects and encodes an association between relative object location (signaled by OVCs) and object identity (signaled by perirhinal neurons) - see <xref ref-type="video" rid="video7">Video 7</xref> for encoding phase. Due to the hippocampal lesion, these associations cannot be bound to place cells. Once one of the two objects is moved (compare to Simulation 1.3) the agent initiates recall and the patterns of OVC re-activation are compared to the corresponding patterns during perception (population vectors correlated, see main text). Recall is initiated at two distinct locations to highlight the following effect of the lesion: Since associations between OVCs and perirhinal neurons are not bound to a specific environmental location a comparison of OVC patterns between perception and recall signals mismatch everywhere for both objects except for the two special locations at which imagery is engaged in the video. At each of those locations, the neural pattern due to the learned association happens to coincide with the pattern during perception for one of the two objects. Hence no object can be singled out for enhanced exploration. Match and Mismatch is signaled equally for both objects (see main text). Please see caption of Video 1 for abbreviations. </p></caption></media><p>Rats also show preferential exploration of a familiar object that was previously experienced in a different environment, compared with one previously experienced in the same environment, and this preference is also abolished by hippocampal lesions (<xref ref-type="bibr" rid="bib97">Mumby et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Eacott and Norman, 2004</xref>; <xref ref-type="bibr" rid="bib86">Langston and Wood, 2010</xref>). We have not simulated different environments (using separate place cell ensembles), but note that ‘remapping’ of PCs between distinct environments (i.e. much reduced overlap of PC population activity; e.g. <xref ref-type="bibr" rid="bib18">Bostock et al., 1991</xref>; <xref ref-type="bibr" rid="bib6">Anderson and Jeffery, 2003</xref>; <xref ref-type="bibr" rid="bib161">Wills et al., 2005</xref>) suggests a mismatch signal for the changed-context object would be present in PC population vectors. Initiating recall of object A, belonging to context 1, in context 2, would re-activate the PC ensemble belonging to context 1, creating an imagined scene from context one which would mismatch the activity of PCs representing context two during perception. A hippocampal lesion precludes such a mismatch signal by removing PCs.</p><p>Finally, it has been argued that object recognition (irrespective of context) is spared after hippocampal but not perirhinal lesions (<xref ref-type="bibr" rid="bib2">Aggleton and Brown, 1999</xref>; <xref ref-type="bibr" rid="bib163">Winters et al., 2004</xref>; <xref ref-type="bibr" rid="bib102">Norman and Eacott, 2004</xref>) which would be compatible with the model given that its perirhinal neuronal population signals an object’s identity irrespective of location.</p></sec><sec id="s3-5"><title>‘Top-down’ activity and trace responses (Simulations 2.1, 2.2)</title><p>Simulations 1.3 and 1.4 dealt with a moved object. Similarly, if a scene element (a boundary or an object) has been removed after encoding, probing the memorized MTL representation can reveal trace activity reflecting the previously encoded and now absent boundary or object.</p><p>Section (Bottom-up vs top-down modes of operation) summarizes how top-down and bottom-up phases are implemented by a modulation of connection strengths (see <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig4">4</xref>, Materials and methods section Embedding Object-representations into a Spatial Context: Attention and Encoding, and Appendix). During perception, the ‘top-down’ connections from the MTL to the transformation circuit and thence to the parietal window are reduced to 5% of their maximum strength, to ensure that learned connections do not interfere with on-going, perceptually driven activity. During imagery, the ‘bottom-up’ connections from the parietal window to the transformation circuit and thence to the MTL are reduced to 5 percent of their maximum strength.</p><p>In rodents, it has been proposed that encoding and retrieval are gated by the theta rhythm (<xref ref-type="bibr" rid="bib65">Hasselmo et al., 2002</xref>): a constantly present modulation of the local field potential during exploration. In humans, theta is restricted to shorter bursts, but is associated with encoding and retrieval (<xref ref-type="bibr" rid="bib41">Düzel et al., 2010</xref>). If rodent theta determines the flow of information (encoding vs retrieval) then it may be viewed as a periodic comparison between memorized and perceived representations, without deliberate recall of a specific item in its context (that is, without changing the point of view). In Simulations 2.1 and 2.2, we implement this scenario. There is no cue to recall anything specific, regular sensory inputs are continuously engaged, and we periodically switch between bottom-up and top-down modes (at roughly theta frequency) to allow for an on-going comparison between perception and recall. Activity due to the modulation of top-down connectivity during perception propagates to the parietal window representations (PWb/o), allowing for a detection of mismatch between sensorily driven and imagery representations.</p><p>In Simulation 2.1, the agent has a set of MTL weights which encode the contextual representation of a square room with an inserted barrier (i.e. a barrier was present in the training phase). However, when the agent explores the environment, the barrier is absent (<xref ref-type="fig" rid="fig10">Figure 10A</xref>). Due to the modulation of top-down connectivity, the memory of the barrier (in form of BVC activity) periodically bleeds into the parietal representation during perception (<xref ref-type="fig" rid="fig10">Figures 10B1</xref>, <xref ref-type="fig" rid="fig2">2</xref> and <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="video" rid="video9">Video 9</xref>). The resultant dynamics carry useful information. First, letting the memory representation bleed into the perceptual one allows an agent, in principle, to localize and attend to a region of space in the egocentric frame of reference (as indicated by parietal window activity) where a change has occurred. A mismatch between the perceived (low bottom-up gain) and partially reconstructed (high bottom-up gain) representations, can signal novelty (compare to Simulations 1.3, 1.4), and could underlie the production of memory-guided attention (e.g. <xref ref-type="bibr" rid="bib95">Moores et al., 2003</xref>; <xref ref-type="bibr" rid="bib138">Summerfield et al., 2006</xref>). Moreover, the theta-like periodic modulation of top-down connectivity causes the appearance of ‘trace’ responses in BVC firing rate maps, indicating the location of previously encoded, now absent, boundary elements (<xref ref-type="fig" rid="fig10">Figures 10C1</xref> and <xref ref-type="fig" rid="fig2">2</xref>)</p><fig id="fig10" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.021</object-id><label>Figure 10.</label><caption><title>‘Top-down’ activity and ‘trace’ responses.</title><p>(<bold>A</bold>) An environment containing a small barrier (red outline) has been encoded in the connection weights in the MTL, but the barrier has been removed before the agent explores the environment again. (<bold>B</bold>) Activity snapshots for PWb (<bold>B1</bold>), BVC (<bold>B2</bold>) and PC (<bold>B3</bold>) populations during exploration. The now absent barrier is weakly represented in parietal window activity due to the periodic modulation of top-down connectivity during perception, although ‘bottom-up’ sensory input due to visible boundaries still dominates (see main text). (<bold>C1</bold>) High gain for top-down connections yields BVC firing rate maps with trace fields due to the missing boundary. Left: BVC firing rate map. Right: An illustration of the BVC receptive field (teardrop shape attached to the agent at a fixed allocentric direction and distance) with the agent shown at two locations where the cell in the left panel fires maximally. (<bold>C2</bold>) Same as C1 for a cell whose receptive field is tuned to a different allocentric direction. (<bold>D1</bold>) Similarly to the missing boundary in A, a missing object (small red circle) can produce ‘trace’ firing in an OVC (<bold>D2</bold>). Every time the agent traverses the location from which the object was encoded (large red circle in D1), learned PC-to-OVC connections periodically reactivate the associated OVC. (<bold>D3</bold>) The same PCs also re-activate the associated perirhinal identity cell (PRo), yielding a spatial trace firing field for a nominally non-spatial perirhinal cell (red circle).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig10-v1"/></fig><media id="video9" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video9.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.022</object-id><label>Video 9.</label><caption><title>This video shows a visualization of the simulated neural activity as the agent moves in a familiar environment.</title><p>However, a previously present boundary has been removed. The agent is supplied with a periodic (akin to rodent theta) modulation of the top-down connection weights (please see main text). The periodic modulation of these connections allows for a probing of the memorized spatial context without engaging in full recall and reveals the memory of the environment to be incongruent with the perceived environment. BVC activity due to the memorized (now removed) boundary periodically 'bleeds' into the egocentric parietal window ¨representation, in principle allowing the agent to attend to the part of environment which has undergone change (location of removed boundary). Time integrated neural activity from this simulation yields firing rate maps which show traces of the removed boundary (see <xref ref-type="fig" rid="fig10">Figure 10</xref> in the manuscript). Note, the video is cut after 1 min to reduce filesize. The full simulation covers approximately 300 s of real time. Please see caption of Video 1 for abbreviations. </p></caption></media><p>Simulation 2.2 (<xref ref-type="fig" rid="fig10">Figures 10D1,D2</xref> and <xref ref-type="video" rid="video10">Video 10</xref>) shows similar’ trace’ responses for OVCs. The agent has a set of MTL weights which encode the scene from Simulation 1.0 (<xref ref-type="fig" rid="fig5">Figure 5</xref>) where it encountered and encoded an object. The object is now absent (small red circle in <xref ref-type="fig" rid="fig10">Figure 10D1</xref>), but the periodic modulation of top-down connectivity reactivates corresponding OVCs, yielding trace fields in firing rate maps. This activity can bleed into the parietal representation during perception (e.g. at simulation time 9:40-10:00 in <xref ref-type="video" rid="video6">Video 6</xref>), albeit only when the location of encoding is crossed by the agent, and with weaker intensity than missing boundary activity (the smaller extent of the OVC representation leads to more attenuation of the pattern as it is processed by the transformation circuit).</p><media id="video10" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video10.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.023</object-id><label>Video 10.</label><caption><title>This video shows a visualization of the simulated neural activity as the agent moves in a familiar environment.</title><p>However, a previously present (and encoded) object has been removed. The agent is supplied with a periodic (akin to rodent theta) modulation of the top-down connection weights (please see main text). The periodic modulation of these connections allows for a probing of the memorized spatial context. With every passing through the encoding location OVC activity (reflecting the now removed object) and perirhinal activity is generated by place cells covering the encoding location. This re-activation yields firing rate maps which show traces of the removed object in OVCs, and induces a spatial firing field for the nominally non-spatially selective perirhinal neuron (compare to <xref ref-type="fig" rid="fig10">Figure 10</xref> in the manuscript). Note, the video is cut after 1 min to reduce filesize. The full simulation covers approximately 300 s of real time. Please see caption of Video 1 for abbreviations. </p></caption></media><p>Interestingly, perirhinal identity neurons, which normally fire irrespective of location, can appear as spatially selective trace cells due to the periodic modulation of top-down connectivity at the location of encoding. <xref ref-type="fig" rid="fig10">Figure 10D3</xref> shows the firing rate map of a perirhinal identity neuron. Every time the memorized representation is probed (high top-down gain), if the agent is near the location of encoding, the learned connection from PCs to perirhinal cells (PRo) lead to perirhinal firing for the absent object, yielding a spatial trace firing field for this nominally non-spatial cell.</p><p>The presence of some memory-related activity during nominally bottom-up (perceptual) processing can have benefits beyond the assessment of change discussed above. For instance, additional activity in the contextual representations (BVCs, PC, PRb neurons) due to pattern completion in the MTL can enhance the firing of BVCs coding for scene elements outside the current field of view. This activity can propagate to the PW, as is readily apparent during full recall/imagery (<xref ref-type="fig" rid="fig5">Figure 5</xref>) but is also present in weaker form during perception. Such activity may support awareness of our spatial surrounding outside of the immediate field of view, or may enhance perceptually driven representations when sensory inputs are weak or noisy.</p></sec><sec id="s3-6"><title>Sampling multiple objects in imagery (Simulation 3.0)</title><p>Humans can focus attention on different elements in an imagined scene, sampling one after another, without necessarily adopting a new imagined viewpoint. This implies that the set of active PCs need not change while different objects are inspected in imagery. Moreover, humans can localize an object in imagined scenes and retrieve its identity (e.g. ‘What was next to the fireplace in the restaurant we ate at?”).</p><p>In Simulation 1.0 (encoding and object-cued recall, <xref ref-type="fig" rid="fig5">Figure 5</xref>), in addition to connection weights from perirhinal (PRo) neurons to PCs and OVCs, the reciprocal weights from OVCs to PRo neurons were also learned. These connections allow the model to sample and inspect different objects in an imagined scene. To illustrate this we place two objects in a scene and allow the agent to encode both visible objects from the same point of view. Encoding still proceeds sequentially. That is, our attention model first samples one object (boosting its activity in the PW) and then the other.</p><p>We propose that encoded objects that are not currently the focus of attention in imagery can attract attention by virtue of their residual activity in the parietal window (weak secondary peak in the PWo population in <xref ref-type="fig" rid="fig11">Figure 11B</xref>). Thus, any of these targets can be focused on by scanning the parietal window and shifting attention to the corresponding location (e.g. ‘the next object on a table’). Boosting the drive to such a cluster of PWo cells in the parietal window leads to corresponding activity in the OVC population (via the retrosplenial transformation circuit). The learned connection from OVCs to perirhinal PRo neurons will then drive PRo activity corresponding to the object which, at the time of encoding, was at the location in peripersonal space which is now the new focus of attention. Mutual inhibition between PRo neurons suppresses the previously active PRo neuron. The result is a top-down drive of perirhinal neurons (as opposed to bottom-up object recognition), which allows inferring the identity of a given object. That is, by shifting its focus of attention in peripersonal space (i.e. in the parietal window) during imagery the agent can infer the identity of scene elements which it did not initially recall.</p><fig id="fig11" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.024</object-id><label>Figure 11.</label><caption><title>Inspecting scene elements in imagery.</title><p>The agent encounters two objects. (<bold>A</bold>) Activity in PWo (left) and OVCs (right) populations when the agent is attending to one of the two objects during encoding. Both objects are encoded sequentially from the same location (time index 0.22 in <xref ref-type="video" rid="video11">Video 11</xref>). The agent then moves past the objects. (<bold>B</bold>) Imagery is engaged by querying for object 1, raising activity in corresponding PRo neurons (far right) and switching into top-down mode (similar to Simulation 1.0, <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="video" rid="video2">Video 2</xref>), leading to full imagery from the point of view at encoding. Residual activity in the OVC population at the location of object 2 (encoded from the same position, that is driven by the same place cells) translates to weak residual activity in the PWo population. (<bold>C</bold>) Applying additional current (i.e. allocating attention) to the PWo cells showing residual activity at the location of object 2 (leftmost blue arrow) and removing the drive to the PRo neuron corresponding to object 1 (because the initial query has been resolved) leads to a build-up of activity at the location of object two in the OVC population (blue arrow between PWo and OVC plots). By virtue of the OVC to PRo connections (blue between OVC and Pro plots), the PRo neuron for object two is driven (and inhibits PRo neuron 1, right-most blue arrows). Thus, the agent has inferred the identity of object 2, after having initiated imagery to visualize object 1, by paying attention to its egocentric location in imagery.</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig11-v1"/></fig><p><xref ref-type="fig" rid="fig11">Figure 11</xref> and <xref ref-type="video" rid="video11">Video 11</xref> show sequential (attention-based) encoding, subsequent recall and attentional sampling of scene elements. The agent sequentially encodes two objects from one location (<xref ref-type="fig" rid="fig11">Figure 11A</xref>), moves on until both objects are out of view, and engages imagery to recall object one in its spatial context (<xref ref-type="fig" rid="fig11">Figure 11B</xref>). The agent can then sample object two by allocating attention to the secondary peak in the parietal window (boosting the residual activity by injecting current in the PWo cells corresponding to the location of object 2). This activity spreads back to the MTL network, via OVCs, driving the corresponding PRo neuron (<xref ref-type="fig" rid="fig11">Figure 11C</xref>). Thus, the agent infers the identity of object 2, by inspecting it in imagery. Attention ensures disambiguation of objects at encoding, while reciprocity of connections in the MTL is necessary to form a stored attractor in spatial memory.</p><media id="video11" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video11.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.025</object-id><label>Video 11.</label><caption><title>This video shows a visualization of the simulated neural activity as the agent sequentially encodes two objects into long-term memory.</title><p>Upon navigating past the objects the agent initiates recall, cueing with the first object. The OVC representations of both objects are bound to the same place cells. These place cells thus generate a secondary peak in the OVC representation corresponding to the non-cued object. This activity propagates to the parietal window. Allocating attention to this secondary peak in the egocentric parietal representation (i.e. injecting current), propagates back to OVCs, which then drive the perirhinal cells for the non-cued object. That is, the agent infers the identity of the second object which is part of the scene (see main text). Please see caption of Video 1 for abbreviations. </p></caption></media></sec><sec id="s3-7"><title>Grid cells and mental navigation (Simulation 4.0)</title><p>The parietal window neurons encode the perceived spatial layout of an environment, in an egocentric frame of reference, as an agent explores it (i.e. a representation of the current point of view). In imagery, this viewpoint onto a scene is reconstructed from memory (top-down mode as opposed to bottom-up mode). We refer to mental navigation as internally driven translation and rotation of the viewpoint in the absence of perceptual input. In Simulation 4.0, we let the agent encode a set of objects into memory and then perform mental navigation with the help of grid and head direction cells.</p><p>Grid cell (GC) firing is thought to update the location represented by place cell firing, driven by signals relating to self-motion (<xref ref-type="bibr" rid="bib104">O'Keefe and Burgess, 2005</xref>; <xref ref-type="bibr" rid="bib92">McNaughton et al., 2006</xref>; <xref ref-type="bibr" rid="bib52">Fuhs and Touretzky, 2006</xref>; <xref ref-type="bibr" rid="bib132">Solstad et al., 2006</xref>). During imagination, we suppose that GC firing is driven by mock motor-efference signals (i.e. imagined movement without actual motor output) and used to translate the activity bump on the sheet of place cells. Pattern completion in the MTL network would then update the BVC population activity accordingly, which will spread through the transformation circuit and update parietal window activity. That is, mock motor efference could smoothly translate the viewpoint in imagery (i.e. scene elements represented in the parietal window flow past the point of view of the agent). Similarly, mock rotational signals to the head direction attractor could rotate the viewpoint in imagery. Both together are sufficient to implement mental navigation.</p><p>GCs are implemented heuristically, approximating the output of more sophisticated models (e.g., <xref ref-type="bibr" rid="bib21">Burgess et al., 2007</xref>; <xref ref-type="bibr" rid="bib20">Burak and Fiete, 2009</xref>; <xref ref-type="bibr" rid="bib28">Bush and Burgess, 2014</xref>). Firing rate maps for 7 modules of 100 cells each are pre-calculated (see Appendix), providing the firing rates of GCs as a function of location. GC to PC weights are pre-calculated as Hebbian associations (to simulate a familiar environment), where the connection strength is maximal if the center of a PC’s receptive field coincides with (one of) the GC’s firing peaks. During (bottom-up) perception and navigation, GC input provides a small contribution to PC activity, which is mainly determined by BVC inputs (<xref ref-type="bibr" rid="bib103">O'Keefe and Burgess, 1996</xref>; <xref ref-type="bibr" rid="bib62">Hartley et al., 2000</xref>; <xref ref-type="bibr" rid="bib87">Lever et al., 2009</xref>), to highlight the ability of the model to self-localize based on sensory inputs. Stronger grid cell input simply makes the location estimate more stable without detriment to the model. In the absence of reliable sensory information strong GC inputs are required to make PCs fire reliably (<xref ref-type="bibr" rid="bib26">Bush et al., 2014</xref>; <xref ref-type="bibr" rid="bib114">Poucet et al., 2014</xref>; <xref ref-type="bibr" rid="bib48">Evans et al., 2016</xref>). Imagery is an extreme case of this situation, where no sensory input is provided to PCs. Consequently, GC input is up-regulated during imagery (similar to other connections in the switch from bottom-up to top-down modes), constituting a major input to PCs. This GC input can then translate the agent’s viewpoint in imagery (via their effect on PCs) without directly affecting the transformation circuit.</p><p><xref ref-type="fig" rid="fig12">Figure 12</xref> and <xref ref-type="video" rid="video12">Video 12</xref> show an example of mental navigation. The agent approaches three objects in sequence, encodes them into memory and then initiates recall cued by object 1. From that (imagined) location, it initiates mental navigation in a straight line. GCs shift the PC activity bump along the trajectory. The allocentric boundary representation (BVCs) follows the shifting PCs (due to pattern completion) and the retrosplenial transformation circuit (RSC/TR) translates the shifting BVC representation into a shifting (i.e. ‘flowing past the agent’) egocentric representation of boundary distance (imagery of motion in an imagined scene, not shown in <xref ref-type="fig" rid="fig12">Figure 12</xref>, however, see <xref ref-type="video" rid="video12">Video 12</xref>). Importantly, the imagined trajectory takes the agent through the area of space at which object three was encoded, however this time coming from a novel direction. The transformation circuit nevertheless instantiates the correct activity in the parietal window for object 3, making it appear to the agent’s right, instead of to it’s left (as during its original encoding, coming from object 2). Not only does the object populate the imagined scene as the agent mentally navigates past it, the event also generates an imagined representation which has never been experienced by the agent.</p><p>Translating an established BVC pattern due to updated perceptual input (in response to real motion) also translates the PC activity bump. In fact, this is how perceptual information updates the estimate of position (self-localization) in a familiar environment (PWb→RSC→BVC→PC) in the model. Similarly, shifting the activity pattern across PCs via GCs in mental navigation can update the parietal window (PWb) during mental navigation (GC→PC→BVC→RSC→PWb). With this account of mental exploration of different routes (including potentially novel imagined experiences; see next section), the model provides a neural implementation of important aspects of ‘scene construction’ (<xref ref-type="bibr" rid="bib64">Hassabis et al., 2007</xref>) and ‘episodic future thinking’ (<xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>), although these concepts also extend beyond the capabilities of the model (see Discussion). The inclusion of GCs allows for a parsimonious account of mental navigation in humans, consistent with observation of grid-like activity during imagined movement through familiar environments (<xref ref-type="bibr" rid="bib14">Bellmund et al., 2016</xref>; Horner et al. 2016).</p><fig id="fig12" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.026</object-id><label>Figure 12.</label><caption><title>Mental navigation with grid cells.</title><p>Left to right: allocentric agent position (black triangle) and recent trajectory (black dashed line); PWo, OVC, and PC population snapshots; GC input to PCs (i.e. GC firing rates multiplied by connection weights from GCs to PCs); PRo neurons. The rightmost panel indicates which objects have been encoded. (<bold>A</bold>) The agent is exploring the environment and has just encoded the second object into memory (right-most bar chart). Object one has been encoded near the start of the trajectory. (<bold>B</bold>) After encoding the third object and moving past it, the agent initiates imagery, recalling object one in its spatial context (top-down mode) from a point of view West of object 1, facing East (red triangle). (<bold>C</bold>) Mock motor efference shifts GC activity (dashed arrow on GC input to PCs) and thence drives the PC activity bump representing (imagined) agent location. The allocentric (BVCs) and egocentric (PWb) boundary representation follow suit (see main text and <xref ref-type="video" rid="video12">Video 12</xref>). As the PC activity bump passes the location at which object 3 was encoded, corresponding OVC activity is elicited by learned connections (and is transformed into PWo activity (solid orange arrows indicate GCs updating PCs, PCs updating OVCs, etc). NB object 3 appears in the reconstructed scene ahead-right of the agent (PWo snapshot, second panel), despite being encoded ahead-left of the agent when it moved southwards from object 2 toward object 3. The corresponding perirhinal neuron is also driven to fire by PCs (orange arrow in PRo panel).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig12-v1"/></fig></sec><sec id="s3-8"><title>Shortcutting and ‘preplay-like’ activity (Simulation 5.0)</title><p>It is a small step from imagined movement to planned navigation. GCs have been suggested to compute the vector to a goal location from the current location (see <xref ref-type="bibr" rid="bib82">Kubie and Fenton, 2012</xref>; <xref ref-type="bibr" rid="bib47">Erdem and Hasselmo, 2012</xref>; <xref ref-type="bibr" rid="bib27">Bush et al., 2015</xref>; <xref ref-type="bibr" rid="bib136">Stemmler et al., 2015</xref>), a capability necessary to explain the ability to take a shortcut across previously unexplored territory (<xref ref-type="bibr" rid="bib144">Tolman, 1948</xref>). We propose that this ability is based on mental (vector-based) navigation supported by GCs. In Simulation 5, we let the agent explore a novel part of the environment, extending a pre-existing representation of a spatial context. Simulation 5.0 consists of three distinct phases: planning movement across a previously unvisited area to a reward location (phase 1); actual navigation of this shortcut (phase 2); and finally mental navigation across the now familiar area (phase 3).</p><p>In phase 1, the agent generates a trajectory along the shortest path to the goal using GCs (i.e. a straight line where the barriers happened to be in the way, <xref ref-type="fig" rid="fig13">Figure 13B</xref>). However, unlike in Simulation 4.0 (<xref ref-type="fig" rid="fig12">Figure 12</xref>), this process differs from mental navigation since the unexplored part of the environment is devoid of any meaningful PC-BVC connections and so a scene cannot be generated in the parietal window (PWb). Extending the medial temporal lobe (MTL) representations requires incorporating additional place cells into the MTL attractor. These (future) place cells are referred to as ‘reservoir cells’ and have no relationship to physical space yet, so visualizing their firing rates in a topographic manner is not possible. However, as the agent generates a trajectory towards its goal using GCs, sparse random GC-to-PC connections cause a subset of the reservoir cells to fire (<xref ref-type="fig" rid="fig13">Figure 13B</xref> and <xref ref-type="video" rid="video13">Video 13</xref>). The activity of reservoir cells does not form an attractor bump, as PC-PC connections have not been learned, but their firing is normalised to a level of activity similar to when an attractor bump is present (implemented by an adaptive feedback current, see Appendix). In <xref ref-type="fig" rid="fig13">Figure 13B</xref> (rightmost panel) reservoir cells are ordered according to their time of maximum firing along the imagined trajectory.</p><fig id="fig13" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.027</object-id><label>Figure 13.</label><caption><title>Planning, taking and imaging a trajectory across an unexplored area.</title><p>The agent is located in an environment where the direct trajectory between two salient locations (purple dots, left column) covers an unexplored part of the environment. PCs potentially firing in the unexplored area (‘reservoir cells’) receive only random connections from GCs (see unstructured grid cell input in column 5). Left to right panel columns: allocentric agent position (triangle); PWb, BVC, PC population rates; GC inputs to PCs (see <xref ref-type="fig" rid="fig12">Figure 12</xref>); and (B-D only) firing of ‘reservoir’ PCs along the trajectory (x axis), stacked and ordered by time of peak firing along the trajectory (y axis). (<bold>A</bold>) Starting situation. (<bold>B</bold>) Phase 1; imagined movement across the obstructed space leads to preplay-like activity in reservoir PCs (rightmost panel). Red arrow indicates the reservoir PCs are driven by grid cells. No egocentric representation can be generated from BVCs because ‘reservoir’ PCs have no connections to BVCs, that is they are not yet part of the MTL attractor. (<bold>C</bold>) Phase 2; the barrier is removed and the agent navigates the trajectory in real space. GCs again drive PCs (thick grey arrow), so the temporal sequence of reservoir cell activity in (A) is recapitulated in the spatial sequence of PC activity. Sensory inputs drive the PW (bottom-up mode) and hence BVCs (grey arrow between panels 2 and 3). Hebbian learning proceeds between PCs and BVCs (dashed grey line), and from GCs to PCs (reinforcing the drive from GCs to PCs, grey arrow between panels 4 and 5). (<bold>D</bold>) Phase 3; having traversed the novel part of the environment, the agent initiates imagery and performs mental navigation along the newly learned trajectory. The learned connections now instantiate the correct BVC and PW activity in top-down mode (orange arrows indicating flow of information, similar to <xref ref-type="fig" rid="fig12">Figure 12</xref>).</p></caption><graphic mime-subtype="postscript" mimetype="application" xlink:href="elife-33752-fig13-v1"/></fig><media id="video12" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video12.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.028</object-id><label>Video 12.</label><caption><title>This video shows a visualization of the simulated neural activity as the agent performs a complex trajectory and encodes three objects into long-term memory along the way.</title><p>Upon navigating past the third object the agent initiates recall, cueing with the first object, and subsequently performs mental navigation (imagined movement in visuo-spatial imagery) with the help of grid cells. Grid cells update the place cell representation along the trajectory. The egocentric parietal representation is updated along the imagined trajectory, that is scene elements are flowing past the point of view of the agent. Note, the imagined trajectory does not correspond to a previously taken route. Nevertheless, when the imagined trajectory takes the agent past the encoding location of object 3, it is instantiated in the OVC and PWo representations (see main text). Grid cell firing rates are shown multiplied by their connection weights to place cells. Please see caption of Video 1 for further abbreviations.</p></caption></media><media id="video13" mime-subtype="mp4" mimetype="video" xlink:href="elife-33752-video13.mp4"><object-id pub-id-type="doi">10.7554/eLife.33752.029</object-id><label>Video 13.</label><caption><title>This video shows a visualization of the simulated neural activity as the agent performs mental navigation across a blocked shortcut.</title><p>Newly recruited cells in the hippocampus exhibit activity reminiscent of preplay. Upon removal of the barrier the agent traverses the shortcut and associates the newly recruited hippocampal cells with the perceptually driven activity in the MTL. Subsequent mental navigation across the short cut yields activity in hippocampal cells reminiscent of replay (see main text). Grid cell firing rates are shown multiplied by their connection weights to place cells. Please see caption of Video 1 for further abbreviations.</p></caption></media><p>In phase 2, the barriers are removed and the agent performs the previously imagined trajectory in real space, using the novel shortcut. The same GCs are active along the trajectory and hence the same reservoir cells which fired before exploring the area, now fire in a spatial sequence along the actual trajectory. Since the agent is now actively perceiving its environment BVCs are driven in a bottom-up manner and Hebbian-like plasticity can strengthen connections between BVCs and reservoir cells as they fire along the trajectory (an analogous mechanism should also associate perirhinal neurons, which is omitted here). Hence, the reservoir cells have now effectively become place cells, with firing fields tied to the agent’s location in space (<xref ref-type="fig" rid="fig13">Figure 13C</xref> and <xref ref-type="video" rid="video13">Video 13</xref>). <xref ref-type="fig" rid="fig13">Figure 13C</xref> (rightmost panel) shows place cell activity along the trajectory during phase 2. Crucially, these cells are plotted in the order of activity shown during the previous imagined navigation (<xref ref-type="fig" rid="fig13">Figure 13B</xref>), indicating ‘pre-play-like’ behavior, in that the sequence of PC firing seen prior to first exploration is subsequently recapitulated during actual navigation (<xref ref-type="fig" rid="fig13">Figure 13C</xref>; <xref ref-type="bibr" rid="bib39">Dragoi and Tonegawa, 2011</xref>; <xref ref-type="bibr" rid="bib167">Ólafsdóttir et al., 2015</xref>).</p><p>Finally, in phase 3, the agent initiates imagery and performs mental navigation along the shortcut (i.e. recalls the episode of traversal), demonstrating that the MTL representation has been extended, and that a scene can be generated (<xref ref-type="fig" rid="fig13">Figure 13D</xref> and <xref ref-type="video" rid="video13">Video 13</xref>). The newly learned connections from reservoir PCs to BVCs complete the MTL representation of the spatial context and the transformation circuit reinstates the corresponding parietal window (PWb) representation (imagery, <xref ref-type="fig" rid="fig10">Figure 10D</xref>, panel 2).</p><p>The ability to plan a route by driving a sweep of PC activity with a sweep of GC activity via established GC-PC connections (<xref ref-type="fig" rid="fig12">Figures 12</xref>–<xref ref-type="fig" rid="fig13">13</xref>) could relate to the observation of ‘forward sweeps’ of PC activity during navigation (<xref ref-type="bibr" rid="bib76">Johnson and Redish, 2007</xref>; <xref ref-type="bibr" rid="bib112">Pfeiffer and Foster, 2015</xref>) and ‘replay’ during rest (<xref ref-type="bibr" rid="bib162">Wilson and McNaughton, 1994</xref>; <xref ref-type="bibr" rid="bib51">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib36">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="bib78">Karlsson and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib31">Carr et al., 2011</xref>). However, both of these phenomena, and the ‘pre-play-like’ activity discussed above, occur at compressed timescales in experimental animals. Thus, modeling forward sweeps, replay or pre-play would require a spiking neuron model able to capture the faster time scale of the sharp-wave ripple events associated with replay and pre-play, and the theta sequences associated with forward sweeps (<xref ref-type="bibr" rid="bib25">Burgess et al., 1994</xref>; <xref ref-type="bibr" rid="bib129">Skaggs et al., 1995</xref>; <xref ref-type="bibr" rid="bib57">Gupta et al., 2012</xref>).</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>We propose a model of how sensory experiences, which are ultimately egocentric in nature, are transformed into viewpoint-invariant representations for long-term spatial memory in the medial temporal lobe (MTL) via processing in parietal and retrosplenial cortices. According to the model, imagery and recollection of scenes correspond to the re-construction of egocentric representations in parietal areas (the parietal window, PWb/o) from MTL representations. The MTL is the repository of viewpoint-invariant knowledge which is used to generate spatially coherent scenes by retrieving information consistent with perception from a single location and orientation. Pattern completion (via attractor dynamics) implements retrieval of a neural representation across the MTL, while head-direction cells enable the translation into egocentric coordinates via a retrosplenial transformation circuit, making use of gain-field neurons (<xref ref-type="bibr" rid="bib130">Snyder et al., 1998</xref>; <xref ref-type="bibr" rid="bib53">Galletti et al., 1995</xref>; <xref ref-type="bibr" rid="bib116">Pouget and Sejnowski, 1997</xref>). Thus, for example, unilateral lesions in parietal regions could cause perceptual hemispatial neglect, and unilateral lesions to parietal or retrosplenial cortex could cause representational hemispatial neglect (in imagery) for a scene for which the MTL representation is complete (<xref ref-type="bibr" rid="bib17">Bisiach and Luzzatti, 1978</xref>; see also <xref ref-type="bibr" rid="bib116">Pouget and Sejnowski, 1997</xref>; <xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>).</p><p>The model can be used to account for human spatial cognition at the level of single neurons far from the sensory periphery: Place cells, head direction cells, gain-field neurons, boundary- and object-vector cells (BVCs and OVCs), and grid cells. Future work should try to integrate the present account of spatial cognition with recent progress concerning spatial coding in parietal areas (<xref ref-type="bibr" rid="bib100">Nitz, 2006</xref>; <xref ref-type="bibr" rid="bib99">Nitz, 2009</xref>; <xref ref-type="bibr" rid="bib101">Nitz, 2012</xref>; <xref ref-type="bibr" rid="bib63">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib159">Whitlock et al., 2012</xref>; <xref ref-type="bibr" rid="bib117">Raposo et al., 2014</xref>; <xref ref-type="bibr" rid="bib155">Vedder et al., 2017</xref>), and a broader view of retrosplenial function (e.g., <xref ref-type="bibr" rid="bib4">Alexander and Nitz, 2015</xref>; <xref ref-type="bibr" rid="bib5">Alexander and Nitz, 2017</xref>). Notably, BVCs were predicted by an early predecessor of the present model (<xref ref-type="bibr" rid="bib62">Hartley et al., 2000</xref>; <xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>). Here, we have introduced OVCs to show how items introduced into a familiar environment may be coded for and incorporated into long-term memory. Intriguingly, OVC-like responses have been reported recently (<xref ref-type="bibr" rid="bib35">Deshmukh and Knierim, 2013</xref>; <xref ref-type="bibr" rid="bib73">Hoydal et al., 2017</xref>). We also explored how long-term memory might be probed to assess novelty. Finally, we incorporated grid cells and investigated their role in exploratory behavior and mental navigation. We can thus begin to frame abstract notions such as episodic future thinking and scene construction in terms of neural mechanisms, although we note that these concepts extend beyond our model to include completely fictional scenes/scenarios (<xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib64">Hassabis et al., 2007</xref>; <xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>).</p><sec id="s4-1"><title>Recall of objects in a spatial context</title><p>We have proposed that items/objects are associated to a given (spatial) context via place cells, which index the local sensory panorama, including local objects. Attaching representations of discrete objects (in the form of object vector cell activity) to a contextual representation via place cells aligns well with neuropsychological experiments that show position specificity in visual object memory (<xref ref-type="bibr" rid="bib71">Hollingworth, 2007</xref>). In such experiments, object memory is superior when the target object is presented at the same position in the scene as it had been viewed originally (also see object novelty Simulations 1.3, 1.4). The hippocampus in particular has been implicated in combining information about objects, locations, and contexts (<xref ref-type="bibr" rid="bib157">Warburton and Brown, 2010</xref>; <xref ref-type="bibr" rid="bib42">Eacott and Gaffan, 2005</xref>; <xref ref-type="bibr" rid="bib10">Barker and Warburton, 2015</xref>), consistent with the model. Similarly, studies suggest the hippocampus and precuneus are necessary for maintaining object-location binding even in working memory (<xref ref-type="bibr" rid="bib113">Piekema et al., 2006</xref>; <xref ref-type="bibr" rid="bib109">Olson et al., 2006</xref>).</p><p>The direction-independence of place cell firing in open environments implies that all possible local views at a given location could be associated with the corresponding place cells, potentially encompassing the boundaries in all directions around that location. Only by supplying head (or gaze) direction, and transforming the activity to the parietal window, a specific point of view can be represented. Note that given the anatomical loci of head direction cells along Papez circuit, the role of head direction as a modulatory factor in the egocentric-allocentric transformation (modeled as within retrosplenial cortex) provides a good explanation for impaired episodic memory resulting from Papez circuit lesions (<xref ref-type="fig" rid="fig7">Figure 7</xref>; <xref ref-type="bibr" rid="bib34">Delay and Brion, 1969</xref>; <xref ref-type="bibr" rid="bib135">Squire and Slater, 1978</xref>; <xref ref-type="bibr" rid="bib134">1989</xref>; <xref ref-type="bibr" rid="bib111">Parker and Gaffan, 1997</xref>; <xref ref-type="bibr" rid="bib3">Aggleton et al., 2016</xref>; <xref ref-type="bibr" rid="bib147">Tsivilis et al., 2008</xref>). It also explains why permanent landmarks should evoke stronger responses in retrosplenial cortex (<xref ref-type="bibr" rid="bib9">Auger et al., 2012</xref>), because permanent landmarks provide a more stable directional reference for the transformation circuit (see also <xref ref-type="bibr" rid="bib15">Bicanski and Burgess, 2016</xref>). In summary, head direction cells likely serve to specify a direction of view, and not a movement direction (<xref ref-type="bibr" rid="bib118">Raudies et al., 2015</xref>), which could instead be expressed in the firing phase of grid cells or place cells (<xref ref-type="bibr" rid="bib91">Maurer et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Cei et al., 2014</xref>).</p><p>The encoding strategy for objects allows an agent to reactivate the set of place cells which were active when the object was encountered and thus reconstruct the local view at encoding in the parietal window. This models the explicit recollection of a spatial scene populated with objects as an act of visuo-spatial imagery. It provides an explanation for the neural activity seen in the MTL, retrosplenial cortex and precuneus during imagery for familiar scenes (<xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib64">Hassabis et al., 2007</xref>; <xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>). The agent could also use the place cells activated during imagery as ‘goal cells’ and use grid cells to calculate a vector to navigate to the remembered location (<xref ref-type="bibr" rid="bib27">Bush et al., 2015</xref>); not simulated here), accounting for the role of the MTL in goal-directed navigation (e.g. reviewed in <xref ref-type="bibr" rid="bib23">Burgess et al., 2002</xref>).</p><p>The present model of explicit recall for items in context is a small step on the long road to understanding episodic memory at the neuronal level. However, not all memories for items requires reconstruction of a spatial scene. Recall of factual information (semantic memory) is not modeled, while memory for the attributes of an object irrespective of its context would require only perirhinal involvement. The BB-model only applies to imagery for coherent spatial scenes, and suggests that this is necessary for episodic recollection in which the past event is ‘re-experienced’ (<xref ref-type="bibr" rid="bib148">Tulving, 1983</xref>), and certainly for remembering the spatial context of encountering an object.</p><p>Key components of the model are the ‘bottom-up’ transition from egocentric perceptual representations to allocentric MTL representations and the ‘top-down’ transition from MTL representations back to egocentric imagery. By informing perception in a top-down manner, the MTL can effectively predict perceptual input in familiar environments, allowing novelty detection and enhancing perception with remembered information. If we view imagery as a top-down reconstruction of perceptual representations, the MTL together with the retrosplenial transformation circuit could be seen as a generative model for scenes, consistent with generative models of memory such as (<xref ref-type="bibr" rid="bib83">Káli and Dayan, 2001</xref>). It has been proposed that the bottom-up/top-down transition between encoding and retrieving occurs rhythmically at the frequency of the theta rhythm (<xref ref-type="bibr" rid="bib66">Hasselmo et al., 1996</xref>; <xref ref-type="bibr" rid="bib24">Burgess et al., 2001a</xref>; <xref ref-type="bibr" rid="bib65">Hasselmo et al., 2002</xref>; <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>; <xref ref-type="bibr" rid="bib38">Douchamps et al., 2013</xref>). Theta might underlie a periodic probing of memorized representations; however, full recollection in imagery can last for long periods of time and need not correspond to specific phases of theta in humans (<xref ref-type="bibr" rid="bib41">Düzel et al., 2010</xref>).</p></sec><sec id="s4-2"><title>Mnemonic effects of newly learned connections and ‘trace cells’</title><p>We have proposed (<xref ref-type="fig" rid="fig10">Figure 10</xref>) that the relative strength of top-down and bottom-up connections can change smoothly and under control of the agent (e.g. via the release of a neuromodulator) to allow memory representations to influence neural activity during perception. This allows the agent to localize and attend to a region of space in the egocentric frame of reference where a given scene element used to be located, even if it has subsequently been moved, changed or removed. Moreover, the neural activity caused by increasing top-down connections can signal where the environment has changed. Interestingly, <xref ref-type="bibr" rid="bib146">Tsao et al. (2013)</xref> recently reported ‘trace cells’ in lateral entorhinal cortex, whose firing reflects the previous presence of a now missing object, while related ‘mis-place’ cells have been reported in CA1 (<xref ref-type="bibr" rid="bib107">O'Keefe, 1976</xref>).</p><p>We have shown that nominally non-spatially selective cells like perirhinal identity neurons can manifest a spatial trace firing field when re-activation occurs at the encoding location (<xref ref-type="fig" rid="fig10">Figure 10D3</xref>). This may help to reconcile the notion that lateral entorhinal cortex processes non-spatial information (<xref ref-type="bibr" rid="bib151">Van Cauter et al., 2013</xref>; <xref ref-type="bibr" rid="bib61">Hargreaves et al., 2005</xref>) with the spatial responses of trace cells (<xref ref-type="bibr" rid="bib146">Tsao et al., 2013</xref>) in lateral entorhinal cortex. However, the trace cells of <xref ref-type="bibr" rid="bib146">Tsao et al. (2013)</xref> do not fire when the object is present, but only in the subsequent absence of the object. Thus they might signal the mismatch between the remembered object and its absence, that is reflecting a comparison of perceptually driven and memory driven firing of the model perirhinal cells.</p><p>Finally, even in the absence of changes to the memorized spatial configuration, mnemonic representations can enhance perception, for example allowing the firing of cells coding for scene elements outside the current field of view. This activity is supported by pattern completion in the MTL, and may support people’s awareness of the presence of boundaries or objects outside of their field of view within a familiar environment.</p></sec><sec id="s4-3"><title>Attention</title><p>Although we do not model the mechanistic origins of attention (see e.g. <xref ref-type="bibr" rid="bib74">Itti and Koch, 2001</xref>), attentional modulation in the present model is crucial for unambiguous representations of multiple objects within a scene. If multiple objects are encoded from the same viewpoint, multiple OVC and perirhinal (PRo) neurons can be co-active, precluding the formation of a unique representation for each object-location conjunction, that is, precluding the solving the object-location binding problem. Thus, we require the objects to be sampled rhythmically and encoded sequentially in the parietal window (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig11">11</xref>), consistent with experimental literature suggesting rhythmic and sequential sampling (<xref ref-type="bibr" rid="bib153">VanRullen et al., 2007</xref>; <xref ref-type="bibr" rid="bib85">Landau and Fries, 2012</xref>; for review see <xref ref-type="bibr" rid="bib154">VanRullen, 2013</xref>). If attentional cycles have a limited duration, then there may be insufficient time for activity to build up in the corresponding neuronal populations and support robust encoding into memory if there are too many objects within a scene, producing a capacity limit (see also <xref ref-type="bibr" rid="bib88">Lisman and Idiart, 1995</xref>; <xref ref-type="bibr" rid="bib13">Bays and Husain, 2008</xref>).</p><p>The attentional modulation described above can also act in imagery (within the parietal window), allowing the agent to inspect different parts of an imagined scene (see also <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>). The model proposes that, in the absence of perceptual inputs, perirhinal neurons can be driven in a top-down fashion from hippocampus, thus reinstating an activity pattern in perirhinal cortex similar to the one present at encoding. Only the co-firing of these perirhinal neurons (PRo) and the corresponding BVCs and OVCs provides a unique representation of a given object in a given context, at a given location. The proposed binding of OVCs and PRo neurons, subject to attention, might also provide a functional interpretation of the hippocampus’ role in memory-guided attention (e.g., <xref ref-type="bibr" rid="bib138">Summerfield et al., 2006</xref>).</p></sec><sec id="s4-4"><title>Mental navigation, short-cutting, and planning</title><p>The model suggests a role for grid cell activity in human spatial cognition. Since both self-motion related inputs (via grid cells) and sensory inputs converge onto place cells, grid cells can update the point of view and allow an agent to translate its imagined location. If imagery can inform degraded perception (e.g. in the dark), obstacles can be identified and a suitable path can be planned. Thus, although mental navigation cannot be equated with path integration, we suggest that they reflect a common grid cell-dependent mechanism, which is required when sensory inputs are absent or unreliable. Indeed, humans likely make use of spatial imagery even in apparently non-visual tasks such as triangle completion in darkness (<xref ref-type="bibr" rid="bib143">Tcheang et al., 2011</xref>), and there is evidence for grid-like brain activity during mental navigation (<xref ref-type="bibr" rid="bib14">Bellmund et al., 2016</xref>; Horner et al. 2016).</p><p>The model of mental navigation provides a mechanistic neural-level account of some aspects of ‘scene construction’ and ‘episodic future thinking’ (<xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>; <xref ref-type="bibr" rid="bib64">Hassabis et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">Buckner, 2010</xref>) with regard to familiar spaces. Mental navigation allows an agent to test future behavior, like the approach of a target from a new direction as depicted in <xref ref-type="video" rid="video12">Video 12</xref>. This suggests that the same neural infrastructure involved in scene perception and reconstruction also subserves planning and hypothesis testing (e.g. asking ‘Which way should I go?’ or ‘What would I encounter if I went that way?’). If grid cells (acting on place cells) change the point of view during imagined movement this must be reconciled with the relationships between grid cells and place cells seen during periods of rest or planning (see e.g., <xref ref-type="bibr" rid="bib168">Ólafsdóttir et al., 2016</xref>; <xref ref-type="bibr" rid="bib108">O'Neill et al., 2017</xref>; <xref ref-type="bibr" rid="bib145">Trettel et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Buzsáki and Chrobak, 1995</xref>).</p><p>Grid cells have been proposed to support the computation of vectors to a goal (<xref ref-type="bibr" rid="bib82">Kubie and Fenton, 2012</xref>; <xref ref-type="bibr" rid="bib47">Erdem and Hasselmo, 2012</xref>; <xref ref-type="bibr" rid="bib27">Bush et al., 2015</xref>; <xref ref-type="bibr" rid="bib136">Stemmler et al., 2015</xref>). That is, they can plan trajectories across known and potentially unknown terrain (shortcuts). We proposed that grid cells recruit new hippocampal cells (future place cells) in previously unexplored parts of a familiar environment (<xref ref-type="fig" rid="fig13">Figure 13</xref> and <xref ref-type="video" rid="video13">Video 13</xref>). Planning a trajectory across unexplored space engenders preplay-like activity in place cells (<xref ref-type="bibr" rid="bib39">Dragoi and Tonegawa, 2011</xref>; <xref ref-type="bibr" rid="bib167">Ólafsdóttir et al., 2015</xref>), whereas mental navigation is reminiscent of ‘replay’ (<xref ref-type="bibr" rid="bib162">Wilson and McNaughton, 1994</xref>; <xref ref-type="bibr" rid="bib51">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib36">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="bib78">Karlsson and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib31">Carr et al., 2011</xref>) or ‘forward sweeps (<xref ref-type="bibr" rid="bib76">Johnson and Redish, 2007</xref>; <xref ref-type="bibr" rid="bib112">Pfeiffer and Foster, 2015</xref>), although the faster propagation speed (e.g. during sharp wave ripples) of these sequences of place cell activity are beyond the scope of the present model. Nevertheless, the model suggests that sweeps of activity in the grid cell population may play are role in these aspects of place cell firing, and could correspond to route planning (<xref ref-type="bibr" rid="bib82">Kubie and Fenton, 2012</xref>; <xref ref-type="bibr" rid="bib47">Erdem and Hasselmo, 2012</xref>; <xref ref-type="bibr" rid="bib27">Bush et al., 2015</xref>; <xref ref-type="bibr" rid="bib165">Yamamoto and Tonegawa, 2017</xref>).</p></sec><sec id="s4-5"><title>Conclusions</title><p>It has been argued that the MTL-retrosplenial-parietal system supports the construction of coherent scenes (<xref ref-type="bibr" rid="bib22">Burgess et al., 2001b</xref>; <xref ref-type="bibr" rid="bib30">Byrne et al., 2007</xref>; <xref ref-type="bibr" rid="bib64">Hassabis et al., 2007</xref>; <xref ref-type="bibr" rid="bib125">Schacter et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">Buckner, 2010</xref>). However, if recollection corresponds to the (re-)construction of something akin to a perceptual experience (the defining characteristic of episodic memory; Tulving 1985), then this places strong spatial constraints on how episodic memory works. A vast number of different combinations of information could be retrieved from the body of long-term knowledge in the MTL, but only a small subset would be consistent with a single point of view, making the episodic ‘re-experiencing’ of events or visuo-spatial imagery congruent with perceptual experiences. The BB-model combines this insight with established knowledge and new hypotheses about how location, orientation, and surrounding environmental features are associated and represented by neural population activity.</p><p>This account includes functional roles for the specific firing characteristics of diverse populations of spatially selective cells across multiple brain regions, and distinguishes the egocentric representations supporting conscious (re-)experience from the more abstract (allocentric) representations involved in supporting computations. The resultant systems-level account provides a strong conceptual framework for considering the interplay between structures in the MTL, retrosplenial cortex, Papez circuit’ and parietal cortex in support of spatial memory. It follows Tulving’s theoretical specification of episodic memory, and - spanning Marr’s theoretical, algorithmic and implementational levels (<xref ref-type="bibr" rid="bib90">Marr and Poggio, 1976</xref>) - bridges the gap between a neuropsychological description of spatial cognition (founded on behavioral and functional imaging data) and the neural representations supporting it.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We acknowledge funding by the ERC Advanced grant NEUROMEM, the Wellcome Trust, the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 720270 Human Brain Project SGA1 and grant agreement No. 785907 Human Brain Project SGA2, and the EC Framework Program 7 Future and Emerging Technologies project SpaceCog. We thank all members of the SpaceCog project, and James Bisby, Daniel Bush and Tim Behrens for useful discussions. The authors declare no competing financial interests.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing Editor, eLife</p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Project administration, Writing—review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.33752.030</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-33752-transrepform-v1.docx"/></supplementary-material><sec id="s8" sec-type="data-availability"><title>Data availability</title><p>Matlab code to build all model components and run all simulations will be made available on GitHub in the following repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/bicanski/HBPcollab/tree/master/SpatialEpisodicMemoryModel">https://github.com/bicanski/HBPcollab/tree/master/SpatialEpisodicMemoryModel</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/HBPcollab/tree/master/SpatialEpisodicMemoryModel">https://github.com/elifesciences-publications/HBPcollab/tree/master/SpatialEpisodicMemoryModel</ext-link>.</p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addis</surname> <given-names>DR</given-names></name><name><surname>Wong</surname> <given-names>AT</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Remembering the past and imagining the future: common and distinct neural substrates during event construction and elaboration</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>1363</fpage><lpage>1377</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.10.016</pub-id><pub-id pub-id-type="pmid">17126370</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aggleton</surname> <given-names>JP</given-names></name><name><surname>Brown</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Episodic memory, amnesia, and the hippocampal-anterior thalamic Axis</article-title><source>Behavioral and Brain Sciences</source><volume>22</volume><fpage>425</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1017/S0140525X99002034</pub-id><pub-id pub-id-type="pmid">11301518</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aggleton</surname> <given-names>JP</given-names></name><name><surname>Pralus</surname> <given-names>A</given-names></name><name><surname>Nelson</surname> <given-names>AJ</given-names></name><name><surname>Hornberger</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Thalamic pathology and memory loss in early Alzheimer's disease: moving the focus from the medial temporal lobe to Papez circuit</article-title><source>Brain</source><volume>139</volume><fpage>1877</fpage><lpage>1890</lpage><pub-id pub-id-type="doi">10.1093/brain/aww083</pub-id><pub-id pub-id-type="pmid">27190025</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname> <given-names>AS</given-names></name><name><surname>Nitz</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Retrosplenial cortex maps the conjunction of internal and external spaces</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1143</fpage><lpage>1151</lpage><pub-id pub-id-type="doi">10.1038/nn.4058</pub-id><pub-id pub-id-type="pmid">26147532</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname> <given-names>AS</given-names></name><name><surname>Nitz</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatially periodic activation patterns of retrosplenial cortex encode route Sub-spaces and distance traveled</article-title><source>Current Biology</source><volume>27</volume><fpage>1551</fpage><lpage>1560</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.04.036</pub-id><pub-id pub-id-type="pmid">28528904</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>MI</given-names></name><name><surname>Jeffery</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Heterogeneous modulation of place cell firing by changes in context</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>8827</fpage><lpage>8835</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-26-08827.2003</pub-id><pub-id pub-id-type="pmid">14523083</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atance</surname> <given-names>CM</given-names></name><name><surname>O'Neill</surname> <given-names>DK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Episodic future thinking</article-title><source>Trends in Cognitive Sciences</source><volume>5</volume><fpage>533</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01804-0</pub-id><pub-id pub-id-type="pmid">11728911</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auger</surname> <given-names>SD</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Assessing the mechanism of response in the retrosplenial cortex of good and poor navigators</article-title><source>Cortex</source><volume>49</volume><fpage>2904</fpage><lpage>2913</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2013.08.002</pub-id><pub-id pub-id-type="pmid">24012136</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auger</surname> <given-names>SD</given-names></name><name><surname>Mullally</surname> <given-names>SL</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Retrosplenial cortex codes for permanent landmarks</article-title><source>PLoS ONE</source><volume>7</volume><elocation-id>e43620</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0043620</pub-id><pub-id pub-id-type="pmid">22912894</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barker</surname> <given-names>GR</given-names></name><name><surname>Warburton</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Object-in-place associative recognition memory depends on glutamate receptor neurotransmission within two defined hippocampal-cortical circuits: a critical role for AMPA and NMDA receptors in the Hippocampus, perirhinal, and prefrontal cortices</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>472</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht245</pub-id><pub-id pub-id-type="pmid">24035904</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning in a geometric model of place cell firing</article-title><source>Hippocampus</source><volume>17</volume><fpage>786</fpage><lpage>800</lpage><pub-id pub-id-type="doi">10.1002/hipo.20324</pub-id><pub-id pub-id-type="pmid">17598149</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Lever</surname> <given-names>C</given-names></name><name><surname>Hayman</surname> <given-names>R</given-names></name><name><surname>Hartley</surname> <given-names>T</given-names></name><name><surname>Burton</surname> <given-names>S</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Jeffery</surname> <given-names>K</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The boundary vector cell model of place cell firing and spatial memory</article-title><source>Reviews in the Neurosciences</source><volume>17</volume><fpage>71</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1515/REVNEURO.2006.17.1-2.71</pub-id><pub-id pub-id-type="pmid">16703944</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname> <given-names>PM</given-names></name><name><surname>Husain</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic shifts of limited working memory resources in human vision</article-title><source>Science</source><volume>321</volume><fpage>851</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1126/science.1158023</pub-id><pub-id pub-id-type="pmid">18687968</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellmund</surname> <given-names>JL</given-names></name><name><surname>Deuker</surname> <given-names>L</given-names></name><name><surname>Navarro Schröder</surname> <given-names>T</given-names></name><name><surname>Doeller</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Grid-cell representations in mental simulation</article-title><source>eLife</source><volume>5</volume><elocation-id>e17089</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.17089</pub-id><pub-id pub-id-type="pmid">27572056</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bicanski</surname> <given-names>A</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Environmental anchoring of head direction in a computational model of retrosplenial cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>11601</fpage><lpage>11618</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0516-16.2016</pub-id><pub-id pub-id-type="pmid">27852770</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bird</surname> <given-names>CM</given-names></name><name><surname>Capponi</surname> <given-names>C</given-names></name><name><surname>King</surname> <given-names>JA</given-names></name><name><surname>Doeller</surname> <given-names>CF</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Establishing the boundaries: the hippocampal contribution to imagining scenes</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>11688</fpage><lpage>11695</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0723-10.2010</pub-id><pub-id pub-id-type="pmid">20810889</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bisiach</surname> <given-names>E</given-names></name><name><surname>Luzzatti</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Unilateral neglect of representational space</article-title><source>Cortex</source><volume>14</volume><fpage>129</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1016/S0010-9452(78)80016-1</pub-id><pub-id pub-id-type="pmid">16295118</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bostock</surname> <given-names>E</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Kubie</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Experience-dependent modifications of hippocampal place cell firing</article-title><source>Hippocampus</source><volume>1</volume><fpage>193</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1002/hipo.450010207</pub-id><pub-id pub-id-type="pmid">1669293</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The role of the Hippocampus in prediction and imagination</article-title><source>Annual Review of Psychology</source><volume>61</volume><fpage>27</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.60.110707.163508</pub-id><pub-id pub-id-type="pmid">19958178</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname> <given-names>Y</given-names></name><name><surname>Fiete</surname> <given-names>IR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate path integration in continuous attractor network models of grid cells</article-title><source>PLoS Computational Biology</source><volume>5</volume><elocation-id>e1000291</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000291</pub-id><pub-id pub-id-type="pmid">19229307</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>An oscillatory interference model of grid cell firing</article-title><source>Hippocampus</source><volume>17</volume><fpage>801</fpage><lpage>812</lpage><pub-id pub-id-type="doi">10.1002/hipo.20327</pub-id><pub-id pub-id-type="pmid">17598147</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>Becker</surname> <given-names>S</given-names></name><name><surname>King</surname> <given-names>JA</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001b</year><article-title>Memory for events and their spatial context: models and experiments</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>356</volume><fpage>1493</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1098/rstb.2001.0948</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The human Hippocampus and spatial and episodic memory</article-title><source>Neuron</source><volume>35</volume><fpage>625</fpage><lpage>641</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00830-9</pub-id><pub-id pub-id-type="pmid">12194864</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name><name><surname>Spiers</surname> <given-names>HJ</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001a</year><article-title>A temporoparietal and prefrontal network for retrieving the spatial context of lifelike events</article-title><source>NeuroImage</source><volume>14</volume><fpage>439</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0806</pub-id><pub-id pub-id-type="pmid">11467917</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>Recce</surname> <given-names>M</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>A model of hippocampal function</article-title><source>Neural Networks</source><volume>7</volume><fpage>1065</fpage><lpage>1081</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(05)80159-5</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bush</surname> <given-names>D</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>What do grid cells contribute to place cell firing?</article-title><source>Trends in Neurosciences</source><volume>37</volume><fpage>136</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2013.12.003</pub-id><pub-id pub-id-type="pmid">24485517</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bush</surname> <given-names>D</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Manson</surname> <given-names>D</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Using grid cells for navigation</article-title><source>Neuron</source><volume>87</volume><fpage>507</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.006</pub-id><pub-id pub-id-type="pmid">26247860</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bush</surname> <given-names>D</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A hybrid oscillatory interference/continuous attractor network model of grid cell firing</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>5065</fpage><lpage>5079</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4017-13.2014</pub-id><pub-id pub-id-type="pmid">24695724</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Chrobak</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Temporal structure in spatially organized neuronal ensembles: a role for interneuronal networks</article-title><source>Current Opinion in Neurobiology</source><volume>5</volume><fpage>504</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(95)80012-3</pub-id><pub-id pub-id-type="pmid">7488853</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byrne</surname> <given-names>P</given-names></name><name><surname>Becker</surname> <given-names>S</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Remembering the past and imagining the future: a neural model of spatial memory and imagery</article-title><source>Psychological Review</source><volume>114</volume><fpage>340</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.114.2.340</pub-id><pub-id pub-id-type="pmid">17500630</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname> <given-names>MF</given-names></name><name><surname>Jadhav</surname> <given-names>SP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>147</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1038/nn.2732</pub-id><pub-id pub-id-type="pmid">21270783</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cei</surname> <given-names>A</given-names></name><name><surname>Girardeau</surname> <given-names>G</given-names></name><name><surname>Drieu</surname> <given-names>C</given-names></name><name><surname>Kanbi</surname> <given-names>KE</given-names></name><name><surname>Zugaro</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reversed theta sequences of hippocampal cell assemblies during backward travel</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>719</fpage><lpage>724</lpage><pub-id pub-id-type="doi">10.1038/nn.3698</pub-id><pub-id pub-id-type="pmid">24667574</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Context and relational episodic encoding in humans</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>693</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.10.012</pub-id><pub-id pub-id-type="pmid">17097284</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Delay</surname> <given-names>J</given-names></name><name><surname>Brion</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1969">1969</year><source>Le Syndrome De Korsakoff. Masson</source></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deshmukh</surname> <given-names>SS</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Influence of local objects on hippocampal representations: landmark vectors and memory</article-title><source>Hippocampus</source><volume>23</volume><fpage>253</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1002/hipo.22101</pub-id><pub-id pub-id-type="pmid">23447419</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diba</surname> <given-names>K</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1038/nn1961</pub-id><pub-id pub-id-type="pmid">17828259</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doeller</surname> <given-names>CF</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Evidence for grid cells in a human memory network</article-title><source>Nature</source><volume>463</volume><fpage>657</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1038/nature08704</pub-id><pub-id pub-id-type="pmid">20090680</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Douchamps</surname> <given-names>V</given-names></name><name><surname>Jeewajee</surname> <given-names>A</given-names></name><name><surname>Blundell</surname> <given-names>P</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>Lever</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Evidence for encoding versus retrieval scheduling in the Hippocampus by theta phase and acetylcholine</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>8689</fpage><lpage>8704</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4483-12.2013</pub-id><pub-id pub-id-type="pmid">23678113</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dragoi</surname> <given-names>G</given-names></name><name><surname>Tonegawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Preplay of future place cell sequences by hippocampal cellular assemblies</article-title><source>Nature</source><volume>469</volume><fpage>397</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1038/nature09633</pub-id><pub-id pub-id-type="pmid">21179088</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dugué</surname> <given-names>L</given-names></name><name><surname>McLelland</surname> <given-names>D</given-names></name><name><surname>Lajous</surname> <given-names>M</given-names></name><name><surname>VanRullen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention searches nonuniformly in space and in time</article-title><source>PNAS</source><volume>112</volume><fpage>15214</fpage><lpage>15219</lpage><pub-id pub-id-type="doi">10.1073/pnas.1511331112</pub-id><pub-id pub-id-type="pmid">26598671</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Düzel</surname> <given-names>E</given-names></name><name><surname>Penny</surname> <given-names>WD</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Brain oscillations and memory</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>143</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.01.004</pub-id><pub-id pub-id-type="pmid">20181475</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eacott</surname> <given-names>MJ</given-names></name><name><surname>Gaffan</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The roles of perirhinal cortex, postrhinal cortex, and the fornix in memory for objects, contexts, and events in the rat</article-title><source>The Quarterly Journal of Experimental Psychology Section B</source><volume>58</volume><fpage>202</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1080/02724990444000203</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eacott</surname> <given-names>MJ</given-names></name><name><surname>Norman</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Integrated memory for object, place, and context in rats: a possible model of episodic-like memory?</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>1948</fpage><lpage>1953</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2975-03.2004</pub-id><pub-id pub-id-type="pmid">14985436</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname> <given-names>H</given-names></name><name><surname>Yonelinas</surname> <given-names>AP</given-names></name><name><surname>Ranganath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The medial temporal lobe and recognition memory</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>123</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094328</pub-id><pub-id pub-id-type="pmid">17417939</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekstrom</surname> <given-names>AD</given-names></name><name><surname>Kahana</surname> <given-names>MJ</given-names></name><name><surname>Caplan</surname> <given-names>JB</given-names></name><name><surname>Fields</surname> <given-names>TA</given-names></name><name><surname>Isham</surname> <given-names>EA</given-names></name><name><surname>Newman</surname> <given-names>EL</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cellular networks underlying human spatial navigation</article-title><source>Nature</source><volume>425</volume><fpage>184</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1038/nature01964</pub-id><pub-id pub-id-type="pmid">12968182</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname> <given-names>RA</given-names></name><name><surname>Vass</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural systems for landmark-based wayfinding in humans</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><fpage>20120533</fpage><pub-id pub-id-type="doi">10.1098/rstb.2012.0533</pub-id><pub-id pub-id-type="pmid">24366141</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erdem</surname> <given-names>UM</given-names></name><name><surname>Hasselmo</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A goal-directed spatial navigation model using forward trajectory planning based on grid cells</article-title><source>European Journal of Neuroscience</source><volume>35</volume><fpage>916</fpage><lpage>931</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2012.08015.x</pub-id><pub-id pub-id-type="pmid">22393918</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>T</given-names></name><name><surname>Bicanski</surname> <given-names>A</given-names></name><name><surname>Bush</surname> <given-names>D</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How environment and self-motion combine in neural representations of space</article-title><source>The Journal of Physiology</source><volume>594</volume><fpage>6535</fpage><lpage>6546</lpage><pub-id pub-id-type="doi">10.1113/JP270666</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname> <given-names>PC</given-names></name><name><surname>Shallice</surname> <given-names>T</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name><name><surname>Frackowiak</surname> <given-names>RSJ</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Brain activity during memory retrieval</article-title><source>Brain</source><volume>119</volume><fpage>1587</fpage><lpage>1596</lpage><pub-id pub-id-type="doi">10.1093/brain/119.5.1587</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>Linden</surname> <given-names>DE</given-names></name><name><surname>Di Salle</surname> <given-names>F</given-names></name><name><surname>Trojano</surname> <given-names>L</given-names></name><name><surname>Esposito</surname> <given-names>F</given-names></name><name><surname>Sack</surname> <given-names>AT</given-names></name><name><surname>Grossi</surname> <given-names>D</given-names></name><name><surname>Zanella</surname> <given-names>FE</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Tracking the mind's image in the brain I: time-resolved fMRI during visuospatial mental imagery</article-title><source>Neuron</source><volume>35</volume><fpage>185</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00747-X</pub-id><pub-id pub-id-type="pmid">12123618</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>DJ</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title><source>Nature</source><volume>440</volume><fpage>680</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/nature04587</pub-id><pub-id pub-id-type="pmid">16474382</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhs</surname> <given-names>MC</given-names></name><name><surname>Touretzky</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A spin glass model of path integration in rat medial entorhinal cortex</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>4266</fpage><lpage>4276</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4353-05.2006</pub-id><pub-id pub-id-type="pmid">16624947</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galletti</surname> <given-names>C</given-names></name><name><surname>Battaglini</surname> <given-names>PP</given-names></name><name><surname>Fattori</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Eye position influence on the parieto-occipital area PO (V6) of the macaque monkey</article-title><source>European Journal of Neuroscience</source><volume>7</volume><fpage>2486</fpage><lpage>2501</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.1995.tb01047.x</pub-id><pub-id pub-id-type="pmid">8845954</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girardeau</surname> <given-names>G</given-names></name><name><surname>Benchenane</surname> <given-names>K</given-names></name><name><surname>Wiener</surname> <given-names>SI</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Zugaro</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Selective suppression of hippocampal ripples impairs spatial memory</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1222</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1038/nn.2384</pub-id><pub-id pub-id-type="pmid">19749750</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girardeau</surname> <given-names>G</given-names></name><name><surname>Zugaro</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hippocampal ripples and memory consolidation</article-title><source>Current Opinion in Neurobiology</source><volume>21</volume><fpage>452</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.02.005</pub-id><pub-id pub-id-type="pmid">21371881</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname> <given-names>MA</given-names></name><name><surname>Milner</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Separate visual pathways for perception and action</article-title><source>Trends in Neurosciences</source><volume>15</volume><fpage>20</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(92)90344-8</pub-id><pub-id pub-id-type="pmid">1374953</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname> <given-names>AS</given-names></name><name><surname>van der Meer</surname> <given-names>MA</given-names></name><name><surname>Touretzky</surname> <given-names>DS</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Segmentation of spatial experience by hippocampal θ sequences</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1032</fpage><lpage>1039</lpage><pub-id pub-id-type="doi">10.1038/nn.3138</pub-id><pub-id pub-id-type="pmid">22706269</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname> <given-names>AS</given-names></name><name><surname>van der Meer</surname> <given-names>MA</given-names></name><name><surname>Touretzky</surname> <given-names>DS</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Hippocampal replay is not a simple function of experience</article-title><source>Neuron</source><volume>65</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.034</pub-id><pub-id pub-id-type="pmid">20223204</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guzowski</surname> <given-names>JF</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name><name><surname>Worley</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Environment-specific expression of the immediate-early gene arc in hippocampal neuronal ensembles</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1120</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1038/16046</pub-id><pub-id pub-id-type="pmid">10570490</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname> <given-names>T</given-names></name><name><surname>Fyhn</surname> <given-names>M</given-names></name><name><surname>Molden</surname> <given-names>S</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hargreaves</surname> <given-names>EL</given-names></name><name><surname>Rao</surname> <given-names>G</given-names></name><name><surname>Lee</surname> <given-names>I</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Major dissociation between medial and lateral entorhinal input to dorsal Hippocampus</article-title><source>Science</source><volume>308</volume><fpage>1792</fpage><lpage>1794</lpage><pub-id pub-id-type="doi">10.1126/science.1110449</pub-id><pub-id pub-id-type="pmid">15961670</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartley</surname> <given-names>T</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>Lever</surname> <given-names>C</given-names></name><name><surname>Cacucci</surname> <given-names>F</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Modeling place fields in terms of the cortical inputs to the Hippocampus</article-title><source>Hippocampus</source><volume>10</volume><fpage>369</fpage><lpage>379</lpage><pub-id pub-id-type="doi">10.1002/1098-1063(2000)10:4&lt;369::AID-HIPO3&gt;3.0.CO;2-0</pub-id><pub-id pub-id-type="pmid">10985276</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassabis</surname> <given-names>D</given-names></name><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Using imagination to understand the neural basis of episodic memory</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>14365</fpage><lpage>14374</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4549-07.2007</pub-id><pub-id pub-id-type="pmid">18160644</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname> <given-names>ME</given-names></name><name><surname>Bodelón</surname> <given-names>C</given-names></name><name><surname>Wyble</surname> <given-names>BP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A proposed function for hippocampal theta rhythm: separate phases of encoding and retrieval enhance reversal of prior learning</article-title><source>Neural Computation</source><volume>14</volume><fpage>793</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1162/089976602317318965</pub-id><pub-id pub-id-type="pmid">11936962</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname> <given-names>ME</given-names></name><name><surname>Wyble</surname> <given-names>BP</given-names></name><name><surname>Wallenstein</surname> <given-names>GV</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Encoding and retrieval of episodic memories: role of cholinergic and GABAergic modulation in the Hippocampus</article-title><source>Hippocampus</source><volume>6</volume><fpage>693</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1996)6:6&lt;693::AID-HIPO12&gt;3.0.CO;2-W</pub-id><pub-id pub-id-type="pmid">9034856</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The role of acetylcholine in learning and memory</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>710</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.09.002</pub-id><pub-id pub-id-type="pmid">17011181</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebscher</surname> <given-names>M</given-names></name><name><surname>Levine</surname> <given-names>B</given-names></name><name><surname>Gilboa</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The precuneus and Hippocampus contribute to individual differences in the unfolding of spatial representations during episodic autobiographical memory</article-title><source>Neuropsychologia</source><volume>110</volume><fpage>123</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.03.029</pub-id><pub-id pub-id-type="pmid">28365362</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henson</surname> <given-names>RN</given-names></name><name><surname>Gagnepain</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Predictive, interactive multiple memory systems</article-title><source>Hippocampus</source><volume>20</volume><fpage>1315</fpage><lpage>1326</lpage><pub-id pub-id-type="doi">10.1002/hipo.20857</pub-id><pub-id pub-id-type="pmid">20928831</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinman</surname> <given-names>JR</given-names></name><name><surname>Chapman</surname> <given-names>GW</given-names></name><name><surname>Hasselmo</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Egocentric representation of environmental boundaries in the striatum</article-title><source> Society for Neuroscience</source></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollingworth</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Object-position binding in visual memory for natural scenes and object arrays</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><volume>33</volume><fpage>31</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.33.1.31</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horner</surname> <given-names>AJ</given-names></name><name><surname>Bisby</surname> <given-names>JA</given-names></name><name><surname>Zotow</surname> <given-names>E</given-names></name><name><surname>Bush</surname> <given-names>D</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Grid-like processing of imagined navigation</article-title><source>Current Biology</source><volume>26</volume><fpage>842</fpage><lpage>847</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.01.042</pub-id><pub-id pub-id-type="pmid">26972318</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hoydal</surname> <given-names>OA</given-names></name><name><surname>Skytøen</surname> <given-names>ER</given-names></name><name><surname>Moser</surname> <given-names>M-B</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Object-vector cells in the medial entorhinal cortex, society for neuroscience</article-title><source>Biorxiv</source><pub-id pub-id-type="doi">10.1101/286286</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname> <given-names>L</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Computational modelling of visual attention</article-title><source>Nature Reviews. Neuroscience</source><volume>2</volume><elocation-id>194</elocation-id><lpage>203</lpage><pub-id pub-id-type="doi">10.1038/35058500</pub-id><pub-id pub-id-type="pmid">11256080</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname> <given-names>J</given-names></name><name><surname>Kahana</surname> <given-names>MJ</given-names></name><name><surname>Ekstrom</surname> <given-names>AD</given-names></name><name><surname>Mollison</surname> <given-names>MV</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A sense of direction in human entorhinal cortex</article-title><source>PNAS</source><volume>107</volume><fpage>6487</fpage><lpage>6492</lpage><pub-id pub-id-type="doi">10.1073/pnas.0911213107</pub-id><pub-id pub-id-type="pmid">20308554</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>A</given-names></name><name><surname>Redish</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>12176</fpage><lpage>12189</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3761-07.2007</pub-id><pub-id pub-id-type="pmid">17989284</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>BF</given-names></name><name><surname>Witter</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Cingulate cortex projections to the parahippocampal region and hippocampal formation in the rat</article-title><source>Hippocampus</source><volume>17</volume><fpage>957</fpage><lpage>976</lpage><pub-id pub-id-type="doi">10.1002/hipo.20330</pub-id><pub-id pub-id-type="pmid">17598159</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Awake replay of remote experiences in the Hippocampus</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>913</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1038/nn.2344</pub-id><pub-id pub-id-type="pmid">19525943</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keck</surname> <given-names>T</given-names></name><name><surname>Toyoizumi</surname> <given-names>T</given-names></name><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Doiron</surname> <given-names>B</given-names></name><name><surname>Feldman</surname> <given-names>DE</given-names></name><name><surname>Fox</surname> <given-names>K</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name><name><surname>Haydon</surname> <given-names>PG</given-names></name><name><surname>Hübener</surname> <given-names>M</given-names></name><name><surname>Lee</surname> <given-names>H-K</given-names></name><name><surname>Lisman</surname> <given-names>JE</given-names></name><name><surname>Rose</surname> <given-names>T</given-names></name><name><surname>Sengpiel</surname> <given-names>F</given-names></name><name><surname>Stellwagen</surname> <given-names>D</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name><name><surname>Turrigiano</surname> <given-names>GG</given-names></name><name><surname>van Rossum</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integrating hebbian and homeostatic plasticity: the current state of the field and future research directions</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160158</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0158</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JA</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>Hartley</surname> <given-names>T</given-names></name><name><surname>Vargha-Khadem</surname> <given-names>F</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Human Hippocampus and viewpoint dependence in spatial memory</article-title><source>Hippocampus</source><volume>12</volume><fpage>811</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1002/hipo.10070</pub-id><pub-id pub-id-type="pmid">12542232</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knauff</surname> <given-names>M</given-names></name><name><surname>Kassubek</surname> <given-names>J</given-names></name><name><surname>Mulack</surname> <given-names>T</given-names></name><name><surname>Greenlee</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Cortical activation evoked by visual mental imagery as measured by fMRI</article-title><source>NeuroReport</source><volume>11</volume><fpage>3957</fpage><lpage>3962</lpage><pub-id pub-id-type="doi">10.1097/00001756-200012180-00011</pub-id><pub-id pub-id-type="pmid">11192609</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubie</surname> <given-names>JL</given-names></name><name><surname>Fenton</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Linear look-ahead in conjunctive cells: an entorhinal mechanism for vector-based navigation</article-title><source>Frontiers in Neural Circuits</source><volume>6</volume><elocation-id>20</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2012.00020</pub-id><pub-id pub-id-type="pmid">22557948</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Káli</surname> <given-names>S</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><chapter-title>Hippocampally-dependent consolidation in a hierarchical model of neocortex</chapter-title><source>Advances in Neural Information Processing Systems</source><fpage>24</fpage><lpage>30</lpage></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambrey</surname> <given-names>S</given-names></name><name><surname>Doeller</surname> <given-names>C</given-names></name><name><surname>Berthoz</surname> <given-names>A</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagining being somewhere else: neural basis of changing perspective in space</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>166</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr101</pub-id><pub-id pub-id-type="pmid">21625010</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname> <given-names>AN</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Attention samples stimuli rhythmically</article-title><source>Current Biology</source><volume>22</volume><fpage>1000</fpage><lpage>1004</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.03.054</pub-id><pub-id pub-id-type="pmid">22633805</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langston</surname> <given-names>RF</given-names></name><name><surname>Wood</surname> <given-names>ER</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Associative recognition and the Hippocampus: differential effects of hippocampal lesions on object-place, object-context and object-place-context memory</article-title><source>Hippocampus</source><volume>20</volume><fpage>1139</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1002/hipo.20714</pub-id><pub-id pub-id-type="pmid">19847786</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lever</surname> <given-names>C</given-names></name><name><surname>Burton</surname> <given-names>S</given-names></name><name><surname>Jeewajee</surname> <given-names>A</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Boundary vector cells in the subiculum of the hippocampal formation</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>9771</fpage><lpage>9777</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1319-09.2009</pub-id><pub-id pub-id-type="pmid">19657030</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname> <given-names>JE</given-names></name><name><surname>Idiart</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Storage of 7 +/- 2 short-term memories in oscillatory subcycles</article-title><source>Science</source><volume>267</volume><fpage>1512</fpage><lpage>1515</lpage><pub-id pub-id-type="doi">10.1126/science.7878473</pub-id><pub-id pub-id-type="pmid">7878473</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marchette</surname> <given-names>SA</given-names></name><name><surname>Vass</surname> <given-names>LK</given-names></name><name><surname>Ryan</surname> <given-names>J</given-names></name><name><surname>Epstein</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Anchoring the neural compass: coding of local spatial reference frames in human medial parietal lobe</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1598</fpage><lpage>1606</lpage><pub-id pub-id-type="doi">10.1038/nn.3834</pub-id><pub-id pub-id-type="pmid">25282616</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>From understanding computation to understanding neural circuitry</article-title><ext-link ext-link-type="uri" xlink:href="https://dspace.mit.edu/bitstream/handle/1721.1/5782/AIM-357.pdf?sequence=2">https://dspace.mit.edu/bitstream/handle/1721.1/5782/AIM-357.pdf?sequence=2</ext-link></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maurer</surname> <given-names>AP</given-names></name><name><surname>Lester</surname> <given-names>AW</given-names></name><name><surname>Burke</surname> <given-names>SN</given-names></name><name><surname>Ferng</surname> <given-names>JJ</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Back to the future: preserved hippocampal network activity during reverse ambulation</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>15022</fpage><lpage>15031</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1129-14.2014</pub-id><pub-id pub-id-type="pmid">25378167</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Battaglia</surname> <given-names>FP</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Path integration and the neural basis of the 'cognitive map'</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>663</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1038/nrn1932</pub-id><pub-id pub-id-type="pmid">16858394</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishkin</surname> <given-names>M</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name><name><surname>Macko</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Object vision and spatial vision: two cortical pathways</article-title><source>Trends in Neurosciences</source><volume>6</volume><fpage>414</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(83)90190-X</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mittelstaedt</surname> <given-names>M-L</given-names></name><name><surname>Mittelstaedt</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Homing by path integration in a mammal</article-title><source>Naturwissenschaften</source><volume>67</volume><fpage>566</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1007/BF00450672</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moores</surname> <given-names>E</given-names></name><name><surname>Laiti</surname> <given-names>L</given-names></name><name><surname>Chelazzi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Associative knowledge controls deployment of visual selective attention</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>182</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1038/nn996</pub-id><pub-id pub-id-type="pmid">12514738</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>Kropff</surname> <given-names>E</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Place cells, grid cells, and the brain's spatial representation system</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>69</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.061307.090723</pub-id><pub-id pub-id-type="pmid">18284371</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumby</surname> <given-names>DG</given-names></name><name><surname>Gaskin</surname> <given-names>S</given-names></name><name><surname>Glenn</surname> <given-names>MJ</given-names></name><name><surname>Schramek</surname> <given-names>TE</given-names></name><name><surname>Lehmann</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Hippocampal damage and exploratory preferences in rats: memory for objects, places, and contexts</article-title><source>Learning &amp; Memory</source><volume>9</volume><fpage>49</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1101/lm.41302</pub-id><pub-id pub-id-type="pmid">11992015</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadasdy</surname> <given-names>Z</given-names></name><name><surname>Nguyen</surname> <given-names>TP</given-names></name><name><surname>Török</surname> <given-names>Á</given-names></name><name><surname>Shen</surname> <given-names>JY</given-names></name><name><surname>Briggs</surname> <given-names>DE</given-names></name><name><surname>Modur</surname> <given-names>PN</given-names></name><name><surname>Buchanan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Context-dependent spatially periodic activity in the human entorhinal cortex</article-title><source>PNAS</source><volume>114</volume><fpage>E3516</fpage><lpage>E3525</lpage><pub-id pub-id-type="doi">10.1073/pnas.1701352114</pub-id><pub-id pub-id-type="pmid">28396399</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nitz</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Parietal cortex, navigation, and the construction of arbitrary reference frames for spatial information</article-title><source>Neurobiology of Learning and Memory</source><volume>91</volume><fpage>179</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2008.08.007</pub-id><pub-id pub-id-type="pmid">18804545</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nitz</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Tracking route progression in the posterior parietal cortex</article-title><source>Neuron</source><volume>49</volume><fpage>747</fpage><lpage>756</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.01.037</pub-id><pub-id pub-id-type="pmid">16504949</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nitz</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spaces within spaces: rat parietal cortex neurons register position across three reference frames</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1365</fpage><lpage>1367</lpage><pub-id pub-id-type="doi">10.1038/nn.3213</pub-id><pub-id pub-id-type="pmid">22960933</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname> <given-names>G</given-names></name><name><surname>Eacott</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Impaired object recognition with increasing levels of feature ambiguity in rats with perirhinal cortex lesions</article-title><source>Behavioural Brain Research</source><volume>148</volume><fpage>79</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/S0166-4328(03)00176-1</pub-id><pub-id pub-id-type="pmid">14684250</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Geometric determinants of the place fields of hippocampal neurons</article-title><source>Nature</source><volume>381</volume><fpage>425</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1038/381425a0</pub-id><pub-id pub-id-type="pmid">8632799</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dual phase and rate coding in hippocampal place cells: theoretical significance and relationship to entorhinal grid cells</article-title><source>Hippocampus</source><volume>15</volume><fpage>853</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1002/hipo.20115</pub-id><pub-id pub-id-type="pmid">16145693</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Dostrovsky</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The Hippocampus as a spatial map. preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O'keefe</surname> <given-names>J</given-names></name><name><surname>Nadel</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>The Hippocampus as a Cognitive Map</source><publisher-loc>Oxford</publisher-loc><publisher-name>Clarendon Press</publisher-name></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Place units in the Hippocampus of the freely moving rat</article-title><source>Experimental Neurology</source><volume>51</volume><fpage>78</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(76)90055-8</pub-id><pub-id pub-id-type="pmid">1261644</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Neill</surname> <given-names>J</given-names></name><name><surname>Boccara</surname> <given-names>CN</given-names></name><name><surname>Stella</surname> <given-names>F</given-names></name><name><surname>Schoenenberger</surname> <given-names>P</given-names></name><name><surname>Csicsvari</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Superficial layers of the medial entorhinal cortex replay independently of the hippocampus</article-title><source>Science</source><volume>355</volume><fpage>184</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1126/science.aag2787</pub-id><pub-id pub-id-type="pmid">28082591</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olson</surname> <given-names>IR</given-names></name><name><surname>Page</surname> <given-names>K</given-names></name><name><surname>Moore</surname> <given-names>KS</given-names></name><name><surname>Chatterjee</surname> <given-names>A</given-names></name><name><surname>Verfaellie</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Working memory for conjunctions relies on the medial temporal lobe</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>4596</fpage><lpage>4601</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1923-05.2006</pub-id><pub-id pub-id-type="pmid">16641239</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panoz-Brown</surname> <given-names>D</given-names></name><name><surname>Corbin</surname> <given-names>HE</given-names></name><name><surname>Dalecki</surname> <given-names>SJ</given-names></name><name><surname>Gentry</surname> <given-names>M</given-names></name><name><surname>Brotheridge</surname> <given-names>S</given-names></name><name><surname>Sluka</surname> <given-names>CM</given-names></name><name><surname>Wu</surname> <given-names>JE</given-names></name><name><surname>Crystal</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rats remember items in context using episodic memory</article-title><source>Current Biology</source><volume>26</volume><fpage>2821</fpage><lpage>2826</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.023</pub-id><pub-id pub-id-type="pmid">27693137</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname> <given-names>A</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Mamillary body lesions in monkeys impair Object-in-Place memory: functional unity of the Fornix-Mamillary system</article-title><source>Journal of Cognitive Neuroscience</source><volume>9</volume><fpage>512</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1162/jocn.1997.9.4.512</pub-id><pub-id pub-id-type="pmid">23968214</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname> <given-names>BE</given-names></name><name><surname>Foster</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Place cells. autoassociative dynamics in the generation of sequences of hippocampal place cells</article-title><source>Science</source><volume>349</volume><fpage>180</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1126/science.aaa9633</pub-id><pub-id pub-id-type="pmid">26160946</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piekema</surname> <given-names>C</given-names></name><name><surname>Kessels</surname> <given-names>RP</given-names></name><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Petersson</surname> <given-names>KM</given-names></name><name><surname>Fernández</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The right hippocampus participates in short-term memory maintenance of object-location associations</article-title><source>NeuroImage</source><volume>33</volume><fpage>374</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.06.035</pub-id><pub-id pub-id-type="pmid">16904344</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poucet</surname> <given-names>B</given-names></name><name><surname>Sargolini</surname> <given-names>F</given-names></name><name><surname>Song</surname> <given-names>EY</given-names></name><name><surname>Hangya</surname> <given-names>B</given-names></name><name><surname>Fox</surname> <given-names>S</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Independence of landmark and self-motion-guided navigation: a different role for grid cells</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130370</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0370</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname> <given-names>A</given-names></name><name><surname>Deneve</surname> <given-names>S</given-names></name><name><surname>Duhamel</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A computational perspective on the neural basis of multisensory spatial representations</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>741</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1038/nrn914</pub-id><pub-id pub-id-type="pmid">12209122</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname> <given-names>A</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Spatial transformations in the parietal cortex using basis functions</article-title><source>Journal of Cognitive Neuroscience</source><volume>9</volume><fpage>222</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1162/jocn.1997.9.2.222</pub-id><pub-id pub-id-type="pmid">23962013</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname> <given-names>D</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A category-free neural population supports evolving demands during decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1784</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1038/nn.3865</pub-id><pub-id pub-id-type="pmid">25383902</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raudies</surname> <given-names>F</given-names></name><name><surname>Brandon</surname> <given-names>MP</given-names></name><name><surname>Chapman</surname> <given-names>GW</given-names></name><name><surname>Hasselmo</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Head direction is coded more strongly than movement direction in a population of entorhinal neurons</article-title><source>Brain Research</source><volume>1621</volume><fpage>355</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2014.10.053</pub-id><pub-id pub-id-type="pmid">25451111</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchey</surname> <given-names>M</given-names></name><name><surname>Wing</surname> <given-names>EA</given-names></name><name><surname>LaBar</surname> <given-names>KS</given-names></name><name><surname>Cabeza</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural similarity between encoding and retrieval is related to memory via hippocampal interactions</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>2818</fpage><lpage>2828</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs258</pub-id><pub-id pub-id-type="pmid">22967731</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robertson</surname> <given-names>RG</given-names></name><name><surname>Rolls</surname> <given-names>ET</given-names></name><name><surname>Georges-François</surname> <given-names>P</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Head direction cells in the primate pre-subiculum</article-title><source>Hippocampus</source><volume>9</volume><fpage>206</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1999)9:3&lt;206::AID-HIPO2&gt;3.0.CO;2-H</pub-id><pub-id pub-id-type="pmid">10401637</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sack</surname> <given-names>AT</given-names></name><name><surname>Sperling</surname> <given-names>JM</given-names></name><name><surname>Prvulovic</surname> <given-names>D</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Di Salle</surname> <given-names>F</given-names></name><name><surname>Dierks</surname> <given-names>T</given-names></name><name><surname>Linden</surname> <given-names>DE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Tracking the mind's image in the brain II: transcranial magnetic stimulation reveals parietal asymmetry in visuospatial imagery</article-title><source>Neuron</source><volume>35</volume><fpage>195</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00745-6</pub-id><pub-id pub-id-type="pmid">12123619</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname> <given-names>E</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Transfer of coded information from sensory to motor networks</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>6461</fpage><lpage>6474</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-10-06461.1995</pub-id><pub-id pub-id-type="pmid">7472409</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargolini</surname> <given-names>F</given-names></name><name><surname>Fyhn</surname> <given-names>M</given-names></name><name><surname>Hafting</surname> <given-names>T</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Witter</surname> <given-names>MP</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex</article-title><source>Science</source><volume>312</volume><fpage>758</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1126/science.1125572</pub-id><pub-id pub-id-type="pmid">16675704</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Save</surname> <given-names>E</given-names></name><name><surname>Poucet</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Role of the parietal cortex in long-term representation of spatial information in the rat</article-title><source>Neurobiology of Learning and Memory</source><volume>91</volume><fpage>172</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2008.08.005</pub-id><pub-id pub-id-type="pmid">18782629</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname> <given-names>DL</given-names></name><name><surname>Addis</surname> <given-names>DR</given-names></name><name><surname>Buckner</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Remembering the past to imagine the future: the prospective brain</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>657</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1038/nrn2213</pub-id><pub-id pub-id-type="pmid">17700624</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname> <given-names>DL</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Koutstaal</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The cognitive neuroscience of constructive memory</article-title><source>Annual Review of Psychology</source><volume>49</volume><fpage>289</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.49.1.289</pub-id><pub-id pub-id-type="pmid">9496626</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shine</surname> <given-names>JP</given-names></name><name><surname>Valdés-Herrera</surname> <given-names>JP</given-names></name><name><surname>Hegarty</surname> <given-names>M</given-names></name><name><surname>Wolbers</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The human retrosplenial cortex and thalamus code head direction in a global reference frame</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>6371</fpage><lpage>6381</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1268-15.2016</pub-id><pub-id pub-id-type="pmid">27307227</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shrager</surname> <given-names>Y</given-names></name><name><surname>Bayley</surname> <given-names>PJ</given-names></name><name><surname>Bontempi</surname> <given-names>B</given-names></name><name><surname>Hopkins</surname> <given-names>RO</given-names></name><name><surname>Squire</surname> <given-names>LR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatial memory and the human hippocampus</article-title><source>PNAS</source><volume>104</volume><fpage>2961</fpage><lpage>2966</lpage><pub-id pub-id-type="doi">10.1073/pnas.0611233104</pub-id><pub-id pub-id-type="pmid">17296931</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skaggs</surname> <given-names>WE</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name><name><surname>Kudrimoti</surname> <given-names>HS</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>A model of the neural basis of the rat's sense of direction</article-title><source>Advances in Neural Information Processing Systems</source><volume>7</volume><fpage>173</fpage><lpage>182</lpage><pub-id pub-id-type="pmid">11539168</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname> <given-names>LH</given-names></name><name><surname>Grieve</surname> <given-names>KL</given-names></name><name><surname>Brotchie</surname> <given-names>P</given-names></name><name><surname>Andersen</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Separate body- and world-referenced representations of visual space in parietal cortex</article-title><source>Nature</source><volume>394</volume><fpage>887</fpage><lpage>891</lpage><pub-id pub-id-type="doi">10.1038/29777</pub-id><pub-id pub-id-type="pmid">9732870</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solstad</surname> <given-names>T</given-names></name><name><surname>Boccara</surname> <given-names>CN</given-names></name><name><surname>Kropff</surname> <given-names>E</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representation of geometric borders in the entorhinal cortex</article-title><source>Science</source><volume>322</volume><fpage>1865</fpage><lpage>1868</lpage><pub-id pub-id-type="doi">10.1126/science.1166466</pub-id><pub-id pub-id-type="pmid">19095945</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solstad</surname> <given-names>T</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>Einevoll</surname> <given-names>GT</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>From grid cells to place cells: a mathematical model</article-title><source>Hippocampus</source><volume>16</volume><fpage>1026</fpage><lpage>1031</lpage><pub-id pub-id-type="doi">10.1002/hipo.20244</pub-id><pub-id pub-id-type="pmid">17094145</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname> <given-names>P</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Angular path integration by moving &quot;hill of activity&quot;: a spiking neuron model without recurrent excitation of the head-direction system</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>1002</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4172-04.2005</pub-id><pub-id pub-id-type="pmid">15673682</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squire</surname> <given-names>LR</given-names></name><name><surname>Amaral</surname> <given-names>DG</given-names></name><name><surname>Zola-Morgan</surname> <given-names>S</given-names></name><name><surname>Kritchevsky</surname> <given-names>M</given-names></name><name><surname>Press</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Description of brain injury in the amnesic patient N.A. based on magnetic resonance imaging</article-title><source>Experimental Neurology</source><volume>105</volume><fpage>23</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/0014-4886(89)90168-4</pub-id><pub-id pub-id-type="pmid">2744126</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squire</surname> <given-names>LR</given-names></name><name><surname>Slater</surname> <given-names>PC</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Anterograde and retrograde memory impairment in chronic amnesia</article-title><source>Neuropsychologia</source><volume>16</volume><fpage>313</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(78)90025-8</pub-id><pub-id pub-id-type="pmid">703946</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stemmler</surname> <given-names>M</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Herz</surname> <given-names>AV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Connecting multiple spatial scales to decode the population activity of grid cells</article-title><source>Science Advances</source><volume>1</volume><elocation-id>e1500816</elocation-id><pub-id pub-id-type="doi">10.1126/science.1500816</pub-id><pub-id pub-id-type="pmid">26824061</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stensola</surname> <given-names>H</given-names></name><name><surname>Stensola</surname> <given-names>T</given-names></name><name><surname>Solstad</surname> <given-names>T</given-names></name><name><surname>Frøland</surname> <given-names>K</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The entorhinal grid map is discretized</article-title><source>Nature</source><volume>492</volume><fpage>72</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1038/nature11649</pub-id><pub-id pub-id-type="pmid">23222610</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>JJ</given-names></name><name><surname>Lepsien</surname> <given-names>J</given-names></name><name><surname>Gitelman</surname> <given-names>DR</given-names></name><name><surname>Mesulam</surname> <given-names>MM</given-names></name><name><surname>Nobre</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Orienting attention based on long-term memory experience</article-title><source>Neuron</source><volume>49</volume><fpage>905</fpage><lpage>916</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.01.021</pub-id><pub-id pub-id-type="pmid">16543137</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname> <given-names>JS</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Ranck</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="1990">1990a</year><article-title>Head-direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis</article-title><source>The Journal of Neuroscience</source><volume>10</volume><fpage>420</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-02-00420.1990</pub-id><pub-id pub-id-type="pmid">2303851</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname> <given-names>JS</given-names></name><name><surname>Muller</surname> <given-names>RU</given-names></name><name><surname>Ranck</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="1990">1990b</year><article-title>Head-direction cells recorded from the postsubiculum in freely moving rats. II. Effects of environmental manipulations</article-title><source>The Journal of Neuroscience</source><volume>10</volume><fpage>436</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-02-00436.1990</pub-id><pub-id pub-id-type="pmid">2303852</pub-id></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname> <given-names>JS</given-names></name><name><surname>Ranck</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Head direction cells in the deep layer of dorsal presubiculum in freely moving rats. InSociety of neuroscience abstract</article-title><source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source><volume>10</volume><fpage>420</fpage><lpage>435</lpage></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The head direction signal: origins and sensory-motor integration</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>181</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112854</pub-id><pub-id pub-id-type="pmid">17341158</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tcheang</surname> <given-names>L</given-names></name><name><surname>Bülthoff</surname> <given-names>HH</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual influence on path integration in darkness indicates a multimodal representation of large-scale space</article-title><source>PNAS</source><volume>108</volume><fpage>1152</fpage><lpage>1157</lpage><pub-id pub-id-type="doi">10.1073/pnas.1011843108</pub-id><pub-id pub-id-type="pmid">21199934</pub-id></element-citation></ref><ref id="bib144"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><volume>55</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="bib145"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Trettel</surname> <given-names>SG</given-names></name><name><surname>Trimper</surname> <given-names>JB</given-names></name><name><surname>Hwaun</surname> <given-names>E</given-names></name><name><surname>Fiete</surname> <given-names>IR</given-names></name><name><surname>Colgin</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Grid cell co-activity patterns during sleep reflect spatial overlap of grid fields during active behaviors</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/198671</pub-id></element-citation></ref><ref id="bib146"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname> <given-names>A</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Traces of experience in the lateral entorhinal cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>399</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.01.036</pub-id><pub-id pub-id-type="pmid">23434282</pub-id></element-citation></ref><ref id="bib147"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsivilis</surname> <given-names>D</given-names></name><name><surname>Vann</surname> <given-names>SD</given-names></name><name><surname>Denby</surname> <given-names>C</given-names></name><name><surname>Roberts</surname> <given-names>N</given-names></name><name><surname>Mayes</surname> <given-names>AR</given-names></name><name><surname>Montaldi</surname> <given-names>D</given-names></name><name><surname>Aggleton</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A disproportionate role for the fornix and mammillary bodies in recall versus recognition memory</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>834</fpage><lpage>842</lpage><pub-id pub-id-type="doi">10.1038/nn.2149</pub-id><pub-id pub-id-type="pmid">18552840</pub-id></element-citation></ref><ref id="bib148"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tulving</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1983">1983</year><source>Elements of Episodic Memory</source><publisher-name>Clarenden Press</publisher-name></element-citation></ref><ref id="bib149"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Two cortical visual systems</article-title><source>Analysis of visual behavior</source><fpage>549</fpage><lpage>586</lpage></element-citation></ref><ref id="bib150"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valyear</surname> <given-names>KF</given-names></name><name><surname>Culham</surname> <given-names>JC</given-names></name><name><surname>Sharif</surname> <given-names>N</given-names></name><name><surname>Westwood</surname> <given-names>D</given-names></name><name><surname>Goodale</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A double dissociation between sensitivity to changes in object identity and object orientation in the ventral and dorsal visual streams: a human fMRI study</article-title><source>Neuropsychologia</source><volume>44</volume><fpage>218</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.05.004</pub-id><pub-id pub-id-type="pmid">15955539</pub-id></element-citation></ref><ref id="bib151"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Cauter</surname> <given-names>T</given-names></name><name><surname>Camon</surname> <given-names>J</given-names></name><name><surname>Alvernhe</surname> <given-names>A</given-names></name><name><surname>Elduayen</surname> <given-names>C</given-names></name><name><surname>Sargolini</surname> <given-names>F</given-names></name><name><surname>Save</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distinct roles of medial and lateral entorhinal cortex in spatial cognition</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>451</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs033</pub-id><pub-id pub-id-type="pmid">22357665</pub-id></element-citation></ref><ref id="bib152"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vann</surname> <given-names>SD</given-names></name><name><surname>Aggleton</surname> <given-names>JP</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>What does the retrosplenial cortex do?</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>792</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1038/nrn2733</pub-id><pub-id pub-id-type="pmid">19812579</pub-id></element-citation></ref><ref id="bib153"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname> <given-names>R</given-names></name><name><surname>Carlson</surname> <given-names>T</given-names></name><name><surname>Cavanagh</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The blinking spotlight of attention</article-title><source>PNAS</source><volume>104</volume><fpage>19204</fpage><lpage>19209</lpage><pub-id pub-id-type="doi">10.1073/pnas.0707316104</pub-id></element-citation></ref><ref id="bib154"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual attention: a rhythmic process?</article-title><source>Current Biology</source><volume>23</volume><fpage>R1110</fpage><lpage>R1112</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.11.006</pub-id><pub-id pub-id-type="pmid">24355791</pub-id></element-citation></ref><ref id="bib155"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vedder</surname> <given-names>LC</given-names></name><name><surname>Miller</surname> <given-names>AMP</given-names></name><name><surname>Harrison</surname> <given-names>MB</given-names></name><name><surname>Smith</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Retrosplenial cortical neurons encode navigational cues, trajectories and reward locations during goal directed navigation</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>3713</fpage><lpage>3723</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw192</pub-id><pub-id pub-id-type="pmid">27473323</pub-id></element-citation></ref><ref id="bib156"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallentin</surname> <given-names>M</given-names></name><name><surname>Roepstorff</surname> <given-names>A</given-names></name><name><surname>Glover</surname> <given-names>R</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Parallel memory systems for talking about location and age in precuneus, caudate and Broca's region</article-title><source>NeuroImage</source><volume>32</volume><fpage>1850</fpage><lpage>1864</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.05.002</pub-id><pub-id pub-id-type="pmid">16828565</pub-id></element-citation></ref><ref id="bib157"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warburton</surname> <given-names>EC</given-names></name><name><surname>Brown</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Findings from animals concerning when interactions between perirhinal cortex, hippocampus and medial prefrontal cortex are necessary for recognition memory</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>2262</fpage><lpage>2272</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.12.022</pub-id><pub-id pub-id-type="pmid">20026141</pub-id></element-citation></ref><ref id="bib158"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welday</surname> <given-names>AC</given-names></name><name><surname>Shlifer</surname> <given-names>IG</given-names></name><name><surname>Bloom</surname> <given-names>ML</given-names></name><name><surname>Zhang</surname> <given-names>K</given-names></name><name><surname>Blair</surname> <given-names>HT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cosine directional tuning of theta cell burst frequencies: evidence for spatial coding by oscillatory interference</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>16157</fpage><lpage>16176</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0712-11.2011</pub-id><pub-id pub-id-type="pmid">22072668</pub-id></element-citation></ref><ref id="bib159"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitlock</surname> <given-names>JR</given-names></name><name><surname>Pfuhl</surname> <given-names>G</given-names></name><name><surname>Dagslott</surname> <given-names>N</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Functional split between parietal and entorhinal cortices in the rat</article-title><source>Neuron</source><volume>73</volume><fpage>789</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.028</pub-id><pub-id pub-id-type="pmid">22365551</pub-id></element-citation></ref><ref id="bib160"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilber</surname> <given-names>AA</given-names></name><name><surname>Clark</surname> <given-names>BJ</given-names></name><name><surname>Forster</surname> <given-names>TC</given-names></name><name><surname>Tatsuno</surname> <given-names>M</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Interaction of egocentric and world-centered reference frames in the rat posterior parietal cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>5431</fpage><lpage>5446</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0511-14.2014</pub-id><pub-id pub-id-type="pmid">24741034</pub-id></element-citation></ref><ref id="bib161"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wills</surname> <given-names>TJ</given-names></name><name><surname>Lever</surname> <given-names>C</given-names></name><name><surname>Cacucci</surname> <given-names>F</given-names></name><name><surname>Burgess</surname> <given-names>N</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Attractor dynamics in the hippocampal representation of the local environment</article-title><source>Science</source><volume>308</volume><fpage>873</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1126/science.1108905</pub-id><pub-id pub-id-type="pmid">15879220</pub-id></element-citation></ref><ref id="bib162"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>MA</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Reactivation of hippocampal ensemble memories during sleep</article-title><source>Science</source><volume>265</volume><fpage>676</fpage><lpage>679</lpage><pub-id pub-id-type="doi">10.1126/science.8036517</pub-id><pub-id pub-id-type="pmid">8036517</pub-id></element-citation></ref><ref id="bib163"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winters</surname> <given-names>BD</given-names></name><name><surname>Forwood</surname> <given-names>SE</given-names></name><name><surname>Cowell</surname> <given-names>RA</given-names></name><name><surname>Saksida</surname> <given-names>LM</given-names></name><name><surname>Bussey</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Double dissociation between the effects of peri-postrhinal cortex and hippocampal lesions on tests of object recognition and spatial memory: heterogeneity of function within the temporal lobe</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>5901</fpage><lpage>5908</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1346-04.2004</pub-id><pub-id pub-id-type="pmid">15229237</pub-id></element-citation></ref><ref id="bib164"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyss</surname> <given-names>JM</given-names></name><name><surname>Van Groen</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Connections between the retrosplenial cortex and the hippocampal formation in the rat: a review</article-title><source>Hippocampus</source><volume>2</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1002/hipo.450020102</pub-id><pub-id pub-id-type="pmid">1308170</pub-id></element-citation></ref><ref id="bib165"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamamoto</surname> <given-names>J</given-names></name><name><surname>Tonegawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Direct Medial Entorhinal Cortex Input to Hippocampal CA1 Is Crucial for Extended Quiet Awake Replay</article-title><source>Neuron</source><volume>96</volume><fpage>217</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.017</pub-id><pub-id pub-id-type="pmid">28957670</pub-id></element-citation></ref><ref id="bib166"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>2112</fpage><lpage>2126</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-06-02112.1996</pub-id><pub-id pub-id-type="pmid">8604055</pub-id></element-citation></ref><ref id="bib167"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname> <given-names>HF</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Saleem</surname> <given-names>AB</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name><name><surname>Spiers</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal place cells construct reward related sequences through unexplored space</article-title><source>eLife</source><volume>4</volume><elocation-id>e06063</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06063</pub-id><pub-id pub-id-type="pmid">26112828</pub-id></element-citation></ref><ref id="bib168"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname> <given-names>HF</given-names></name><name><surname>Carpenter</surname> <given-names>F</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Coordinated grid and place cell replay during rest</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>792</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1038/nn.4291</pub-id><pub-id pub-id-type="pmid">27089021</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1 </title><sec id="s7" sec-type="appendix"><title>BB-Model Details</title><boxed-text><object-id pub-id-type="doi">10.7554/eLife.33752.031</object-id><sec id="s7-1"><title>Neuron model</title><p>All neuron populations in the BB-model, with the exception of for grid cells, are composed of rate-coded neurons and implemented according to the following equations.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Where <bold><italic>x</italic></bold> is the vector of activations (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, vectors/matrices displayed in bold) for all neurons belonging to the population marked by the subscript <italic>i</italic> (e.g. PCs, BVC, etc.). Within a population all neurons are identical. The superscript indicates the temporal dimension, with t + 1 referring to the updated state variable for the next time step (step size <italic>dt</italic>). <italic>τ</italic> is the decay time-constant of the rate equation. The sigmoid with parameters <italic>α</italic>, <italic>β</italic> (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) serves as a non-linearity to map activations onto firing rates. The term <bold><italic>k<sub>i</sub></italic></bold> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> contains all population specific inputs. <xref ref-type="disp-formula" rid="equ3">Equations 3</xref> through 13 summarize the inputs to the model populations.<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here (and below) <bold><italic>W<sub>i,j</sub></italic></bold> is the matrix of connection weights from population <italic>i</italic> to <italic>j</italic>, <italic>φ<sub>i,j</sub></italic> is a gain factor, and <bold><italic>r<sub>j</sub></italic></bold> refers to the vector of firing rates of population <italic>j. I<sub>FB</sub></italic> is a feedback current ensuring a set total of activity in the place cell sheet (numerical value 15). <italic>I<sub>mod</sub></italic> and <italic>P<sub>mod</sub></italic> refer to neuromodulation for bottom-up vs top-down modes of operation, and the mode is set determined externally. that is setting these values according to behavioural needs of the agent (perception vs imagery/recollection) implements the switch between bottom-up and top-down modes. <italic>P<sub>mod</sub></italic> is one in bottom-up mode of operation and 0.05 in top-down mode. <italic>I<sub>mod</sub></italic> is 0.05 in bottom-up mode of operation and one in top-down mode. Abbreviations: PC; place cells, BVC; boundary vector cells, OVC; object vector cells; PRb; boundary selective perirhinal neurons, PRo; object selective perirhinal neurons, PW; parietal window neurons, TR; transformation circuit neurons, HDC; head direction cells, GC; grid cells.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>B is the ‘bleed’ parameter for a smooth modulation of bottom-up vs top-down connectivity, during perception (simulations 2.1, 2.2). Sums over transformation sublayers run from 1 to 20, the number of distinct sublayers (see description of transformation circuit in main text and below).<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><italic>I<sub>cue</sub></italic> is an externally supplied (i.e. not causally determined by other model components) trigger current to initiate recall in imagery.</p><p><italic>I<sub>PRo</sub></italic> and <italic>I<sub>PRb</sub></italic> are externally supplied inputs to perirhinal identity neurons that represent the result of a recognition process along the ventral visual stream which is not explicitly modelled, and both inputs are only present in bottom-mode (i.e. during perception). <italic>I<sub>PRo</sub></italic> is binary (object attended and present vs not attended/not present), while the magnitude of <italic>I<sub>PRb</sub></italic> depends linearly on the extent of the boundary that is visible and its distance to the agent.<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>∑</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>∑</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>B</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><italic>PWb/o<sub>bath</sub></italic> is an inhibitory input based on the total activity in the PWb/o population (sum of the population vector in <xref ref-type="disp-formula" rid="equ8 equ9">Equations 8 and 9)</xref>. I<sub>PWb/o</sub><sup>agent</sup> refers to the sensory/perceptual inputs to the PWb/o populations. That is, these input currents are generated in response to the presence of boundaries/objects in the field of view in order to be injected into the corresponding populations.<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo> <mml:mi/><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub> <mml:mi/><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula></p><p> connects onto the different sublayers of the transformation circuit (small hexagon in <xref ref-type="fig" rid="fig4">Figure 4</xref>), ensuring suppression of activity in all sublayers except where the positive modulatory input from HDCs ensures that inhibition is overcome.<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>∑</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mtext> </mml:mtext><mml:mi>ϵ</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mn>20</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>∑</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext> </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mtext> </mml:mtext><mml:mi>ϵ</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mn>20</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The superscript <italic>i</italic> in <xref ref-type="disp-formula" rid="equ11 equ12">Equations 11 and 12</xref> refers to the individual sublayers of the retrosplenial transformation circuit (<italic>i</italic> ranging from 1 to 20). For convenience and in order to visualize object (item) and boundary (contextual) related representations separately the transformation is applied separately to the PWb/o representations but the same connectivity is used. TRb/o<sub>bath</sub> are analogous to <italic>PWb/o<sub>bath</sub></italic>.<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mi>w</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>w</mml:mi><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><italic>cw</italic> and <italic>ccw</italic> in <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> are 0 or 1 depending on whether on the agent is performing a clockwise or counterclockwise turn, respectively. The scaling factor <italic>φ<sub>rot</sub></italic> is set to ensure a match between the agent’s rotations speed and the translation of the activity packet in the head direction ring attractor.</p><p>The firing rate dynamics of GCs are not modelled. GCs exist as firing rate maps which span the environment. GC rates are sampled from these rate maps by looking up the pixel value closest to the agent’s location. See section Grid cell rate maps, mental navigation, and preplay setup for the generation of the grid maps.</p><p>See <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> for population sizes.</p><table-wrap id="app1table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.33752.032</object-id><label>Appendix 1—table 1.</label><caption><title>Model Parameters.</title><p>Top to bottom: <italic>α</italic>, <italic>β</italic> sigmoid parameters; <italic>φ</italic> connection gains; Φ constants subtracted from given weight matrices (e.g. PC to PC connections) to yield global inhibition; bath parameters; range thresholds for object encoding; <italic>l</italic> learning rates for simulation 5; <italic>S</italic> sparseness of connections for reservoir PCs; <italic>σ<sub>ρ</sub>, σ<sub>ϑ</sub></italic> spatial dispersion of the rate function for BVCs. The additive constant (<italic>σ<sub>ρ</sub></italic> = (<italic>r</italic> + 8) * <italic>σ<sub>0</sub></italic>) corresponds to half the range of BVC grid and prevents <italic>σ<sub>ρ</sub></italic> from converging to zero close to the agent. <italic>N<sub>i</sub></italic> population sizes. Products of numbers reflect geometric and functional aspects. E.g. receptive fields of PCs tile 2 × 2 m arena with 44 × 44 cells. Polar grids are given by 16 radial distance units (see A.2) and 51 angular distance units. For the transformation circuit this number is multiplied by the number of transformation sublayers, that is 20.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">α</th><th valign="top">5</th></tr></thead><tbody><tr><td valign="top">β</td><td valign="top">0.1</td></tr><tr><td valign="top">α<sub>IP</sub></td><td valign="top">50</td></tr><tr><td valign="top">β<sub>IP</sub></td><td valign="top">0.1</td></tr><tr><td valign="top">φ<sub>PWb-TR</sub></td><td valign="top">50</td></tr><tr><td valign="top">φ<sub>TR-PWb</sub></td><td valign="top">35</td></tr><tr><td valign="top">φ<sub>TR-BVC</sub></td><td valign="top">30</td></tr><tr><td valign="top">φ<sub>BVC-TR</sub></td><td valign="top">45</td></tr><tr><td valign="top">φ<sub>HD-HD</sub></td><td valign="top">15</td></tr><tr><td valign="top">φ<sub>HD-IP</sub></td><td valign="top">10</td></tr><tr><td valign="top">φ<sub>HD-TR</sub></td><td valign="top">15</td></tr><tr><td valign="top">φ<sub>HDrot</sub></td><td valign="top">2</td></tr><tr><td valign="top">φ<sub>IP-TR</sub></td><td valign="top">90</td></tr><tr><td valign="top">φ<sub>PC-PC</sub></td><td valign="top">25</td></tr><tr><td valign="top">φ<sub>PC-BVC</sub></td><td valign="top">1100</td></tr><tr><td valign="top">φ<sub>PC-PRb</sub></td><td valign="top">6000</td></tr><tr><td valign="top">φ<sub>BVC-PC</sub></td><td valign="top">440</td></tr><tr><td valign="top">φ<sub>BVC-PRb</sub></td><td valign="top">75</td></tr><tr><td valign="top">φ<sub>PRb-PC</sub></td><td valign="top">25</td></tr><tr><td valign="top">φ<sub>PRb-BVC</sub></td><td valign="top">1</td></tr><tr><td valign="top">φ<sub>GC-PC</sub></td><td valign="top">3</td></tr><tr><td valign="top">φ<sub>PWo-TR</sub></td><td valign="top">60</td></tr><tr><td valign="top">φ<sub>TR-PWo</sub></td><td valign="top">30</td></tr><tr><td valign="top">φ<sub>TR-OVC</sub></td><td valign="top">60</td></tr><tr><td valign="top">φ<sub>OVC-TR</sub></td><td valign="top">30</td></tr><tr><td valign="top">φ<sub>PC-OVC</sub></td><td valign="top">1.7</td></tr><tr><td valign="top">φ<sub>PRo-OVC</sub></td><td valign="top">6</td></tr><tr><td valign="top">φ<sub>PC-PRo</sub></td><td valign="top">1</td></tr><tr><td valign="top">φ<sub>OVC-PC</sub></td><td valign="top">5</td></tr><tr><td valign="top">φ<sub>OVC-oPR</sub></td><td valign="top">5</td></tr><tr><td valign="top">φ<sub>PRo-PC</sub></td><td valign="top">100</td></tr><tr><td valign="top">φ<sub>PRo-PRo</sub></td><td valign="top">115</td></tr><tr><td valign="top">φ<sub>inh-PC</sub></td><td valign="top">0.4</td></tr><tr><td valign="top">φ<sub>inh-BVC</sub></td><td valign="top">0.2</td></tr><tr><td valign="top">φ<sub>inh-PRb</sub></td><td valign="top">9</td></tr><tr><td valign="top">φ<sub>inh-PRo</sub></td><td valign="top">1</td></tr><tr><td valign="top">φ<sub>inh-HD</sub></td><td valign="top">0.4</td></tr><tr><td valign="top">φ<sub>inh-TR</sub></td><td valign="top">0.075</td></tr><tr><td valign="top">φ<sub>inh-TRo</sub></td><td valign="top">0.1</td></tr><tr><td valign="top">φ<sub>inh-PW</sub></td><td valign="top">0.1</td></tr><tr><td valign="top">φ<sub>inh-OVC</sub></td><td valign="top">0.5</td></tr><tr><td valign="top">φ<sub>inh-PWo</sub></td><td valign="top">1</td></tr><tr><td valign="top">Φ<sub>PC-PC</sub></td><td valign="top">0.4</td></tr><tr><td valign="top">Φ<sub>BVC-BVC</sub></td><td valign="top">0.2</td></tr><tr><td valign="top">Φ<sub>PR-PR</sub></td><td valign="top">9</td></tr><tr><td valign="top">Φ<sub>HD-HD</sub></td><td valign="top">0.4</td></tr><tr><td valign="top">Φ<sub>OVC-OVC</sub></td><td valign="top">0.5</td></tr><tr><td valign="top">Φ<sub>PRo-PRo</sub></td><td valign="top">01</td></tr><tr><td valign="top">PW<sub>bath</sub></td><td valign="top">0.1</td></tr><tr><td valign="top">PW<sub>bath</sub></td><td valign="top">0.2</td></tr><tr><td valign="top">TR<sub>bath</sub></td><td valign="top">0.088</td></tr><tr><td valign="top">Object enc. threshold</td><td valign="top">18 cm</td></tr><tr><td valign="top">Object enc. Threshold (3.1)</td><td valign="top">36 cm</td></tr><tr><td valign="top"><italic>l<sub>GC-resPC</sub></italic></td><td valign="top">0.65*10^−5</td></tr><tr><td valign="top"><italic>l<sub>resPC-BVC</sub></italic></td><td valign="top">0.65*10^−5</td></tr><tr><td valign="top"><italic>l<sub>BVC-resPC</sub></italic></td><td valign="top">0.65*10^−5</td></tr><tr><td valign="top"><italic>S<sub>GC-resPC</sub></italic></td><td valign="top">3%</td></tr><tr><td valign="top"><italic>S<sub>resPC-resPC</sub></italic></td><td valign="top">6%</td></tr><tr><td valign="top"><italic>σ<sub>ρ</sub></italic></td><td valign="top">(r + 8) * <italic>σ<sub>0</sub></italic></td></tr><tr><td valign="top"><italic>σ<sub>0</sub></italic></td><td valign="top">0.08</td></tr><tr><td valign="top"><italic>σ<sub>ϑ</sub></italic></td><td valign="top">0.2236</td></tr><tr><td valign="top"><italic>N<sub>PC</sub></italic></td><td valign="top">44 × 44</td></tr><tr><td valign="top"><italic>N<sub>BVC</sub></italic></td><td valign="top">16 × 51</td></tr><tr><td valign="top"><italic>N<sub>TRb/o</sub></italic></td><td valign="top">20 × 16×51</td></tr><tr><td valign="top"><italic>N<sub>OVC</sub></italic></td><td valign="top">16 × 51</td></tr><tr><td valign="top"><italic>N<sub>PRb/o</sub></italic></td><td valign="top">Dependent on simulation environment</td></tr><tr><td valign="top"><italic>N<sub>PWb/o</sub></italic></td><td valign="top">16 × 51</td></tr><tr><td valign="top"><italic>N<sub>IP</sub></italic></td><td valign="top">1</td></tr><tr><td valign="top"><italic>N<sub>HD</sub></italic></td><td valign="top">100</td></tr><tr><td valign="top"><italic>N<sub>GC</sub></italic></td><td valign="top">100 per module</td></tr><tr><td valign="top"><italic>N<sub>reservoir</sub></italic></td><td valign="top">437</td></tr></tbody></table></table-wrap></sec><sec id="s7-2"><title>Receptive fields of place cells and boundary vector cells</title><p>In the training phase for the contextual representation (see section Connection Profiles) BVC and PWb neurons have activation functions of the following type. If a boundary segment is located at the coordinates (ρ<italic>,ϑ)</italic>, then the activity of each boundary selective cell is proportional to the distance of its receptive field from that boundary segment. If <italic>(ρ<sub>i</sub>, ϑ<sub>i</sub>)</italic> are the polar coordinates of the receptive field of the i-th BVC or PWb neuron, then the firing rate <italic>r</italic> is calculated according to the following equation:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>V</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></disp-formula>where <italic>σ<sub>ϑ</sub></italic> and <italic>σ<sub>ρ</sub></italic> define the spatial dispersion of the rate function <italic>r</italic>. The radial dispersion increases with distance (i.e. <italic>σ<sub>ρ</sub></italic> is a function of the radius; see e.g. Barry and Burgess 2007).</p><p>The radial separation of distance bins (see <xref ref-type="fig" rid="fig2">Figure 2A2</xref>) increases linearly from 0.21 to 1.71 along the radius of length 16 distance units (corresponding to approx. 145 cm for the 2 × 2 m environment). Internal to the model a distance unit is given by 2/<italic>N<sub>PC</sub></italic> (see place cell resolution below). The same function is used to calculate the perceptual input to the parietal window due to objects and boundaries during simulation and to calculate activations of parietal window neurons and retrosplenial cells during the setup of the transformation circuit (see below). The receptive fields of BVCs, OVCs, PWb, PWo neurons and retrosplenial cells tile the space in polar coordinates with a radial resolution of 1 receptive fields per arbitrary distance unit (range: 0–16, see above) and an angular resolution of 51 receptive fields over 2π radians.</p><p>Similarly, to set up the PC weights in the training phase PC rates are calculated via the following equation:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math></disp-formula>where (<italic>x,y</italic>) is the location of the agent and (<italic>x<sub>i</sub>,y<sub>i</sub></italic>) the location of the receptive field of the PC in question. The firing fields of PCs tile the environment in a Cartesian grid with resolution 0.5 (i.e. two PCs per arbitrary distance unit). However, note that during simulations PCs are never driven by this activation function. Only BVCs, PR neurons and GCs drive PCs during simulation, unlike PWb/o neurons which must receive sensory/perceptual inputs in bottom-up mode.</p></sec><sec id="s7-3"><title>Connection profiles</title><p>The encoding procedure (section Bottom-up vs top-down modes of operation) describes how object related connections are learned. The contextual representation of BVC, PC and PRb neurons, as well as the connections to and from the transformation circuit are set up in a training phase prior to running any simulations. To set up the transformation circuit randomly oriented boundary segments are chosen (20.000 times per transformation sublayer for a total of 400.000 instances), and the corresponding firing rates (calculated according to <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>) for PWb neurons and the transformation circuit sublayers are instantiated. For each transformation circuit sublayer the randomly generated activity pattern is rotated by a different angle (rotation angle chosen from 20 evenly spaced head directions). Connection weights are then calculated as outer products of the population vectors, yielding a matrix of Hebbian-like associations between the populations. The connections from the retrosplenial transformation circuit to BVCs are one-to-one connections between BVCs and the cells in each of the 20 transformation sublayers (i.e. the connections are given by the identity matrix) since this connection only needs to convey the outcome of the gain modulation across the RSC sublayers. That is, rotations of activity patterns occur on the connection to and from the parietal window. <xref ref-type="video" rid="video1">Video 1</xref> shows all sublayers of the transformation circuit, subject to gain modulation from HDCs as a simulated agent navigates a simple environment, see also <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. The entire transformation of egocentric boundary inputs to BVCs effectively constitutes a model of BVC generation from sensory inputs. Finally, connections between HDCs and the 20 transformation circuit sublayers are calculated algorithmically, by associating each sublayer with one of 20 evenly spaced HD activity bumps on the head direction ring.</p><p>With a functioning transformation circuit, and after specifying the location and extent of extended boundaries in the environment, the agent is placed at a random location and orientation in the environment and the activations of BVCs and PCs are calculated via <xref ref-type="disp-formula" rid="equ14 equ15">equations 14 and 15</xref>. PRb activations are instantiated based on the identity of the visible landmark segments. Connection weights between these three populations (supporting the contextual representation) are again calculated as outer products of the corresponding population vectors, yielding matrices of Hebbian-like associations between the populations.</p><p>Weights are normalized such that the sum total of weights converging on a given target neuron is 1, which is assumed to be the result of some homeostatic process, a widely agreed upon feature of synaptic plasticity (<xref ref-type="bibr" rid="bib79">Keck et al., 2017</xref>). Weights are scaled by scalar gain factors <italic>φ<sub>i</sub></italic> (see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>) to produce appropriate responses in targets of afferent connections.</p></sec><sec id="s7-4"><title>Grid cell rate maps, mental navigation, and preplay setup</title><p>Grid cells are implemented as firing rate maps. Each map consists of a matrix of the same dimensions as the PC sheet (44 × 44 pixels) and is computed as 60 degrees offset, superimposed cosine waves using the following set of equations.<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>Here <italic>b<sub>0</sub></italic>, <italic>b<sub>1</sub></italic> and <italic>b<sub>2</sub></italic> are the normal vectors for the cosine waves. <italic>Rj</italic> is the standard 2D rotation matrix where the index j ranges from 1 to 7 and refers to the rotation angle of the matrix (7 random orientations for 7 grid modules, here 0, π/3, π/4, π/2, π/6, 1.2π, 1.7π). <italic>F</italic> is the frequency of the grids, starting at 0.0028*2π. The scales of successive grids are related by the scaling factor <inline-formula><mml:math id="inf1"><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:math></inline-formula> (Stensola et al. 2012). For each grid scale offsets are sampled uniformly along the principle axes of two adjacent equilateral triangles on the grid (i.e. the rhomboid made of 4 grid vertices).</p><p>Motion through GC maps (i.e. a GC sweep) during mental navigation and preplay is implemented by sampling the GC rate along the imagined trajectory superimposed on the GC rate map. The firing rate value (i.e. the pixel of the rate map) is determined by rounding the x and y values of the imagined trajectory to the nearest integer value. This sampling is equivalent to a shift of a hexagonal pattern of activation on a 2D sheet of entorhinal cells, as suggested in mechanistic models of grid cells (<xref ref-type="bibr" rid="bib20">Burak and Fiete, 2009</xref>).</p><p>For simulation 5.0 (planning; <xref ref-type="video" rid="video13">Video 13</xref> and <xref ref-type="fig" rid="fig13">Figure 13</xref>) the reservoir place cells are supplied with random afferent connections from grid cells (sparseness 3%), and are also randomly interconnected amongst themselves (sparseness 6%). Place cells representing the familiar context and reservoir PCs inhibit each other (inhibitory connections 50% stronger than the default inhibition among place cells representing the context). Weights among reservoir place cells are normalized to the mean of the total amount of positive weights converging onto a typical place cell representing the familiar context. Grid cell weights to reservoir place cells are similarly normalized (80% stronger than default). These additions suffice to produce random, preplay-like activity in reservoir place cells as soon as the central peak of the grid cell ensemble begins to drive the reservoir. The inhibitory connections to and from the context network assure that either the reservoir place cells or the context network wins out. No changes to the adaptive feedback current are necessary (<italic>I<sub>FB</sub></italic> in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). Finally, during preplay connections from BVCs and perirhinal neurons to place cells are turned off to avoid interference which can arise due to the very simple layout of the environment (many boundary configurations experienced by the agent are similar). No other changes to the default model are necessary. To visualize the spatio-temporal sequence of the firing of reservoir place cells during the three phases of simulation 5.0 (planning, perception, recall; see main text) the firing of reservoir cells is recorded along the imagined or real trajectory, and stacked (rightmost panels in <xref ref-type="fig" rid="fig13">Figure 13</xref>) to yield figures akin to typical preplay/replay experiments. The firing rates are normalized and thresholded at 10% of the maximum firing rate for clarity. That is, cells that do not fire, or fire at very low rates are not shown. Due to learning during the actual traversal of the novel part of the environment (phase 2, perception) some cells can increase their firing rate above the threshold. As a consequence the number of cells which is plotted in the stacked rate maps grows marginally between phase 1 (preplay) and phase 2. However, ordering PCs in phases 2 and 3 according to the sequence derived from the preplay is done before thresholding. Hence the correct order derived from phase 1 (preplay) is applied to the cells recorded in phases 2 and 3.</p></sec><sec id="s7-5"><title>Agent and attention models</title><p>To ensure an unambiguous representation of an object at a given location (see main text) we implement a heuristic model of directed attention. A fixed length for an attentional cycle (600 ms) is allocated and divided by the number of visible objects, yielding a time per object <italic>t<sub>O</sub></italic>. The PWo population is then driven for <italic>t<sub>O</sub></italic> ms with the cueing current <inline-formula><mml:math id="inf2"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>) for each visible object in sequence.</p><p>The agent moves in straight lines within the environment, following a path defined by a list of coordinates. Upon reaching a target the rotation towards the next subgoal is performed, followed by the next segment of translation. The rotational velocity is implicitly given by a fixed offset of the translation weights for the HD ring attractor (approximately 18 degrees; see e.g. <xref ref-type="bibr" rid="bib166">Zhang, 1996</xref>; <xref ref-type="bibr" rid="bib133">Song and Wang, 2005</xref>; <xref ref-type="bibr" rid="bib15">Bicanski and Burgess, 2016</xref> for more sophisticated methods of integrating rotational velocity). Translational velocity is fixed at 25 cm per second.</p><p>The agent model is agnostic about the size of the arena and nature of the agent. It can be viewed as rodent like agent or alternatively a human-like agent. The environment is covered by 44 × 44 PCs. that is 1/44 of the length/width of the environment corresponds to one distance unit. Assuming a timestep of e.g. 1ms and an arena size of approximately 2 × 2 m<sup>2</sup> for a rodent-like agent yields a translation speed of approximately 10 cm/s. Assuming a human-like agent in an environment of approximately 10 × 10 m<sup>2</sup> yields a translation speed of approximately 56 cm/s, corresponding to a slow paced walk for a human subject. In either case the speed is orders of magnitude below the time scale of neural rate dynamics.</p></sec></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.33752.035</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Colgin</surname><given-names>Laura</given-names></name><role>Reviewing Editor</role><aff><institution>The University of Texas at Austin, Center for Learning and Memory</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;A Model of Spatial Memory and Imagery – From Single Neurons to Cognition&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This paper describes a model that proposes a framework to explain how multiple, highly complex regions interact to produce egocentric and allocentric representations of space. Although all reviewers found the model to be interesting and novel, they also agreed that the Results, Materials and methods, and rationale for the model were often not presented in a clear and concise manner that would be optimal for a broad readership. Reviewers also felt that the simulation movies were insufficient to demonstrate the importance of the model and request a number of essential revisions, including incorporation of clear performance goals for the model, together with appropriate quantitative measures.</p><p>Essential revisions:</p><p>1) A primary shortcoming is that the model is not used to simulate any specific behavioral tasks. Instead, after describing the architecture, the paper presents a series of several simulations (depicted by figures and videos) to demonstrate examples of neural population activity (over time periods lasting tens of seconds) during a few selected types of navigational behavior. These example simulations do not provide adequate support for strong claims made in the Discussion section, where it is argued that the model accounts for a wide range of findings including position specificity in visual object memory (Hollingworth, 2007), impaired episodic memory resulting from Papez circuit lesions (Delay and Brion 1969), neural activity seen during imagery for scenes in the MTL, retrosplenial cortex and precuneus (Burgess, Maguire et al., 2001; Hassabis et al. 2007; Schacter et al. 2007), and 'some aspects' of 'scene construction' and 'episodic future thinking' (Schacter et al., 2007; Hassabis et al., 2007; Buckner 2010). To support such claims, the model should more faithfully reproduce experimental designs from these prior studies (as is, there does not seem to be any quantitative metric by which these broad claims can be evaluated). It is also implied that the model accounts for &quot;trace cells&quot; that fire when the agent visits the prior location of a missing object (Tsao et al., 2015), and although trace-like activity is shown in some example simulations, it is not specifically stated which neural population in the model would correspond to trace cells, nor are simulated trace cell firing rate maps presented. It is additionally claimed that the model accounts for preplay and replay activity of place cells, but it appears that simulated preplay and replay events do not occur on a compressed time scale in the model as they do in real rodents, which is not addressed. In other words, the heavy reliance on qualitative (rather than quantitative) performance assessments make it difficult to offer anything more than a subjective evaluation of the model's capabilities. A more objective evaluation might be possible if the authors run more simulations to quantitatively compare simulated vs. real neural activity (or simulated vs. real task performance), explore how the model's performance degrades under realistic noise or uncertainty conditions, etc.</p><p>2) Several key mechanisms – such as sequential shifting of attentional focus (which is essential for solving the place-object binding problem), neuromodulation (which allows the model to transition between sensory processing and mental imagery modes), and population activity in the grid cell map (which is essential for generating preplay/replay trajectories) – are not explicitly simulated by the model. Rather, these signals are provided &quot;for free&quot; as inputs to the network. When reciting the list of phenomena that the model can explain (see prior point), the authors should take care to include only phenomena that fall within the purview of what is actually being simulated by the network, rather what is being provided for free.</p><p>3) The depiction and explanation of BVC and PWb data (initially shown in Figure 2A2, C1, and C2 and also described in subsequent figures) is unclear. Why are small receptive fields shown close to the agent in Figure 2A2? What does this mean? In the Video 1, receptive fields do not appear to get smaller or bigger as the agent moves around. Are the cells with receptive fields close to the agent the ones that fire at the actual boundaries, not a distance away from the boundaries? If not, in Figure 2C1, why are BVCs shown to be firing, presumably at the north and east boundaries of the environment, when the agent is in the center of the environment (i.e., not in the boundary)? Why are so many BVCs firing at the same time when the agent is in a particular location? Are these different cells that fire at different distances from the boundaries? Figure 2A2 is described as showing receptive fields for BVCs, but these receptive fields are usually more rectangular in shape, whereas they are depicted as circular here. In &quot;bottom up&quot; mode (e.g., Figure 5, top) it appears that only BVCs encoding boundaries ahead of the animal (those in the visual field?) are active. Is this consistent with the firing of real neurons (such as border cells)? Is there any evidence that real BVCs are active only when a rodent faces toward but not away from their coded boundary?</p><p>4) The authors show a schematic of their model in Figure 4, which includes top-down and bottom-up connections between different brain areas. Lacking from the paper are multiple citations to anatomical studies that demonstrate that these connections are realistic (e.g., Jones and Witter, 2007, for projections from retrosplenial cortex to deep layers of entorhinal cortex). For example, has it been shown that the entorhinal cortex projects back to the retrosplenial cortex?</p><p>5) In Figure 7D, why isn't anything activated for the new object?</p><p>6) The authors go to great lengths to realistically model MTL activity based on rodent literature. However, it was not clear how the model relates to rodent literature regarding parietal coding (e.g. Harvey, Coen and Tank, 2012; Raposo, Kaufman and Churchland, 2014; Whitlock et al., 2012) and retrosplenial coding (Vedder, Miller, Harrison, Smith, Cerebral Cortex, 2017; Alexander and Nitz, 2017; Alexander and Nitz, 2015). Can the model account for the type of coding observed in navigating rodents in these regions? Or are there specific predictions the model can make about what would be observed or how these types of observed neural codes can be interpreted in the context of spatial cognition?</p><p>7) In some places, terms are used that assume prior knowledge to a degree that can make the paper difficult to parse. Some examples include 'gain-field circuit' (Introduction), the current model as an extension of the 'BBB model' (Introduction), 'heuristically implemented', 'mock-motor-efference'. Terms like this could use additional (but very brief) explanation when introduced.</p><p>8) The paper can be a bit long in places – particularly the sections on preplay and replay.</p><p>9) Figure 4: was activity from place cells to grid cells considered?</p><p>10) The presence of object specific coding appears to be a 'strong prediction' of the model, as object coding reported thus far is minimally selective for specific objects. The authors detail potential places where this activity might be found but they may want to consider that these types of representations lay outside the cortex or emerge as a population level representation (i.e. would not be observable at the level of single cell tuning curve responses).</p><p>[Editors’ note: this article was subsequently rejected after the authors submitted their revisions but they were invited to resubmit after an appeal against the decision.]</p><p>Thank you for submitting your work entitled &quot;A Neural-Level Model of Spatial Memory and Imagery&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by one peer reviewer, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The reviewers have opted to remain anonymous.</p><p>We regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>After consultation, we feel that the paper remains too cumbersome for a general audience, and some points remain unclear.</p><p><italic>Reviewer #3:</italic></p><p>In this paper, Bicanski and Burgess present a network model to propose how multiple populations of spatially tuned neurons are functionally interconnected with one another to form a spatial memory network with multiple capabilities.</p><p>A core feature of the model is that it proposes how bidirectional transformations between the egocentric and allocentric reference frames might be performed by networks in the retrosplenial cortex. The paper ambitiously attempts to demonstrate numerous capabilities for the model, such as mapping a familiar environment, learning the locations of unique landmark objects in such an environment, detecting novelty when changes occur in an environment, and simulating effects of brain lesions upon memory and navigation. The paper incorporates 13 videos, 13 figures, and 18 methods equations. Despite its length (nearly 60 pages), the paper does not provide enough detail for readers to fully understand some key aspects of the model (see below). This should <italic>not</italic> be interpreted as an entreaty to make the paper even longer. Rather, the paper should probably be split up into at least two publications, each dedicated to exploring specific capabilities of the model with more clarity and depth.</p><p>As noted in the prior review, simulations of mental navigation and route planning do not seem to be rooted in the model's core ability to perform bidirectional egocentric / allocentric transformations. Instead, it is the addition (&quot;for free&quot;) of a grid cell network connected to place cells endows the model with this capability. How does the egocentric / allocentric transformation network contribute to the mental navigation and route-planning process? What is the functional purpose for transforming allocentric replay events back into the egocentric coordinate frame to generate imagery? The grid cell driven simulations do not even appear until the subsection “Grid cells and mental navigation (Simulation 4.0)”, and given how information dense the preceding pages are, perhaps the addition of the grid cell network and accompanying simulations might be better suited to a second paper, dedicated more specifically to the topic of route planning?</p><p>Another problem raised in review, which has not been adequately addressed, is that the sequential shifting of attentional focus is not explicitly simulated by the model, and not explained fully enough for readers to comprehend how this process contributes to the simulation results. To solve the problem of binding each object or boundary's location to its identity, the model adopts a sequential shift of attention strategy. If I understand correctly, each population of boundary (BVC) or object (OVC) cells is activated one at a time in conjunction with its corresponding boundary (PRb) or object (PRo) identity cell. The length of each attentional cycle is stated to be 600 time units. How does information about multiple objects and boundaries get integrated across multiple attentional cycles to form a stable and non-fluctuating representation of the environment as a whole? Do PCs have slow activity decay kinetics that can span multiple attentional cycles? How is such temporal integration accounted for in analyses like that shown in Figure 8, where momentary &quot;snapshots&quot; of population vectors are being correlated with one another across encoding vs. recall? Because of the sequential attention mechanism, one would expect that an incomplete representation of the environment (i.e., just one object and one boundary) would be active at any given time. So I don't quite see how it is possible to use an instantaneous snapshot of the population vector to perform these correlation analyses, unless the snapshot is being taken at the end of some temporal integration process that spans multiple attention cycles?</p><p>Finally, since the heart of the model is the retrosplenial transformation network, it is a bit surprising that no simulation results are shown to demonstrate the predicted firing properties of retrosplenial neurons that perform the egocentric / allocentric transformation (in either direction) during imagery and recall. Do any testable predictions for unit recording studies arise from the firing properties of these model neurons? If so, then it seems like they should be included in the paper.</p><p>In summary, there are some innovative features of this model that would of interest to the research community, but the format of the paper (including its length) may not be best suited for publication in <italic>eLife</italic>. The authors might wish to consider splitting the paper up into two publications (e.g., one on coordinate transformation, and one on mental navigation and route following) so that some of the missing details of the model can be more thoroughly described without adding even more to the current manuscript's excessive length.</p><p>[Editors’ note: what now follows is the decision letter after the authors submitted for further consideration.]</p><p>Thank you for resubmitting your work entitled &quot;A Neural-Level Model of Spatial Memory and Imagery&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably evaluated by Michael Frank (Senior Editor), a Reviewing Editor, and one new reviewer.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p><italic>Reviewer #1:</italic></p><p>In the revised manuscript, the authors have done a lot of work to respond to previous criticisms, including adding new results to respond to the initial criticisms.</p><p>There is much value and novelty in this model, which provides a potential explanation of how egocentric and allocentric representations of space are reconciled. Also, as the authors point out in their appeal, the excessive length of the manuscript is due to results that were added to satisfy the original reviews. I feel confident that the specific feedback provided below will allow the authors to clarify points that remain puzzling.</p><p>1) I didn't really understand what the &quot;transformation sublayers&quot; are. Are they between areas or in the retrosplenial cortex part of the model? How are they different from &quot;individual sublayers&quot; (subsection “The Head Direction Attractor Network and the Transformation Circuit”, second paragraph)?</p><p>2) In all of the figures and videos involving the top-down mode/recall/imagery, why does firing occur for all directions of boundaries in PWbs and BVCs?</p><p>3) In Figure 12A, why are OVCs to the east firing when object 2 is in the northeast of allocentric space?</p><p>4) In Video 8, I don't understand at time = 1.67 seconds how the OVC pattern matches that during perception for object 2. It looks like object 1. OVC activity at NW at 1.67 seconds looks the same as OVC activity at NW at 6.52 seconds, the time at which the text states that activity matches that during perception for object 1. Related to this, in Video 12, at time = 11.52 s, OVC do not seem to be representing object 1 (i.e., it is not in the SE). The mental navigation representation of object 3 in OVC at 15.21 seconds looks the same as the mental navigation representation of object 1 at time = 11.36 seconds.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.33752.036</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>This paper describes a model that proposes a framework to explain how multiple, highly complex regions interact to produce egocentric and allocentric representations of space. Although all reviewers found the model to be interesting and novel, they also agreed that the Results, Materials and methods, and rationale for the model were often not presented in a clear and concise manner that would be optimal for a broad readership. Reviewers also felt that the simulation movies were insufficient to demonstrate the importance of the model and request a number of essential revisions, including incorporation of clear performance goals for the model, together with appropriate quantitative measures.</p></disp-quote><p>We hope our changes and additions to the manuscript in response to the comments provided remedy any criticisms. We now replicate specific experimental findings to supplement the previously submitted simulations and movies and expand the quantitative treatment of results. Please see in particular our extensive response to comment 1.</p><p>Quantification</p><p>In addition to the replication of several experimental paradigms (i.e. replication of behaviour/phenomenology), we have introduced a quantitative measure to assess the performance of the model. Our model’s main claim is that it constitutes a model of spatial memory and imagery specified at the level of single neurons. Hence we now quantify the similarity between the model representations during encoding and during recall, by correlating the population vectors during these two distinct states for the model’s populations. That is, we now measure the extent of re-activation of population vectors to quantify the extent of memory or novelty being signaled by the various neural populations. The magnitude of the correlation is compared to correlations of the recalled pattern to randomly sampled times while the model operates in bottom-mode (i.e. during perception). <xref ref-type="fig" rid="respfig1">Author response image 1</xref> shows an example from simulation 1.0 (object-cued recall). This measure can also be compared between simulations with and without noise (see end of replies to comment 1). Perirhinal neurons are not shown because of their small number (4 for the 4 walls, 1 for the object) and because the correlation is trivially 1 when cueing with the object (current is injected into the object-specific perirhinal neuron to induce recall).</p><fig id="respfig1"><object-id pub-id-type="doi">10.7554/eLife.33752.034</object-id><label>Author response image 1.</label><caption><title>Example of pattern comparison via correlations of population vectors from simulation 1.0 (object cued recall).</title><p>White bars show the correlation between the neural patterns during imagery/recall and those during encoding (RvE), while black bars show the average correlation between the neural patterns during imagery/recall and random patterns (sampled every 100 ms; RvRP). Note that OVCs and PCs exhibit correlation values close to one, indicating faithful reproduction of patterns. BVC correlations are somewhat diminished because recall fully reactivates all boundaries indiscriminately compared to a limited field of view during perception with only modest reactivation outside the field of view. PW neurons show correlations below one because at recall reinstatement in parietal areas requires the egocentric allocentric transformation (i.e. OVC signals passed through retrosplenial cells), which smears out the pattern compared to perceptual instatement in the parietal window (i.e. imagined representations are not as precise as their counterparts during perception).</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-33752-resp-fig1-v1"/></fig><p>This quantification of the mismatch between population vectors reflecting perceived and recalled representations could potentially be compared to experimental measures of overlap between neuronal populations (e.g. Guzowski et al., 1999 in animals, or ‘representational similarity’ measures in fMRI, e.g. Ritchey et al., 2013) if perception and recall can be separated experimentally. We use the same measure to infer the extent of novelty-related exploration (see response 1A, and original submission) and to assess robustness with regards to noise (please see reply section 1G).</p><p>Comment 1 requires the most comprehensive response and we split our reply in multiple parts (A-H) to focus on the individual points raised.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) A primary shortcoming is that the model is not used to simulate any specific behavioral tasks. Instead, after describing the architecture, the paper presents a series of several simulations (depicted by figures and videos) to demonstrate examples of neural population activity (over time periods lasting tens of seconds) during a few selected types of navigational behavior. These example simulations do not provide adequate support for strong claims made in the Discussion section, where it is argued that the model accounts for a wide range of findings including position specificity in visual object memory (Hollingworth, 2007), impaired episodic memory resulting from Papez circuit lesions (Delay and Brion 1969), neural activity seen during imagery for scenes in the MTL, retrosplenial cortex and precuneus (Burgess, Maguire et al., 2001; Hassabis et al. 2007; Schacter et al. 2007), and 'some aspects' of 'scene construction' and 'episodic future thinking' (Schacter et al., 2007; Hassabis et al., 2007; Buckner 2010). To support such claims, the model should more faithfully reproduce experimental designs from these prior studies (as is, there does not seem to be any quantitative metric by which these broad claims can be evaluated).</p></disp-quote><p>We agree with the referees that our more general simulations (designed to showcase the capabilities of the model) should be supplemented by replications of published experiments to better link our claims to the cited literature. We have added several items to the Results section, replicating behaviour/phenomenology from both human and rodent studies, focusing and on Papez’ circuit lesions and object novelty respectively. We have also restructured the section on top-down activity and trace cells, conducting new simulations replacing the previous simulations 2.1-4.</p><p>1A) Object novelty, hippocampal vs. perirhinal lesions</p><p>We now simulate experiments showing that hippocampal vs. perirhinal lesions differentially affect change detection, and suggest that object vector cells play a crucial role. In experiments the effects of lesions are commonly tested by exploiting the spontaneous preference that rats display for exploring novel/altered stimuli compared to familiar/unchanged ones. We assume that the relative amount of exploration will be proportional to the mismatch signal in the model (exploiting the introduction of a quantitative measure, please see above), and simulate key aspects of the following study.</p><p>Mumby et al. (2002) tested (among other manipulations) whether rats can detect that one of two objects has been shifted to a new location in a given environment. We have added simulations of this type of task (a moved object) with and without an intact hippocampus to the manuscript (simulations 1.3 and 1.4). The agent experiences an environment with two objects in which one is later moved. The mismatch signal is quantified as the difference in the firing of object vector cells during bottom-up mode (perception at the location of encoding) and during recall (imagery, at the same location).</p><p>Intact hippocampus:With an intact hippocampus (Simulation 1.3 and Video 7) the agent can generate a meaningful novelty signal, since the perceived location of the object relative to the agent is different from the recalled position. We compare the mismatch signal between the two objects in the arena and set the next movement target for the agent (to have a visual manifestation of the outcome of the comparison in the video) to the object for which the biggest mismatch has been detected (lowest correlation of population vectors). Figure 9 depicts the crucial aspects of this simulation and has been added to the manuscript as an application of the model to a real task. That detection of a change in position requires the hippocampus is consistent with place cells binding the relative location of an object (via object vector cells) to perirhinal neurons signalling the identity of an object.</p><p>Hippocampal lesion:Hippocampal lesions (Simulation 1.4 and Video 8) are implemented by setting the firing rates of hippocampal neurons to zero at all times. A hippocampal lesion precludes the generation of a meaningful novelty signal because the agent is incapable of generating a coherent point of view, since no appropriate BVC configuration can be selected by the now missing hippocampal input. A connection between object vector cells and perirhinal neurons (see Figure 3D in original manuscript) can still form in the lesioned agent. Thus upon recall some OVC activity is present (in imagery) due to a connection between perirhinal neurons and OVCs. However, such activity is not location specific. That is, without the reference frame of BVCs/PCs (place cells selecting active BVCs) this residual spurious activity cannot specify absolute object location since it can be generated everywhere (see Figure 2F-H). It only tells agent it has seen this object at given relative distance, but not where. Hence the mismatch signal is equal for both objects, and consequently exploration time would be split roughly 50/50 between the two objects.</p><p>On rare occasions, by chance, an object could be at the correct distance for the agent to give a good match between the perceived location and the unanchored OVC activity elicited by perirhinal neurons in imagery (Figure 9G, H). However, such an incidental match can occur for both the un-moved, as well as the moved object. Note that this account is corroborated by human data, which shows that some focal hippocampal amnesics can recall a familiar arrangement of objects if the point of view between test phase and sample phase is matched (King et al., 2002; but see also Shrager et al., 2007).</p><p>Contextual effects:Rats show preferential exploration of a familiar object that was previously experienced in a different environment, compared with one previously experienced in the same environment, and this preference is also abolished by hippocampal lesions (Mumby et al., 2002; Eacott and Norman 2004; Langston and Wood 2009). We have not simulated different environments (using separate place cell ensembles), but note that the ‘remapping’ of place cells between distinct environments (i.e. much reduced overlap of place cell population activity, Bostock, Muller and Kubie, 1991; Andersen and Jeffery 2003; Wills et al., 2005) suggests a mismatch signal for the changed-context object can be derived by comparing PC population vectors. Initiating recall of object 1, belonging to context 1, in context 2, would drive the place cells ensemble belonging to context 1, creating an imagined scene in context 1. During perception place cells representing context 2 would instead continue to be active. That is, since two environments would be represented by distinct place cells ensembles in the model (approximating remapping conditions) this would trivially generate a mismatch signal, and a hippocampal lesion obviously precludes such a mismatch signal specific to either the same-context or changed-context object.</p><p>Perirhinal lesions:Finally, it has been argued that object recognition (irrespective of context) is spared after hippocampal lesions but not perirhinal lesions (Aggleton and Brown, 1999; Winters et al., 2004; Norman and Eacott, 2004). These results are certainly compatible with the model given that its perirhinal neuronal population signals an object’s identity irrespective of location.</p><p>1B) The effect of Papez’ Circuit lesions</p><p>The second type of experimental finding we now replicate is the effect of lesions along Papez’ Circuit, outside of the hippocampus, in causing amnesia (Delay and Brion 1969; Squire and Slater 1978, Squire et al., 1989; Parker and Gaffan 1997; Aggleton et al., 2016). Lesions to the fornix and mammillary bodies severely impact recollection, although recognition is less affected (Tsivillis et al., 2008). Although Papez’ Circuit includes more regions than are covered by our model, in the context of spatial representations Papez’ circuit brain areas are notable for containing head direction cells. That is, the mammillary bodies (more specifically the lateral mammillary nucleus, LMN), anterior dorsal thalamus, retrosplenial cortex, parts of the subicular complex and medial entorhinal cortex. Thus lesioning Papez’ circuit corresponds to (at least) removing the head direction signal in our model. This signal is crucial for the operation of the transformation circuit. (In reality the disruption of inputs from the medial and lateral septum, conveying theta rhythmicity and cholinergic tone could also be important, but are not part of the current model.)</p><p>We have conducted new simulations to illustrate the ability of the model to account for the known results of such lesions (Simulations 1.1, 1.2 and Videos 5, 6). The lesion is modelled by setting the input provided by head direction cells to the retrosplenial transformation circuit to zero at all times.</p><p>In the bottom-up mode of operation (perception) a lesion to the head direction cell ensemble removes drive to the transformation circuit and consequently to the boundary vector cells and object vector cells. That is, the perceived location of an object (present in the egocentric parietal representation) cannot elicit activity in the MTL and thus cannot be encoded into memory. I.e. there is no BVC/OVC activity which could be associated with currently active place cells. In the top-down mode of operation (recall)there are two effects. 1) Since no new elements can be encoded into memory there is nothing to recall (anterograde amnesia) (Simulation 1.1), and 2) Even if there are pre-existing memories (e.g. of an object encoded prior to the lesion) and place cells can be driven via a learned connection from perirhinal neurons (e.g. when cued with the object encoded prior to the lesion; Simulation 1.2), no meaningful representation can be instantiated in parietal areas, preventing recollection/imagery, thus causing retrograde amnesia for hippocampus-dependent episodic memories (other forms of retrieval of neocortical memories would not necessarily be affected). Simulations 1.1 and 1.2 show that the egocentric neural correlates of objects and boundaries present in the visual field would persist in the parietal window only as long the agent perceives them (they could also be held in working memory, which we do not model). Figure 7 depicts the key elements of simulations 1.1 and 1.2.</p><p>Note that perirhinal cells and any presumed upstream ventral visual stream inputs are spared, and hence an agent could still report the identity of an attended object.</p><p>Also note that grid cell firing would also be disrupted by removal of head direction inputs (Winter, Clark and Taube, 2015).</p><p>Winter SS, Clark BJ, Taube JS. Disruption of the head direction cell network impairs the parahippocampal grid cell signal. Science. 2015 Feb 5:1259591</p><p>1C) Position specificity and Hollingworth (2007)</p><p>In Hollingworth (2007) participants viewed images of natural scenes or object arrays in a change detection task and memory performance was higher when the target object position remained the same from study to test. This advantage was lost when the relative spatial relationships among contextual objects was disrupted. The author concludes that episodic scene representations are formed through the binding of objects to scene locations, and object position is defined relative to a larger spatial representation coding the relative locations of contextual objects (also see Hannula et al., 2006). This seems like a very good match to the role played by object vector cells (OVCs) in our model. OVCs are bound to a context via place cells, which means that all objects, when represented by OVC firing (in conjunction with perirhinal identity cells), preserve their relative arrangements (see e.g. Simulations 1.3, 1.4, and 3.0 for two objects encoded from the same position).</p><p>Hannula DE, Tranel D, Cohen NJ. The long and the short of it: relational memory impairments in amnesia, even at short lags. Journal of Neuroscience. 2006 Aug 9;26(32):8352-9.</p><p>The rationale behind the inclusion of this citation on our part was that transferring Hollingworth’s paradigm into 3D (e.g. overlooking an arrangement of objects in a 3D scene), maps perfectly onto the toy environment our virtual agent moves in, and would provide a good example of the use of object vector cells. In fact, detecting a change of an object’s position within the scene maps 1:1 to the simulation conducted for 1A, which exploits position specificity conferred by object vector cells. Hence we hope the referees agree that the inclusion of the object novelty paradigm described in 1A warrants the citation of Hollingworth (2007).</p><p>1D) 'Scene construction' and 'episodic future thinking'</p><p>We address the concerns about our suggestion that the model serves as an explanation for some aspects of 'scene construction' and 'episodic future thinking' separately.</p><p>First, we clarify that the model provides a potential explanation for the neural activity seen in MTL, retrosplenial cortex and precuneus (e.g. Hassabis, Kumaran and Maguire, 2007) during imagery for previously experienced scenes (i.e. when subjects were instructed to perform recall of a past event) in terms of the overlap between the model structures and the reported brain regions (see also, Burgess et al., 2001; Schacter, Addis and Buckner, 2007). However, we now note that the explanatory power of the model with regards to imagined future (or potentially non-existent) scenes is more limited (please also see our brief discussion of episodic future thinking below).</p><p>With regard to the neural activity seen in experiments during imagery for scenes in the MTL Hassabis, Kumaran and Maguire (2007) note that “the imaginary constructions produced by four of the five patients ([hippocampal amnesics]) were greatly reduced in richness and content compared with those of controls. The impairment was especially pronounced for the measure of spatial coherence, indicating that the constructions of the amnesic patients tended to consist of isolated fragments of information, rather than connected scenes.” It is currently impossible to map our quantification onto subjective, participant-dependent metrics like ‘spatial coherence’ as used in Hassabis, Kumaran and Maguire (2007). However, we have shown that a hippocampal lesion (see 1A) at best allows for the association of fragments of a scene (e.g. perirhinal cells being linked OVCs). No coherent scene can be constructed in the model without hippocampal place cells binding all scene elements together. We would cautiously suggest that such a deficit could potentially underlie such lack of spatial coherence reported by amnesics in Hasssabis, Kumaran and Maguire (2007) with regard to previously experienced scenes.</p><p>Concerning the model providing an explanation for some aspects of 'episodic future thinking' (Schacter, Addis and Buckner, 2007; Hassabis, Kumaran and Maguire, 2007; Buckner 2010), we apologise if this was overstated. Episodic future thinking explicitly links memory with planning for future behaviour. “Brain regions that have traditionally been associated with memory appear to be similarly engaged when people imagine future experiences” Schacter, Addis and Buckner (2007). Our model does describe how spatial scenes can be imagined, and how this process relies on structures traditionally associated with memory. However, our simulations do not include imagination of completely novel scenes, and we now take care to state that the concept of episodic future thinking extends beyond our simulations. Nevertheless, our simulations do include imagination of novel trajectories within a familiar environment, which could be useful for planning future actions. For example, in simulation 4.0 (Figure 12; mental navigation), the agent imagines performing new trajectory and object 3 appears in the egocentric frame of reference in a relative position to the agent not experienced during encoding. We believe this constitutes a small (albeit crucial) step towards a neural account of episodic future thinking. We are now more careful in our phrasing regarding claims about episodic future thinking.</p><disp-quote content-type="editor-comment"><p>It is also implied that the model accounts for &quot;trace cells&quot; that fire when the agent visits the prior location of a missing object (Tsao et al., 2015), and although trace-like activity is shown in some example simulations, it is not specifically stated which neural population in the model would correspond to trace cells, nor are simulated trace cell firing rate maps presented.</p></disp-quote><p>1E) Trace cells</p><p>The reviewers are right that the topic of trace cells merits some clarification. In response to this comment we have restructured the section of the manuscript previously devoted to novelty and top-down activity. Novelty is now covered separately, by simulations 1.3,1.4 (replication of Mumby et al. 2002). What used to be simulation set 2 (simulations 2.1-4 in the previous version of the manuscript) is now devoted to a more in-depth discussion of trace responses.</p><p>The trace property is not necessarily tied to a specific anatomical area or cell type. We use the term trace cell to refer to any cell that used to fire in response to an object or boundary and subsequently fires in its absence. As requested we now show examples of rate maps which predict trace fields as a result of repeated memory probes which may occur roughly with the frequency of theta in rodents.</p><p>We define a memory probe as follows: a smooth modulation of top-down connectivity during perception, subject to the following criteria: 1) Sensory inputs are not disengaged, and 2) there is no cue to recall anything specific (like for instance an object in its context). This kind of probe of MTL representations can inform the perceptually driven egocentric parietal representation about absent scene elements (please see main manuscript text and Figure 10A, B). The exact duration of the probe is not important as long as it leaves enough time for activity to build up in the MTL. For the trace response simulations the memory probe occurs with a frequency of rodent theta (though no mechanism of theta generation is modelled) because it has previously been suggested that theta orchestrates the flow of information (Hasselmo, Bodelón and Wyble, 2002), at least in rodents.</p><p>However, note that some trace cell firing could also result from non-spatially selective cells. We now show that nominally non-spatially selective cells like perirhinal identity neurons can manifest a spatial trace firing field if encoding is restricted to a limited region of space (cf. Figure 10D3). This finding may help to reconcile the common notion that lateral entorhinal cortex processes non-spatial information (Van Cauter et al., 2012) with spatial responses of trace cells (Tsao, Moser and Moser, 2013) in lateral entorhinal cortex. However, unlike perirhinal neurons in the model the trace cells of Tsao et al. (2015) do not fire when the object is present, but only subsequently, in the absence of the object. Thus they might signal the mismatch between the expected presence of an object and its actual absence. However, crucially such a signal may be based on trace firing of cells similar to perirhinal cells in the model. I.e. an object specific cell which was active at the time of encoding must necessarily participate in the generation of a mismatch signal.</p><disp-quote content-type="editor-comment"><p>It is additionally claimed that the model accounts for preplay and replay activity of place cells, but it appears that simulated preplay and replay events do not occur on a compressed time scale in the model as they do in real rodents, which is not addressed.</p></disp-quote><p>1F) Replay and Preplay</p><p>We agree that the section on “replay” and “preplay” would benefit from a more careful presentation. We simplified the corresponding section considerably (which also addresses point 8 below, shortening the section), making clear the limitations of the model to explain these types of phenomena.</p><p>The same results are reported as before but we more correctly refer to them as simulations of planning and recall. The main aim of simulation 5.0 (Figure 13) was to show how the model can cope with shortcuts, extending its cognitive map into un-explored territory by incorporating new place cells from a reservoir. That is, the agent engages in planning, performing mental navigation along the shortest path to the goal by using a forward sweep of grid cell activity to drive corresponding activity in established place cells (across familiar ground) or in proto-place cells (across unexplored ground). We now simply point out that the internally-generated sequential activity of place cells is reminiscent of forward replay or preplay (referring to ‘preplay-like’ activity) but do not claim (and state so explicitly) to have fully modelled those phenomena.</p><p>The planning stage engenders preplay-like activity in place cells (Dragoi and Tonegawa 2011; Olafsdottir et al., 2016), whereas the mental navigation phase is reminiscent of replay (Wilson and McNaughton 1994; Foster and Wilson 2006; Diba and Buzsaki 2007; Karlsson and Frank 2009; Carr, Jadhav and Frank 2011) or ‘forward sweeps (Johnson and Redish 2007; Pfeiffer and Foster 2015), although the faster propagation of these sequences compared to those seen during actual navigation is not addressed. Both replay and preplay usually occur during sharp wave ripple events (at a compressed time scale). To account for sharp wave ripple events and theta sequences requires a spiking neuron model. Nevertheless, the causal role played by grid cells in the model suggests that some forms of replay in place cells could be driven by a sweep of activity in the grid cell population along a given trajectory, and may correspond to route planning (Kubie and Fenton 2012; Erdem and Hasselmo 2012; Bush et al. 2015; Yamamoto and Tonegawa 2017).</p><p>It is possible that a compressed time scale could be simulated if grid cell sweeps occur at faster timescales within SWRs or theta cycles, and without re-activation of parietal representations. But such extensions are outside the scope of the model (spiking neurons, theta rhythmicity, SWRs) and are not simulated.</p><disp-quote content-type="editor-comment"><p>In other words, the heavy reliance on qualitative (rather than quantitative) performance assessments make it difficult to offer anything more than a subjective evaluation of the model's capabilities. A more objective evaluation might be possible if the authors run more simulations to quantitatively compare simulated vs. real neural activity (or simulated vs. real task performance), explore how the model's performance degrades under realistic noise or uncertainty conditions, etc.</p></disp-quote><p>1G) Comparison to real tasks, quantification, and noise</p><p>We hope to have addressed the reviewers’ main concerns regarding quantification and comparisons to real tasks by replicating several finding reported in the literature (please see 1A, B, E), and by providing a quantitative measure of the model’s performance (correlation of population vectors). We have also provided more examples of neural activity shown in the form of rate maps (trace cells) for comparison with experimental data. We note that the design of the model was also constrained by known data in terms of the tuning curves used for spatial cells and the assumed anatomical connections between regions.</p><p>Noise/robustness</p><p>As far as noise and robustness are concerned, we perform two additional sets of simulations (all modifications of simulation 1.0, object-cued recall). In the first simulation set we randomly chose cells throughout the model to be permanently deactivated (equal proportions of place cells, grid cells, OVCs, BVCs, parietal and retrosplenial neurons etc.). This allows us to assess the robustness of the model’s capability to perform encoding and recall in visuo-spatial imagery with regards to neuron loss. Simulations with neuron loss could also serve as a model for the effects of diffuse damage, as might occur in anoxia, Alzheimer’s disease or normal aging, although we do not attempt to model any specific neurological condition. We have repeated this simulation 20 times (each time with newly drawn random numbers) and present the average correlation between the population vectors at encoding vs. recall in Figure 8.</p><p>We have tested silencing up to 20% of the cells per affected model component. The agent is still able to encode and recall the corresponding representations (cf. Figure 8). The ability to maintain a stable attractor state among place cells and head direction cells is critical to the functioning of the model, while damage in the remaining (feed-forward) model components manifests in gradual degradation in the ability to represent the locations of objects and boundaries (please see accompanying Video 3). For example, if certain parts of the parietal window suffer from neuron loss, the reconstruction in imagery is impaired only at the locations in peri-personal space which were coded for by the now missing neurons (indeed, this can be the basis for a model of hemi-spatial neglect, Byrne, Becker and Burgess 2007). The place cell populations were more robust to silencing than the head-direction population, simply because they were simulated in comparatively greater numbers of neurons and therefore exhibit redundancy.</p><p>Similarly, the model is also robust to adding firing rate noise (up to 20% of peak firing rate) to all cells. Correlations between patterns at encoding and recall remain similar to the noise-free case. Figure 8 shows the correlation between the population vectors at encoding vs. recall for all model components for 20 iterations. We include one representative example from both simulation sets as new videos (Videos 3 and 4).</p><disp-quote content-type="editor-comment"><p>2) Several key mechanisms – such as sequential shifting of attentional focus (which is essential for solving the place-object binding problem), neuromodulation (which allows the model to transition between sensory processing and mental imagery modes), and population activity in the grid cell map (which is essential for generating preplay/replay trajectories) – are not explicitly simulated by the model. Rather, these signals are provided &quot;for free&quot; as inputs to the network. When reciting the list of phenomena that the model can explain (see prior point), the authors should take care to include only phenomena that fall within the purview of what is actually being simulated by the network, rather what is being provided for free.</p></disp-quote><p>We wholeheartedly agree with the reviewers. It was certainly not our intention to claim that the model explains the mechanistic origins of attention, how grid cell firing patterns are generated by the brain, or the exact nature of the proposed neuromodulatory signal. We now emphasise more strongly in the main text that these phenomena are not explained by the model at a mechanistic level. However, we believe it remains fair to say that the model proposes what role attention plays is memory encoding. This statement can be divorced from the mechanistic origins of attention and thus constitutes a valid prediction. Similarly, assuming grid cell-like inputs to the model (e.g. in the form of pre-calculated firing rate maps) allows for predictions about the role of grid cells (e.g. in mental navigation, driving the point of view) without needlessly replicating one of the numerous articles that have proposed models of the mechanistic origin of grid cell signals.</p><disp-quote content-type="editor-comment"><p>3) The depiction and explanation of BVC and PWb data (initially shown in Figure 2A2, C1, and C2 and also described in subsequent figures) is unclear. Why are small receptive fields shown close to the agent in Figure 2A2? What does this mean? In the Video 1, receptive fields do not appear to get smaller or bigger as the agent moves around. Are the cells with receptive fields close to the agent the ones that fire at the actual boundaries, not a distance away from the boundaries? If not, in Figure 2C1, why are BVCs shown to be firing, presumably at the north and east boundaries of the environment, when the agent is in the center of the environment (i.e., not in the boundary)? Why are so many BVCs firing at the same time when the agent is in a particular location? Are these different cells that fire at different distances from the boundaries? Figure 2A2 is described as showing receptive fields for BVCs, but these receptive fields are usually more rectangular in shape, whereas they are depicted as circular here. In &quot;bottom up&quot; mode (e.g., Figure 5, top) it appears that only BVCs encoding boundaries ahead of the animal (those in the visual field?) are active. Is this consistent with the firing of real neurons (such as border cells)? Is there any evidence that real BVCs are active only when a rodent faces toward but not away from their coded boundary?</p></disp-quote><p>We have endeavoured to clarify these aspects in the first presentation of boundary vector cell and PW firing. Figure 2 was poorly explained previously and we take care now to be more precise. In particular the relationship between receptive field centers mapped on a polar grid and the firing of individual cells is now hopefully clearer.</p><p>Further clarifications</p><p>- The videos and figures never show receptive fields (with the exception of Figure 2A2 now), but neural firing of cells arranged according to where in space their receptive fields are centered.</p><p>- In the previously submitted version of the manuscript, receptive fields only scaled along the angular axis with distance (constant angular resolution) because we had modelled PW neurons and BVCs after the previously published model of Byrne, Becker and Burgess (2007). However, a better fit to the known data on BVCs is that the centers of receptive fields should also exhibit increasing radial separation as shown in panel A2 now. This has now been remedied. Similarly, the radial dispersion of the receptive fields (given by σ_rho in Equation 14) now also scales with distance from the center, such that far away receptive fields still cover multiple bins/grid points. We have rerun all model simulations with this updated topology of polar receptive fields (see Appendix), which provides a closer fit to Barry and Burgess (2007) and Lever et al. (2009).</p><p>- There are BVCs for all distances, as suggested by the arrangement of the receptive fields as a polar grid around the agent. This is why BVCs fire for boundaries of the environment, when the agent is e.g. in the center of the environment.</p><p>- Many BVCs firing at the same time collectively represent all the currently perceived boundaries. That is, for extended boundaries multiple grid points/receptive fields are covered by a boundary and hence multiple cells fire. Without extra allocation of attention to specific portions of a boundary BVC firing thus represents, to a first approximation, a context of the environment (in conjunction with place cells).</p><p>- The shape of the receptive is not rectangular but rather given by the product of exponentials in Equation 14. Panel A2 hopefully makes this clearer.</p><p>- To emphasise the clear distinction between perception and recall/imagery we have restricted perceptual activity to the field of view. To the best of our knowledge there currently is not enough data to assess if BVCs or border cells fire only when a boundary is actively perceived as long an animal is not engaged in recall. However note that boundaries outside the field of view are weakly represented due to pattern completion (see for instance in panel C1). A periodically recurring memory probe as outlined for the simulation that yielded BVC trace rate maps (see 1E) can increase this effect further.</p><disp-quote content-type="editor-comment"><p>4) The authors show a schematic of their model in Figure 4, which includes top-down and bottom-up connections between different brain areas. Lacking from the paper are multiple citations to anatomical studies that demonstrate that these connections are realistic (e.g., Jones and Witter, 2007, for projections from retrosplenial cortex to deep layers of entorhinal cortex). For example, has it been shown that the entorhinal cortex projects back to the retrosplenial cortex?</p></disp-quote><p>We thank the referees for catching this oversight. We now include proper citations for all the connections. Jones and Witter (2007) report that “projections from the remaining cingulate areas [including retrosplenial cortex] preferentially target the postrhinal and medial entorhinal cortices as well as the presubiculum and parasubiculum”. Since we make no strong claims about the locus of OVCs we believe this reference is compatible with the present model, regardless of whether BVCs/OVCs are preferentially located in the subicular complex (Lever et al., 2009; with OVC co-located by analogy) or in medial entorhinal cortex (Solstad et al., 2008; Høydal et al., 2017).</p><p>Regarding the back projection to retrosplenial cortex, Wyss and van Groen (1992) have reported such connections. We briefly discuss these connections when we discuss the possible locus of OVCs.</p><disp-quote content-type="editor-comment"><p>5) In Figure 7D, why isn't anything activated for the new object?</p></disp-quote><p>Figure 7D depicted the neural representations during imagery and the novel object had not yet been encoded into memory. In addition, if it had been, it would have needed to be in the focus of attention to be accompanied by, for instance, perirhinal activity. For a similar simulation, note that, if it had been encoded in addition to an old, removed object, the situation would have been covered by what was previously Figure 8 (sampling multiple items in imagery, now Figure 11). However, we have restructured the corresponding manuscript section to focus on trace cells, which can be compared to experimental findings and which yield explicit predictions.</p><disp-quote content-type="editor-comment"><p>6) The authors go to great lengths to realistically model MTL activity based on rodent literature. However, it was not clear how the model relates to rodent literature regarding parietal coding (e.g. Harvey, Coen and Tank, 2012; Raposo, Kaufman and Churchland, 2014; Whitlock et al., 2012) and retrosplenial coding (Vedder, Miller, Harrison, Smith, Cerebral Cortex, 2017; Alexander and Nitz, 2017; Alexander and Nitz, 2015). Can the model account for the type of coding observed in navigating rodents in these regions? Or are there specific predictions the model can make about what would be observed or how these types of observed neural codes can be interpreted in the context of spatial cognition?</p></disp-quote><p>We agree with the reviewers that the rodent literature concerning the parietal side of the model has been neglected in the text. We have instead been more inspired by the human side of the literature for this model component. This is mainly due to the fact that neurons resembling parietal window cells, as presented in the model, have only recently been reported (Hinman, Chapman and Hasselmo 2017), compared to vast literature on place cells, grid cells, head direction cells, and, to some extent, boundary (vector) cells. Some findings, e.g. the firing patterns correlated with multiple frames of reference (e.g. Wilber et al., 2014) have parallels with the model (in this case with our retroplenial model component. However, although intriguing correlations have been reported in parietal cortex (Nitz 2006, 2009, 2012; Harvey, Coen and Tank 2012; Whitlock et al., 2012; Raposo, Kaufman and Churchland 2014; Vedder et al., 2016) it remains difficult (compared to place cells or grid cells) to identify a unified representational code underlying all of the reported recordings. It is however an intriguing topic for future work. Also because trying to incorporate these findings would considerably inflate the already lengthy manuscript.</p><disp-quote content-type="editor-comment"><p>7) In some places, terms are used that assume prior knowledge to a degree that can make the paper difficult to parse. Some examples include 'gain-field circuit' (Introduction), the current model as an extension of the 'BBB model' (Introduction), 'heuristically implemented', 'mock-motor-efference'. Terms like this could use additional (but very brief) explanation when introduced.</p></disp-quote><p>We now clarify these terms upon first occurrence. The BBB abbreviation has been removed since it only occurred once.</p><disp-quote content-type="editor-comment"><p>8) The paper can be a bit long in places – particularly the sections on preplay and replay.</p></disp-quote><p>We have shortened these sections. We realize that the rest of the paper is a bit long in places and have endeavoured to be more concise where possible, within the constraints of the requested corrections and additional work.</p><disp-quote content-type="editor-comment"><p>9) Figure 4: was activity from place cells to grid cells considered?</p></disp-quote><p>We did not consider this type of activity for multiple reasons. 1) On a practical level, since grid cells are implemented as firing rate maps it is not clear how any influence of place cells on grid cells could be implemented. 2) On a theoretical level, it has been argued that place cells (when driven by sensory inputs) can help anchor grid cells, which exhibit drift due to path integration errors. Regarding the dynamics of the model, since grid cells are represented by static firing rate maps, there was no need for stabilizing place cell inputs to grid cells. We believe the functional meaning of the connection from place cells to grid cells is not relevant for the model in its current form, although intriguing new work on the issue (e.g. grid cells performing PCA on place cell representations; Dordek et al., 2016) might warrant a closer look in future work.</p><disp-quote content-type="editor-comment"><p>10) The presence of object specific coding appears to be a 'strong prediction' of the model, as object coding reported thus far is minimally selective for specific objects. The authors detail potential places where this activity might be found but they may want to consider that these types of representations lay outside the cortex or emerge as a population level representation (i.e. would not be observable at the level of single cell tuning curve responses).</p></disp-quote><p>We are not quite sure of the question here. Our model does indeed take into account that ‘object coding reported thus far is minimally selective for specific objects.’ A given object vector cell fires for any object, as long as an object is at a certain distance in a certain direction: it is not selective for specific objects. This is a key prediction, similar to the original prediction that BVCs respond to any boundary at the correct distance and direction (Hartley et al., 2000). Object specific coding is only present in the perirhinal identity cells, and there are many reports of this type of coding there (e.g. Miller et al., 1993; Xiang and Brown 1998; Zhu et al., 1995). Only the conjunction of OVC firing and perirhinal identity cells uniquely defines a specific object at a specific place relative to the agent. Uniquely specifying absolute position requires place cells as well. Thus, we agree that, although individual cells have well-defined tuning curves, it is a population code of OVCs, PCs and perirhinal cells which uniquely defines the neural representation of an object in its place.</p><p>Miller, E. K., Li, L. &amp; Desimone, R. Activity of neurons in anterior inferior temporal cortex during a short-term memory task. J. Neurosci. 13, 1460–1478 (1993).</p><p>Xiang, J. Z. &amp; Brown, M. W. Differential neuronal encoding of novelty, familiarity and recency in regions of the anterior temporal lobe. Neuropharmacology 37, 657–676 (1998).</p><p>Zhu, X. O., Brown, M. W. &amp; Aggleton, J. P. Neuronal signalling of information important to visual recognition memory in rat rhinal and neighbouring cortices. Eur.J. Neurosci. 7, 753–765 (1995).</p><p>[Editors’ note: the authors’ responses to first round of re-review follow]</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>In this paper, Bicanski and Burgess present a network model to propose how multiple populations of spatially tuned neurons are functionally interconnected with one another to form a spatial memory network with multiple capabilities.</p><p>A core feature of the model is that it proposes how bidirectional transformations between the egocentric and allocentric reference frames might be performed by networks in the retrosplenial cortex. The paper ambitiously attempts to demonstrate numerous capabilities for the model, such as mapping a familiar environment, learning the locations of unique landmark objects in such an environment, detecting novelty when changes occur in an environment, and simulating effects of brain lesions upon memory and navigation. The paper incorporates 13 videos, 13 figures, and 18 methods equations. Despite its length (nearly 60 pages), the paper does not provide enough detail for readers to fully understand some key aspects of the model (see below). This should not be interpreted as an entreaty to make the paper even longer. Rather, the paper should probably be split up into at least two publications, each dedicated to exploring specific capabilities of the model with more clarity and depth.</p><p>As noted in the prior review, simulations of mental navigation and route planning do not seem to be rooted in the model's core ability to perform bidirectional egocentric / allocentric transformations. Instead, it is the addition (&quot;for free&quot;) of a grid cell network connected to place cells endows the model with this capability. How does the egocentric / allocentric transformation network contribute to the mental navigation and route-planning process? What is the functional purpose for transforming allocentric replay events back into the egocentric coordinate frame to generate imagery? The grid cell driven simulations do not even appear until the subsection “Grid cells and mental navigation (Simulation 4.0)”, and given how information dense the preceding pages are, perhaps the addition of the grid cell network and accompanying simulations might be better suited to a second paper, dedicated more specifically to the topic of route planning?</p></disp-quote><p>Contrary to the reviewer’s new statement (not mentioned in the first review), mental navigation and route planning do indeed require the model's core ability to perform bidirectional egocentric / allocentric transformations. As stated multiple times throughout the manuscript one of the core hypotheses of the present model is that an agent only has conscious access to the egocentric representation of space around itself, which embodies a point of view. This hypothesis is strongly supported by striking, classical empirical findings like hemispatial neglect. Thus, updating the (imagined) location of the agent by the action of grid cells on place cells in the medial temporal lobe still requires the egocentric-allocentric transformation to allow imagery of the simulated outcome. Without this transformation, there would be no mental navigation.</p><disp-quote content-type="editor-comment"><p>Another problem raised in review, which has not been adequately addressed, is that the sequential shifting of attentional focus is not explicitly simulated by the model, and not explained fully enough for readers to comprehend how this process contributes to the simulation results. To solve the problem of binding each object or boundary's location to its identity, the model adopts a sequential shift of attention strategy. If I understand correctly, each population of boundary (BVC) or object (OVC) cells is activated one at a time in conjunction with its corresponding boundary (PRb) or object (PRo) identity cell. The length of each attentional cycle is stated to be 600 time units. How does information about multiple objects and boundaries get integrated across multiple attentional cycles to form a stable and non-fluctuating representation of the environment as a whole? Do PCs have slow activity decay kinetics that can span multiple attentional cycles? How is such temporal integration accounted for in analyses like that shown in Figure 8, where momentary &quot;snapshots&quot; of population vectors are being correlated with one another across encoding vs. recall? Because of the sequential attention mechanism, one would expect that an incomplete representation of the environment (i.e., just one object and one boundary) would be active at any given time. So I don't quite see how it is possible to use an instantaneous snapshot of the population vector to perform these correlation analyses, unless the snapshot is being taken at the end of some temporal integration process that spans multiple attention cycles?</p></disp-quote><p>With regard to the details of the attentional modulation, the OVCs are subject to the attentional modulation because we simulate the encoding of novel objects into a familiar context. Attention is required at encoding to allow the relevant representations to be bound together (in this case OVCs, place cells and object-identity neurons). At retrieval, the associations have been formed, and a complete scene representation is stably present. Population vectors are sampled at the time of encoding, which is a precise moment in simulation: there is no need for averaging across a cycle because the encoded object is in the focus of attention at the time of encoding. These population vectors are then compared to the stable representations during retrieval/imagery.</p><disp-quote content-type="editor-comment"><p>Finally, since the heart of the model is the retrosplenial transformation network, it is a bit surprising that no simulation results are shown to demonstrate the predicted firing properties of retrosplenial neurons that perform the egocentric / allocentric transformation (in either direction) during imagery and recall. Do any testable predictions for unit recording studies arise from the firing properties of these model neurons? If so, then it seems like they should be included in the paper.</p></disp-quote><p>The firing properties of retrosplenial neurons have been shown in Video 1, and we would happily add a static supplementary figure (e.g. of rate maps) to formulate an additional prediction.</p><disp-quote content-type="editor-comment"><p>In summary, there are some innovative features of this model that would of interest to the research community, but the format of the paper (including its length) may not be best suited for publication in eLife. The authors might wish to consider splitting the paper up into two publications (e.g., one on coordinate transformation, and one on mental navigation and route following) so that some of the missing details of the model can be more thoroughly described without adding even more to the current manuscript's excessive length.</p></disp-quote><p>In addition, the second round reviewer requests full models of attention, grid cells, and parietal cortex. We do not think that would be possible, even if split over 2 papers. More importantly, we do not see how the absence of these additional models details diminishes the thorough and comprehensive theoretical account of spatial memory we have proposed.</p><p>[Editors’ note: the author responses to the re-review follow.]</p><disp-quote content-type="editor-comment"><p>Thank you for resubmitting your work entitled &quot;A Neural-Level Model of Spatial Memory and Imagery&quot; for further consideration at eLife. Your revised article has been favorably evaluated by Michael Frank (Senior Editor), a Reviewing Editor, and one new reviewer.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>Reviewer #1:</p><p>In the revised manuscript, the authors have done a lot of work to respond to previous criticisms, including adding new results to respond to the initial criticisms.</p><p>There is much value and novelty in this model, which provides a potential explanation of how egocentric and allocentric representations of space are reconciled. Also, as the authors point out in their appeal, the excessive length of the manuscript is due to results that were added to satisfy the original reviews. I feel confident that the specific feedback provided below will allow the authors to clarify points that remain puzzling.</p><p>1) I didn't really understand what the &quot;transformation sublayers&quot; are. Are they between areas or in the retrosplenial cortex part of the model? How are they different from &quot;individual sublayers&quot; (subsection “The Head Direction Attractor Network and the Transformation Circuit”, second paragraph)?</p></disp-quote><p>The transformation sublayers are the same as the individual sublayers. The whole transformation circuit consists of 20 individual sublayers (please see Video 1 and the new supplementary figure, Figure 2—figure supplement 1), each maximally modulated by a different head direction. We apologise for the confusion and have standardized the terminology throughout, using the term ‘sublayer’ consistently with retrosplenial cortex. The term ‘circuit’ is used to refer to the whole of the retrosplenial model component, and the generic term ‘layer’ is replaced by the term ‘population’. We have also partially rewritten (and shortened) subsection “The Head Direction Attractor Network and the Transformation Circuit” to further clarify the structure of the retrosplenial component further.</p><disp-quote content-type="editor-comment"><p>2) In all of the figures and videos involving the top-down mode/recall/imagery, why does firing occur for all directions of boundaries in PWbs and BVCs?</p></disp-quote><p>This is because the simulated environment is familiar, so that the agent has experienced it from many different points of view at each location. Thus there are reciprocal connections between place cells and BVCs for all boundaries, for a given location. Since, during imagery, PWb neurons are driven by BVC activity (via the transformation circuit) they fire for boundaries in all directions (see Figure 2—figure supplement 1), even though the PWb neurons behind the agent would not be driven by perception. We now make extra note of this at the first occurrence of recall/imagery (near Figure 5).</p><disp-quote content-type="editor-comment"><p>3) In Figure 12A, why are OVCs to the east firing when object 2 is in the northeast of allocentric space?</p></disp-quote><p>The object is in the North East of the map, but relative to the agent it lies East. Hence OVCs representing the location of object 2 are firing East of the origin of the OVC plot because the object is East of the agent. The OVC grid of receptive fields (like the BVC grid) is anchored to the agent (i.e. the origin is the location of the agent and moves with it). The allocentric aspect lies in the independence of BVC/OVC responses with regard to the orientation of the agent (e.g. whether the object is to the left or the right). We now make an extra note of this in the discussion of receptive field topology (in the caption of Figure 2). Hopefully the new Figure 2—figure supplement 1 also helps to clarify the nature of these receptive fields.</p><disp-quote content-type="editor-comment"><p>4) In Video 8, I don't understand at time = 1.67 seconds how the OVC pattern matches that during perception for object 2. It looks like object 1. OVC activity at NW at 1.67 seconds looks the same as OVC activity at NW at 6.52 seconds, the time at which the text states that activity matches that during perception for object 1. Related to this, in Video 12, at time = 11.52 s, OVC do not seem to be representing object 1 (i.e., it is not in the SE).</p></disp-quote><p>In Video 8 the time in imagery (during the 1<sup>st</sup> recall episode) can be divided into 2 phases. From roughly 1.60 to 2.70s object 1 is being recalled (with corresponding OVC pattern). From 2.70 to 2.80s object 2 is being recalled (with corresponding OVC pattern). Only for object two there is a match to the perceptual representation (experienced prior to recall). The confusion is likely due to the fact that the annotation appears already at time 1.60 but actually refers mainly to phase two, when object 2 is recalled after object 1. We apologize for the inaccuracy and have now split the annotation to be more precise.</p><disp-quote content-type="editor-comment"><p>The mental navigation representation of object 3 in OVC at 15.21 seconds looks the same as the mental navigation representation of object 1 at time = 11.36 seconds.</p></disp-quote><p>This is again due to the fact that the OVC grid (similar to the BVC grid) translates with the agent, but is independent of the orientation of the agent. At both times (around 11.35s and around 15.21s) the object is roughly South East to the agent at a similar distance (though not precisely at the same angle) but, both times with different place cell representations, which disambiguate the representational code overall. We hope the changes we made in response to comment 3 also clarify this case.</p></body></sub-article></article>