<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">54519</article-id><article-id pub-id-type="doi">10.7554/eLife.54519</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Behavioral evidence for memory replay of video episodes in the macaque</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-170539"><name><surname>Zuo</surname><given-names>Shuzhen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8917-8352</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-170540"><name><surname>Wang</surname><given-names>Lei</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6224-6474</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-170545"><name><surname>Shin</surname><given-names>Jung Han</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8237-2144</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-170542"><name><surname>Cai</surname><given-names>Yudian</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-184857"><name><surname>Zhang</surname><given-names>Boqiang</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-78114"><name><surname>Lee</surname><given-names>Sang Wan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6266-9613</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-104849"><name><surname>Appiah</surname><given-names>Kofi</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-170544"><name><surname>Zhou</surname><given-names>Yong-di</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-104057"><name><surname>Kwok</surname><given-names>Sze Chai</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7439-1193</contrib-id><email>sze-chai.kwok@st-hughs.oxon.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><aff id="aff1"><label>1</label><institution>Shanghai Key Laboratory of Brain Functional Genomics, Key Laboratory of Brain Functional Genomics Ministry of Education, School of Psychology and Cognitive Science, East China Normal University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>Program of Brain and Cognitive Engineering, Korea Advanced Institute of Science and Technology</institution><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff3"><label>3</label><institution>Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology</institution><addr-line><named-content content-type="city">Daejeon</named-content></addr-line><country>Republic of Korea</country></aff><aff id="aff4"><label>4</label><institution>Department of Computer Science, University of York</institution><addr-line><named-content content-type="city">York</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution>School of Psychology, Shenzhen University</institution><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution>Shanghai Key Laboratory of Magnetic Resonance, East China Normal University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff7"><label>7</label><institution>NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution>University of Toronto</institution><country>Canada</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>20</day><month>04</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e54519</elocation-id><history><date date-type="received" iso-8601-date="2019-12-17"><day>17</day><month>12</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-04-20"><day>20</day><month>04</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Zuo et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Zuo et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-54519-v2.pdf"/><abstract><p>Humans recall the past by replaying fragments of events temporally. Here, we demonstrate a similar effect in macaques. We trained six rhesus monkeys with a temporal-order judgement (TOJ) task and collected 5000 TOJ trials. In each trial, the monkeys watched a naturalistic video of about 10 s comprising two across-context clips, and after a 2 s delay, performed TOJ between two frames from the video. The data are suggestive of a non-linear, time-compressed forward memory replay mechanism in the macaque. In contrast with humans, such compression of replay is, however, not sophisticated enough to allow these monkeys to skip over irrelevant information by compressing the encoded video globally. We also reveal that the monkeys detect event contextual boundaries, and that such detection facilitates recall by increasing the rate of information accumulation. Demonstration of a time-compressed, forward replay-like pattern in the macaque provides insights into the evolution of episodic memory in our lineage.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>drift diffusion model framework</kwd><kwd>event boundary detection</kwd><kwd>forward replay-like pattern</kwd><kwd>naturalistic material</kwd><kwd>time compression of memory traces</kwd><kwd>temporal order judgement</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012166</institution-id><institution>973 Program</institution></institution-wrap></funding-source><award-id>2013CB329501</award-id><principal-award-recipient><name><surname>Zhou</surname><given-names>Yong-di</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002338</institution-id><institution>Ministry of Education of the People's Republic of China</institution></institution-wrap></funding-source><award-id>Humanities and Social Sciences 16YJC190006</award-id><principal-award-recipient><name><surname>Kwok</surname><given-names>Sze Chai</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Macaque monkeys temporally compress past experiences and use a forward-replay mechanism during judgment of temporal-order between episodes.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Accumulating evidence indicates that non-human primates possess the ability to remember temporal relationships among events (<xref ref-type="bibr" rid="bib48">Templer and Hampton, 2013</xref>; <xref ref-type="bibr" rid="bib13">Gower, 1992</xref>; <xref ref-type="bibr" rid="bib5">Charles et al., 2004</xref>). The apes can remember movies based on the temporal order of scenes (<xref ref-type="bibr" rid="bib33">Morimura and Matsuzawa, 2001</xref>) and keep track of the past time of episodes (<xref ref-type="bibr" rid="bib30">Martin-Ordas et al., 2010</xref>), whereas macaque monkeys possess serial (<xref ref-type="bibr" rid="bib49">Terrace et al., 2003</xref>) and ordinal positions expertise for multi-item lists (<xref ref-type="bibr" rid="bib6">Chen et al., 1997</xref>) and are able to categorize sequences of fractal images by their ordinal number (<xref ref-type="bibr" rid="bib39">Orlov et al., 2000</xref>). However, keeping track of and remembering the positional coding, and forming associative chaining (<xref ref-type="bibr" rid="bib47">Templer et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Long and Kahana, 2019</xref>) of lists of arbitrary items might diverge from how a semantically linked, temporally relational representation of real-life events is maintained and utilized.</p><p>In the human literature, it has been shown that episodes can be replayed sequentially on the basis of learned structures (<xref ref-type="bibr" rid="bib25">Liu et al., 2019</xref>), sensory information (<xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>), and pictorial content (<xref ref-type="bibr" rid="bib52">Wimmer et al., 2019</xref>). These findings suggest the possibility that monkeys can rely on a similar mechanism in recalling events that are linked temporally during temporal order judgement (TOJ). However, the extent to which mechanisms of temporal order judgement overlap across humans and monkeys remains undefined. One hypothesis is that the macaques can similarly rely on a scanning model for information retrieval – akin to serial replay of episodes – to perform temporal order judgements (<xref ref-type="bibr" rid="bib13">Gower, 1992</xref>; <xref ref-type="bibr" rid="bib5">Charles et al., 2004</xref>). In this model, after encoding streams of events, the animal performs retrieval by replaying the stream of information in a forward direction. In this way, retrieval time (RT) would be positively correlated to the temporal distance between the beginning of the stream and the target location. Recent findings in rodents (<xref ref-type="bibr" rid="bib41">Panoz-Brown et al., 2018</xref>) and in humans of memory replay during cued-recall tasks across fragments of video episodes are characterized, however, by replay that proceeds in a forward manner and is temporally compressed (<xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>). A feature of this more sophisticated mechanism is that memory replay is a fluidic process that allows subjects to skip flexibly across sub-events (<xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>). Subjects can omit non-informative parts of episodes and replay a shorter episode (shorter than physical perception) in memory, which contains less information. This interpretation is supported by other works on the mental simulation of paths (<xref ref-type="bibr" rid="bib3">Bonasia et al., 2016</xref>) and video episodes (<xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>). This latter model constitutes a global compression of parts of episodes (that allows skipping across sub-events) and is regarded as substantially superior to a strict forward-replay mechanism.</p><p>In order to simulate the dynamic flow of information that occurs in real-life scenarios, we used naturalistic videos as experimental material to study the mechanism of memory retrieval of event order in the monkeys. These videos are more realistic than the arbitrary items or images that were used in previous studies (<xref ref-type="bibr" rid="bib48">Templer and Hampton, 2013</xref>; <xref ref-type="bibr" rid="bib34">Naya et al., 2017</xref>). We used a temporal order judgement paradigm to examine whether and to what extent the pattern underlying memory retrieval conforms to a time-compressed, forward-replay mechanism. In each trial, monkeys watched a naturalistic video composed of two clips, and following a 2 s retention delay, made a temporal order judgement to choose the frame that was shown earlier in the video between two frames extracted from that video (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The two frames were either extracted from the same clip or from two different clips of the video. Given that analyses on response latency can provide insights into the extent to which the monkeys’ behavior might conform to the two putative replay models outlined above, we looked into the RT data. By applying representational similarity analyses (RSA), the Linear Approach to Threshold with Ergodic Rate (LATER) model and generalized linear models to the RT data, we examined the presence of replay-like behavioral patterns in the monkeys. Specifically, if monkeys recall the frames by their ordinal positions, this would imply a linear increase in their retrieval times. By contrast, if the memory search entails a complex processing of the content determined by their semantically linked, temporally relational linkage within the cinematic footage, we should observe evidence of some non-linear pattern.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>TOJ task schema and RT results.</title><p>(<bold>A</bold>) In each trial, the monkey watched a video (8–12 s, comprising two 4–6 s video clips), and following a 2 s retention delay, made temporal order judgement between two probe frames extracted from the video. The monkeys were required to choose the frame that was presented earlier in the video for water reward. (<bold>B</bold>) Task performance of six monkeys. Proportion correct for the six monkeys (left); mean reaction times for three trial types (right). Error bars are standard errors of the means over monkeys. *** denotes p&lt;0.001. (<bold>C</bold>) Linear plots of reaction time (RT) for each monkey as a function of chosen frame location, see also <xref ref-type="table" rid="table1">Table 1</xref>. (<bold>D</bold>) Linear plots of RT as a function of chosen frame location for each human participant, see also <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>. In panels (<bold>C</bold>) and (<bold>D</bold>), black lines and orange lines refer to lists of non-primate video clips and primate video clips, respectively (with five repetitions collapsed for monkeys and two repetitions collapsed for human participants). All responses in the within-context condition are shown, with cyan and magenta dots denoting whether the chosen probe frames were extracted from Clip 1 or Clip 2, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Performance of human participants and speed accuracy trade off results.</title><p>(<bold>A</bold>) (Left) Task performance of seven human participants. The proportions of correct responses for the across-context condition are significantly higher than those for the within-context condition (all p&lt;0.001). (Right) Mean reaction times for the three trial types differ from each other, across- vs. within-Clip1 vs. within-Clip2: F (2, 18) = 18.65, p&lt;0.001, and RT is significantly faster in the across-context condition than in the within-context conditions (Clip 1 and Clip 2). Error bars are standard errors of the means over participants. *** denotes p&lt;0.001. (<bold>B</bold>) Speed accuracy trade-off analysis. The monkeys showed a mild numeral increase trend in inverse efficiency score across the four segments but this did not reach statistical significance (left); the humans show a lower inverse efficiency score for the video parts right after a boundary (F (3, 24) = 4.17, p=0.016), with post hoc tests showing significant difference between bars 2 and 3 (right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig1-figsupp1-v2.tif"/></fig></fig-group><p>Our results suggested that macaque monkeys might adopt a time-compressed, replay-like pattern to search within the representation of continuous information. This time-compression characteristic refers to durations of memory replay that are significantly shorter than the length of the videos. We found that while both species recall the video content non-linearly, there is an aspect of discrepancy between the two species in which the monkeys do not compress the cinematic events globally as effectively as in humans, whereas human participants possess an ability to skip irrelevant information within the video. Finally, we revealed that the monkeys can make use of context changes to facilitate memory retrieval, thus increasing their rate of information accumulation in a drift diffusion model framework.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Human-like forward replay in macaques</title><p>All six monkeys learned to perform the temporal order judgement task with dynamic cinematic videos as encoded content (<xref ref-type="fig" rid="fig1">Figure 1B</xref> left and <xref ref-type="video" rid="video1">Video 1</xref>). The six monkeys performed the task with a significantly above chance level with an overall accuracy of 67.9% ± 1.5% (mean ± SD). The human participants performed the task on average at 92.7% ± 1.2% (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>, left). Note that there are two main kinds of TOJ trials: ‘within-context’ and ‘across-context’ trials (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Here, we are first concerned with the response times (RT) data from ‘within-context’ trials, which allow us to examine TOJ mechanisms, whereas RT data from ‘across-context’ trials were used to test for effects arising from context changes (event boundaries), as is discussed in subsequent subsections.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-54519-video1.mp4"><label>Video 1.</label><caption><title>A monkey performing an example trial.</title><p>The monkey performs an across-context trial with a correct response (rewarded with liquid).</p></caption></media><p>We first addressed our main hypothesis by examining changes in RT using only within-context trials. One possibility is that the monkeys perform TOJ by relying on some form of memory replay, recalling the events coded at specific ordinal positions in a serial manner. To test this hypothesis, in each trial we explicitly looked at the relationship between RT and the location of the frame within the video that the monkeys chose (‘chosen frame location’, as indexed by the ordinal frame numbers in the video). Considering a range of nuisance variables that might affect these relationships (see also full GLM results in Figure 6), we ran the linear regression analysis of reciprocal latency as a function of chosen frame location/temporal similarity, while including a range of variables as nuisance regressors for each monkey separately. We found a negative relationship between reciprocal latency and chosen frame location in all monkeys (all p&lt;0.001 <xref ref-type="fig" rid="fig1">; Figure 1C</xref> and <xref ref-type="table" rid="table1">Table 1</xref> upper panel), and between reciprocal latency and temporal similarity (all p&lt;0.001; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref> and <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> upper panel). This suggests that the monkeys are systemically faster in identifying frames that are located earlier in the video. Moreover, we replicated these results when considering only correct trials or only incorrect trials separately (<xref ref-type="table" rid="table1">Table 1</xref> middle/bottom panel for chosen frame location; <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> middle/bottom panels for temporal similarity), suggesting that the putative response latency patterns are not affected by the memory outcome. These patterns of result are also replicated using logarithmically transformed RT data (all p&lt;0.001).</p><p>To address the direction and speed of memory replay, reaction times at retrieval were compared between TOJ frames that are within Clip 1 versus Clip 2. During TOJ retrieval, frames that were presented in Clip 1 (mean reaction time = 1.59 s) were retrieved significantly faster than those that were experienced in Clip 2 (mean reaction time = 1.88 s) (one-tailed <italic>t</italic>5 = −4.533; p<italic>=</italic>0.003; Cohen’s d = −0.54; 95% CI: –infinity to −0.158 s; log(RT): one-tailed <italic>t</italic>5 = −6.473; p=6.558 × e<sup>−4</sup>; Cohen’s d = −0.69; 95% CI: –infinity to −0.114 s). Again, these findings confirm that the replay of the video takes place in a forward direction.</p><p>Moreover, since the latency required to respond to the chosen frames is much smaller than the duration of the videos themselves, the replay of the video must have been conducted at a compressed speed (i.e., memory replay was faster than perception during video-watching). The difference in reaction time between the very first frame and the last frame was averaged at 942 ms (range: 468–1859 ms). This is equivalent to 94.2 ms to scan through each second of the video, and corresponds to a compression factor of 10.61 during replay in these monkeys (compression factor for each monkey: Jupiter = 13.59, Mars = 7.39, Saturn = 14.80, Mercury = 21.37, Uranus = 17.78, Neptune = 5.38, see <xref ref-type="fig" rid="fig1">Figure 1C</xref>). This is comparable to a compression factor of 13.7 observed in humans, corroborating the notion of forward replay and those findings in humans (<xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>).</p><p>The results suggest that the monkeys’ judgments are faster when responding to probe frames that are located in the earlier parts of the videos, and they also illustrate that the search might not follow a linear function. We formally tested for any ‘non-linearity’ of the fit. We began by testing for a linear relationship and progressively moved to higher-order relationships (quadratic, cubic). At each step, we assessed the significance of the new predictor, having accounted for all the variance fitted by the lower-level predictors. We reported the significance of different trends (linear, quadratic, cubic) for the two species (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). This non-linear (quadratic and cubic) relationship in the monkeys is important because it rules out alternative explanations in which the main effects simply result from positional effects. Rather, the non-linear change in slope indicates that other factors are in play, and that the monkeys might group (or parse) the content according to the relational storyline structure (e.g., the two-clip video design), and do not merely recall the frames/items as of their ordinal positions in a fixed linear manner. In comparison, this analysis also highlights differences between human and macaque monkeys. We found that the human data fits best, and only, with the cubic model (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>), suggesting that humans might have treated the video and TOJ differently from the macaques. This difference is reflected in the global compression capability of the human participants (see next section).</p><p>In summary, we found that our monkeys might have performed TOJ of video episodes using a forward search of ordered elements in the mnemonic representation at the time of memory test with a non-linear, time-compressed function.</p><p>We also ran the same sets of analyses on human participant data for comparison between the two species. Showing a completely opposite pattern, regression analyses on human subjects showed a negative relationship between temporal similarity and reciprocal latency for all participants (all p&lt;0.001, one subject with p=0.055, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref> and <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> upper panel). These results imply that the more similar the two frames to be judged, the longer time needed for retrieve temporal order information. There was also no observable reciprocal latency/chosen frame location slope in the human data (if anything, it shows an opposite trend; see <xref ref-type="fig" rid="fig1">Figure 1D</xref> and <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> bottom panel). It is notable that when reciprocal latency as a function of chosen frame location is analyzed in the humans (as shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref> compared with <xref ref-type="fig" rid="fig2">Figure 2A</xref> for monkeys) a very different pattern emerges, suggesting some form of mechanistic discrepancy between the species. We will examine these aspects in detail in the next section.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Moving average analysis based on reciprocal latency and accuracy for monkeys (left panel) and human participants (right panel).</title><p>(<bold>A</bold>) Reciprocal latency for monkeys as a function of chosen frame location for the average of all animals (upper panel) in the within-context condition, with the results for six individual monkeys are shown in the lower panel. The relationship between chosen frame location and RT follows a non-linear pattern. (<bold>B</bold>) Reciprocal latency for human participants as a function of chosen frame location for the average of all human subjects (upper panel) in the within-context condition, with results for individual subjects are shown in the lower panel. In panels (<bold>A</bold>) and (<bold>B</bold>), the shaded region denotes confidence intervals. (<bold>C</bold>) Proportion of correct answers for individual monkeys as a function of target frame location in the within-context condition. (<bold>D</bold>) Proportion of correct answers for individual human subjects as a function of target frame location in the within-context condition. In panels (<bold>C</bold>) and (<bold>D</bold>), the horizontal blue lines denote chance-level accuracy. Blue vertical lines in these plots denote the mean boundary location between Clip 1 and Clip 2 (116<sup>th</sup> frame).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Relationship between temporal similarity and reciprocal latency for within-context trials in (<bold>A</bold>) monkeys and (<bold>B</bold>) humans.</title><p>Reciprocal latency as a function of temporal similarity for the average of all individuals (upper panel) and for each individual (bottom panel). The temporal similarity between two frames is mathematically the inverse of their respective frame locations in the video.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig2-figsupp1-v2.tif"/></fig></fig-group><p>We also ran a sliding-window average analysis to illustrate how accuracy varies as a function of the target frames' location within within-context trials (<xref ref-type="fig" rid="fig2">Figure 2C</xref> for monkeys; <xref ref-type="fig" rid="fig2">Figure 2D</xref> for humans). It is clear that humans can make use of the across-clip boundary to facilitate their TOJ. Interestingly, in paradigms that use discreet images for encoding, the TOJ performance reported in the human literature is distinct from those making use of continuous streaming movie material. Specifically, it has been reported that accuracy and response times are worse in TOJ for items are that separated by a boundary during encoding in the literature (<xref ref-type="bibr" rid="bib11">Ezzyat and Davachi, 2014</xref>; <xref ref-type="bibr" rid="bib15">Heusser et al., 2018</xref>), whereas we show that TOJ performance is better for frames taken from an across-context condition than for frames taken from a within-context condition in both the proportion of correct answers and RTs (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). This might be because remembering ‘discrete segment of events’ is facilitated by making use of the contextual or perceptual cues in the video frames in across-context condition. We will pursue this aspect further in a latter part of this manuscript, in which we focus on contrasting within-context trials with across-context trials (results and discussion related to Figure 5).</p><p>We also noted that the accuracy might vary along the course of the videos. In order to rule out the possibility that the putative RT effects are not driven by a trade-off in accuracy, we segmented each video into four segments (i.e., each clip was segmented into two segments on the basis of target frame location) and calculated the inverse efficiency score [RT (ms)/percentage correct (%)] for each segment for each individual. The monkeys showed a mild numeral increase in inverse efficiency score across the four segments but this trend did not reach statistical significance (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, left panel) [<italic>F</italic> (3, 20) = 0.10, p=0.96], suggesting that the increase in RT towards the end of the video did not contribute to better accuracy. By contrast, we found that the humans showed a lower inverse efficiency score for the video parts immediately after a boundary (bars 2 and 3 in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, right panel) [<italic>F</italic> (3, 24) = 4.17, p=0.016, a post hoc test shows significant difference between bar 2 and bar 3). This pattern of boundary effect aligns with the little ‘blip’ in proportion correct that occurs shortly after the beginning of Clip 2 in the humans (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). These characteristics might be related to their ability to detect the boundaries.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>One sample t-tests of the slopes of reciprocal latency as a function of chosen frame location for each monkey after having entered a range of nuisance variables as regressor-of-no-interest (see also <xref ref-type="fig" rid="fig6">Figure 6</xref>).</title><p>The three panels correspond to analyses performed using all trials (top), only correct trials (middle), and only incorrect trials (bottom). The same slope patterns were observed irrespective of response correctness.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Monkeys</th><th>Beta</th><th>SEM</th><th>t-statistics</th><th><italic>p</italic>-value</th><th colspan="2">95% confidence interval <break/>Lower Upper</th></tr></thead><tbody><tr><td colspan="7" valign="top">Slope of reciprocal latency/chosen frame location tested against zero (all trials)</td></tr><tr><td>Jupiter</td><td>–0.203</td><td>0.021</td><td>–9.751</td><td>&lt;0.001</td><td>–0.244</td><td>–0.163</td></tr><tr><td>Mars</td><td>–0.369</td><td>0.025</td><td>–14.950</td><td>&lt;0.001</td><td>–0.417</td><td>–0.320</td></tr><tr><td>Saturn</td><td>–0.157</td><td>0.027</td><td>–5.810</td><td>&lt;0.001</td><td>–0.210</td><td>–0.104</td></tr><tr><td>Mercury</td><td>–0.207</td><td>0.052</td><td>–3.958</td><td>&lt;0.001</td><td>–0.309</td><td>–0.104</td></tr><tr><td>Uranus</td><td>–0.164</td><td>0.022</td><td>–7.595</td><td>&lt;0.001</td><td>–0.207</td><td>–0.122</td></tr><tr><td>Neptune</td><td>–0.197</td><td>0.031</td><td>–6.361</td><td>&lt;0.001</td><td>–0.257</td><td>–0.136</td></tr><tr><td colspan="7" valign="top">Slope of reciprocal latency/chosen frame location tested against zero (correct trials)</td></tr><tr><td>Jupiter</td><td>–0.185</td><td>0.025</td><td>–7.393</td><td>&lt;0.001</td><td>–0.234</td><td>–0.136</td></tr><tr><td>Mars</td><td>–0.272</td><td>0.028</td><td>–9.879</td><td>&lt;0.001</td><td>–0.326</td><td>–0.218</td></tr><tr><td>Saturn</td><td>–0.092</td><td>0.032</td><td>–2.857</td><td>0.004</td><td>–0.155</td><td>–0.029</td></tr><tr><td>Mercury</td><td>–0.246</td><td>0.065</td><td>–3.777</td><td>&lt;0.001</td><td>–0.374</td><td>–0.118</td></tr><tr><td>Uranus</td><td>–0.153</td><td>0.024</td><td>–6.259</td><td>&lt;0.001</td><td>–0.201</td><td>–0.105</td></tr><tr><td>Neptune</td><td>–0.150</td><td>0.039</td><td>–3.858</td><td>&lt;0.001</td><td>–0.226</td><td>–0.074</td></tr><tr><td colspan="7" valign="top">Slope of reciprocal latency/chosen frame location tested against zero (Incorrect trials)</td></tr><tr><td>Jupiter</td><td>–0.175</td><td>0.027</td><td>–6.619</td><td>&lt;0.001</td><td>–0.227</td><td>–0.123</td></tr><tr><td>Mars</td><td>–0.366</td><td>0.031</td><td>–11.705</td><td>&lt;0.001</td><td>–0.428</td><td>–0.305</td></tr><tr><td>Saturn</td><td>–0.191</td><td>0.035</td><td>–5.386</td><td>0.002</td><td>–0.261</td><td>–0.122</td></tr><tr><td>Mercury</td><td>–0.075</td><td>0.077</td><td>–0.975</td><td>0.330</td><td>–0.227</td><td>0.076</td></tr><tr><td>Uranus</td><td>–0.140</td><td>0.029</td><td>–4.816</td><td>&lt;0.001</td><td>–0.197</td><td>–0.083</td></tr><tr><td>Neptune</td><td>–0.209</td><td>0.041</td><td>–5.148</td><td>&lt;0.001</td><td>–0.288</td><td>–0.129</td></tr></tbody></table></table-wrap></sec><sec id="s2-2"><title>Discrepancy with humans: compression of replay is local but not global</title><p>It has been shown in the humans that memory replay is not a straightforward recapitulation of the original experience. Subjects can skip through their memories, on a faster time scale across segments of a video episode than within-segment, by skipping flexibly over salient elements such as video boundaries within episodes (<xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>). We propose two possible models with respect to whether the compression is global or not over the whole video. If there is a global compression of the video during replay, the time to initiate replay of Clip 2 would be sooner than the endpoint of replay for Clip 1, as the animal would be able to skip over the whole of Clip 1 to the beginning of Clip 2 (Global-compression model, <xref ref-type="fig" rid="fig3">Figure 3A</xref>, right panel). However, if the monkeys are not equipped with the ability to skip video segments during the replay process, we would expect a linear increase of retrieval time with chosen frame location, irrespective of the boundary (Strict forward model, <xref ref-type="fig" rid="fig3">Figure 3A</xref> left). We tested statistically whether the time to initiate replay of Clip 2 was shorter than the duration of Clip 1.</p><p>We divided each video into eight equal segments and computed cross-correlations derived from pairs of averaged condition-wise RTs based on chosen frame locations using a representational similarity analysis (see section 'Representational similarity analysis (RSA)' in 'Materials and methods' for details). The RT for TOJ between each segment of the video increases linearly according to their position in encoding (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left). We tested these against a hypothetical ‘Strict forward’ model and found significant correlation with the Strict forward model (<italic>r</italic> = 0.66, p=0.009), but not with the Global compression model (<italic>r</italic> = −0.16, p=0.802) (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left). These statistics also remain significant for the Strict forward model when we divided the video into either 10 (p=0.030) or 14 equal segments (p=0.020). The same patterns are also obtained when considering correct trials (Strict forward model: <italic>r</italic> = 0.37, p=0.040; Global compression model: <italic>r</italic> = −0.02, p=0.545) or incorrect trials (Strict forward model: <italic>r</italic> = 0.45, p=0.010; Global compression model: <italic>r</italic> = −0.06, p=0.545) separately. Contrarily, these correlational patterns with the Strict forward model are not observed in the human subjects (<italic>r</italic> = −0.11, p=0.703), but rather we observe a trend favoring the global compression model instead (<italic>r</italic> = 0.39, p=0.069) (<xref ref-type="fig" rid="fig3">Figure 3B</xref> right). When contrasting the the two models using pairwise comparisons, the two models are both statistically significant for monkeys (p&lt;0.01) and humans (p&lt;0.05), confirming the significance of the winning model.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Model comparison using representative similarity analysis.</title><p>(<bold>A</bold>) Visualization of two candidate models as representational dissimilarity matrices (RDMs). Patterns of reaction time (rank-transformed values) as a function of chosen frame location for the the two hypothetical models. The colors of Clip 1 and Clip 2 evolve increasingly with the temporal progression of the video (left), and their respective hypothetical RDMs (right). The reduction in RT (indicated by an arrow) between Clip 1 and Clip 2 is defined as ‘offset’; the magnitude of such an ‘offset’ is arbitrary (but see further analysis in <xref ref-type="fig" rid="fig4">Figure 4</xref>). (<bold>B</bold>) We segmented the videos into eight equal segments, and the RDMs show pairwise Euclidean distances between these different segments for the species group average (monkeys: left; humans: right) and for each individual separately (monkeys: left bottom; humans: right bottom). RDM correlation tests between behavioral RDMs and two candidate RDMs show that the monkeys replay the footage using a Strict forward strategy (<italic>r</italic> = 0.66, p=0.009), and provide little evidence for the Global compression strategy (<italic>r</italic> = −0.16, p=0.802). Humans show an opposite pattern from the macaques. In the humans, the Global compression model shows a higher correlation with behavioral RDM (marginally insignificant <italic>r</italic> = 0.39, p=0.069) than with the Strict forward model (<italic>r</italic> = −0.11, p=0.703). Pairwise comparisons show that the two models are both statistically significant for monkeys (p&lt;0.01) and humans (p&lt;0.05). Error bars indicate the standard errors of the means based on 100 iterations of randomization. <italic>P</italic> values are FDR-corrected (***, p&lt;0.001, **, p&lt;0.01, *, p&lt;0.05).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig3-v2.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The Strict forward model provides a better fit to the RT data in monkeys but not in humans.</title><p>(<bold>A</bold>) ‘Offsets’ are defined as the magnitude of reduced RT when the frames were in Clip 2. 11 hypothetical models with their reaction time patterns (top) and RDMs (bottom). We systemically varied the ‘offset’ parameter while keeping a constant slope. These models progressively range from an absolute Global compression model (model 1, most left) to a Strict-forward model (model 6, middle), and beyond (7<sup>th</sup> to 11<sup>th</sup> models, right). The numerals below the RDMs denote the magnitude of the respective offsets. (<bold>B</bold>) Each monkey’s data were tested against each of these 11 hypothetical models. The Spearman correlations increase as a function of offset magnitude between Clip 1 and Clip 2 until reaching an asymptote when the offset value is around zero, which corresponds to the Strict forward model (model 6 in panel (<bold>A</bold>); see also <xref ref-type="fig" rid="fig3">Figure 3</xref>). Individuals’ RT RDMs are shown in insets. (<bold>C</bold>) Each human participant’s data were also tested against each of these 11 hypothetical models. The Spearman correlations decrease as a function of offset magnitude between Clip 1 and Clip 2 until reaching an asymptote when the offset value is around zero. This analysis confirms the hypothetical discrepancy between the two species (see also <xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig4-v2.tif"/></fig><p>To control for the confound that the two species might be differentially susceptible to the effect of having different numbers of trials in the experiment, we performed a control analysis that made use of only the first two repetitions of data to equate the number of trials between the two species. With these equated subsets of data, we re-calculated the correlation between monkey RT RDM and the two hypothetical models (i.e., the Strict forward model vs the Global compression model). The results showed a significant correlation with the Strict forward model (r = 0.56, p=0.049), but not with the Global compression model (r = −0.10, p=0.643) in the monkeys. With this smaller set of data, we also directly compared the correlations between the two models for the monkeys and found significant differences between them (p&lt;0.05, FDR-corrected). These results replicated the findings observed when the full set of data were used, suggesting that more numerous exposures to the same stimuli did not affect the main results.</p></sec><sec id="s2-3"><title>Factors modulating the model: ‘offsets’ for search and memory-search RT slope</title><p>We defined the reduced RT to initiate replay for Clip 2 as ‘offsets’ in initiating search in Clip 2 by skipping the non-informative Clip 1 (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). With respect to the detailed differences between the two models, one may wonder whether and how the ‘offsets’ between Clip 1 and Clip 2 might influence the results. Especially for the Global compression model, changes of this parameter will cause changes in the RDMs. To address this concern, we simulated an array of RDMs by systemically varying the offset parameter and produced 11 hypothetical models, ranging from an absolute Global compression model (model 1, most left in <xref ref-type="fig" rid="fig4">Figure 4A</xref>) to a Strict-forward model (model 6, middle in <xref ref-type="fig" rid="fig4">Figure 4A</xref>), and beyond (7<sup>th</sup> to 11<sup>th</sup> models, right in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). We then tested each individual monkeys’ data with each of these 11 models. The results show that the Spearman correlation values between the monkey’s data and hypothetical RDMs reach an asymptote of around <italic>r</italic> = 0.8 as the offset parameter tends to zero, and notably, that the correlation values only improve minimally with increasing offsets (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, with each individual’s RT RDM displayed as an inset). These results suggest that the monkeys have processed the video as a holistic chunk of information rather than taking advantage of skipping the non-informative first clip when the two probe frames were in Clip 2. For comparison, we also tested human participant’s data against each of these 11 hypothetical models and found a completely opposite pattern in the humans (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, with each individual’s RT RDM displayed as an inset). Taken together, we reveal a discrepancy between human and macaque performance in terms of their ability to compress past (irrelevant) information during TOJ.</p></sec><sec id="s2-4"><title>Context changes (event boundaries) increase the rate of rise in decision information</title><p>Thus far, we have focused on how the monkeys retrieve the order of frames when information was equated within contexts, but how contextual changes might aid TOJ processes remains to be examined. It was evident that the monkeys retrieved the temporal order of frames with numerally different speeds for the three trial-types: across-Clip 1 and Clip 2 vs. within-Clip1 vs. within-Clip2[<italic>F </italic>(2, 15) = 2.32, p=0.132 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, right)]. Thus, we then compared the latency distribution of within-context and across-context conditions. and we hypothesized that a context shift would change the rate of rise of information accumulation (shift model) without altering the decision threshold (swivel model) within the Linear Approach to Threshold with Ergodic Rate (LATER) model (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). We compared across-context and within-context trials specifically and fitted the two types of LATER models to each monkey’s data separately [<xref ref-type="fig" rid="fig5">Figure 5B</xref>, see section 'LATER (linear approach to threshold with ergodic rate) modelling' in 'Materials and methods'], together with a ‘two fits’ model, which supposes that the reaction times for the two conditions are independent of each other, and a null model, which assumes that there is no effect of manipulation. Using the Bayesian information criterion (BIC) as an index of model comparison, the results consistently indicate that the shift model is better than the swivel model for all six monkeys [range of ΔBIC = (14.57, 300.07); <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>, see section 'Model comparison' in 'Materials and methods'). These results further indicate that contextual changes do not alter the judgement threshold for decisions (providing no evidence for a swivel pattern). By contrast, this pattern is not seen in the human participants (<xref ref-type="fig" rid="fig5">Figure 5C</xref> and <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>), suggesting that the two species might not treat the information given by the event boundary during TOJ in the same manner. Within a drift diffusion model framework, the results suggest that monkeys accumulate information for memory decisions at a faster rate when the frames were extracted from two different clips than when the frames were extracted from the same-context clip.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>LATER model fitting of RT in across-context and within-context conditions for both species.</title><p>(<bold>A</bold>) Cartoon of the LATER model cartoon illustrating that a decision signal triggered by a stimulus rises from its start level, at a rate of information accumulation <italic>r</italic>, to the threshold. Once it reaches the threshold, a decision is initiated. The rate of rise <italic>r</italic> varies from trial to trial, obeying the Gaussian distribution (variation denoted as green shaded area). (<bold>B</bold>) Contextual change effect on the distribution of response latency for the monkeys; data from Monkey ‘Mars’ was chosen for larger display. (<bold>C</bold>) Contextual change effect on the distribution of response latency for humans; data from Subject 1 was chosen for larger display. The red and blue dashed lines show the best fits (maximum likelihood) of across-context trials and within-context trials, respectively (see also <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig5-v2.tif"/></fig></sec><sec id="s2-5"><title>Confirmatory GLMs and control analyses for the putative patterns</title><p>To verify whether the effects are attributed to basic stimulus features such as the perceptual differences inherent in the across-context condition. We then generated several generalized linear models to quantify the effect sizes of several principal variables (see section 'Generalized linear models (GLM)' in 'Materials and methods'). In the within-context condition, given that the monkeys would replay their experience to judge the relative temporal order of probe frames (‘replay hypothesis’), we used the temporal characteristics of probe frames, as represented by chosen frame location (or temporal similarity, which is essentially an inverse of frame location) as the independent variables. In the across-context condition, we included a perceptual similarity measure, which was based on feature points extracted by the SURF algorithm (SURF similarity, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C</xref>), in the GLM to reflect the extent to which the monkeys were able to capitalize on using contextual boundaries for TOJ judgment. In addition, we also entered a number of independent variables as regressors: a binary regressor indicating whether the video includes primate content or not, a binary regressor indicating that a video is played forward or backward, the five repetitions (or two repetitions for humans) of the video-trials, physical location of the selected probe (left or right), time elapsed within a session, chosen frame location, temporal similarity, perceptual similarity (SURF), temporal distance, and response of the subjects (correct/incorrect).</p><p>The within-context GLM shows that monkeys' RT was indeed significantly faster when the probe frame was located earlier in the video, p&lt;0.001 (or in equivalent terms, when two frames were temporally closer, p=0.004), confirming our main finding that the monkeys might have adopted a forward scanning strategy for information retrieval (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, left). By contrast, the across-context GLM shows that there was no significant effect of the chosen frame location on RT. Rather, the monkeys retrieve their memories significantly faster for probe frames that are contextually (or perceptually) distinct, p=0.004 (<xref ref-type="fig" rid="fig6">Figure 6A</xref> , right). Human results are shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref> for comparison.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Full GLM analysis including a number of variables that might affect reciprocal latency separately for within-context and across-context conditions.</title><p>(<bold>A</bold>) Monkey data. (<bold>B</bold>) Human data. We included ten regressors, namely, a binary regressor indicating whether the video category is primate or non-primate (video category), a binary regressor indicating that a video is played forward or backward (play order), the repeated exposure of the trial (Monkey: 1–5; Human: 1–2) (exposure), the physical location of the selected probe on screen (left or right) (touch side), time elapsed within a session (elapsed time; to rule out fatigue or attentional confounds), chosen frame location, temporal similarity, SURF similarity as a perceptual similarity measure (perceptual similarity), temporal distance between two probe frames, and response of subjects (correct/incorrect). In the monkeys, the results confirm that chosen frame location is the most significant regressor in within-context trials, whereas perceptual similarity is the most significant regressor in across-context trials. ***, p&lt;0.001, **, p&lt;0.01, *, p&lt;0.05.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>GLM results on the effects of image similarity measures on reciprocal latency for within-context and across-context conditions for (<bold>A</bold>) the monkeys and (<bold>B</bold>) humans, and (<bold>C</bold>) an example of SURF similarity.</title><p>Among the several indices (difference of distribution in RGB-histogram, Histogram of Oriented Gradients (HOG) similarity and SURF similarity), the SURF similarity measure was significantly correlated with reciprocal latency in the across-context condition in both species (monkeys: p=0.0015; humans: p&lt;0.001). Higher perceptual dissimilarity leads to shorter RT. In panel (<bold>C</bold>), SURF uses various scales and orientations to identify unique features or key-points in an image. The cartoon illustrates how features from two images can be matched irrespective of their scale. Thus, if the same feature exits in another image that is smaller/larger in size or even at different orientations, SURF can still identify that feature (or key-point) as corresponding or similar in both images. We can see that features of the inset are matched to the T-shirt on the basis of how strongly they are related, but some feature points (mostly a minority) may still incorrectly match other parts of the image.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig6-figsupp1-v2.tif"/><permissions><copyright-statement>© 2011 Chuck Lorre Productions and Warner Bros. Television. All rights reserved</copyright-statement><copyright-year>2011</copyright-year><copyright-holder>Chuck Lorre Productions and Warner Bros. Television</copyright-holder><license><license-p>The still image in panel (C) is taken from 'The Big Bang Theory', in the episode titled 'The Agreement Dissection', with permission. It is not covered by the CC-BY 4.0 license and further reproduction of this panel would need permission from the copyright holder.</license-p></license></permissions></fig></fig-group><p>Finally, considering that the monkeys performed a larger number of trials than the human participants (5000 vs. 2000 trials), we plotted the accuracy data for each testing day for each monkey (and for humans too, <xref ref-type="fig" rid="fig7">Figure 7</xref>) and observed no apparent increase in performance over the course of the 50 testing days. This absence of performance change could also be due to the extensive training that the monkeys had received prior to this experiment (~21,150 trials in total for each monkey across 141 sessions on average, see <xref ref-type="table" rid="table2">Table 2</xref>). We therefore deem the main effects reported here as unlikely to be attributable to behavioral or strategic changes over the course of main experiment.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Sessional accuracy data expressed as proportion correct for each individual.</title><p>(<bold>A</bold>) Monkey data. (<bold>B</bold>) Human data. No obvious increase in performance was observed over the course of testing days in the experiment for either monkeys or humans.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54519-fig7-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In light of recent reports on the neural correlates underlying how humans and rodents replay their past experiences (<xref ref-type="bibr" rid="bib25">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Panoz-Brown et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Davidson et al., 2009</xref>), here, we demonstrate parallel behavioral findings in macaque monkeys looking at dynamic cinematic material. Previous reports of macaques succeeding in TOJ indicated their ability to remember the order of events (<xref ref-type="bibr" rid="bib48">Templer and Hampton, 2013</xref>; <xref ref-type="bibr" rid="bib30">Martin-Ordas et al., 2010</xref>; <xref ref-type="bibr" rid="bib35">Ninokura et al., 2003</xref>) and even to monitor the quality of representations of temporal relations among item images meta-cognitively (<xref ref-type="bibr" rid="bib46">Templer et al., 2018</xref>). For example, <xref ref-type="bibr" rid="bib39">Orlov et al., 2000</xref> suggested that monkeys can categorize stimuli by their ordinal number to aid recall of order, and <xref ref-type="bibr" rid="bib48">Templer and Hampton, 2013</xref> showed that monkeys retrieve the temporal order information on the basis of the order of events rather than elapsed time.</p><p>One possible common mechanism underlying these performances is that monkeys use a forward search to identify targets in memory representation (<xref ref-type="bibr" rid="bib13">Gower, 1992</xref>). Taking advantage of the latency data obtained during TOJ on naturalistic materials, we provide new behavioral evidence in support of the hypothesis proposed by <xref ref-type="bibr" rid="bib13">Gower, 1992</xref> that the monkeys can replay their memory in a serial forward manner. Our analysis further clarifies that this replay process is conducted in a time-compressed manner. Notwithstanding task differences, both humans and macaques execute retrieval with forward replay with a comparable compression factor (factors of ~11 in macaques vs. ~13 in humans, <xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>) (but see also <xref ref-type="bibr" rid="bib52">Wimmer et al., 2019</xref>). Another cross-species similarity rests on the observation that the RT patterns are independent of retrieval success. We have previously showed in analogous TOJ paradigms in humans that TOJ task-specific BOLD activation and behavioral RT patterns are independent of retrieval accuracy (<xref ref-type="bibr" rid="bib20">Kwok et al., 2012</xref>; <xref ref-type="bibr" rid="bib21">Kwok and Macaluso, 2015a</xref>). We interpreted these effects as process-based rather than content-based. At present, the latency results probably also point towards some ‘search’ or replay processes, and any ultimate incorrect responses are thus likely to be caused by memory noises injected during encoding and/or during delay maintenance.</p><p>Despite the cross-species similarity, our revelation of a critical discrepancy between humans and macaques carries an important theoretical implication: humans can do both local and global compression, whereas monkeys are not able to attain global compression. The implication is that mental time travel is not all-or-none. There could be multiple layers underpinning the concept of mental time travel, which entail the ability to relive the past (<xref ref-type="bibr" rid="bib45">Suddendorf et al., 2009</xref>; <xref ref-type="bibr" rid="bib50">Tulving, 1985</xref>) and skipping over unimportant information (<xref ref-type="bibr" rid="bib32">Michelmann et al., 2019</xref>). The latter aspect allows humans to recall our memories flexibly far into our past and suggest powerful computational efficiencies that may facilitate memory storage and recall. By contrast, we did not find any evidence in the macaques of an ability to make use of salient boundary cues to skip unimportant details. It thus remains unknown how far back in time monkeys can replay their memories. It has been shown recently that humans can spontaneously replay experience on the basis of learned structures, with fast structural generalization to new experiences facilitated by representing structural information in a format that is independent of its sensory consequences (<xref ref-type="bibr" rid="bib25">Liu et al., 2019</xref>). The lack of global compression in the monkeys of their video experience implies that the monkeys might not be able to use factorized representations to allow components to be recombined in more ways than were experienced (<xref ref-type="bibr" rid="bib2">Behrens et al., 2018</xref>). However, by establishing that neither the number of intervening frames nor the passage of time per se determines the RT pattern (probably a mixed effect resultant from a combination of both), we ruled out order or positional memory as the underlying mechanism supporting TOJ in this task.</p><p>Although we have argued for the presence of replay-like patterns in the monkey during TOJ, we are aware that replay is a neural phenomenon supported by the activity of individual neurons and that implicates the offline reactivation of sequences of hippocampal place cells that reflect past and future trajectories (<xref ref-type="bibr" rid="bib42">Pfeiffer and Foster, 2013</xref>; <xref ref-type="bibr" rid="bib16">Jadhav et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Ólafsdóttir et al., 2018</xref>). On the basis of the behavioral data presented here, the question of where the putative replay may be occurring anatomically is important for the broader field. On the one hand, in rodent research, replay has been linked to sharp wave-ripples in the hippocampal formation (<xref ref-type="bibr" rid="bib24">Lee and Wilson, 2002</xref>; <xref ref-type="bibr" rid="bib12">Foster and Wilson, 2006</xref>). On the other hand, recent MEG studies on humans' replay have implicated several cortical areas such as the occipito-parietal cortex (<xref ref-type="bibr" rid="bib31">Michelmann et al., 2016</xref>), the vmPFC (<xref ref-type="bibr" rid="bib25">Liu et al., 2019</xref>), and the visual cortex (<xref ref-type="bibr" rid="bib52">Wimmer et al., 2019</xref>), in addition to the MTL including the hippocampus (<xref ref-type="bibr" rid="bib25">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Wimmer et al., 2019</xref>). These observations, made under simultaneous whole-brain recordings in humans, are in line with the idea that replay may be coordinated between the hippocampus and neocortical areas (<xref ref-type="bibr" rid="bib17">Ji and Wilson, 2007</xref>).</p><p>A further caveat is that although most of the rodents studies on replay focus on spontaneous replay patterns at rest (<xref ref-type="bibr" rid="bib25">Liu et al., 2019</xref>), during sleep (<xref ref-type="bibr" rid="bib24">Lee and Wilson, 2002</xref>; <xref ref-type="bibr" rid="bib27">Louie and Wilson, 2001</xref>) or during a task-free state (<xref ref-type="bibr" rid="bib12">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib18">Karlsson and Frank, 2009</xref>), our monkeys perform their replay-like recall of the videos as an effortful operation to solve a TOJ task. This is more akin to studies using stimuli embedded in episodes (<xref ref-type="bibr" rid="bib52">Wimmer et al., 2019</xref>) or short video-episodes (<xref ref-type="bibr" rid="bib31">Michelmann et al., 2016</xref>) for their ecological validity. Our combined results thus constitute a novel connection between various kinds of replay-like behaviors that are shared between rodents and humans, and provide a primate model for anatomical investigation. This cognitive discrepancy should be further elucidated using electrophysiological methods probing into the MTL (<xref ref-type="bibr" rid="bib7">Davidson et al., 2009</xref>; <xref ref-type="bibr" rid="bib12">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib18">Karlsson and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib8">Diba and Buzsáki, 2007</xref>) and the neocortices (<xref ref-type="bibr" rid="bib34">Naya et al., 2017</xref>).</p><p>We observed one further interesting phenomenon here, which is that the monkeys are able to detect contextual changes to facilitate TOJ. We show that the expedited TOJ in across-context condition was facilitated by contextual details, which in turn results in an increased rate of rise of signal towards memory decision in a drift-diffusion process. Humans studies show that contextual changes lead to segmentation of ongoing information (<xref ref-type="bibr" rid="bib28">Magliano et al., 2001</xref>; <xref ref-type="bibr" rid="bib14">Hard et al., 2006</xref>). Our results provide evidence consistent with event segmentation in the macaque monkeys and imply that these monkeys might be capable of parsing the footage using contextual information, akin to what has been shown in humans (<xref ref-type="bibr" rid="bib9">DuBrow and Davachi, 2013</xref>; <xref ref-type="bibr" rid="bib44">Sols et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Ezzyat and Davachi, 2011</xref>; <xref ref-type="bibr" rid="bib22">Kwok and Macaluso, 2015b</xref>) and rodents (<xref ref-type="bibr" rid="bib40">Panoz-Brown et al., 2016</xref>).</p><p>Memory replay is an elaborate mental process and our demonstration of a time-compressed, forward replay-like pattern in the macaque monkeys, together with their primordial rigidity in compressing the experienced past, provides promise for mapping the evolution of episodic memory in our lineage.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><sec id="s4-1-1"><title>Macaque monkeys</title><p>Six male rhesus macaques (<italic>Macaca mulatta</italic>) (5.49 ± 0.5 kg) with a mean age of 3.5 years at the start of testing participated in this study. They were initially housed in a group of 6 in a specially built spacious enclosure (max capacity = 12–16 adults) with enrichment elements such as a swing and climbing structures present until the study began. The monkeys were then housed in pairs during the experimentation period according to their social hierarchy and temperament. They were fed twice a day with portions of 180 g monkey chow and pieces of fruits (8:30am/4:00pm). Water was available ad libitum except on experimental days. They were routinely offered treats such as peanuts, raisins and various kinds of seeds in their home cage for forage purpose. The monkeys were procured from a nationally accredited colony located in the Beijing outskirts, where the monkeys were bred and reared. The animals were thus ecologically naive to the natural wilderness and should not have had any previous encounter with other creatures except humans and their companion. The room in which they are housed is operated with an automated 12:12 (7am/7pm) light-dark cycle and kept within temperate around 18–23°C and humidity of 60–80%.</p></sec><sec id="s4-1-2"><title>Human subjects</title><p>Seven participants (mean age = 19.57 ± 1.13, 6 female) took part in the experiment. The participants were recruited from the undergraduate population in East China Normal University. The participants provided informed consent and were compensated 400 RMB for their time.</p><p>The experimental protocol was approved by the Institutional Animal Care and Use Committee (permission code: M020150902 and M020150902-2018) and the University Committee on Human Research Protection (permission code: HR 023–2017) at East China Normal University. All experimental protocols and animal welfare adhered with the ‘NIH Guidelines for the Care and Use of Laboratory Animals’.</p></sec></sec><sec id="s4-2"><title>Training history and task performance</title><p>There were five stages of training on the TOJ task and the numbers of days per monkey are reported in <xref ref-type="table" rid="table2">Table 2</xref>. With these extended periods of training, the monkeys’ performances were unlikely to change over the course of the main experiment (see also <xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Number of days accrued in each training stage.</title><p>When we trained the two additional monkeys (Uranus and Neptune), we made them skip the 4th and 5th stages entirely. The performance patterns of these two monkeys were not different from those of the initial four.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Monkeys</th><th>Stage 1</th><th>Stage 2</th><th>Stage 3</th><th>Stage 4</th><th>Stage 5</th></tr></thead><tbody><tr><td>Jupiter</td><td>12</td><td>39</td><td>16</td><td>65</td><td>51</td></tr><tr><td>Mars</td><td>29</td><td>7</td><td>15</td><td>48</td><td>42</td></tr><tr><td>Saturn</td><td>25</td><td>42</td><td>31</td><td>64</td><td>95</td></tr><tr><td>Mercury</td><td>13</td><td>38</td><td>17</td><td>61</td><td>49</td></tr><tr><td>Uranus</td><td>14</td><td>17</td><td>11</td><td>-</td><td>-</td></tr><tr><td>Neptune</td><td>13</td><td>21</td><td>11</td><td>-</td><td>-</td></tr></tbody></table></table-wrap></sec><sec id="s4-3"><title>Apparatus and testing cubicle</title><sec id="s4-3-1"><title>Macaque monkeys</title><p>The testing was conducted in an automated test apparatus controlled by two Windows computers (OptiPlex 3020, Dell). The subject sat, head-unrestrained, in a wheeled, specially made Plexiglas monkey chair (29.4 cm × 30.8 cm × 55 cm) fixed in position in front of a 17-inch infrared touch-sensitive screen (An-210W02CM, Shenzhen Anmite Technology Co., Ltd, China) with a refresh rate of 60 Hz. The distance between the subject's head and the screen was kept at ~20 cm. The touch-sensitive screen was mounted firmly on a custom-made metal frame (18.5 cm × 53.2 cm) on a large platform (100 cm × 150 cm × 76 cm). Water reward delivery was controlled by an automated water-delivery rewarding system (5-RLD-D1, Crist Instrument Co., Inc, U.S.) and each delivery was accompanied by an audible click. An infrared camera and video recording system (EZVIZ-C2C, Hangzhou Ezviz Network Co., Ltd, China) allowed the subject to be monitored while it was engaged in the task. The entire apparatus was housed in a sound-proof experimental cubicle that was dark apart from the background illumination from the touch screen.</p></sec><sec id="s4-3-2"><title>Human subjects</title><p>We used the same model of computers and touch-sensitive screens for the human participants. The human subjects sat ~30 cm in front of another identical 17-inch infrared touch-sensitive screen, with stimuli presented with the same computer used in the monkeys’ experiment.</p></sec></sec><sec id="s4-4"><title>Source of video materials and preparation</title><p>A collection of documentary films on wild animals was gathered from YouTube. The films were <italic>Monkey Kingdom</italic> (Disney), <italic>Monkey Planet</italic> (Episode 1–3; BBC), <italic>Planet Earth</italic> (Episode 1–11; BBC), <italic>Life</italic> (Episode 1–10; BBC), and <italic>Snow Monkey</italic> (PBS Nature). In total, 28 hr of footage was gathered. We applied Video Studio X8 (Core Corporation) to parse the footage into smaller segments. Experimenters then applied the following criteria to edit out ~2500 unique clips manually: 1) the clip must contain a continuous flow of depiction of events (i.e., no scene transition); 2) at least one living creature must be included; 3a) at least one of the animals contained must be in obvious motion; 3b) the trajectories of these motions must be unidirectional (i.e., no back and forth motion of the same subject); 4) clips with snakes were discarded. From this library, we then selected 2000 clips (all of 4 s – 6 s) for the final test, and a small number of additional clips were also prepared for the training stages. Each of the monkeys was assigned with a pseudo-randomized remix of a unique 1000 video-trials set from the 2000-video library, thus eliminating video idiosyncrasy associated with any experimental conditions. For each individual monkey, the videos assigned to be in an experimental condition were not used/shown in another condition, so that a particular monkey would only view that video repeatedly under the same condition.</p></sec><sec id="s4-5"><title>Task and experimental procedure</title><p>We combined naturalistic material with a TOJ paradigm that is widely used in episodic memory research (<xref ref-type="bibr" rid="bib48">Templer and Hampton, 2013</xref>; <xref ref-type="bibr" rid="bib29">Manns et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Kwok and Macaluso, 2015c</xref>). In each trial, the monkey initiated a trial by pressing a colored rectangle in the center of the screen (0.15 ml water). An 8–12 s video (consisting of two 4–6 s clips) was then presented (0.15 ml water), and following a 2 s retention delay, two frames extracted from the video were displayed bilaterally on the screen for TOJ. The monkeys were trained to choose the frame that was shown earlier in the video (see <xref ref-type="video" rid="video1">Video 1</xref>). A touch to the target frame resulted in 1.5 ml water as reward, the incorrect frame was removed, and the target frame remained alone for 5 s as positive feedback. A touch to the incorrect frame removed both frames from the screen and blanked the screen for 20 s without water delivery. As the monkeys could self-start the trials, we did not set an explicit inter-trial interval. Correction trial procedures were not used in the main test.</p><p>We collected 50 daily sessions of data. Each session contained 100 trials, giving us a 5000 trials per monkey. The 5000 trials contained a break-down of four factors: Boundary (Within vs. Across), Play Order (Normal vs. Reverse), Temporal distance (TD, 25 levels), and Exposure (R1–R5), giving out a 2 * 2 * 25 * 5 within-subject design. The two frames for TOJ could be extracted from the same clip (Within) or from distinct clips (Across). TD was an ordinal variable, with 25 levels ranging between a minimum of 1000 ms (equivalent to 25 frames) and a maximum of 3880 ms (equivalent of to frames). Each TD level increased progressively with three frames in each step. The ten lists contained five lists of primate animals and five lists of non-primate animals (Category: Primate/Non-Primate). The six monkeys were counter-balanced in their order of receiving the two kinds of material: three monkeys (Jupiter, Mars, and Saturn) were tested first on Non-Primate lists, whereas the other three (Mercury, Neptune, and Uranus) were tested first on Primate lists. The whole experiment lasted for 68 days. There were three blocks of reset (3 days, 9 days and 6 days) in-between testing days. The task was programmed in PsychoPy2 implemented in Python.</p><p>The same experimental design was adopted by the human participants. They re-used the 6 unique video-trials/list sets and TOJ frames (one unique set per monkey) correspondingly for the human subjects (the extra subject 7 re-used set 1). The only critical difference was that human participants performed only two exposures (R1–R2) rather than five. Identically, each session contained 100 trials, and with 20 daily sessions of testing, we acquired 2000 trials per participant for analysis.</p></sec><sec id="s4-6"><title>Data analysis: modelling, temporal and perceptual similarity, and model comparison</title><p>Trials with RT longer than 10 s (1.45%) or shorter than 0.7 s (2.47%) were excluded from the analyses. Both correct and incorrect trials were entered for all analyses. For Neptune, data from one day of primate list 5 (exposure 4) and from two days of primate lists 4 and 5 (exposure 5) were lost because of machine breakdown and thus only 4700 trials were included for Neptune.</p><sec id="s4-6-1"><title>Representational similarity analysis (RSA)</title><p>To create RDMs for the representational pattern of response time, we evenly divided each video into eight segments on the basis of chosen frame location and averaged the response times within each segment for each monkey individually. For each matrix, we then computed the Euclidean distance of average response time for each pair of the segments. In order to show the relative distance among all pairs of segments intuitionally, we further transformed the matrix by replacing each element with the rank number in the distribution of all the elements, and linearly scaled into [0,1]. To compare similarity between RT patterns and the candidate models, we computed Spearman correlation between the model and group averaged RT RDMs with 100 randomized iterations by bootstrapping. We then used two-sided Wilcoxon signed-rank test to test the null hypothesis that the correlation between data RDM and the two hypothetical models are equal. The statistical threshold was set at p&lt;0.05 (FDR corrected).</p></sec><sec id="s4-6-2"><title>LATER (linear approach to threshold with ergodic rate) modelling</title><p>The observations that the brain needs more time than that requires for nerves to transport information and that trial-by-trial RTs vary considerably have stimulated researchers to make use of distribution of RT to examine mental processes (<xref ref-type="bibr" rid="bib28">Magliano et al., 2001</xref>). Latency, as an indicator of decision processes, provides a source of insight into the underlying decision mechanisms (<xref ref-type="bibr" rid="bib27">Louie and Wilson, 2001</xref>; <xref ref-type="bibr" rid="bib18">Karlsson and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib8">Diba and Buzsáki, 2007</xref>). The Linear Approach to Threshold with Ergodic Rate (LATER) model is a widely used model that taps into these processes (<xref ref-type="bibr" rid="bib37">Noorani and Carpenter, 2016</xref>). The LATER model stipulates that the winner signals that reach threshold faster would trigger the decision, resulting in shorter latency. Accordingly, changing the rate of rise would cause the line to ‘shift’ along the abscissa without changing its slope. By contrast, lines plotted by latency distribution would ‘swivel’ around an intercept when the threshold changes (<xref ref-type="bibr" rid="bib43">Reddi and Carpenter, 2000</xref>; <xref ref-type="fig" rid="fig5">Figure 5A</xref>). To explore the underlying mechanisms of different experiment conditions, we plotted the reciprocal of RT (i.e., a multiplicative inverse of RT, 1/RT) as a function of their z-scores, thus making the distributions follow a Gaussian distribution (<xref ref-type="bibr" rid="bib36">Noorani and Carpenter, 2011</xref>). Hence, we fitted the main component for each condition. The main component is a ramp to the threshold with rate of rise <italic>r</italic>. The distance between start level S<sub>0</sub> and threshold S<sub>T</sub> is defined as <italic>θ</italic>, and the rate of rise <italic>r</italic> follows a Gaussian distribution of mean, <italic>μ</italic>, and standard deviation, <italic>σ1</italic>.</p><p>For model comparison, we fitted four different models to the data. The ‘null’ model fits the RT from within-context vs. across-context conditions with the same parameters, implying no effect of manipulation. The ‘two fits’ model set all the parameters to be free, which supposes that the RTs of the two conditions are independent of each other. The ‘Shift’ model only allows the slope of main component <italic>μ</italic> to change according to different conditions on the assumption that the manipulation will change the rate of rise. The ‘Swivel’ model only allows <italic>θ</italic> to change according to different conditions on the assumption that the subject sets different thresholds for different conditions.</p></sec><sec id="s4-6-3"><title>Generalized linear models (GLM)</title><p>We ran GLM to compare the effect sizes of independent variables on the dependent variable. The mean (<italic>μ</italic>) of the outcome distribution <inline-formula><mml:math id="inf1"><mml:mi mathvariant="normal">Y</mml:mi> <mml:mi mathvariant="normal"/></mml:math></inline-formula> depends on the independent variables <inline-formula><mml:math id="inf2"><mml:mi mathvariant="normal">X</mml:mi></mml:math></inline-formula>, according to the following formula:<disp-formula id="equ1"><mml:math id="m1"><mml:mi mathvariant="normal">E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf3"><mml:mi mathvariant="normal">Y</mml:mi></mml:math></inline-formula> is a distribution of outcomes, β is an unknown parameter to be estimated, and <inline-formula><mml:math id="inf4"><mml:mi>g</mml:mi></mml:math></inline-formula> is a link function (Gaussian function). The dependent variable (Y) is reciprocal latency, and the independent variables are as follows: a binary regressor indicating whether the video includes primate content or not, a binary regressor indicating that a video is played forward or backward, the five repetitions (or two repetitions for humans) of the video-trials, physical location of the selected probe (left or right), time elapsed within session, chosen frame location, temporal similarity, perceptual similarity (SURF), temporal distance, and the response of the subjects (correct/incorrect).</p></sec><sec id="s4-6-4"><title>Model comparison</title><p>To obtain the best fit among these models, we used Bayesian Information Criterion (BIC) as a criterion for model selection among these four models. The formula for BIC is: BIC = −2(logL) + numParam* log(numObs), where L is the maximum likelihood for the model, and numParam and numObs represent the number of free parameters and the number of samples, respectively. We computed ΔBIC as the strength of the evidence, which indicates the extent to which the selected model is superior to other models. Different ranges of ΔBIC show different level of evidence: a value of ΔBIC larger than two shows positive evidence, and a value of ΔBIC larger than six indicates strong evidence (<xref ref-type="bibr" rid="bib19">Kass and Raftery, 1995</xref>).</p></sec><sec id="s4-6-5"><title>Temporal similarity</title><p>For each trial, we calculated temporal similarity (TS) as an index of the discriminability of probe frames. Temporal similarity between two probe frames extracted from the video is calculated by the ratio of the two frames’ temporal separation between their occurrence in the video and the time of testing. The temporal similarity of any two memory traces can be calculated as: TS = delay2/delay1, where delay2 &lt; delay1 (<xref ref-type="bibr" rid="bib4">Brown et al., 2007</xref>).</p></sec><sec id="s4-6-6"><title>Perceptual similarity</title><p>We made use of three main parameters to measure the perceptual dissimilarity between TOJ frames for the GLMs. First, RGB-histogram is computed as the Sum-of Square-Difference (SSD) error between image pairs for the three color channels (RGB). For each color channel, the intensity values range from 0 to 255 (i.e., 256 bins), and we computed the total number of pixels at each intensity value and then the SSD for all 256 bins for each image pair. The smaller the value of the SSD, the more similar the two images (image pair) were. Second, for HOG similarity, we constructed a histogram of directions of gradient over fixed-sized grids across the entire image. A vector is generated from each grid cell and correlated with HOG features from another image. Third, Speeded Up Robust Features (SURF) (<xref ref-type="bibr" rid="bib1">Bay et al., 2008</xref>) uses Box Filter using integral images (<xref ref-type="bibr" rid="bib51">Viola and Jones, 2004</xref>) to approximate Laplacian-of-Gaussian (LoG). Wavelet responses in both horizontal and vertical directions are used to assign orientation in SURF. SURF consists of fixing a reproducible orientation that is based on information from a circular region around the interest point. A descriptor vector is generated around the interest point using the integral image, which matches with descriptor vectors extracted from a compared image. SURF uses various scales and different orientation to identify unique features or key-points in an image. If the same feature exits in another image that is smaller or larger in size or even at a different orientation, SURF identifies that feature (or key-point) as corresponding or similar in both images (<xref ref-type="bibr" rid="bib1">Bay et al., 2008</xref>; see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C</xref> for illustration). The Euclidean distance is used to measure the similarity between two descriptor vectors from images.</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing - original draft, Writing - review and editing, Data interpretation</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Investigation, Methodology, Data interpretation</p></fn><fn fn-type="con" id="con3"><p>Software, Methodology, Data interpretation</p></fn><fn fn-type="con" id="con4"><p>Investigation, Methodology</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Data curation, Investigation</p></fn><fn fn-type="con" id="con6"><p>Methodology, Data interpretation</p></fn><fn fn-type="con" id="con7"><p>Software, Methodology</p></fn><fn fn-type="con" id="con8"><p>Funding acquisition, Data interpretation</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Writing - original draft, Project administration, Writing - review and editing, Data interpretation</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: The experimental protocol was approved by the the University Committee on Human Research Protection (permission code: HR 023-2017) at East China Normal University. The participants provided informed consent.</p></fn><fn fn-type="other" id="fn2"><p>Animal experimentation: The experimental protocol was approved by the Institutional Animal Care and Use Committee (permission code: M020150902 &amp; M020150902-2018) at East China Normal University. All experimental protocols and animal welfare adhered with the &quot;NIH Guidelines for the Care and Use of Laboratory Animals&quot;.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="sdata1"><label>Source data 1.</label><caption><title>Source data for all figures and tables.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-54519-data1-v2.xlsx"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Data description tables used to illustrate all of the key variables contained in the ‘Source Data 1.xlsx’.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-54519-supp1-v2.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>For monkeys: one sample <italic>t</italic>-test results for the slopes of reciprocal latency as a function of temporal similarity, having entered a range of nuisance variables as regressor-of-no-interest.</title><p>The three panels correspond to analyses performed using all trials (top), only correct trials (middle), and only incorrect trials (bottom). The same slope patterns were observed irrespective of correctness, as is consistent with the analyses of slopes of reciprocal latency as a function of chosen frame location for each monkey. Related to <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-54519-supp2-v2.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Hierarchical multiple regression results for individual monkeys (left panel) and for human participants (right panel) showing reaction time as a function of chosen frame location (CFL).</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-54519-supp3-v2.docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>For human participants: one sample <italic>t</italic>-test results for the slopes of reciprocal latency as a function of temporal similarity (upper panel) and the slopes of reciprocal latency as a function of chosen frame location (bottom panel) against zero, after having entered a range of nuisance variables as regressor-of-no-interest.</title><p>Related to <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-54519-supp4-v2.docx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>LATER model fitting results of six monkeys and seven human participants.</title><p>For ease of comparison, we computed the respective ΔBIC to index the strength of evidence for each model. Note that the model with the lowest BIC is the winning model. In all 6 monkeys, the shift model is superior to the other three models, whereas this effect is not consistent in the humans. Related to <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-54519-supp5-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-54519-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data is available at Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.3r2280gcc">https://doi.org/10.5061/dryad.3r2280gcc</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Zuo</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Shin</surname><given-names>JH</given-names></name><name><surname>Cai</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>SW</given-names></name><name><surname>Appiah</surname><given-names>K</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Kwok</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Behavioral evidence for memory replay of video episodes in macaque monkeys</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.3r2280gcc</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bay</surname> <given-names>H</given-names></name><name><surname>Ess</surname> <given-names>A</given-names></name><name><surname>Tuytelaars</surname> <given-names>T</given-names></name><name><surname>Van Gool</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Speeded-Up robust features (SURF)</article-title><source>Computer Vision and Image Understanding</source><volume>110</volume><fpage>346</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2007.09.014</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname> <given-names>TEJ</given-names></name><name><surname>Muller</surname> <given-names>TH</given-names></name><name><surname>Whittington</surname> <given-names>JCR</given-names></name><name><surname>Mark</surname> <given-names>S</given-names></name><name><surname>Baram</surname> <given-names>AB</given-names></name><name><surname>Stachenfeld</surname> <given-names>KL</given-names></name><name><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What is a cognitive map? organizing knowledge for flexible behavior</article-title><source>Neuron</source><volume>100</volume><fpage>490</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.002</pub-id><pub-id pub-id-type="pmid">30359611</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonasia</surname> <given-names>K</given-names></name><name><surname>Blommesteyn</surname> <given-names>J</given-names></name><name><surname>Moscovitch</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Memory and navigation: compression of space varies with route length and turns</article-title><source>Hippocampus</source><volume>26</volume><fpage>9</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1002/hipo.22539</pub-id><pub-id pub-id-type="pmid">26418606</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname> <given-names>GD</given-names></name><name><surname>Neath</surname> <given-names>I</given-names></name><name><surname>Chater</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A temporal ratio model of memory</article-title><source>Psychological Review</source><volume>114</volume><fpage>539</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.114.3.539</pub-id><pub-id pub-id-type="pmid">17638496</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charles</surname> <given-names>DP</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Buckley</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Impaired recency judgments and intact novelty judgments after fornix transection in monkeys</article-title><source>Journal of Neuroscience</source><volume>24</volume><fpage>2037</fpage><lpage>2044</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3796-03.2004</pub-id><pub-id pub-id-type="pmid">14985446</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>S</given-names></name><name><surname>Swartz</surname> <given-names>KB</given-names></name><name><surname>Terrace</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Knowledge of the ordinal position of list items in rhesus monkeys</article-title><source>Psychological Science</source><volume>8</volume><fpage>80</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1997.tb00687.x</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname> <given-names>TJ</given-names></name><name><surname>Kloosterman</surname> <given-names>F</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hippocampal replay of extended experience</article-title><source>Neuron</source><volume>63</volume><fpage>497</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.027</pub-id><pub-id pub-id-type="pmid">19709631</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diba</surname> <given-names>K</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1038/nn1961</pub-id><pub-id pub-id-type="pmid">17828259</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DuBrow</surname> <given-names>S</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The influence of context boundaries on memory for the sequential order of events</article-title><source>Journal of Experimental Psychology: General</source><volume>142</volume><fpage>1277</fpage><lpage>1286</lpage><pub-id pub-id-type="doi">10.1037/a0034024</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ezzyat</surname> <given-names>Y</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What constitutes an episode in episodic memory?</article-title><source>Psychological Science</source><volume>22</volume><fpage>243</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1177/0956797610393742</pub-id><pub-id pub-id-type="pmid">21178116</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ezzyat</surname> <given-names>Y</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Similarity breeds proximity: pattern similarity within and across contexts is related to later mnemonic judgments of temporal proximity</article-title><source>Neuron</source><volume>81</volume><fpage>1179</fpage><lpage>1189</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.042</pub-id><pub-id pub-id-type="pmid">24607235</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>DJ</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title><source>Nature</source><volume>440</volume><fpage>680</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/nature04587</pub-id><pub-id pub-id-type="pmid">16474382</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gower</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Short-term memory for the temporal order of events in monkeys</article-title><source>Behavioural Brain Research</source><volume>52</volume><fpage>99</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/S0166-4328(05)80329-8</pub-id><pub-id pub-id-type="pmid">1472291</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hard</surname> <given-names>BM</given-names></name><name><surname>Tversky</surname> <given-names>B</given-names></name><name><surname>Lang</surname> <given-names>DS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Making sense of abstract events: building event schemas</article-title><source>Memory &amp; Cognition</source><volume>34</volume><fpage>1221</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.3758/BF03193267</pub-id><pub-id pub-id-type="pmid">17225504</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heusser</surname> <given-names>AC</given-names></name><name><surname>Ezzyat</surname> <given-names>Y</given-names></name><name><surname>Shiff</surname> <given-names>I</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Perceptual boundaries cause mnemonic trade-offs between local boundary processing and across-trial associative binding</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>44</volume><fpage>1075</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1037/xlm0000503</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadhav</surname> <given-names>SP</given-names></name><name><surname>Kemere</surname> <given-names>C</given-names></name><name><surname>German</surname> <given-names>PW</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Awake hippocampal sharp-wave ripples support spatial memory</article-title><source>Science</source><volume>336</volume><fpage>1454</fpage><lpage>1458</lpage><pub-id pub-id-type="doi">10.1126/science.1217230</pub-id><pub-id pub-id-type="pmid">22555434</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname> <given-names>D</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Coordinated memory replay in the visual cortex and Hippocampus during sleep</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1038/nn1825</pub-id><pub-id pub-id-type="pmid">17173043</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Awake replay of remote experiences in the Hippocampus</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>913</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1038/nn.2344</pub-id><pub-id pub-id-type="pmid">19525943</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname> <given-names>RE</given-names></name><name><surname>Raftery</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayes factors</article-title><source>Journal of the American Statistical Association</source><volume>90</volume><fpage>773</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwok</surname> <given-names>SC</given-names></name><name><surname>Shallice</surname> <given-names>T</given-names></name><name><surname>Macaluso</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Functional anatomy of temporal organisation and domain-specificity of episodic memory retrieval</article-title><source>Neuropsychologia</source><volume>50</volume><fpage>2943</fpage><lpage>2955</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.07.025</pub-id><pub-id pub-id-type="pmid">22877840</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwok</surname> <given-names>SC</given-names></name><name><surname>Macaluso</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Immediate memory for &quot;when, where and what&quot;: Short-delay retrieval using dynamic naturalistic material</article-title><source>Human Brain Mapping</source><volume>36</volume><fpage>2495</fpage><lpage>2513</lpage><pub-id pub-id-type="doi">10.1002/hbm.22787</pub-id><pub-id pub-id-type="pmid">25773646</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwok</surname> <given-names>SC</given-names></name><name><surname>Macaluso</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Exogenous features versus prior experiences modulate different subregions of the right IPL during episodic memory retrieval</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>11248</elocation-id><pub-id pub-id-type="doi">10.1038/srep11248</pub-id><pub-id pub-id-type="pmid">26057929</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwok</surname> <given-names>SC</given-names></name><name><surname>Macaluso</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015c</year><article-title>Scale invariance of temporal order discrimination using complex, naturalistic events</article-title><source>Cognition</source><volume>140</volume><fpage>111</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2015.04.007</pub-id><pub-id pub-id-type="pmid">25909581</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Memory of sequential experience in the Hippocampus during slow wave sleep</article-title><source>Neuron</source><volume>36</volume><fpage>1183</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01096-6</pub-id><pub-id pub-id-type="pmid">12495631</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name><name><surname>Behrens</surname> <given-names>TEJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human replay spontaneously reorganizes experience</article-title><source>Cell</source><volume>178</volume><fpage>640</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.06.012</pub-id><pub-id pub-id-type="pmid">31280961</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>NM</given-names></name><name><surname>Kahana</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hippocampal contributions to serial-order memory</article-title><source>Hippocampus</source><volume>29</volume><fpage>252</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1002/hipo.23025</pub-id><pub-id pub-id-type="pmid">30178573</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname> <given-names>K</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Temporally Structured Replay of Awake Hippocampal Ensemble Activity during Rapid Eye Movement Sleep</article-title><source>Neuron</source><volume>29</volume><fpage>145</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00186-6</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magliano</surname> <given-names>JP</given-names></name><name><surname>Miller</surname> <given-names>J</given-names></name><name><surname>Zwaan</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Indexing space and time in film understanding</article-title><source>Applied Cognitive Psychology</source><volume>15</volume><fpage>533</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1002/acp.724</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manns</surname> <given-names>JR</given-names></name><name><surname>Howard</surname> <given-names>MW</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Gradual Changes in Hippocampal Activity Support Remembering the Order of Events</article-title><source>Neuron</source><volume>56</volume><fpage>530</fpage><lpage>540</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.08.017</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin-Ordas</surname> <given-names>G</given-names></name><name><surname>Haun</surname> <given-names>D</given-names></name><name><surname>Colmenares</surname> <given-names>F</given-names></name><name><surname>Call</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Keeping track of time: evidence for episodic-like memory in great apes</article-title><source>Animal Cognition</source><volume>13</volume><fpage>331</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1007/s10071-009-0282-4</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michelmann</surname> <given-names>S</given-names></name><name><surname>Bowman</surname> <given-names>H</given-names></name><name><surname>Hanslmayr</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The temporal signature of memories: identification of a general mechanism for dynamic memory replay in humans</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002528</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002528</pub-id><pub-id pub-id-type="pmid">27494601</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michelmann</surname> <given-names>S</given-names></name><name><surname>Staresina</surname> <given-names>BP</given-names></name><name><surname>Bowman</surname> <given-names>H</given-names></name><name><surname>Hanslmayr</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Speed of time-compressed forward replay flexibly changes in human episodic memory</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>143</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0491-4</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morimura</surname> <given-names>N</given-names></name><name><surname>Matsuzawa</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Memory of movies by chimpanzees (Pan Troglodytes)</article-title><source>Journal of Comparative Psychology</source><volume>115</volume><fpage>152</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1037/0735-7036.115.2.152</pub-id><pub-id pub-id-type="pmid">11459162</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naya</surname> <given-names>Y</given-names></name><name><surname>Chen</surname> <given-names>H</given-names></name><name><surname>Yang</surname> <given-names>C</given-names></name><name><surname>Suzuki</surname> <given-names>WA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contributions of primate prefrontal cortex and medial temporal lobe to temporal-order memory</article-title><source>PNAS</source><volume>114</volume><fpage>13555</fpage><lpage>13560</lpage><pub-id pub-id-type="doi">10.1073/pnas.1712711114</pub-id><pub-id pub-id-type="pmid">29192021</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ninokura</surname> <given-names>Y</given-names></name><name><surname>Mushiake</surname> <given-names>H</given-names></name><name><surname>Tanji</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Representation of the temporal order of visual objects in the primate lateral prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>89</volume><fpage>2868</fpage><lpage>2873</lpage><pub-id pub-id-type="doi">10.1152/jn.00647.2002</pub-id><pub-id pub-id-type="pmid">12740417</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noorani</surname> <given-names>I</given-names></name><name><surname>Carpenter</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Full reaction time distributions reveal the complexity of neural decision-making</article-title><source>European Journal of Neuroscience</source><volume>33</volume><fpage>1948</fpage><lpage>1951</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07727.x</pub-id><pub-id pub-id-type="pmid">21645090</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noorani</surname> <given-names>I</given-names></name><name><surname>Carpenter</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The LATER model of reaction time and decision</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>64</volume><fpage>229</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.02.018</pub-id><pub-id pub-id-type="pmid">26915927</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname> <given-names>HF</given-names></name><name><surname>Bush</surname> <given-names>D</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The role of hippocampal replay in memory and Planning</article-title><source>Current Biology</source><volume>28</volume><fpage>R37</fpage><lpage>R50</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.10.073</pub-id><pub-id pub-id-type="pmid">29316421</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orlov</surname> <given-names>T</given-names></name><name><surname>Yakovlev</surname> <given-names>V</given-names></name><name><surname>Hochstein</surname> <given-names>S</given-names></name><name><surname>Zohary</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Macaque monkeys categorize images by their ordinal number</article-title><source>Nature</source><volume>404</volume><fpage>77</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1038/35003571</pub-id><pub-id pub-id-type="pmid">10716445</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panoz-Brown</surname> <given-names>D</given-names></name><name><surname>Corbin</surname> <given-names>HE</given-names></name><name><surname>Dalecki</surname> <given-names>SJ</given-names></name><name><surname>Gentry</surname> <given-names>M</given-names></name><name><surname>Brotheridge</surname> <given-names>S</given-names></name><name><surname>Sluka</surname> <given-names>CM</given-names></name><name><surname>Wu</surname> <given-names>JE</given-names></name><name><surname>Crystal</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rats remember items in context using episodic memory</article-title><source>Current Biology</source><volume>26</volume><fpage>2821</fpage><lpage>2826</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.023</pub-id><pub-id pub-id-type="pmid">27693137</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panoz-Brown</surname> <given-names>D</given-names></name><name><surname>Iyer</surname> <given-names>V</given-names></name><name><surname>Carey</surname> <given-names>LM</given-names></name><name><surname>Sluka</surname> <given-names>CM</given-names></name><name><surname>Rajic</surname> <given-names>G</given-names></name><name><surname>Kestenman</surname> <given-names>J</given-names></name><name><surname>Gentry</surname> <given-names>M</given-names></name><name><surname>Brotheridge</surname> <given-names>S</given-names></name><name><surname>Somekh</surname> <given-names>I</given-names></name><name><surname>Corbin</surname> <given-names>HE</given-names></name><name><surname>Tucker</surname> <given-names>KG</given-names></name><name><surname>Almeida</surname> <given-names>B</given-names></name><name><surname>Hex</surname> <given-names>SB</given-names></name><name><surname>Garcia</surname> <given-names>KD</given-names></name><name><surname>Hohmann</surname> <given-names>AG</given-names></name><name><surname>Crystal</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Replay of episodic memories in the rat</article-title><source>Current Biology</source><volume>28</volume><fpage>1628</fpage><lpage>1634</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.006</pub-id><pub-id pub-id-type="pmid">29754898</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname> <given-names>BE</given-names></name><name><surname>Foster</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><volume>497</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1038/nature12112</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddi</surname> <given-names>BAJ</given-names></name><name><surname>Carpenter</surname> <given-names>RHS</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The influence of urgency on decision time</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>827</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1038/77739</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sols</surname> <given-names>I</given-names></name><name><surname>DuBrow</surname> <given-names>S</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name><name><surname>Fuentemilla</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Event Boundaries Trigger Rapid Memory Reinstatement of the Prior Events to Promote Their Representation in Long-Term Memory</article-title><source>Current Biology</source><volume>27</volume><fpage>3499</fpage><lpage>3504</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.09.057</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suddendorf</surname> <given-names>T</given-names></name><name><surname>Addis</surname> <given-names>DR</given-names></name><name><surname>Corballis</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Mental time travel and the shaping of the human mind</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>364</volume><fpage>1317</fpage><lpage>1324</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0301</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Templer</surname> <given-names>VL</given-names></name><name><surname>Brown</surname> <given-names>EK</given-names></name><name><surname>Hampton</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rhesus monkeys metacognitively monitor memories of the order of events</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>11541</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-30001-y</pub-id><pub-id pub-id-type="pmid">30068995</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Templer</surname> <given-names>VL</given-names></name><name><surname>Gazes</surname> <given-names>RP</given-names></name><name><surname>Hampton</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Co-operation of long-term and working memory representations in simultaneous chaining by rhesus monkeys ( <italic>Macaca mulatta</italic> )</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>72</volume><fpage>2208</fpage><lpage>2224</lpage><pub-id pub-id-type="doi">10.1177/1747021819838432</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Templer</surname> <given-names>VL</given-names></name><name><surname>Hampton</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cognitive mechanisms of memory for order in rhesus monkeys (<italic>Macaca mulatta</italic>)</article-title><source>Hippocampus</source><volume>23</volume><fpage>193</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1002/hipo.22082</pub-id><pub-id pub-id-type="pmid">23197396</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terrace</surname> <given-names>HS</given-names></name><name><surname>Son</surname> <given-names>LK</given-names></name><name><surname>Brannon</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Serial expertise of rhesus macaques</article-title><source>Psychological Science</source><volume>14</volume><fpage>66</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.01420</pub-id><pub-id pub-id-type="pmid">12564756</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tulving</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Memory and consciousness</article-title><source>Canadian Psychology/Psychologie Canadienne</source><volume>26</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1037/h0080017</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viola</surname> <given-names>P</given-names></name><name><surname>Jones</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Robust Real-Time face detection</article-title><source>International Journal of Computer Vision</source><volume>57</volume><fpage>137</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1023/B:VISI.0000013087.49260.fb</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wimmer</surname> <given-names>GE</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Vehar</surname> <given-names>N</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Episodic memory retrieval is supported by rapid replay of episode content</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/758185</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54519.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing Editor</role><aff><institution>University of Toronto</institution><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Strange</surname><given-names>Bryan</given-names> </name><role>Reviewer</role><aff><country>Spain</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Memory replay enables humans recall the past in a flexible and temporally-compressed manner. The extent to which this process is shared with nonhuman primates is unknown. Kwok and colleagues demonstrate that, like humans, macaque monkeys temporally compress past experiences with a non-linear forward-replay mechanism. However, replay in macaques was strictly forward; unlike humans, macaque monkeys lacked a global compression mechanism that enabled the flexibility to skip irrelevant information. This work helps to map the evolution the episodic memory.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Behavioral evidence for memory replay of video episodes in macaque monkeys&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Bryan Strange (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>All reviewers agreed that this is an interesting and timely paper, and they universally congratulated you on the Herculean effort involved in conducting this research. The reviewers did raise several issues that must be addressed prior to publication. Their reviews are included below.</p><p>The manuscript presents a number of very complex analyses, which were at times difficult to digest. In the revision, we encourage the authors to improve the overall clarity of the manuscript by providing more (and clearer) details on the methods. Each reviewer has highlighted specific requests for more information and additional analyses to clarify results. Clearer linkage between the Materials and methods and corresponding Results would also help to improve the manuscript's clarity.</p><p>The reviewers raised questions about the behavioural performance measures (e.g., the need to more carefully consider the accuracy data, the inclusion of both correct and incorrect trials, and how stimulus repetition may affect the results). In several instances, the reviewers suggested additional interpretations of the data.</p><p>We hope that these comments are helpful as you prepare your revision, and look forward to receiving the revised manuscript.</p><p><italic>Reviewer #1:</italic>In this manuscript, Zuo and colleagues present novel behavioural data from macaque monkeys with the aim of investigating the presence of replay during temporal order judgments for previously seen video clips. The question and paradigm are interesting and timely. The comparisons between monkey and human strategies and performance are especially intriguing. However, I have some questions about the behavioural performance and measures. The statistical models should also be reported more clearly and the link to predictions should be more explicit.</p><p>1) Additional details about the testing paradigm and overall behavioural results are needed. Were the monkeys trained ahead of time (and if so, how long did the training take)? With such a protracted period, it would be interesting to know whether the monkeys' performance improved or changed over the course of the experiment. As their performance is not at ceiling level, it seems possible that they improve overall, or that they start relying on different aspects to make their judgments. Day of testing could therefore also be included as a predictor in the models.</p><p>Regarding the monkeys' accuracy: in the subsection “Task performance”, the authors report that the monkeys' overall accuracy was ~68%. Was this the overall accuracy calculated across all 50 testing days? In contrast, the monkeys' accuracy plotted in Figure 2C is barely above chance level – while it's clear that the pattern displayed in the figure will be noisy as the data are split according to target frame location, the lines hover close to chance level and don't seem to average out to 68%. I may be missing something, but unless the data in Figure 2C represent only a subset, for example, this needs to be explained in more detail. Related to the point above, it would generally be helpful if the authors provided a measure of the monkeys' accuracy over the course of testing.</p><p>This is also important as the human participants' accuracy was at ceiling after a single testing session (based on the aforementioned subsection) – did the trend of the monkeys' performance remain the same across the 50 testing days (i.e., no global compression), or did this only develop after extensive experience on the task?</p><p>2) On a related note, it seems confusing to include both correct and incorrect trials in most of the analyses. In the GLM analyses (subsection “Human-like forward replay in macaques”, Supplementary file 2), the patterns were indeed quite similar for correct and incorrect trials, but it still seems unusual to include trials where the monkeys may not have replayed the content correctly as they reached an incorrect decision. Further, the different amounts of data entered into different models (correct trials only vs. all trials) make it more difficult to compare them. I would recommend that only correct trials are included in all analyses, or additional control analyses are needed to ensure that all findings remain the same when only correct trials are used.</p><p>3) The comparison between monkey and human data in Figure 3 is especially interesting. It is intriguing to see that human participants display global compression, but monkeys do not, and this interpretation seems to be well-supported by the data. However, the authors state that monkeys 'reach their memory decision threshold more quickly when probe frames are extracted from the two different contexts' and that these findings 'parallel closely established findings in the humans'. This pattern is in fact the opposite to that observed in humans; when two stimuli are separated by a boundary, humans are slower to reach a decision (e.g. Ezzyat and Davachi, 2014; Heusser et al., 2018, to name just two recent papers). This interpretation should be revised as the data from the present manuscript in fact suggest that boundaries affect human and monkey decision making processes in opposite directions.</p><p>4) The manuscript contains a relatively large number of (at times complex) analyses. As the more complex models are such an important aspect of the paper, they should be explained in more detail and the link between Materials and methods and Results should be clear. I found it somewhat difficult to align the description of the analytic approach in the Materials and methods section with the reporting of the results. In general, more detail in the description of the methods is needed. This especially applies to the model and variable descriptions. A few key points below:</p><p>– In the subsection “Task and experimental procedure”, the authors state that the experiment comprised four factors: boundary, play order, temporal distance, and exposure. First, I found it unusual that the authors reported temporal distance as a factor with 25 levels, since modeling it as a categorical variable assumes that all 25 levels are independent of one another. If this was not the case, it should be clarified in text, but if this variable was indeed modeled as a categorical factor, this should be changed to continuous or ordinal.</p><p>Second, in the subsection “Generalized linear models (GLM)”, the authors then report a set of variables included in the GLM. If I'm not mistaken, these are only regressed out of the very last GLM (subsection “Confirmatory GLMs for the putative patterns”). While the evidence from this analysis is clear and the significance of the key factors of interest does not change, I think including these regressors in each of the analyses and treating them as nuisance regressors would be more convincing/parsimonious.</p><p>– The description of the LATER model fitting was somewhat confusing. For example, it wasn't immediately clear that 'both conditions' (subsection “LATER (linear approach to threshold with ergodic rate) modelling”) refers to within vs. across clips. I also found it somewhat confusing that one of the models was called 'unconstrained' in the Materials and methods, but 'two fits' in Supplementary file 5 (if that is indeed the same model). Finally, in the Materials and methods, the authors refer to Figure 4A, which I believe should be Figure 6A. I am not a modeler, but it would nonetheless be important for the model descriptions to be linked back to the experimental predictions to make the connection between model parameters and behaviour clearer.</p><p>– How exactly was reciprocal latency calculated? I was not familiar with the term beforehand so I looked it up in the literature, but I would suggest that all key variables are defined in an accessible manner.</p><p>5) Related to the above: in the Introduction, the authors say that a 'non-linear' pattern would be predicted, which led me to expect a comparison of linear and non-linear model fits. Similarly, in the Materials and methods, the authors state that they used BIC to 'obtain the best fit among these models' – as this section immediately follows the section on GLMs, I assumed the models would be compared. However, the only BIC value reported is that comparing the 'shift' and 'swivel' models. I believe BIC can be used to compare any model types (i.e. linear and non-linear), but if this is not the case, the approach should be clarified. Unless I'm missing something, it appears that the reciprocal latency was modelled in a linear model (subsection “Human-like forward replay in macaques”), but the 'non-linearity' of the fit was only assessed visually. As the linear analyses all had significant outcomes, it seems important to provide a benchmark of 'goodness of fit' for different models. Reporting the significance of different trends (e.g. linear, quadratic, cubic) could be helpful.</p><p><italic>Reviewer #2:</italic>In this study, Zuo et al. examined existence of memory replay during the retrieval of video clip material encoded continuously during periods of ~10 seconds. The study combined several computational modeling approaches and complex analytical procedures on reaction time data to test whether monkeys showed forward replay of the encoded material and whether the replayed memory content showed a structured pattern modulated by context changes during encoding, as shown previously in humans. Their results, though similar with humans in many aspects, also revealed striking differences, being the Inability to skip irrelevant Information In their replay a major one.</p><p>I found the study very well written and the topic of research of interest among many in the neuroscientific community. The analytical approach is sophisticated and the implementation sounding. I do have however some concerns that would require further attention though, which are listed below:</p><p>1) Many of the findings described in human studies related to sequence of images or video clips that are presented only once. Here, animals are shown repeatedly with the video clips. To what extent the repetition is affecting the underlying neural mechanisms and consequently the actual findings? I am aware the study included somehow this in their GLM model (Supplementary Figure 3) but in my view, this should require some more work. The question that one would like to see answered here would be: To what extent differences between humans and macaques are susceptible to be affected by the large amount of repeated material used in monkeys? I am aware many of the analysis included in the study require large number of trials for each individual but maybe authors can explore this issue across participants?</p><p>2) Several details concerning the experimental design tested in humans are lacking. Which are the differences between the two species when it comes to the experimental design? This information should be clear in the manuscript so that differences and similarities between species can be fully evaluated.</p><p>3) I was expecting that most of the analysis were implemented in monkeys' and in humans' data. Is there any particular reason to skip some of them in humans RTs (for example: effects of context change (within vs. across) and GLM confirmatory analysis)?</p><p>4) Correlation results between models should be directly compared and show they differed significantly to be able to attribute a winner one (i.e., Figure 3).</p><p>5) I found the results showing that many of the effects were equally robust for correct and incorrect trials a bit confusing. In my understanding, the behavioural manifestation of how memory content is organized and replayed should be specifically evident for when retrieval access has been successful, as otherwise it may be difficult to discard the possibility that the observations are driven by a more general task oriented operation. Can the authors please justify why it would be relevant that many of their central findings were valid for correct and incorrect trials? And if so, wouldn't it be also relevant to show the same results in humans' data?</p><p><italic>Reviewer #3:</italic>By training 6 macaques on a cinematic video-clip task, Kwok and colleagues have leveraged reaction time (RT) data to make inferences on the ability of non-human primates to make temporal order judgment (TOJ). RT analyses, using LATER and drift-diffusion modelling, enabled to authors to suggest potential mechanistic underpinnings to the behavioural effects they observe. Irrespective of performance, RTs were faster if the still pertained to earlier segments of the video clip. The effect was non-linear, as the correlation was significant for log-transformed RTs. Furthermore, the relationship of still presentation latency to RT was around 10:1, which the authors interpret as time compression in replay. Humans, on the other hand do not show this relationship.</p><p>This is an interesting paper, and the cross-species differences are very clearly depicted and highly interesting. I commend the authors on what is a tremendous effort in terms of stimulus preparation and animal training. I have some comments that would need to be addressed before recommending publication.</p><p>1) The authors have concentrated mainly on RTs, but when accuracy is considered (plotted in Figure 2C) it is clear that for many target frame locations, the macaques are performing at chance (horizontal blue line). In at least 3 of the macaques, accuracy seems worse for earliest stills from clip 1. Could there be a speed-accuracy trade-off underlying faster RTs for these early stills? I appreciate that, overall, performance was above chance (it is somewhat atypical to report this at the beginning of the Materials and methods section), but the possible confound of monkeys making fast responses with little memory content to these early still probes needs addressing.</p><p>2) Furthermore, there is clearly an improvement in accuracy for stills that are at the beginning of clip 2. The authors mention this as &quot;a blip&quot; but provide no statistics. This is an interesting boundary effect that could be reported better and integrated with point 3.</p><p>3) Representational similarity analyses were used to demonstrate that &quot;global compression&quot; of individual video clips is not evident in macaques, who appear to show increasing RTs to stills drawn from over the course of the 2 clips (i.e. the strictly forward model). There is a non-significant trend towards global compression effects in humans. It is clear, though, that macaques and humans respond differently. What makes things a bit confusing is that LATER modeling indicated that macaques show an important boundary effect. Memory decision threshold is reached more quickly if probe stills come from different clips. I wonder whether this has something to do with perceptual similarity effects reported later for the GLM analyses, but I did not get a good feel for what this perceptual similarity parameter is measuring.</p><p>4) I am undecided as to whether Figure 5 and associated Results section really add to the findings. It is clear from the preceding figures that slope is markedly different in the two species.</p><p>5) In view of the indirect nature of the inference, certain statements such as &quot;The monkeys apply a non-linear forward, time-compressed replay mechanism during the temporal-order judgement” (Abstract) need to be toned down.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your article &quot;Behavioral evidence for memory replay of video episodes in the macaque&quot; for consideration by <italic>eLife</italic>. Your revised article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor The following individual involved in review of your submission has agreed to reveal their identity: Bryan Strange (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>The reviewers were very satisfied with the revisions and we would like to congratulate you on a fine paper. The number of testing days for each animal is testimony to Herculean effort put into this work, and we believe that it will be a very important contribution to understanding cross-species differences in memory replay.</p><p>It was noted that with the additional analyses, the paper now shows more robust evidence for a forward scanning strategy in non-human primates. The additional analyses regarding training/testing session over time were also appreciated, in particular including response accuracy as a covariate and the trend analysis.</p><p>Only a few relatively minor comments remain:</p><p>1) The observation that the findings largely hold up when the data are split by correct and incorrect trials (Table 1) certainly supports the authors' decision to include all trials in their analysis, regardless of correctness. However, since the findings are so similar for correct and incorrect trials, the authors may wish to discuss why this pattern might be observed even on incorrect trials. Is the assumption that the monkeys are replaying the content correctly but then reaching an incorrect decision or that the information was incorrectly encoded? In other words, if the latency data reflects memory processes, an incorrect decision here would suggest that the initially encoded temporal order was incorrect. Either way, this seems like an interesting finding and Discussion point.</p><p>2) Regarding the analysis of linear, quadratic, and cubic trends: it is indeed encouraging to see that the non-linear (quadratic and cubic) trends are significant in the monkeys. However, interestingly, only the cubic trend seems to be significant in the human sample (linear and quadratic are not). Since one of the important contributions of this paper is a direct comparison between monkeys and humans, we think it would be helpful if the authors also addressed this difference in the manuscript. We also suggest that the manuscript text more explicitly stated what type of non-linear relationship was observed (i.e., in the subsection “Human-like forward replay in macaques” where these results are reported).</p><p>There were two requests for greater clarity:</p><p>1) The Introduction sets up the notion of linear vs. non-linear models for RTs and the authors state that they adjudicate between the two aspects of the replay models comparing between human and macaque data. While I appreciate that the non-linear human component refers to the global compression, the fact that monkeys appear to have performed TOJ using a forward search with non-linear compression, might confuse some readers. It was recommended that this be made explicit in the Introduction.</p><p>2) The legend of Figure 6—figure supplement 1 needs to include more explanation; please correct. Is the point of this to indicate that there is not a direct mapping of all features between inset and T-shirt images (i.e. the coloured lines don't always go to the same point of the corresponding image)?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54519.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…]</p><p>1) Additional details about the testing paradigm and overall behavioural results are needed. Were the monkeys trained ahead of time (and if so, how long did the training take)? With such a protracted period, it would be interesting to know whether the monkeys' performance improved or changed over the course of the experiment. As their performance is not at ceiling level, it seems possible that they improve overall, or that they start relying on different aspects to make their judgments. Day of testing could therefore also be included as a predictor in the models.</p></disp-quote><p>We now provided more details on these aspects. The monkeys had received training before the main test reported in the manuscript. There were five stages of learning on the TOJ task and the numbers of days are reported in (Table 2). With these extended periods of training, we think that the monkeys were unlikely to change over the course of the experiment in terms of their base TOJ ability (see also our reply below). In order to check this, we plotted out the proportion correct per day per individual: (A) Monkey data. (B) Human data. We observe no obvious increase in performance over the course of testing days for both monkeys and humans (Figure 7). This absence of performance changes is due to the extensive training the monkeys had received prior to this experiment (~ 21150 trials in total per monkey across 141 sessions on average, see Table 2). We therefore deem the main effects reported here as unlikely attributable to behavioral or strategic changes over the course of main experiment.</p><disp-quote content-type="editor-comment"><p>Regarding the monkeys' accuracy: in the subsection “Task performance”, the authors report that the monkeys' overall accuracy was ~68%. Was this the overall accuracy calculated across all 50 testing days? In contrast, the monkeys' accuracy plotted in Figure 2C is barely above chance level – while it's clear that the pattern displayed in the figure will be noisy as the data are split according to target frame location, the lines hover close to chance level and don't seem to average out to 68%. I may be missing something, but unless the data in Figure 2C represent only a subset, for example, this needs to be explained in more detail.</p></disp-quote><p>The results displaying ~68% are the aggregated data from all trials (both within- and across-) from all 50 testing days, whereas the data in Figure 2C contained only within-context trials. These are now clarified more explicitly in the revised manuscript (subsection “Human-like forward replay in macaques”). The analyses in this paper are broadly made of two parts: using only within-context trials and comparing within-context vs. across-context trials.</p><disp-quote content-type="editor-comment"><p>Related to the point above, it would generally be helpful if the authors provided a measure of the monkeys' accuracy over the course of testing. This is also important as the human participants' accuracy was at ceiling after a single testing session (based on the aforementioned subsection) – did the trend of the monkeys' performance remain the same across the 50 testing days (i.e., no global compression), or did this only develop after extensive experience on the task?</p></disp-quote><p>We plotted the accuracy data for each testing day for each monkey and human as Figure 7. We do not observe any obvious increase in performance over the course of the 50 testing days. Together with the extensive training sessions/trials prior to the experiment (over 20,000 trials per monkey). We deem the main effects reported in the manuscript as not attributable to changes over the course of main experiment.</p><p>Moreover, to further address the possibility that the two species might be differentially susceptible to the effect of having different numbers of trials in the experiment, we performed a control analysis making use of only the first 2 repetitions of data to equate the number of trials between the two species. With these equated subsets of data, we re-calculated the correlation between monkey RT RDM and two hypothetical model (i.e., Strict forward model vs. Global compression model). The results replicated the same significant correlation with the Strict forward model (r = 0.56, P = 0.049), but not with the Global compression model (r = -0.10, P = 0.643). These results replicated the findings as of when the full set of data was used, suggesting that the more numerous exposure to the same stimuli did not affect the main results. These results are now added in the revised manuscript (subsection “Discrepancy with humans: Compression of replay is local but not global”).</p><disp-quote content-type="editor-comment"><p>2) On a related note, it seems confusing to include both correct and incorrect trials in most of the analyses. In the GLM analyses (subsection “Human-like forward replay in macaques”, Supplementary file 2), the patterns were indeed quite similar for correct and incorrect trials, but it still seems unusual to include trials where the monkeys may not have replayed the content correctly as they reached an incorrect decision. Further, the different amounts of data entered into different models (correct trials only vs. all trials) make it more difficult to compare them. I would recommend that only correct trials are included in all analyses, or additional control analyses are needed to ensure that all findings remain the same when only correct trials are used.</p></disp-quote><p>In order to clarify this issue, we first made use of the GLMs, but now adding a new binary regressor (response: correct or incorrect) into the full GLMs. In this way, we checked how correctness might influencethe overall response latency per condition per species.The results indicate that response correctness influences the overall response latency considerably especially in the across-context condition in both species, but to a lesser extent for within-context condition in the monkeys (Figure 6). However, in order to directly address the main issue of “changes in response latency as a function of chosen frame location”, we improved the linear regression analyses by including other variables as nuisance regressors (including response correctness) (cf. reviewer 1 comment 4 below). We found a significantly positive relationship between RT and chosen frame location in all monkeys, all P &lt; 0.001 (more significant than without the nuisance regressors). We have accordingly modified Table 1 (top panel) with these new results. Moreover, we have also conducted these analyses considering only correct trials (or only incorrect trials) separately. The new results are now also reported in Table 1 (middle and bottom panels respectively) (subsection “Human-like forward replay in macaques”). These results support that possibility that the retrieval RT patterns as function of chosen frame location are not affected by the memory outcome. The same has been done with response latency/temporal similarity slope analysis too (Supplementary file 2).</p><disp-quote content-type="editor-comment"><p>3) The comparison between monkey and human data in Figure 3 is especially interesting. It is intriguing to see that human participants display global compression, but monkeys do not, and this interpretation seems to be well-supported by the data. However, the authors state that monkeys 'reach their memory decision threshold more quickly when probe frames are extracted from the two different contexts' and that these findings 'parallel closely established findings in the humans'. This pattern is in fact the opposite to that observed in humans; when two stimuli are separated by a boundary, humans are slower to reach a decision (e.g. Ezzyat and Davachi, 2014; Heusser et al., 2018 to name just two recent papers). This interpretation should be revised as the data from the present manuscript in fact suggest that boundaries affect human and monkey decision making processes in opposite directions.</p></disp-quote><p>We have now added an analysis on the human data with the LATER and did not observe the same pattern as the monkeys (Figure 5C and Supplementary file 3). It is worth mentioning that in paradigms making use of discreet images for encoding, the TOJ performance thus far reported in the human literature is distinct from those making use of continuous streaming video material. Specifically, it has been reported that accuracy and response times are worse in TOJ for items are that separated by a boundary during encoding in the literature (Ezzyat and Davachi, 2014; Heusser et al., 2018), whereas we show that TOJ performance is better between frames taken from an across-context condition than from a within-context condition (Figure 1—figure supplement 1A). This is likely due to the ease of remembering ‘discrete segment of events’ when making use of the contextual or perceptual clues in the video frames in an across-context condition (see also Figure 6—figure supplement 1 that contextual information affects TOJ). We have added this discussion briefly in the revised manuscript (subsection “Human-like forward replay in macaques”).</p><disp-quote content-type="editor-comment"><p>4) The manuscript contains a relatively large number of (at times complex) analyses. As the more complex models are such an important aspect of the paper, they should be explained in more detail and the link between Materials and methods and Results should be clear. I found it somewhat difficult to align the description of the analytic approach in the Materials and methods section with the reporting of the results. In general, more detail in the description of the methods is needed. This especially applies to the model and variable descriptions. A few key points below:</p></disp-quote><p>Thank you for this suggestion. Apart from addressing the specific sub-points mentioned below, we have re-organized some content from the Materials and methods section and provided some more detailed clarification of the methods. For example, to clarify our methods in parameterizing perceptual similarity information between TOJ frames, we have provided an illustration of what the main perceptual similarity parameter indexes in Figure 6—figure supplement 1C.</p><disp-quote content-type="editor-comment"><p>– In the subsection “Task and experimental procedure”, the authors state that the experiment comprised four factors: boundary, play order, temporal distance, and exposure. First, I found it unusual that the authors reported temporal distance as a factor with 25 levels, since modeling it as a categorical variable assumes that all 25 levels are independent of one another. If this was not the case, it should be clarified in text, but if this variable was indeed modeled as a categorical factor, this should be changed to continuous or ordinal.</p></disp-quote><p>Temporal distance is an ordinal variable. We have clarified this in the revised manuscript (subsection “Task and experimental procedure”).</p><disp-quote content-type="editor-comment"><p>Second, in the subsection “Generalized linear models (GLM)”, the authors then report a set of variables included in the GLM. If I'm not mistaken, these are only regressed out of the very last GLM (subsection “Confirmatory GLMs for the putative patterns”). While the evidence from this analysis is clear and the significance of the key factors of interest does not change, I think including these regressors in each of the analyses and treating them as nuisance regressors would be more convincing/parsimonious.</p></disp-quote><p>We now ran the linear regression analysis of response latency as a function of chosen frame location/temporal similarity having included other variables as nuisance regressors for each monkey separately. We now found a significantly positive relationship between RT and chosen frame location in all monkeys, all P &lt; 0.001 (more significant than without these nuisance variables). We have modified Table 1 (top panel) with these new results. Moreover, related to comment 2 above, we have also conducted these analyses considering only correct trials (and only incorrect trials) separately. The results are reported in Table 1 (middle and bottom panels respectively) (subsection “Human-like forward replay in macaques”). We also ran the same sets of analyses on human participant data for comparison between the two species. The results are reported in Supplementary file 4.</p><disp-quote content-type="editor-comment"><p>– The description of the LATER model fitting was somewhat confusing. For example, it wasn't immediately clear that 'both conditions' (subsection “LATER (linear approach to threshold with ergodic rate) modelling”) refers to within vs. across clips. I also found it somewhat confusing that one of the models was called 'unconstrained' in the Materials and methods, but 'two fits' in Supplementary file 5 (if that is indeed the same model). Finally, in the Materials and methods, the authors refer to Figure 4A, which I believe should be Figure 6A. I am not a modeler, but it would nonetheless be important for the model descriptions to be linked back to the experimental predictions to make the connection between model parameters and behaviour clearer.</p></disp-quote><p>We apologize for the confusion and the mistaken reference. To clarify, the ‘unconstrained’ model is the same as ‘two fits’ model, and we have now only used the term ‘two fits’ model throughout the main text and the table. We have also corrected the reference to correct figure in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>– How exactly was reciprocal latency calculated? I was not familiar with the term beforehand so I looked it up in the literature, but I would suggest that all key variables are defined in an accessible manner.</p></disp-quote><p>Reciprocal latency is the multiplicative inverse of raw reaction time (i.e., 1/RT). We added its definition in the Materials and methods (subsection “LATER (linear approach to threshold with ergodic rate) modelling”).</p><disp-quote content-type="editor-comment"><p>5) Related to the above: in the Introduction, the authors say that a 'non-linear' pattern would be predicted, which led me to expect a comparison of linear and non-linear model fits. Similarly, in the Materials and methods, the authors state that they used BIC to 'obtain the best fit among these models' – as this section immediately follows the section on GLMs, I assumed the models would be compared. However, the only BIC value reported is that comparing the 'shift' and 'swivel' models. I believe BIC can be used to compare any model types (i.e. linear and non-linear), but if this is not the case, the approach should be clarified. Unless I'm missing something, it appears that the reciprocal latency was modelled in a linear model (subsection “Human-like forward replay in macaques”), but the 'non-linearity' of the fit was only assessed visually. As the linear analyses all had significant outcomes, it seems important to provide a benchmark of 'goodness of fit' for different models. Reporting the significance of different trends (e.g. linear, quadratic, cubic) could be helpful.</p></disp-quote><p>We formally tested for any ‘non-linearity’ of the fit. We ran curvilinear regression analyses using reaction time as the dependent variable as chosen frame location to estimate the significance of different trends. We started testing for the linear relationship and progressively moved to higher-order relationships (quadratic, cubic). At each step, we assessed the significance of the new predictor, having accounted for all the variance fitted by the lower-level predictors. We reported the significance of different trends (linear, quadratic, cubic) for the two species (Supplementary file 5). This non-linear relationship is important because it rules out alternative explanations that the main effects are simply resultant from positional effects. Rather, the non-linear change in slope indicates there are other factors in play that the monkeys might group (or parse) the content according to the relational storyline structure (e.g., the 2-clip video design), and do not merely recall the frames/items as of their ordinal positions in a fixed linear manner.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…]</p><p>1) Many of the findings described in human studies related to sequence of images or video clips that are presented only once. Here, animals are shown repeatedly with the video clips. To what extent the repetition is affecting the underlying neural mechanisms and consequently the actual findings? I am aware the study included somehow this in their GLM model (Supplementary Figure 3) but in my view, this should require some more work. The question that one would like to see answered here would be: To what extent differences between humans and macaques are susceptible to be affected by the large amount of repeated material used in monkeys? I am aware many of the analysis included in the study require large number of trials for each individual but maybe authors can explore this issue across participants?</p></disp-quote><p>In a control analysis, we now made use of only the first 2 repetitions of data to equate the number of trials between the two species (note that the same experimental design was adopted by the human participants, see our reply to comment 2 below). Now with these equated subsets of data, we re-calculated the correlation between monkey RT RDM and two hypothetical model (i.e., Strict forward model vs. Global compression model). The results replicated the significant correlation with Strict forward model (r=0.56, P=0.049), but not with the Global compression model (r=-0.10, P=0.643). With this smaller set of data, we also directly compared the correlations between the two models and found significant differences between them (P&lt;0.05, FDR-corrected, cf. reply to comment 4 below). These results replicated the findings as of when the full set of data was used, suggesting that having the more numerous exposure to the same stimuli did not affect the main results. These results are now added in the revised manuscript (subsection “Discrepancy with humans: Compression of replay is local but not global”).</p><p>It may be worth mentioning that despite having administered a large amount of training to the animals prior to this study, the monkeys’ performance have stabilized over the course of the experiment (see our reply to reviewer 1, point 1 and revised Materials and methods, subsection “Training history and task performance”). We have also taken care to ensure that all the videos used in the training stages were sampled differently from those employed in the main test, and that each of the monkeys was assigned with a pseudo-randomized remix of a unique 1000 video-trials set from the 2000-video library, thus eliminating video idiosyncrasy to be associated with any experimental conditions. With the large pool of unseen videos we have generated, only five repeated presentations were needed to accrue the relatively large numbers of trials (5000 per animal), which we believe is not too deviant from accepted standards in psychological studies in human.</p><disp-quote content-type="editor-comment"><p>2) Several details concerning the experimental design tested in humans are lacking. Which are the differences between the two species when it comes to the experimental design? This information should be clear in the manuscript so that differences and similarities between species can be fully evaluated.</p></disp-quote><p>The experimental design is essentially the same as the monkeys. The human participants re-used the 6 unique video-trials/list sets and TOJ frames (one unique set per monkey) correspondingly (the extra subject 7 re-used set 1). The only critical difference was that human participants performed only two exposures (R1 – R2) rather than five. Identically, each session contained 100 trials and with 20 daily sessions of testing, we have acquired 2000 trials per participant for analysis. Human participants used the same model of computers and touch-sensitive screens as monkeys. The human subjects sat ~30cm in front of another identical 17-inch infrared touch-sensitive screen, with stimuli presented with the same computer used in the monkeys’ experiment. These details are now further clarified in the revised manuscript (subsections “Apparatus and testing cubicle”, “Source of video materials and preparation” and “Task and experimental procedure”).</p><disp-quote content-type="editor-comment"><p>3) I was expecting that most of the analysis were implemented in monkeys' and in humans' data. Is there any particular reason to skip some of them in humans RTs (for example: effects of context change (within vs. across) and GLM confirmatory analysis)?</p></disp-quote><p>We have now completed the full comparison between the two species’ data with a series of new analyses with human data. These include the following main analyses: (i) LATER modelling for within- vs. across-conditions (Figure 5 and Supplementary file 3) and (ii) GLM confirmatory analyses (Figure 6B and Figure 6—figure supplement 1B). In addition, we also completed the comparison on (iii) the relationship between temporal similarity and reciprocal latency for within-context trials in humans (Figure 2—figure supplement 1B), (iv) proportion corrects and RT analyses for the humans (Figure 1—figure supplement 1A and Figure 7). These altogether help elucidate the differences and similarities between the species more explicitly.</p><disp-quote content-type="editor-comment"><p>4) Correlation results between models should be directly compared and show they differed significantly to be able to attribute a winner one (i.e., Figure 3).</p></disp-quote><p>We used two-sided Wilcoxon signed-rank tests to test the null hypothesis that the correlation between data RDM and the two hypothetical models are equal. Pairwise comparisons between the two models are statistically significant for monkeys (P&lt;0.01) and humans (P&lt;0.05), confirming the significant differences between the models in both cases. These are now modified in the revised manuscript and in Figure 3 (subsections “Discrepancy with humans: Compression of replay is local but not global”, and “Representational similarity analysis (RSA)”).</p><disp-quote content-type="editor-comment"><p>5) I found the results showing that many of the effects were equally robust for correct and incorrect trials a bit confusing. In my understanding, the behavioural manifestation of how memory content is organized and replayed should be specifically evident for when retrieval access has been successful, as otherwise it may be difficult to discard the possibility that the observations are driven by a more general task oriented operation. Can the authors please justify why it would be relevant that many of their central findings were valid for correct and incorrect trials? And if so, wouldn't it be also relevant to show the same results in humans' data?</p></disp-quote><p>Making use of the GLMs, we now added a new binary regressor (response: correct or incorrect) into the full GLMs. In this way, we checked how correctness might influencethe overall response latency per condition per species.The results indicate that response correctness influences response latency considerably especially in the across-context condition in both species, but to a lesser extent for within-context condition in the monkeys (Figure 6). However, to directly address the main issue of “changes in response latency as a function of chosen frame location”, we also ran the linear regression analyses but now including other variables as nuisance regressors (including response correctness). We found a significantly negative relationship between reciprocal latency and chosen frame location in all monkeys, all P &lt; 0.001 (more significant than without those nuisance variables). We have modified Table 1 (top panel) with these new results. Moreover, we have also re-conducted these analyses considering only correct trials (or only incorrect trials) separately. The results are now reported in Table 1 (middle and bottom panels respectively) (subsection “Human-like forward replay in macaques”). These results support the possibility that the response latency patterns are not affected by the memory outcome.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>[…]</p><p>1) The authors have concentrated mainly on RTs, but when accuracy is considered (plotted in Figure 2C) it is clear that for many target frame locations, the macaques are performing at chance (horizontal blue line). In at least 3 of the macaques, accuracy seems worse for earliest stills from clip 1. Could there be a speed-accuracy trade-off underlying faster RTs for these early stills? I appreciate that, overall, performance was above chance (it is somewhat atypical to report this at the beginning of the Materials and methods section), but the possible confound of monkeys making fast responses with little memory content to these early still probes needs addressing.</p></disp-quote><p>In order to rule out that the putative RT effects are not driven by a trade-off in accuracy, we segmented each video into four segments (i.e., each clip was parsed into two halves based on target frame location separately) and calculated the inverse efficiency score [RT (ms) / percentage correct (%)] for each segment per individual. The monkeys showed a mild numeral increase trend in inverse efficiency score across the four segments but did not reach statistical significance (Figure 1—figure supplement 1B, left panel) (F (3, 20) = 0.10, P = 0.96), suggesting that the increase in RT towards the end of the video did not contribute to better accuracy. In contrast, we found that the humans show a lower inverse efficiency score for the video parts right after a boundary (F (3, 24) = 4.17, P = 0.016), with post hoc tests showing significant difference between bars 2 and 3 (Figure 1—figure supplement 1B, right panel). This difference between bars 2 and 3 suggests that the participants are generally faster in their decision (while having accuracy controlled for) right after a clip boundary. This pattern of boundary effect aligns with the little “blip” in proportion correct right after the beginning of Clip 2 in the humans (Figure 2B, right panel). This discussion is now added in the revised manuscript (subsection “Human-like forward replay in macaques”).</p><disp-quote content-type="editor-comment"><p>2) Furthermore, there is clearly an improvement in accuracy for stills that are at the beginning of clip 2. The authors mention this as &quot;a blip&quot; but provide no statistics. This is an interesting boundary effect that could be reported better and integrated with point 3.</p></disp-quote><p>Together with an additional analysis making use of inverse efficiency score (cf. reply to comment 1), it is clear that humans can make use of the across-clip boundary to facilitate their TOJ. Interestingly, in paradigms making use of discreet images for encoding, the TOJ performance thus far reported in the human literature is distinct from those making use of continuous streaming video material. Specifically, accuracy and response times are worse in TOJ for items are that separated by a boundary during encoding (Ezzyat and Davachi, 2014; Heusser et al., 2018), whereas TOJ performance is better between frames taken from an across-context condition than from a within-context condition (Figure 1—figure supplement 1A). This might be due to the ease of remembering ‘discrete segment of events’ when making use of the contextual or perceptual cues in the video frames in an across-context condition (see also Figure 6—figure supplement 1 that contextual information affects TOJ). We have added this discussion briefly in the revised manuscript (subsection “Human-like forward replay in macaques”).</p><disp-quote content-type="editor-comment"><p>3) Representational similarity analyses were used to demonstrate that &quot;global compression&quot; of individual video clips is not evident in macaques, who appear to show increasing RTs to stills drawn from over the course of the 2 clips (i.e. the strictly forward model). There is a non-significant trend towards global compression effects in humans. It is clear, though, that macaques and humans respond differently. What makes things a bit confusing is that LATER modeling indicated that macaques show an important boundary effect. Memory decision threshold is reached more quickly if probe stills come from different clips. I wonder whether this has something to do with perceptual similarity effects reported later for the GLM analyses, but I did not get a good feel for what this perceptual similarity parameter is measuring.</p></disp-quote><p>Thank you for raising this consideration. While both species make use of the perceptual dissimilarity to some extent (see Figure 6—figure supplement 1), their benefits from utilizing the perceptual information seemingly differ. Specifically, we have now added an analysis on the human data with the LATER and indeed did not observe the same pattern as the monkeys (Figure 5C and Supplementary file 3; cf. reviewer 1’s comment 3), suggesting that the two species might not treat the information given by the event boundary at TOJ in the same manner. Moreover, to clarify our methods in parameterizing perceptual similarity information between TOJ frames, we have provided a more intuitive illustration of what the main perceptual similarity parameter (SURF) indexes in Figure 6—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>4) I am undecided as to whether Figure 5 and associated Results section really add to the findings. It is clear from the preceding figures that slope is markedly different in the two species.</p></disp-quote><p>Thank you for this suggestion. Indeed, reviewer 1 also concurs with this concern. Removal of Figure 5 is justified given that most of the information conveyed by Figure 5 is shown in Figures 3 and 4.</p><disp-quote content-type="editor-comment"><p>5) In view of the indirect nature of the inference, certain statements such as &quot;The monkeys apply a non-linear forward, time-compressed replay mechanism during the temporal-order judgement” (Abstract) need to be toned down.</p></disp-quote><p>Yes, indeed. Thank you for this suggestion. We have made changes in the revised manuscript to emphasize that the data (or analyses) patterns are suggestive of a possibility of memory replay in the animals, or similar statements to that effect (e.g., Abstract, Introduction, Results and Discussion).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>1) The observation that the findings largely hold up when the data are split by correct and incorrect trials (Table 1) certainly supports the authors' decision to include all trials in their analysis, regardless of correctness. However, since the findings are so similar for correct and incorrect trials, the authors may wish to discuss why this pattern might be observed even on incorrect trials. Is the assumption that the monkeys are replaying the content correctly but then reaching an incorrect decision or that the information was incorrectly encoded? In other words, if the latency data reflects memory processes, an incorrect decision here would suggest that the initially encoded temporal order was incorrect. Either way, this seems like an interesting finding and Discussion point.</p></disp-quote><p>Thank you for pointing this out. We are also intrigued by this finding. We have previously showed in analogous TOJ paradigms in humans that TOJ task specific BOLD activation and behavioural RT patterns are independent of retrieval accuracy (Ezzyat and Davachi, 2014; Heusser et al., 2018). We interpreted these effects as process-based rather than content-based. At present, the latency results likely also point towards some “search” or replay memory processes, and any ultimate incorrect responses would thus likely be caused by memory noises injected during encoding and/or during delay maintenance. We have now discussed these issues more explicitly, as evidence for cross-species correspondence, in the revised text (Discussion).</p><disp-quote content-type="editor-comment"><p>2) Regarding the analysis of linear, quadratic, and cubic trends: it is indeed encouraging to see that the non-linear (quadratic and cubic) trends are significant in the monkeys. However, interestingly, only the cubic trend seems to be significant in the human sample (linear and quadratic are not). Since one of the important contributions of this paper is a direct comparison between monkeys and humans, we think it would be helpful if the authors also addressed this difference in the manuscript. We also suggest that the manuscript text more explicitly stated what type of non-linear relationship was observed (i.e., in the subsection “Human-like forward replay in macaques” where these results are reported).</p></disp-quote><p>This analysis also helps highlight differences between humans and macaque monkeys. As we found that the human data fits best, and only, with the cubic model, it suggests humans might have treated the video and TOJ differently from the macaques. This difference is reflected in the global compression capability in the human participants. We link this part to the reporting of its subsequent section in the revised manuscript (subsection “Human-like forward replay in macaques”).</p><p>To stipulate the non-linear relationship more precisely, we have added “quadratic and cubic” to qualify the non-linear pattern in monkeys.</p><disp-quote content-type="editor-comment"><p>There were two requests for greater clarity:</p><p>1) The Introduction sets up the notion of linear vs. non-linear models for RTs and the authors state that they adjudicate between the two aspects of the replay models comparing between human and macaque data. While I appreciate that the non-linear human component refers to the global compression, the fact that monkeys appear to have performed TOJ using a forward search with non-linear compression, might confuse some readers. It was recommended that this be made explicit in the Introduction.</p></disp-quote><p>We clarify this by stating that while both species do recall the video content non-linearly, there is an aspect of discrepancy between the two species, in which the monkeys do not compress the cinematic events globally as effectively as in humans, whereas human participants possess an ability to skip irrelevant information of the video. This is now clarified in the revised manuscript (Introduction).</p><disp-quote content-type="editor-comment"><p>2) The legend of Figure 6—figure supplement 1 needs to include more explanation; please correct. Is the point of this to indicate that there is not a direct mapping of all features between inset and T-shirt images (i.e. the coloured lines don't always go to the same point of the corresponding image)?</p></disp-quote><p>We modified the legend of Figure 6—figure supplement 1C. Specifically, we clarified that the cartoon is to illustrate that how features from two images can be matched irrespective of their scale.We can see that features of the inset are matched to the T-shirt on how strongly they are related, but some feature points (mostly minority) may still incorrectly match other parts of the image.</p></body></sub-article></article>