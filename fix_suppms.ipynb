{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook exists to specifically fix an issue caused by the mdpi review spider:\n",
    "\n",
    "files containing supplementary materials are wrongly logged in the metadata: all PDF/DOCX files are linked to every round, but in reality each only belongs to one round.\n",
    "\n",
    "in this notebook the JSON files with sub-article metadata are fixed and updated, and the `metadata.json` files are updated to match them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getj(a):\n",
    "    \"\"\"\n",
    "    Takes a path to a directory and returns the metadata.json file as a dictionary.\n",
    "\n",
    "    :param a: path to an article's folder.\n",
    "    :return: A json object as a dict.\n",
    "    :rtype: dict\n",
    "\n",
    "    \"\"\"\n",
    "    path = os.path.join(a, \"metadata.json\")\n",
    "    if not os.path.exists(path):\n",
    "        print(\"no meta for a =\", a)\n",
    "        return pd.NA\n",
    "    fp = open(path, encoding='utf-8')\n",
    "    j = json.load(fp)\n",
    "    fp.close()\n",
    "    return j\n",
    "\n",
    "# count number of files in sub-articles for each reviewed\n",
    "def count_suba_files(x):\n",
    "    if not x.has_suba: return 0\n",
    "    else: return len(os.listdir(x.path+\"/sub-articles\"))\n",
    "\n",
    "def load_arts_dir(dirpath, load_meta = False):\n",
    "    \"\"\"\n",
    "    The load_arts_dir function takes a directory path and returns a dataframe df with the following columns:\n",
    "        doi - the name of each subdirectory in dirpath, which should be dois\n",
    "        path - the full path to each subdirectory in dirpath\n",
    "        has_suba - whether or not there is a 'sub-articles' folder within each article's folder\n",
    "        num_suba_files - how many files are contained within 'sub-articles' if it exists\n",
    "        meta and meta.has_reviews - only if load_meta is True\n",
    "\n",
    "    Setting the load_meta parameter to True will mean that all JSON files will be loaded into df as dicts and stored in the 'meta' column.\n",
    "    This could take Extremely long to run, depending on the number of directories in the provided path.\n",
    "    \n",
    "    :param dirpath: Used to Specify the directory path of the articles.\n",
    "    :param load_meta=False: Whether to Load the metadata from the articles into the column 'meta'.\n",
    "    :return: A pandas dataframe\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'doi': os.listdir(dirpath)})  # folder names should be dois\n",
    "    df = df.loc[df.doi.map(lambda x: os.path.isdir(os.path.join(dirpath, x)))]\n",
    "    df['path'] = df.doi.map(lambda x: os.path.join(dirpath, x))  # path to the folder relative to cwd\n",
    "    # check if articles have a sub-articles folder\n",
    "    df['has_suba'] = df.path.map(lambda p: os.path.exists(os.path.join(p, \"sub-articles\")))\n",
    "    df['num_suba_files'] = df.apply(count_suba_files, axis=1)\n",
    "    if load_meta:\n",
    "        df['meta'] = df.path.apply(getj)  # dangerous line\n",
    "        df['meta.has_reviews'] = df.meta.map(lambda a: a['has_reviews'])\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dir = os.path.join(\"output\", \"mdpi\", \"reviewed\")  # this is where the reviewed articles should be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this takes ~ 11 minutes on my PC\n",
    "rarts = load_arts_dir(r_dir, load_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 135772 articles in output\\mdpi\\reviewed\n"
     ]
    }
   ],
   "source": [
    "print(\"we have\",len(rarts),\"articles in\",r_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata_add_suba(rart):\n",
    "    \"\"\"\n",
    "    The update_metadata_add_suba function takes an article and updates the metadata.json file\n",
    "    with sub-article information if it exists.\n",
    "    \n",
    "    :return: A metadata object.\n",
    "    \"\"\"\n",
    "    meta = rart.meta\n",
    "    if not rart.has_suba: return meta\n",
    "    if rart.num_suba_files == 0: return meta  # shouldn't happen really \n",
    "\n",
    "    sub_a_path = os.path.join(rart.path, 'sub-articles')\n",
    "    \n",
    "    sub_articles = []\n",
    "    for json_file in [f for f in os.listdir(sub_a_path) if f.endswith(\".json\")]:\n",
    "        filepath = os.path.join(sub_a_path, json_file)\n",
    "        j = json.load(open(filepath, 'rb'))\n",
    "        sub_articles.append(j)\n",
    "    \n",
    "    if len(sub_articles) > 0: \n",
    "        meta['sub_articles'] = sub_articles\n",
    "        with open(os.path.join(rart.path, \"metadata.json\"), 'w', encoding=\"utf-8\") as fp:\n",
    "            json.dump(meta, fp, ensure_ascii=False)\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rarts['num_rounds'] = rarts.meta.apply(lambda m: max([int(x['round']) for x in m['sub_articles']]))\n",
    "rarts['meta.num_sub_a'] = rarts.meta.apply(lambda m: len(m['sub_articles']))\n",
    "rarts['meta.sub_articles'] = rarts.meta.apply(lambda m: m['sub_articles'])\n",
    "assert (rarts['num_suba_files'] >= 2*rarts['meta.num_sub_a']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "rarts['meta.num_sub_a_with_supp_m'] = rarts.meta.apply(lambda m: sum(['supplementary_materials' in x for x in m['sub_articles']]))\n",
    "assert (rarts['meta.num_sub_a_with_supp_m'] == rarts['meta.num_sub_a']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need to fix articles with more than one sub-article\n",
    "rarts = rarts.loc[rarts['meta.num_sub_a'] > 1]\n",
    "# actually, we only care about articles that have more than 2*num_rounds files in /sub-articles \n",
    "rarts = rarts.loc[rarts['num_suba_files'] > 2*rarts['num_rounds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each article, get all supplementary materials (in array of dicts)\n",
    "def get_all_supp_ms(sub_articles):\n",
    "    supplementary_materials = []\n",
    "    gone_ids = []  # to avoid duplicates: keep track of id's that were already appended \n",
    "    for sub_a in sub_articles:\n",
    "        for sup_m in sub_a['supplementary_materials']:\n",
    "            if sup_m['id'] in gone_ids: continue\n",
    "            gone_ids.append(sup_m['id'])\n",
    "            supplementary_materials.append(sup_m)\n",
    "    return supplementary_materials\n",
    "\n",
    "rarts['r.all_supplementary_materials'] = rarts['meta.sub_articles'].apply(get_all_supp_ms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each article, get the filenames of the reviews in plaintext\n",
    "def get_plaintexts(sub_articles):\n",
    "    plaintexts = []\n",
    "    for sub_a in sub_articles:\n",
    "        plaintexts += [sup_m['filename'] for sup_m in sub_a['supplementary_materials'] if sup_m['title'].endswith(\"plaintext.\")]\n",
    "    return plaintexts\n",
    "\n",
    "rarts['r.plaintext_filenames'] = rarts['meta.sub_articles'].apply(get_plaintexts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_orig_filenames(rart):\n",
    "    # read each plaintext file and find all original filenames of the supplementary materials for each round\n",
    "    # the result may be empty\n",
    "    all_orig_filenames = []\n",
    "    for i, filename in enumerate(rart['r.plaintext_filenames']):\n",
    "        filepath = os.path.join(rart['path'], 'sub-articles', filename)\n",
    "        \n",
    "        orig_filenames = {}\n",
    "\n",
    "        with open(filepath, 'r', encoding='utf-8') as fp:\n",
    "            lines = re.findall(r\"File: .+\", fp.read())\n",
    "        names = [line.split(\": \")[-1].strip() for line in lines]\n",
    "        for name in names:\n",
    "            if name in orig_filenames: orig_filenames[name] += 1\n",
    "            else: orig_filenames[name] = 1\n",
    "        all_orig_filenames.append(orig_filenames)\n",
    "    return all_orig_filenames\n",
    "\n",
    "# takes very long\n",
    "rarts['r.all_orig_filenames'] = rarts.apply(get_all_orig_filenames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_supp_materials(rart):\n",
    "    \"\"\"\n",
    "    The reassign_supp_materials function takes a rarticle, and returns a new, fixed list of sub-articles for that article.\n",
    "    \n",
    "    The purpose is to reassign supplementary materials to their respective sub-article. \n",
    "    This is necessary because in some cases, supplementary materials are assigned to all sub-articles.\n",
    "    \n",
    "    :return: A new list of sub-articles.\n",
    "    \n",
    "    \"\"\"\n",
    "    sub_articles = rart['meta.sub_articles']\n",
    "    for sub_a in sub_articles[1:]:\n",
    "        assert len(sub_articles[0]['supplementary_materials']) == len(sub_a['supplementary_materials'])\n",
    "    plaintext_filenames = rart['r.plaintext_filenames']\n",
    "    assert len(sub_articles) == len(plaintext_filenames)\n",
    "    all_supp_ms = rart['r.all_supplementary_materials']\n",
    "    assert len(all_supp_ms) > len(sub_articles)\n",
    "    all_orig_filenames = rart['r.all_orig_filenames']\n",
    "    assert len(all_orig_filenames) == rart['num_rounds']\n",
    "\n",
    "    new_sub_articles = []\n",
    "    gone_ids = []\n",
    "\n",
    "    for i, d_round in enumerate(all_orig_filenames):\n",
    "        new_sub_a = sub_articles[i].copy()\n",
    "        new_sub_a['supplementary_materials'] = []\n",
    "        assert int(new_sub_a['round']) == i+1\n",
    "\n",
    "        for sup_m in all_supp_ms:\n",
    "            if sup_m['id'] in gone_ids: continue\n",
    "            # case when it's one of the plaintext files:\n",
    "            if 'original_filename' not in sup_m.keys() or sup_m['original_filename'] not in d_round:\n",
    "                if sup_m['id'].split('.r')[-1] == new_sub_a['round']:\n",
    "                    new_sub_a['supplementary_materials'].append(sup_m)\n",
    "                    gone_ids.append(sup_m['id'])\n",
    "            # case when it's any other type of file\n",
    "            elif d_round[sup_m['original_filename']] > 0:\n",
    "                d_round[sup_m['original_filename']] -= 1\n",
    "                new_sub_a['supplementary_materials'].append(sup_m)\n",
    "                gone_ids.append(sup_m['id'])\n",
    "        new_sub_articles.append(new_sub_a)\n",
    "            \n",
    "    return new_sub_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAA\\AppData\\Local\\Temp\\ipykernel_17288\\208820163.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rarts['meta.sub_articles'] = rarts.apply(reassign_supp_materials, axis=1)\n"
     ]
    }
   ],
   "source": [
    "rarts['meta.sub_articles'] = rarts.apply(reassign_supp_materials, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata_new_suba(rart):\n",
    "    meta = rart['meta']\n",
    "    num_rounds = rart['num_rounds']\n",
    "    if num_rounds == 1 or len(rart['r.all_supplementary_materials']) == num_rounds:  # shouldn't happen really\n",
    "        return meta  \n",
    "    \n",
    "    # read each JSON file with  sub-article metadata and update its contents\n",
    "    sub_articles = rart['meta.sub_articles']\n",
    "    for i, sub_a in enumerate(sub_articles):\n",
    "        r_no = i+1\n",
    "        meta_filename = f\"{rart['doi']}.r{r_no}.json\"\n",
    "        filepath = os.path.join(rart['path'], 'sub-articles', meta_filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as fp:\n",
    "            json.dump(sub_a, fp, ensure_ascii=False)\n",
    "\n",
    "    meta['sub_articles'] = sub_articles\n",
    "    \n",
    "    # update the metadata.json file itself\n",
    "    with open(os.path.join(rart['path'], \"metadata.json\"), 'w', encoding=\"utf-8\") as fp:\n",
    "        json.dump(meta, fp, ensure_ascii=False)\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAA\\AppData\\Local\\Temp\\ipykernel_17288\\1683937324.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rarts['meta'] = rarts.apply(update_metadata_new_suba, axis=1)\n"
     ]
    }
   ],
   "source": [
    "rarts['meta'] = rarts.apply(update_metadata_new_suba, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
