<sub-article xmlns:ns0="http://www.w3.org/1999/xlink" article-type="author-comment" id="pone.0222271.r002">
<front-stub>
<article-id pub-id-type="doi">10.1371/journal.pone.0222271.r002</article-id>
<title-group>
<article-title>Author response to Decision Letter 0</article-title>
</title-group>
<related-object document-id="10.1371/journal.pone.0222271" document-id-type="doi" document-type="peer-reviewed-article" id="rel-obj002" link-type="rebutted-decision-letter" object-id="10.1371/journal.pone.0222271.r001" object-id-type="doi" object-type="decision-letter" />
<custom-meta-group>
<custom-meta>
<meta-name>Submission Version</meta-name>
<meta-value>1</meta-value>
</custom-meta>
</custom-meta-group>
</front-stub>
<body>
<p>
<named-content content-type="author-response-date">7 Aug 2019</named-content>
</p>
<p>Responses to additional Editor Comments</p>
<p>Language improvements needed:</p>
<p>, in occurrence order are listed here.</p>
<p>Line 2 &#8220;reside in&#8221; better than &#8220;reside on&#8221;? Or maybe both (in and on) </p>
<p>Reply: Changed as requested to &#8220;in and on&#8221;</p>
<p>l. 10 delete &#8220;the&#8221; </p>
<p>Reply: Changed as requested</p>
<p>l. 14 word order better: &#8220;obtain directly&#8221; </p>
<p>Reply: Changed as requested</p>
<p>L.22 delete &#8220;percentage&#8221;? </p>
<p>Reply: Changed as requested</p>
<p>l.. 30 vague antecedent: &#8220;it&#8221;  </p>
<p>Reply: Added &#8220;the algorithm&#8221; in the text instead of &#8220;it&#8221;</p>
<p>l. 57 &#8220;in&#8221; in place of &#8220;on&#8221; in description of the Figure Also line 80, line 290, 300, 305, 306 </p>
<p>Reply: Changed as requested</p>
<p>I would appreciate more explanation in a legend to Figure 1 how values for pattern branch and frequency branch are deduced.</p>
<p>Also: what is meant by &#8220;fully connected&#8221;? </p>
<p>Reply: We have expanded the caption of Figure 1 and explicitly clarified the meaning of &#8220;fully-connected&#8221; both in figure caption and the text. </p>
<p>l. 58 I prefer &#8220;puts out&#8221; and elsewhere</p>
<p>Reply: In the context of computer science, &#8220;to output&#8221; is more commonly used than &#8220;to put out&#8221; in the machine learning community. In fact, Google Scholar searches for &#8220;the model outputs a&#8221; and &#8220;model puts out a&#8221; show that the first phrasing (using &#8220;to output&#8221;) is much more common. The exact sentence &#8220;the model outputs a&#8221; was used to i) to avoid responses where &#8220;outputs&#8221; is a noun  and ii) to minimize confounding results due to Google not paying attention to punctuation and capitalization -- results such as &#8220;the model outputs. The&#8221;. Thus, using &#8220;to output&#8221; seems grammatically valid, widely-used, and we would prefer to use it in the context of our manuscript. </p>
<p>l. 80 number disagreement &#8220;these type&#8221;; also l. 309 &#8220;a&#8230;models&#8217; </p>
<p>Reply: Changed as requested</p>
<p>l. 160 &#8220;We repeated out to&#8230;&#8221; ??? meaning not clear </p>
<p>Reply: Changed, the original sentence was indeed nonsensical with some excessive or missing part. </p>
<p>l. 183-4 needs articles; &#8216;the former, &#8216;the latter&#8217;. </p>
<p>Reply: Changed as requested</p>
<p>L. 210-211 is not clear. Rewrite</p>
<p>Reply: The sentence was rewritten as &#8220;However, it is important to note that ViraMiner does not have a database to rely on and must thus capture a different kind of information of genome composition than alignment based methods.&#8221;</p>
<p>l. 262-3 needs work  </p>
<p>Reply:  For clarity, we have now added a few sentences that put convolutional networks in context by mentioning their particularities. Also, we expand on the learning procedure. Notice that the nature of convolutions in case of 1D input is explained later in the subsection. </p>
<p>l.318-321 is not clear to me</p>
<p>Reply: This has been expanded in Methods and Materials</p>
<p>--------------------------------------------------------------------------------------------</p>
<p>Response to Reviewers</p>
<p>Reviewer #1: </p>
<p>This is a very well written manuscript with promising results of 19 human metagenomes, however, there are some items that I considered they need to be addressed before publication:</p>
<p>1) Although the machine learning model shows promising results, it would be interesting to the reader to know which genome regions of the viruses were used by the trained model to identify the potential new viruses. Is there any bias towards coat protein, RNA dependent RNA polymerase or any other genes to determine the presence of the virus in the dataset?</p>
<p>Can you include in the results what are the hallmark genes that were mostly found in the identified sequences? This would possibly require HMMER to be run using the unknown sequences of your 19 metagenomes after blastn.</p>
<p>Reply: We thank the reviewer for these comments. We ran HMMER with pfam database on the viral 300bp sequences and the table below represents the found proteins. The table only contains the most frequent proteins, because we want to have at least 10 data points when calculating average metrics. The second column counts the occurrences of the protein in the entire dataset (train+val+test). The third column counts the occurrences of the protein among val+test set samples. The fourth column shows the average probability of being viral according to the model for val+test set samples of the given protein (i.e. the average output value).  The fifth column gives the quartiles of these score values. The sixth column gives the AUROC (using test+val sequences containing given protein and all test set non-viruses). </p>
<p>There are clear differences among the values in the fourth, fifth and sixth column. More importantly, all these values are an improvement over the average performance over all viral sequences (given in table caption). Hence, indeed, containing parts of certain proteins helps the model to identify the sequence as viral. </p>
<p>This table is now provided as supplementary table for the paper. </p>
<p>S1 Table. Most common HHMER-identified viral proteins. For the most commonly found viral proteins, the table shows how many 300bp sequences were found in the entire dataset (train+val+test) and same count for only val+test sets. The measures in the last 3 columns are calculated using a combination of validation and test set, validation set was included to increase the number of samples (for more reliable measures). Average score corresponds to the model&#8217;s mean output on the given (val+test) sequences. The quartiles show how these scores are distributed. The AUROC is calculated using all test-set non-viral sequences and only the val+test viral sequences corresponding to the given row.  For comparison, the baseline values - using all test+val viral sequences, containing genes or not - is 0.390 for average score, [0.016; 0.127; 0.915] for quartiles and 0.923 for AUROC. It can be concluded that the model achieves improved performance on sequences containing these commonly found genes.</p>
<p>----------------</p>
<p>2) Why didn&#8217;t you try contig lengths larger than 300, for instance 1000, 5000, 10000? Would your model perform better with those contig lengths?</p>
<p>Reply: As mentioned in &#8220;Data processing and labeling&#8221; subsection, we also tried sequence length 500, but it performed clearly worse than 300. In initial experiments we also briefly tested sequence length 1000, but the results were even weaker. We hypothesize that this is due to having less data points with longer (more restrictive) sequence lengths. </p>
<p>With sequence length 500 we have 3 times less data than with sequence length 300. With sequence length 1000 we have 20 times less data than with length 300. Empirical results show that in the current case having more data is the most important.</p>
<p>-----------------</p>
<p>3) The tool requires further validation with more data. I understand that you are using 19 metagenomes and using partitions/baselines to train, and the AUROC can be considered a good parameter to evaluate your model. However, it is imperative to certainly know what is in your metagenome to be able to validate the current technology. I&#8217;d suggest to generate simulated human metagenomes using taxon profiles similar to the ones that you used in the training model. I would suggest using NeSSM, ART, MetaSim for the simulations to determine how your trained model performs in completely new datasets.</p>
<p>Reply: We believe that the true effectiveness of our models can only be measured by their performance on real data, preferably originating from a sequencing experiment that has not been used for training the model. Leaving an entire dataset out from the training procedure (using training set) and model picking (using validation data) procedure achieves just that.</p>
<p>That said, we agree that working with real data, there is always some risk of unknown biases making results nicer than they should be. We have thus repeated our experiments on simulated data, as requested. </p>
<p>We considered all three simulation tools mentioned by the reviewer and decided to use ART, as it is most understandable and easy to use. The article now contains a new methods section describing the simulation procedure and parameters, and a results subsection describing the results obtained.</p>
<p>The results showed that the model that was trained our 19 metagenomic experiments produced AUROC 0.751 on the simulated data. Even though the model performs clearly above the random level, it is indeed a more moderate performance compared to the model performance on the main test set. However,  consider that the simulated dataset was generated based on randomly picked viral reference genomes from GenBank without any prior selection.</p>
<p>Using a ViraMiner model both trained and tested on simulated data, the test AUROC increases to 0.921. We believe that this further proves that the architecture is useful and able to generalize to different datasets.</p>
<p>---------------</p>
<p>Per line comments:</p>
<p>Line 40: Builds on top of the CNN architecture of Ren [25]. </p>
<p>Reply: Changed as requested </p>
<p>Line 101: Spell out AUROC as Area Under the Receiver Operating Characteristics the first time in the text. </p>
<p>Reply: Changed as requested </p>
<p>Line 185: You can say something like: &#8220;trained to identify viruses infecting prokaryotic </p>
<p>organisms&#8221; </p>
<p>Reply: We thank the reviewer for pointing this out. Changed as requested </p>
<p>Line 251: replace producing by produced.  </p>
<p>Reply: Changed as requested </p>
<p>-----------------------------------------------------------------------------------------------------------------------</p>
<p>Reviewer #2: </p>
<p>This article proposed a new machine learning approach for characterizing unknown metagenomics contigs. The approach using ANN with raw DNA sequences as inputs is unique and novel. The authors demonstrated that the proposed approach &#8220;viralMiner&#8221; performs better than random forests and kmer as baseline. The writing is excellent as well. Because of the novelty of the approach, I recommend the paper accepted after minor revision.</p>
<p>Several minor areas can be improved:</p>
<p>1) The AUC is 0.92, however, the real performance 0.9 accuracy and 0.32 recall is not as impressive. I believe these numbers are much worse than blast, so I recommend emphasize this in the abstract.</p>
<p>Reply:      </p>
<p>Blast was used as reference method (i.e. always correct by definition) and a comparison of performance with the reference method is not useful. In the abstract, we suggest to use our method only after having applied conventional alignment methods (BLAST) in order to further investigate the sequences that are left unlabeled. </p>
<p>Also, as seen from precision-recall curve, instead of precision-recall pair 0.9&amp;0.32, any other pair (e.g. 0.95&amp;0.24 or 1.0&amp;0.151) could have been chosen. It is not possible to foresee which particular precision-recall ratio is the most important to the reader. Hence, we are reluctant to stress one particular pair of values by mentioning them in the abstract.</p>
<p>In this manuscript we have decided to use AUROC as the main metric, because it does not depend neither on class distribution nor on a particular classification threshold. </p>
<p> ---------</p>
<p>2) The real strength of this approach is to detect &#8220;unknown&#8221; contigs which cannot be detected by blast. However the training test validation experiments did not evaluate anything that is &#8220;unknown&#8221;. Maybe the authors can hold out some viral classes in the training and test if the machine learning approach can detect &#8220;unknown&#8221; contigs?</p>
<p>Reply:  We thank the reviewer for this comment. The primary purpose use case of ViraMiner is to identify distant homologs that alignment-based methods cannot detect reliably. To maximally detect those highly divergent viruses, we think that all viral classes should be included in the training dataset. In comparison, detecting new yet unknown viral classes is a much more complicated task since one viral family can be very different from the others. Note that even though the CNN-based method does not use the same type of similarity measures as BLAST or HMMER3, it still must rely on some kind of common features.  </p>
<p>Nevertheless, we have now investigated if ViraMiner is able to identify viruses from a viral class that it had not been trained on. For this purpose, we trained and validated the ViraMiner model on a dataset where all anelloviruses were removed.  We used the same hyperparameters (layer sizes) as above, with the same stepwise training strategy - i.e. no new hyperparameter search was performed. The test set contained non-viral samples (10% of all non-viruses) and the left-out anelloviruses. </p>
<p>In this setting, from the model's point of view anelloviruses are a completely unknown and unseen type of viruses. On this test set, frequency branch alone achieves the best result, with 0.755 AUROC. This is clearly above random performance and translates into getting 11 of the top 20 predictions correct on a test set with 5% prevalence. We conclude that even when dealing with viral sequences that are distant from our training (and validation) samples, using ViraMiner as recommendation system increases the chances of identifying viruses.</p>
<p>-----------</p>
<p>3) A table could be added to show all training viral classes.</p>
<p>Reply: Such table is now provided in the manuscript.</p>
<p>S2 Table. Viral classes in the entire dataset. The first column shows the viral classes (families) found in the dataset, the second column represents number of viruses found by Blast. The sequences are cut into 300 bp long sequences and the third column counts the numbers after the cut. Cutting longer contigs into smaller pieces means that the resulting 300bp training sequences represent different parts of the same virus. &#8220;Others&#8221;, at the last row of table, includes sequences that have by Blast been classified as definitely being viral, but have not been assigned a viral family yet.</p>
<supplementary-material id="pone.0222271.s003" mimetype="application/pdf" position="float" ns0:href="info:doi/10.1371/journal.pone.0222271.s003" ns0:type="simple">
<label>Attachment</label>
<caption>
<p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.pdf</named-content></p>
</caption>
</supplementary-material>
</body>
</sub-article>
