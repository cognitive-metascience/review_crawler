<article xmlns:ns0="http://www.w3.org/1999/xlink" xmlns:ns1="http://www.w3.org/1998/Math/MathML" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-19-19151</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0220161</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Memory recall</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Memory recall</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Database and informatics methods</subject><subj-group><subject>Bioinformatics</subject><subj-group><subject>Sequence analysis</subject><subj-group><subject>Sequence motif analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Database and informatics methods</subject><subj-group><subject>Bioinformatics</subject><subj-group><subject>Sequence analysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Noise reduction</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Probabilistic associative learning suffices for learning the temporal structure of multiple sequences</article-title>
<alt-title alt-title-type="running-head">Probabilistic associative learning and temporal sequences</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" ns0:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5937-7537</contrib-id>
<name name-style="western">
<surname>Martinez</surname> <given-names>Ramon H.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing &#8211; original draft</role>
<role content-type="http://credit.casrai.org/">Writing &#8211; review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" ns0:type="simple">
<name name-style="western">
<surname>Lansner</surname> <given-names>Anders</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing &#8211; review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" ns0:type="simple">
<name name-style="western">
<surname>Herman</surname> <given-names>Pawel</given-names></name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing &#8211; original draft</role>
<role content-type="http://credit.casrai.org/">Writing &#8211; review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Computational Brain Science Lab, KTH Royal Institute of Technology, Stockholm, Sweden</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Mathematics Department, Stockholm University, Stockholm, Sweden</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" ns0:type="simple">
<name name-style="western">
<surname>Morrison</surname> <given-names>Abigail</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1" />
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Research Center J&#252;lich, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email ns0:type="simple">rhmm@kth.se</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>1</day>
<month>8</month>
<year>2019</year>
</pub-date>
<volume>14</volume>
<issue>8</issue>
<elocation-id>e0220161</elocation-id>
<history>
<date date-type="received">
<day>22</day>
<month>5</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>8</day>
<month>7</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Martinez et al</copyright-holder>
<license ns0:href="http://creativecommons.org/licenses/by/4.0/" ns0:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" ns0:href="http://creativecommons.org/licenses/by/4.0/" ns0:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" ns0:href="info:doi/10.1371/journal.pone.0220161" />
<abstract>
<p>From memorizing a musical tune to navigating a well known route, many of our underlying behaviors have a strong temporal component. While the mechanisms behind the sequential nature of the underlying brain activity are likely multifarious and multi-scale, in this work we attempt to characterize to what degree some of this properties can be explained as a consequence of simple associative learning. To this end, we employ a parsimonious firing-rate attractor network equipped with the Hebbian-like Bayesian Confidence Propagating Neural Network (BCPNN) learning rule relying on synaptic traces with asymmetric temporal characteristics. The proposed network model is able to encode and reproduce temporal aspects of the input, and offers internal control of the recall dynamics by gain modulation. We provide an analytical characterisation of the relationship between the structure of the weight matrix, the dynamical network parameters and the temporal aspects of sequence recall. We also present a computational study of the performance of the system under the effects of noise for an extensive region of the parameter space. Finally, we show how the inclusion of modularity in our network structure facilitates the learning and recall of multiple overlapping sequences even in a noisy regime.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by grants from the Swedish Science Council (Vetenskapsr&#229;det, VR2018-05360), Swedish e-Science Research Center (SeRC) and the EuroSPIN Erasmus Mundus doctoral programme. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="11" />
<table-count count="1" />
<page-count count="28" />
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value> All relevant data are within the manuscript and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>1 Introduction</title>
<p>From throwing spears in the savanna to the performance of a well rehearsed dance, human behavior reflects an intrinsic sequential structure. In this light, is not surprising that sequential activity has been found in the neural dynamics across different anatomical brain areas such as the cortex [<xref ref-type="bibr" rid="pone.0220161.ref001">1</xref>&#8211;<xref ref-type="bibr" rid="pone.0220161.ref004">4</xref>], the basal ganglia [<xref ref-type="bibr" rid="pone.0220161.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0220161.ref005">5</xref>&#8211;<xref ref-type="bibr" rid="pone.0220161.ref010">10</xref>], the hippocampus [<xref ref-type="bibr" rid="pone.0220161.ref011">11</xref>&#8211;<xref ref-type="bibr" rid="pone.0220161.ref015">15</xref>] and the HVC area in songbirds [<xref ref-type="bibr" rid="pone.0220161.ref016">16</xref>, <xref ref-type="bibr" rid="pone.0220161.ref017">17</xref>]. Moreover, sequential activity is not only present in a wide range of neuroanatomical areas but is also associated with an ample repertoire of behaviors and cognitive processes including sensory perception [<xref ref-type="bibr" rid="pone.0220161.ref018">18</xref>, <xref ref-type="bibr" rid="pone.0220161.ref019">19</xref>], memory [<xref ref-type="bibr" rid="pone.0220161.ref020">20</xref>&#8211;<xref ref-type="bibr" rid="pone.0220161.ref022">22</xref>], motor behavior [<xref ref-type="bibr" rid="pone.0220161.ref023">23</xref>, <xref ref-type="bibr" rid="pone.0220161.ref024">24</xref>] and decision making [<xref ref-type="bibr" rid="pone.0220161.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0220161.ref025">25</xref>]. In our view, the entanglement of sequential activity with cognitive processes and behavior strongly suggests that sequential activity is an essential component of the information processing capabilities of the brain and therefore demands better understanding. A plausible hypothesis for the ubiquity of sequential activity is a common learning mechanism for the construction of temporal representations at the network level. Inspired by experimental evidence we propose the following constraints and properties for the neural representations and the underlying network mechanisms: First, the recall dynamics of a sequence should reflect key temporal features of the input or training signal [<xref ref-type="bibr" rid="pone.0220161.ref026">26</xref>]. Second, the network should enable temporal scaling, that is, once a sequential representation has been learned, internal neural network&#8217;s mechanisms should suffice to contract or dilate its recall duration [<xref ref-type="bibr" rid="pone.0220161.ref027">27</xref>, <xref ref-type="bibr" rid="pone.0220161.ref028">28</xref>]. Finally, as the same neural network circuits have been observed to exhibit many sequential trajectories accounting for different behaviors [<xref ref-type="bibr" rid="pone.0220161.ref012">12</xref>], it is desirable for the network to posses mechanisms to store and recall multiple and, to some extent, overlapping sequences [<xref ref-type="bibr" rid="pone.0220161.ref029">29</xref>].</p>
<p>There is evidence that sequential activity can be characterized as a succession of meta-stable cell assemblies in the cortex [<xref ref-type="bibr" rid="pone.0220161.ref021">21</xref>]. Attractor neural networks have a long standing tradition as models of sequential activity with meta-stable states corresponding to attractor patterns [<xref ref-type="bibr" rid="pone.0220161.ref030">30</xref>, <xref ref-type="bibr" rid="pone.0220161.ref031">31</xref>]. Hopfield in his seminal work [<xref ref-type="bibr" rid="pone.0220161.ref032">32</xref>, <xref ref-type="bibr" rid="pone.0220161.ref033">33</xref>] already noted that an asymmetric connectivity in a recurrent attractor network was conducive to sequential recall. However, in the most basic implementation, the asynchronous update dynamics of these Hopfield models resulted in mixed patterns, thereby gradually diluting sequential recall with time [<xref ref-type="bibr" rid="pone.0220161.ref034">34</xref>]. To overcome such limitations, temporal traces of the activity were utilized successfully as a mechanism to keep the meta-stable states active for long enough to ensure a successful transition between the patterns and some models even allow for temporal rescaling of the dynamics [<xref ref-type="bibr" rid="pone.0220161.ref035">35</xref>, <xref ref-type="bibr" rid="pone.0220161.ref036">36</xref>]. However, such models are unable to properly integrate the temporal structure of the input due to the discrete nature of their learning rule. A more sophisticated approach relies on systematically considering all the possible delays of the input and calculate all the resulting cross-correlations [<xref ref-type="bibr" rid="pone.0220161.ref037">37</xref>, <xref ref-type="bibr" rid="pone.0220161.ref038">38</xref>]. While in principle these models are able to learn arbitrary variations in the temporal structure of the input, in practice they are limited by an explosion in the number of parameters as the connectivity matrix scales with the size of the longest transition. In this work we propose an attractor model that uses the following properties to overcome the aforementioned problems: 1) It exploits temporal traces for learning in a probabilistic framework [<xref ref-type="bibr" rid="pone.0220161.ref039">39</xref>]. The temporal nature of the traces allows us to capture the temporal structure of the input, while avoiding an explosion in the number of parameters by collapsing the temporal structure into statistical estimates of the connectivity. 2) The sequence transition mechanism rests on the meta-stability of the attractor dynamics by means of intrinsic adaptation of the network units coupled with a competition mechanism that biases the transition in the correct direction. At the same time the intrinsic adaptation allows for the internal control and rescaling of the recall dynamics. 3) The use of a modular structure in our network facilitates both flexible learning and recall of overlapping representations.</p>
<p>Several network models have been proposed to account for sequential activity. While Veliz-Cuba et al [<xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>] reported that their network could learn the temporal structure of the input, it required a fine-tuned relationship between synaptic, dynamic and homeostatic parameters. Additionally, their model lacked a mechanism for temporal rescaling and the question of learning multiple sequences was not addressed. In a more recent approach by Pereira and Brunel (2018) [<xref ref-type="bibr" rid="pone.0220161.ref041">41</xref>], persistent or sequential activity dynamics could be learned depending on the temporal structure of the input. However, the proposed network did not solve the problem of temporal scaling nor the acquisition of multiple sequences. Using spike-time-dependent plasticity (STDP) with heterosynaptic competition Fiete et al. [<xref ref-type="bibr" rid="pone.0220161.ref042">42</xref>] demonstrated the capability of their model to learn multiple sequences from random activity but handling input with specific temporal structure was not elaborated in their work. Furthermore, Byrnes et Al [<xref ref-type="bibr" rid="pone.0220161.ref043">43</xref>] addressed the problem of learning overlapping sequences but their approach did not scale well as it relied on a single unique representation for every sequence even if they had overlapping elements. Finally, Murray et al. [<xref ref-type="bibr" rid="pone.0220161.ref044">44</xref>] proposed an inhibitory network inspired by the basal ganglia that achieves temporal rescaling by means of the interplay between synaptic fatigue and external input. In this model, however, the problem of handling multiple sequences could be solved only by assuming the existence of such representations in an upstream network, which we consider as a strongly limiting factor.</p>
<p>Inspired by our previous modelling efforts to study sequence [<xref ref-type="bibr" rid="pone.0220161.ref039">39</xref>] and word list learning phenomena [<xref ref-type="bibr" rid="pone.0220161.ref045">45</xref>] we propose here a modular attractor memory neural network model that learns sequential representations by means of the combination of the Bayesian Confidence Propagating Neural Network (BCPNN) learning mechanism [<xref ref-type="bibr" rid="pone.0220161.ref046">46</xref>] and asymmetrical temporal synaptic traces. We proceed by first presenting the network and its dynamics. Then, we derive analytical formulae for the temporal structure of the recall process in noiseless conditions. We also describe how learning is accomplished in the network through the use of synaptic traces and study how the temporal structure of the input is accounted for in the recall dynamics by means of the BCPNN learning rule. We follow up with a systematic characterization of the effects of noise on the sequence recall capability of the network. Finally, we elaborate on how the modularity of the network enables learning overlapping sequences and discuss key limitations.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>2 Results</title>
<sec id="sec003">
<title>2.1 Sequence recall</title>
<p>Following previous work on cortical attractor memory modelling [<xref ref-type="bibr" rid="pone.0220161.ref039">39</xref>, <xref ref-type="bibr" rid="pone.0220161.ref045">45</xref>] we present here a network capable of learning, recalling and processing sequential activity. We utilize a population model of the cortex where each unit represents a population of excitatory neurons in the superficial layer of a cortical column. Consistently with the mesoscale neuroanatomical organization, those units are organized into hypercolumns, where a winner-takes-all (WTA) mechanism representing lateral inhibition keeps the activity within the hypercolumnar module normalized [<xref ref-type="bibr" rid="pone.0220161.ref047">47</xref>]. The topological organization of the model is presented in <xref ref-type="fig" rid="pone.0220161.g001">Fig 1A</xref>. The circuit implements attractor dynamics [<xref ref-type="bibr" rid="pone.0220161.ref048">48</xref>] that leads the evolution of the network towards temporary or permanent patterns of activity (pattern refers to a particular collection of active units in the network, see <xref ref-type="fig" rid="pone.0220161.g001">Fig 1A</xref>). We refer to these stable or meta-stable states as the stored patterns of the network. The patterns themselves are defined by self-recurrent excitatory connectivity that tends to maintain the pattern in place once activated (represented by <italic>w</italic><sub><italic>self</italic></sub> in <xref ref-type="fig" rid="pone.0220161.g001">Fig 1B</xref>). The patterns can naturally be thought of as cell assemblies distributed among the hypercolumns in the network. The WTA mechanism renders the activity of the units mutually exclusive within the hypercolumns and therefore ensures sparse activity [<xref ref-type="bibr" rid="pone.0220161.ref049">49</xref>]. Sequential activation of patterns can be induced by feed-forward excitation (represented by <italic>w</italic><sub><italic>next</italic></sub> in <xref ref-type="fig" rid="pone.0220161.g001">Fig 1B</xref>) coupled with an adaptation mechanism whose role is to cease current pattern activity thereby counteracting the pattern retention effects of the self-recurrent connectivity.</p>
<fig id="pone.0220161.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Network architecture and connectivity underlying sequential pattern activation.</title>
<p>Network architecture and connectivity underlying sequential pattern activation. (A) network topology. Units <inline-formula id="pone.0220161.e001"><alternatives><graphic id="pone.0220161.e001g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e001" ns0:type="simple" /><ns1:math display="inline" id="M1"><ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mi>i</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msubsup></ns1:math></alternatives></inline-formula> are organized into hypercolumns <italic>h</italic><sub>1</sub>, &#8230;, <italic>h</italic><sub><italic>H</italic></sub>. At each point in time only one unit per hypercolumn is active due to a WTA mechanism. Each memory pattern is formed by a set of <italic>H</italic> recurrently connected units distributed across hypercolumns. For simplicity and without compromising the generality we adopt the following notation for patterns <inline-formula id="pone.0220161.e002"><alternatives><graphic id="pone.0220161.e002g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e002" ns0:type="simple" /><ns1:math display="inline" id="M2"><ns1:mrow><ns1:msub><ns1:mi>P</ns1:mi> <ns1:mn>1</ns1:mn></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mo form="prefix" stretchy="false">(</ns1:mo> <ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mn>1</ns1:mn> <ns1:mn>1</ns1:mn></ns1:msubsup> <ns1:mo>,</ns1:mo> <ns1:mo>&#8230;</ns1:mo> <ns1:mo>,</ns1:mo> <ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mn>1</ns1:mn> <ns1:mi>H</ns1:mi></ns1:msubsup> <ns1:mo form="postfix" stretchy="false">)</ns1:mo></ns1:mrow></ns1:math></alternatives></inline-formula>. We depict stereotypical network connectivity by showing all the units that emanate from unit <inline-formula id="pone.0220161.e003"><alternatives><graphic id="pone.0220161.e003g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e003" ns0:type="simple" /><ns1:math display="inline" id="M3"><ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mn>1</ns1:mn> <ns1:mn>1</ns1:mn></ns1:msubsup></ns1:math></alternatives></inline-formula>. The unit has excitatory projections to the proximate units in the sequence (connections from <inline-formula id="pone.0220161.e004"><alternatives><graphic id="pone.0220161.e004g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e004" ns0:type="simple" /><ns1:math display="inline" id="M4"><ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mn>1</ns1:mn> <ns1:mn>1</ns1:mn></ns1:msubsup></ns1:math></alternatives></inline-formula> to <inline-formula id="pone.0220161.e005"><alternatives><graphic id="pone.0220161.e005g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e005" ns0:type="simple" /><ns1:math display="inline" id="M5"><ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mn>2</ns1:mn> <ns1:mn>1</ns1:mn></ns1:msubsup></ns1:math></alternatives></inline-formula> and <inline-formula id="pone.0220161.e006"><alternatives><graphic id="pone.0220161.e006g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e006" ns0:type="simple" /><ns1:math display="inline" id="M6"><ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mn>3</ns1:mn> <ns1:mn>1</ns1:mn></ns1:msubsup></ns1:math></alternatives></inline-formula> and the corresponding units in other hypercolumns) and inhibitory projections to both the units that are farther ahead in the sequence (<inline-formula id="pone.0220161.e007"><alternatives><graphic id="pone.0220161.e007g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e007" ns0:type="simple" /><ns1:math display="inline" id="M7"><ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mn>1</ns1:mn> <ns1:mn>1</ns1:mn></ns1:msubsup></ns1:math></alternatives></inline-formula> to <inline-formula id="pone.0220161.e008"><alternatives><graphic id="pone.0220161.e008g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e008" ns0:type="simple" /><ns1:math display="inline" id="M8"><ns1:msubsup><ns1:mi>u</ns1:mi> <ns1:mn>4</ns1:mn> <ns1:mn>1</ns1:mn></ns1:msubsup></ns1:math></alternatives></inline-formula>) and the units that are not in the sequence at all (gray units). (B) abstract representation of the relevant connectivity for sequence dynamics. Please note that only connections from <italic>P</italic><sub>2</sub> are shown.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g001" ns0:type="simple" />
</fig>
<p>We model the dynamics of the units with a population model equation [<xref ref-type="bibr" rid="pone.0220161.ref050">50</xref>]. As described in <xref ref-type="disp-formula" rid="pone.0220161.e010">Eq 1</xref> the current <italic>s</italic> changes according to the base rate <italic>&#946;</italic><sub><italic>j</italic></sub> (also called the bias term) plus the total incoming current from all other N units, <inline-formula id="pone.0220161.e009"><alternatives><graphic id="pone.0220161.e009g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e009" ns0:type="simple" /><ns1:math display="inline" id="M9"><ns1:mrow><ns1:mfrac><ns1:mn>1</ns1:mn> <ns1:mi>H</ns1:mi></ns1:mfrac> <ns1:msubsup><ns1:mo>&#8721;</ns1:mo> <ns1:mi>i</ns1:mi> <ns1:mi>N</ns1:mi></ns1:msubsup> <ns1:msub><ns1:mi>w</ns1:mi> <ns1:mrow><ns1:mi>i</ns1:mi> <ns1:mi>j</ns1:mi></ns1:mrow></ns1:msub> <ns1:msub><ns1:mi>o</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub></ns1:mrow></ns1:math></alternatives></inline-formula>, normalized by the number of hypercolumns H. The binary activation variable <italic>o</italic><sub><italic>j</italic></sub> represents unit activation and is related to the current through a WTA mechanism implemented with a max operation as in <xref ref-type="disp-formula" rid="pone.0220161.e011">Eq 2</xref>. This mechanism selects the unit receiving the maximum current at each hypercolumn and activates it. We introduce intrinsic adaptation as a mechanism controlled by the variable <italic>a</italic> in <xref ref-type="disp-formula" rid="pone.0220161.e012">Eq 3</xref> to induce pattern deactivation. <italic>d&#958;</italic> represents additive white noise with variance <italic>&#963;</italic>. An extra current <italic>I</italic><sub><italic>j</italic></sub>(<italic>t</italic>) is used to model external input into the system. For the sake of generality, it is important to stress that our current based population model is equivalent to a rate-based formalism, as shown in [<xref ref-type="bibr" rid="pone.0220161.ref051">51</xref>].
<disp-formula id="pone.0220161.e010"><alternatives><graphic id="pone.0220161.e010g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e010" ns0:type="simple" /><ns1:math display="block" id="M10"><ns1:mtable displaystyle="true"><ns1:mtr><ns1:mtd columnalign="right"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>s</ns1:mi></ns1:msub> <ns1:mstyle displaystyle="true" scriptlevel="0"><ns1:mfrac><ns1:mrow><ns1:mi>d</ns1:mi> <ns1:msub><ns1:mi>s</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow> <ns1:mrow><ns1:mi>d</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:mfrac></ns1:mstyle></ns1:mrow></ns1:mtd> <ns1:mtd><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:msub><ns1:mi>&#946;</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>+</ns1:mo> <ns1:mfrac><ns1:mn>1</ns1:mn> <ns1:mi>H</ns1:mi></ns1:mfrac> <ns1:munderover><ns1:mo>&#8721;</ns1:mo> <ns1:mrow><ns1:mi>i</ns1:mi></ns1:mrow> <ns1:mi>N</ns1:mi></ns1:munderover> <ns1:msub><ns1:mi>w</ns1:mi> <ns1:mrow><ns1:mi>i</ns1:mi> <ns1:mi>j</ns1:mi></ns1:mrow></ns1:msub> <ns1:msub><ns1:mi>o</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>g</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub> <ns1:msub><ns1:mi>a</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>s</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>+</ns1:mo> <ns1:mi>&#963;</ns1:mi> <ns1:mi>d</ns1:mi> <ns1:msub><ns1:mi>&#958;</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>+</ns1:mo> <ns1:msub><ns1:mi>I</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:math></alternatives> <label>(1)</label></disp-formula>
<disp-formula id="pone.0220161.e011"><alternatives><graphic id="pone.0220161.e011g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e011" ns0:type="simple" /><ns1:math display="block" id="M11"><ns1:mrow><ns1:mtable><ns1:mtr><ns1:mtd><ns1:mrow /></ns1:mtd></ns1:mtr><ns1:mtr><ns1:mtd><ns1:mrow><ns1:msub><ns1:mi>o</ns1:mi><ns1:mi>j</ns1:mi></ns1:msub><ns1:mo>=</ns1:mo><ns1:mo>{</ns1:mo><ns1:mtable><ns1:mtr><ns1:mtd><ns1:mrow><ns1:mn>1</ns1:mn><ns1:mo>,</ns1:mo></ns1:mrow></ns1:mtd><ns1:mtd><ns1:mrow><ns1:msub><ns1:mi>s</ns1:mi><ns1:mi>j</ns1:mi></ns1:msub><ns1:mo>=</ns1:mo><ns1:munder><ns1:mrow><ns1:mtext>max</ns1:mtext></ns1:mrow><ns1:mrow><ns1:mi>h</ns1:mi><ns1:mi>y</ns1:mi><ns1:mi>p</ns1:mi><ns1:mi>e</ns1:mi><ns1:mi>r</ns1:mi><ns1:mi>c</ns1:mi><ns1:mi>o</ns1:mi><ns1:mi>l</ns1:mi><ns1:mi>u</ns1:mi><ns1:mi>m</ns1:mi><ns1:mi>n</ns1:mi></ns1:mrow></ns1:munder><ns1:mrow><ns1:mo>(</ns1:mo><ns1:mstyle mathsize="normal" mathvariant="bold"><ns1:mi>s</ns1:mi></ns1:mstyle><ns1:mo>)</ns1:mo></ns1:mrow><ns1:mo>,</ns1:mo></ns1:mrow></ns1:mtd></ns1:mtr><ns1:mtr><ns1:mtd><ns1:mrow><ns1:mn>0</ns1:mn><ns1:mo>,</ns1:mo></ns1:mrow></ns1:mtd><ns1:mtd><ns1:mrow><ns1:mtext>otherwise</ns1:mtext></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:mrow></ns1:math></alternatives> <label>(2)</label></disp-formula>
<disp-formula id="pone.0220161.e012"><alternatives><graphic id="pone.0220161.e012g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e012" ns0:type="simple" /><ns1:math display="block" id="M12"><ns1:mtable displaystyle="true"><ns1:mtr><ns1:mtd columnalign="right"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub> <ns1:mstyle displaystyle="true" scriptlevel="0"><ns1:mfrac><ns1:mrow><ns1:mi>d</ns1:mi> <ns1:msub><ns1:mi>a</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow> <ns1:mrow><ns1:mi>d</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:mfrac></ns1:mstyle></ns1:mrow></ns1:mtd> <ns1:mtd><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:msub><ns1:mi>o</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>a</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:math></alternatives> <label>(3)</label></disp-formula></p>
<p>It has long been recognized that an attractor model with asymmetric connectivity produces sequential dynamics [<xref ref-type="bibr" rid="pone.0220161.ref052">52</xref>]. In that vein, we explain now how an asymmetric connectivity matrix coupled with the dynamics of our model brings about sequential activity.</p>
<p>In <xref ref-type="fig" rid="pone.0220161.g002">Fig 2A</xref> we show a case of successful sequential recall in the network with the connectivity matrix depicted in <xref ref-type="fig" rid="pone.0220161.g002">Fig 2D</xref>. Here we handcrafted the connectivity matrix to illustrate the unfolding of the following dynamics. Once the first pattern gets activated (<italic>o</italic><sub><italic>i</italic></sub> = 1) as a result of an external cue (current input <italic>I</italic>(<italic>t</italic>) to all the units belonging to the pattern) the adaptation current <italic>a</italic><sub><italic>i</italic></sub> depicted in <xref ref-type="fig" rid="pone.0220161.g002">Fig 2B</xref> starts growing and, in consequence, the self-excitatory current <italic>s</italic><sub><italic>i</italic></sub> becomes smaller. At some point, the self-excitatory current <italic>s</italic><sub><italic>i</italic></sub> is going to become weaker than the feed-forward current <italic>s</italic><sub><italic>i</italic>+1</sub>, which the next pattern in the sequence receives. Then, the competitive WTA mechanism mediates the activation of the next pattern (<italic>o</italic><sub><italic>i</italic>+1</sub> = 1) and suppresses the current one (<italic>o</italic><sub><italic>i</italic></sub>) by competition. These dynamics are self-sustained and the cycle repeats until the end of the sequence. We depict the profile of such transitions in <xref ref-type="fig" rid="pone.0220161.g002">Fig 2C</xref>. The total time that the pattern stays activated is defined as the persistence time <italic>T</italic><sub><italic>per</italic></sub> (as used in [<xref ref-type="bibr" rid="pone.0220161.ref053">53</xref>]) and depends on the interplay between the connectivity matrix, the bias term and the adaptation. We present typical values of the network parameters in <xref ref-type="table" rid="pone.0220161.t001">Table 1</xref>.</p>
<fig id="pone.0220161.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g002</object-id>
<label>Fig 2</label>
<caption>
<title>An instance of sequence recall in the model.</title>
<p>(A) Sequential activity of units initiated by the cue. (B) The time course of the adaptation current for each unit. (C) The total current <italic>s</italic>, note that this quantity crossing the value of <italic>w</italic><sub><italic>next</italic></sub><italic>o</italic> (depicted here with a dotted line) marks the transition point from one pattern to the next. (D) The connectivity matrix where we have included pointers to the most important quantities <italic>w</italic><sub><italic>self</italic></sub> for the self-excitatory weight, <italic>w</italic><sub><italic>next</italic></sub> for the inhibitory connection to the next element, <italic>w</italic><sub><italic>rest</italic></sub> for the largest connection in the column after <italic>w</italic><sub><italic>next</italic></sub> and <italic>w</italic><sub><italic>prev</italic></sub> for the connection to the last pattern that was active in the sequence.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g002" ns0:type="simple" />
</fig>
<table-wrap id="pone.0220161.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.t001</object-id>
<label>Table 1</label>
<caption>
<title>Network&#8217;s parameters and quantities.</title>
</caption>
<alternatives>
<graphic id="pone.0220161.t001g" mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.t001" ns0:type="simple" />
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle" />
<col align="left" valign="middle" />
<col align="left" valign="middle" />
</colgroup>
<thead>
<tr>
<th align="center">Symbol</th>
<th align="center">Name</th>
<th align="center">Values</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><italic>&#964;</italic><sub><italic>s</italic></sub></td>
<td align="center">Synaptic time constant</td>
<td align="center">10 <italic>ms</italic></td>
</tr>
<tr>
<td align="center"><italic>&#964;</italic><sub><italic>a</italic></sub></td>
<td align="center">Adaptation time constant</td>
<td align="center">250 <italic>ms</italic></td>
</tr>
<tr>
<td align="center"><italic>g</italic><sub><italic>a</italic></sub></td>
<td align="center">Adaptation gain</td>
<td align="center">0 &#8722; 2.5 (units of <italic>w</italic>, control)</td>
</tr>
<tr>
<td align="center">
<inline-formula id="pone.0220161.e013">
<alternatives>
<graphic id="pone.0220161.e013g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e013" ns0:type="simple" />
<ns1:math display="inline" id="M13">
<ns1:msub>
<ns1:mi>&#964;</ns1:mi>
<ns1:msub>
<ns1:mi>z</ns1:mi>
<ns1:mrow>
<ns1:mi>p</ns1:mi>
<ns1:mi>r</ns1:mi>
<ns1:mi>e</ns1:mi>
</ns1:mrow>
</ns1:msub>
</ns1:msub>
</ns1:math>
</alternatives>
</inline-formula>
</td>
<td align="center">Pre synaptic z-filter time constant</td>
<td align="center">5 &#8722; 150 <italic>ms</italic></td>
</tr>
<tr>
<td align="center">
<inline-formula id="pone.0220161.e014">
<alternatives>
<graphic id="pone.0220161.e014g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e014" ns0:type="simple" />
<ns1:math display="inline" id="M14">
<ns1:msub>
<ns1:mi>&#964;</ns1:mi>
<ns1:msub>
<ns1:mi>z</ns1:mi>
<ns1:mrow>
<ns1:mi>p</ns1:mi>
<ns1:mi>o</ns1:mi>
<ns1:mi>s</ns1:mi>
<ns1:mi>t</ns1:mi>
</ns1:mrow>
</ns1:msub>
</ns1:msub>
</ns1:math>
</alternatives>
</inline-formula>
</td>
<td align="center">Post synaptic z-filter time constant</td>
<td align="center">5 <italic>ms</italic></td>
</tr>
<tr>
<td align="center"><italic>&#964;</italic><sub><italic>p</italic></sub></td>
<td align="center">Probability traces time constant</td>
<td align="center">5 <italic>s</italic></td>
</tr>
<tr>
<td align="center"><italic>&#963;</italic></td>
<td align="center">Standard deviation of s values</td>
<td align="center">0 &#8722; 3</td>
</tr>
<tr>
<td align="center"><italic>T</italic><sub><italic>per</italic></sub></td>
<td align="center">Persistence time</td>
<td align="center">50 &#8722; 3000 <italic>ms</italic> (controlled)</td>
</tr>
<tr>
<td align="center"><italic>T</italic><sub><italic>p</italic></sub></td>
<td align="center">Pulse time</td>
<td align="center">100 <italic>ms</italic></td>
</tr>
<tr>
<td align="center">IPI</td>
<td align="center">Inter pulse interval</td>
<td align="center">0 <italic>ms</italic></td>
</tr>
<tr>
<td align="center">H</td>
<td align="center">Number of hypercolumns</td>
<td align="center">variable</td>
</tr>
<tr>
<td align="center">N</td>
<td align="center">Total number of units</td>
<td align="center">variable</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec004">
<title>2.2 Persistence time</title>
<p>Two important characteristics of sequence dynamics are the order in which the patterns are activated (the serial order) and the temporal structure of those activations (the temporal order) [<xref ref-type="bibr" rid="pone.0220161.ref054">54</xref>]. In our model the serial order is determined by the differential connectivity between the units belonging to the currently activated pattern and those of all the other patterns. In general, the next pattern activated will be the one for which the quantity &#916;<italic>w</italic><sub><italic>next</italic></sub> = <italic>w</italic><sub><italic>self</italic></sub> &#8722; <italic>w</italic><sub><italic>next</italic></sub> is smaller. The persistence time or temporal information of the sequence on the other hand is determined by the interplay between the connectivity of the network and the dynamical parameters of the network. We now proceed to characterize this relationship analytically. From the deterministic trajectories (see <xref ref-type="supplementary-material" rid="pone.0220161.s002">S1 Appendix</xref>) we can find the time point at which the currents from two subsequent units are equal, <italic>s</italic><sub><italic>i</italic></sub>(<italic>t</italic>) = <italic>s</italic><sub><italic>i</italic>+1</sub>(<italic>t</italic>), as this results in the transition and thus determines the persistence time, <italic>T</italic><sub><italic>per</italic></sub>. Solving for <italic>t</italic> we can estimate the persistence time, <italic>T</italic><sub><italic>per</italic></sub>, in terms of the other network parameters:
<disp-formula id="pone.0220161.e015"><alternatives><graphic id="pone.0220161.e015g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e015" ns0:type="simple" /><ns1:math display="block" id="M15"><ns1:mtable displaystyle="true"><ns1:mtr><ns1:mtd columnalign="right"><ns1:msub><ns1:mi>T</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>r</ns1:mi></ns1:mrow></ns1:msub><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub> <ns1:mo form="prefix">log</ns1:mo> <ns1:mo>(</ns1:mo> <ns1:mfrac><ns1:mn>1</ns1:mn> <ns1:mrow><ns1:mn>1</ns1:mn> <ns1:mo>-</ns1:mo> <ns1:mi>B</ns1:mi></ns1:mrow></ns1:mfrac> <ns1:mo>)</ns1:mo> <ns1:mo>+</ns1:mo> <ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub> <ns1:mo form="prefix">log</ns1:mo> <ns1:mo>(</ns1:mo> <ns1:mfrac><ns1:mn>1</ns1:mn> <ns1:mrow><ns1:mn>1</ns1:mn> <ns1:mo>-</ns1:mo> <ns1:mfrac><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>s</ns1:mi></ns1:msub> <ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub></ns1:mfrac></ns1:mrow></ns1:mfrac> <ns1:mo>)</ns1:mo></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:math></alternatives> <label>(4)</label></disp-formula>
<disp-formula id="pone.0220161.e016"><alternatives><graphic id="pone.0220161.e016g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e016" ns0:type="simple" /><ns1:math display="block" id="M16"><ns1:mtable displaystyle="true"><ns1:mtr><ns1:mtd columnalign="left"><ns1:mi>B</ns1:mi></ns1:mtd><ns1:mtd columnalign="left"><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:mfrac><ns1:mrow><ns1:msub><ns1:mi>w</ns1:mi> <ns1:mrow><ns1:mi>s</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>l</ns1:mi> <ns1:mi>f</ns1:mi></ns1:mrow></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>w</ns1:mi> <ns1:mrow><ns1:mi>n</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>x</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub> <ns1:mo>+</ns1:mo> <ns1:msub><ns1:mi>&#946;</ns1:mi> <ns1:mrow><ns1:mi>s</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>l</ns1:mi> <ns1:mi>f</ns1:mi></ns1:mrow></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>&#946;</ns1:mi> <ns1:mrow><ns1:mi>n</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>x</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:mrow> <ns1:msub><ns1:mi>g</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub></ns1:mfrac></ns1:mrow></ns1:mtd></ns1:mtr> <ns1:mtr><ns1:mtd /><ns1:mtd columnalign="left"><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:mfrac><ns1:mrow><ns1:mo>&#916;</ns1:mo> <ns1:msub><ns1:mi>w</ns1:mi> <ns1:mrow><ns1:mi>n</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>x</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub> <ns1:mo>+</ns1:mo> <ns1:mo>&#916;</ns1:mo> <ns1:msub><ns1:mi>&#946;</ns1:mi> <ns1:mrow><ns1:mi>n</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>x</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:mrow> <ns1:msub><ns1:mi>g</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub></ns1:mfrac></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:math></alternatives> <label>(5)</label></disp-formula></p>
<p>The parameter B in <xref ref-type="disp-formula" rid="pone.0220161.e016">Eq 5</xref> condenses information regarding the connectivity <italic>w</italic>, bias terms <italic>&#946;</italic>, and adaptation strength <italic>g</italic><sub><italic>a</italic></sub>. From <xref ref-type="disp-formula" rid="pone.0220161.e015">Eq 4</xref> we can infer that <italic>T</italic><sub><italic>per</italic></sub> is defined only for 0 &lt; <italic>B</italic> &lt; 1. This sets the conditions for how the weights, bias and external input interact with the adaptation parameters in order for the sequence to be learned and recalled. The straightforward interpretation for <italic>B</italic> &lt; 1 is that the adaptation has to be strong enough to overcome the effects of the other currents, while <italic>B</italic> &gt; 0 sets the connectivity conditions for sequence recall to occur (<italic>w</italic><sub><italic>self</italic></sub> &gt; <italic>w</italic><sub><italic>next</italic></sub>). As illustrated in <xref ref-type="fig" rid="pone.0220161.g003">Fig 3A</xref> <italic>T</italic><sub><italic>per</italic></sub> is small for <italic>B</italic> &#8776; 0 and diverges to infinity as <italic>B</italic> &#8776; 1. This facilitates the interpretation of <italic>B</italic> as a unitless parameter whose natural interpretation is the inverse of transition speed, as shown in the examples provided in <xref ref-type="fig" rid="pone.0220161.g003">Fig 3B and 3C</xref>.</p>
<fig id="pone.0220161.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Systematic study of persistence time <italic>T</italic><sub><italic>per</italic></sub>.</title>
<p>(A) <italic>T</italic><sub><italic>per</italic></sub> dependence of B. The blue solid line represents the theoretical prediction described in <xref ref-type="disp-formula" rid="pone.0220161.e015">Eq 4</xref> and the orange bullets are the result of simulations. Inset depicts what happens close to <italic>B</italic> = 0 where we can see that the lower limit is the time constant of the units <italic>&#964;</italic><sub><italic>s</italic></sub>. (B) An example of sequence recall where <italic>T</italic><sub><italic>per</italic></sub> = 100 <italic>ms</italic>. This example corresponds to configuration marked the black star in (A). (C) example of sequence recall with <italic>T</italic><sub><italic>per</italic></sub> = 500 <italic>ms</italic>. This example corresponds to the configuration marked with a black triangle in (A). (D) Recall of a sequence with variable temporal structure (varying <italic>T</italic><sub><italic>per</italic></sub>. The values of <italic>T</italic><sub><italic>per</italic></sub> are 500, 200, 1200, 100, and 400 ms respectively.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g003" ns0:type="simple" />
</fig>
<p>Controlling the individual persistence times of different patterns (the temporal structure) through short-term dynamics has been discussed previously in the literature [<xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>]. In our network the temporal structure of the sequence is also controlled by the adaptation dynamics. We illustrate this in <xref ref-type="fig" rid="pone.0220161.g003">Fig 3D</xref> where by choosing specific values for the adaptation gain, <italic>g</italic><sub><italic>a</italic></sub>, precise control of the <italic>T</italic><sub><italic>per</italic></sub> is achieved for every attractor.</p>
<p>For illustration purposes, <xref ref-type="disp-formula" rid="pone.0220161.e015">Eq 4</xref> is given for the case of orthogonal patterns and a single hypercolumn. In the general case with multiple hypercolumns it is possible that not all local transitions within a pattern (in different hypercolumns) occur at the same time. Moreover, as we recall sequences with non-repeating patterns the adaptation effects are not specified. A full treatment, that handles both the modular effects of non-overlapping elements and adaptation effects is provided in the supplementary material (see <xref ref-type="supplementary-material" rid="pone.0220161.s002">S1 Appendix</xref>).</p>
</sec>
<sec id="sec005">
<title>2.3 Learning</title>
<p>So far we have shown that our model can support sequence recall and control of the temporal structure through the adaptation dynamics. We now show that when the network is subject to the right spatio-temporal input structure then associative Hebbian learning is sufficient to induce the learning of the asymmetric connectivity structure characteristic of sequence recall [<xref ref-type="bibr" rid="pone.0220161.ref052">52</xref>]. Based on previous work [<xref ref-type="bibr" rid="pone.0220161.ref039">39</xref>], we use the BCPNN learning rule in its incremental on-line version [<xref ref-type="bibr" rid="pone.0220161.ref055">55</xref>] with learning mediated through asymmetric synaptic time traces. The version of the BCPNN learning rule utilized here is an adaptation of the discrete learning rule (presented in [<xref ref-type="bibr" rid="pone.0220161.ref046">46</xref>]) to a continuous setting.
<disp-formula id="pone.0220161.e017"><alternatives><graphic id="pone.0220161.e017g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e017" ns0:type="simple" /><ns1:math display="block" id="M17"><ns1:mtable displaystyle="true"><ns1:mtr><ns1:mtd columnalign="right"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mstyle displaystyle="true" scriptlevel="0"><ns1:mfrac><ns1:mrow><ns1:mi>d</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub></ns1:mrow> <ns1:mrow><ns1:mi>d</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:mfrac></ns1:mstyle></ns1:mrow><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:msub><ns1:mi>o</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub></ns1:mrow></ns1:mtd> <ns1:mtd columnalign="left"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mstyle displaystyle="true" scriptlevel="0"><ns1:mfrac><ns1:mrow><ns1:mi>d</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow> <ns1:mrow><ns1:mi>d</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:mfrac></ns1:mstyle></ns1:mrow><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:msub><ns1:mi>o</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:math></alternatives> <label>(6)</label></disp-formula>
<disp-formula id="pone.0220161.e018"><alternatives><graphic id="pone.0220161.e018g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e018" ns0:type="simple" /><ns1:math display="block" id="M18"><ns1:mtable displaystyle="true"><ns1:mtr><ns1:mtd columnalign="right"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>p</ns1:mi></ns1:msub> <ns1:mstyle displaystyle="true" scriptlevel="0"><ns1:mfrac><ns1:mrow><ns1:mi>d</ns1:mi> <ns1:msub><ns1:mi>p</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub></ns1:mrow> <ns1:mrow><ns1:mi>d</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:mfrac></ns1:mstyle></ns1:mrow><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>p</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub> <ns1:mspace width="2.em" /><ns1:mspace width="1.em" /><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>p</ns1:mi></ns1:msub> <ns1:mstyle displaystyle="true" scriptlevel="0"><ns1:mfrac><ns1:mrow><ns1:mi>d</ns1:mi> <ns1:msub><ns1:mi>p</ns1:mi> <ns1:mrow><ns1:mi>i</ns1:mi> <ns1:mi>j</ns1:mi></ns1:mrow></ns1:msub></ns1:mrow> <ns1:mrow><ns1:mi>d</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:mfrac></ns1:mstyle> <ns1:mo>=</ns1:mo> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>p</ns1:mi> <ns1:mrow><ns1:mi>i</ns1:mi> <ns1:mi>j</ns1:mi></ns1:mrow></ns1:msub></ns1:mrow></ns1:mtd> <ns1:mtd columnalign="left"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>p</ns1:mi></ns1:msub> <ns1:mstyle displaystyle="true" scriptlevel="0"><ns1:mfrac><ns1:mrow><ns1:mi>d</ns1:mi> <ns1:msub><ns1:mi>p</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow> <ns1:mrow><ns1:mi>d</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:mfrac></ns1:mstyle></ns1:mrow><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>-</ns1:mo> <ns1:msub><ns1:mi>p</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:math></alternatives> <label>(7)</label></disp-formula>
<disp-formula id="pone.0220161.e019"><alternatives><graphic id="pone.0220161.e019g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e019" ns0:type="simple" /><ns1:math display="block" id="M19"><ns1:mtable displaystyle="true"><ns1:mtr><ns1:mtd columnalign="right"><ns1:msub><ns1:mi>w</ns1:mi> <ns1:mrow><ns1:mi>i</ns1:mi> <ns1:mi>j</ns1:mi></ns1:mrow></ns1:msub><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:mo form="prefix">log</ns1:mo> <ns1:mo>(</ns1:mo> <ns1:mfrac><ns1:msub><ns1:mi>p</ns1:mi> <ns1:mrow><ns1:mi>i</ns1:mi> <ns1:mi>j</ns1:mi></ns1:mrow></ns1:msub> <ns1:mrow><ns1:msub><ns1:mi>p</ns1:mi> <ns1:mi>i</ns1:mi></ns1:msub> <ns1:msub><ns1:mi>p</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub></ns1:mrow></ns1:mfrac> <ns1:mo>)</ns1:mo></ns1:mrow></ns1:mtd> <ns1:mtd columnalign="left"><ns1:msub><ns1:mi>&#946;</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub><ns1:mrow><ns1:mo>=</ns1:mo> <ns1:mo form="prefix">log</ns1:mo> <ns1:mo>(</ns1:mo> <ns1:msub><ns1:mi>p</ns1:mi> <ns1:mi>j</ns1:mi></ns1:msub> <ns1:mo>)</ns1:mo></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:math></alternatives> <label>(8)</label></disp-formula></p>
<p>In the spirit of associative learning the BCPNN rule sets positive weights of recurrent connections between units that statistically tend to co-activate and creates inhibitory connections (negative weights) between those that do not. This is reflected in <xref ref-type="disp-formula" rid="pone.0220161.e019">Eq 8</xref>, where the connections are determined with a logarithmic ratio between the probability of co-activation (<italic>p</italic><sub><italic>ij</italic></sub>) and the product of the activation probabilities (<italic>p</italic><sub><italic>i</italic></sub> and <italic>p</italic><sub><italic>j</italic></sub>). Note that if the events are independent the weight between them is zero (<italic>p</italic><sub><italic>ij</italic></sub> = <italic>p</italic><sub><italic>i</italic></sub> <italic>p</italic><sub><italic>j</italic></sub>). Nevertheless, basic associative learning can only bind units that are active simultaneously. In order to bind units that are not simultaneously active in time we need an extra mechanism of temporal integration [<xref ref-type="bibr" rid="pone.0220161.ref052">52</xref>]. To overcome this we combine the BCPNN learning rule with the introduction of the z-traces in order to create temporal associations between units that are contiguous in time [<xref ref-type="bibr" rid="pone.0220161.ref056">56</xref>]. The z-traces, defined in <xref ref-type="disp-formula" rid="pone.0220161.e017">Eq 6</xref>, which can be thought of as synaptic traces, are a low-passed filtered version of the unit activations <italic>o</italic> and dynamically track the activation as shown in the top of <xref ref-type="fig" rid="pone.0220161.g004">Fig 4B</xref>. To approximate the probabilities of activation (<italic>p</italic><sub><italic>i</italic></sub> and <italic>p</italic><sub><italic>j</italic></sub>) and co-activation (<italic>p</italic><sub><italic>ij</italic></sub>) the z-traces are accumulated over time in agreement with <xref ref-type="disp-formula" rid="pone.0220161.e018">Eq 7</xref>, which implements an on-line version of the exponentially weighted moving average (EWMA). As illustrated in <xref ref-type="fig" rid="pone.0220161.g004">Fig 4A</xref>, asymmetry in the connectivity matrix arises from having two z-traces, a pre-synaptic trace with a slow time constant <inline-formula id="pone.0220161.e020"><alternatives><graphic id="pone.0220161.e020g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e020" ns0:type="simple" /><ns1:math display="inline" id="M20"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> and a fast post-synaptic trace with a fast time constant <inline-formula id="pone.0220161.e021"><alternatives><graphic id="pone.0220161.e021g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e021" ns0:type="simple" /><ns1:math display="inline" id="M21"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pone.0220161.ref039">39</xref>]. In short, the z-traces work as a temporal proxy for unit activation that allow us to use the probabilistic framework of the BCPNN rule to learn the sequential structure of the input.</p>
<fig id="pone.0220161.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Sequence learning paradigm.</title>
<p>(A) Relationship between the connectivity matrix <italic>w</italic> and the z-traces. The weight <italic>w</italic><sub><italic>ij</italic></sub> from unit <italic>i</italic> to unit <italic>j</italic> is determined by the probability of co-activation of those units which in turn is proportional to the overlap between the z-traces (show in dark red). The symmetric connection <italic>w</italic><sub><italic>ij</italic></sub> is calculated through the same process but with the traces flipped (here shown in dark blue). Note that the asymmetry of the weights is a direct consequence of the asymmetry of the z-traces. (B) Schematic of the training protocol. In the top we show how the activation of the patterns (in gray) induces the z-traces. In the bottom we show the structure of the training protocol where the pulse time <italic>T</italic><sub><italic>p</italic></sub> and the inter-pulse interval (IPI) are shown for further reference. (C) We trained a network with only five units in a single hypercolumn for illustration. The first three epochs (50 in total) of the training protocol are shown for reference. The values of the parameters during training were set to <italic>T</italic><sub><italic>p</italic></sub> = 100 <italic>ms</italic>, <italic>IPI</italic> = 0 <italic>ms</italic>, <inline-formula id="pone.0220161.e022"><alternatives><graphic id="pone.0220161.e022g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e022" ns0:type="simple" /><ns1:math display="inline" id="M22"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mn>50</ns1:mn> <ns1:mspace width="0.222em" /><ns1:mi>m</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:math></alternatives></inline-formula> and <inline-formula id="pone.0220161.e023"><alternatives><graphic id="pone.0220161.e023g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e023" ns0:type="simple" /><ns1:math display="inline" id="M23"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mn>5</ns1:mn> <ns1:mspace width="0.222em" /><ns1:mi>m</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:math></alternatives></inline-formula>. (D) The matrix at the end of the training (after 50 epochs). (E) Evolution of the probability values during the first three epochs of training. The probability values of the pre (<italic>p</italic><sub><italic>i</italic></sub>), post (<italic>p</italic><sub><italic>j</italic></sub>) and joint probability (<italic>p</italic><sub><italic>ij</italic></sub>) evolve with every presentation. Note that the same color code is used in images C, E and F. (F) Long-term evolution of the probabilities with respect to the number of epochs. The values of the probability traces eventually reach a steady state. (G) Short-term evolution of the weight matrix at the points marked in the first epoch in C. Note that the colors are subjected to the same colorbar reference as in D.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g004" ns0:type="simple" />
</fig>
<p>The training protocol shown in <xref ref-type="fig" rid="pone.0220161.g004">Fig 4B</xref> is driven by the temporal nature of the input and can be characterized by two quantities: the time that the network is exposed to a pattern (this is implemented by clamping the units belonging to the corresponding pattern through <italic>I</italic> in <xref ref-type="disp-formula" rid="pone.0220161.e010">Eq 1</xref>) called the pulse time, <italic>T</italic><sub><italic>p</italic></sub>, and the time between the presentations of two patterns referred as the inter-pulse-interval (IPI). In the following we use a homogeneous training protocol where the values of the <italic>T</italic><sub><italic>p</italic></sub> and IPI are the same for every pattern in the sequence.</p>
<p>The network&#8217;s weights were learned using a training protocol where the patterns were presented sequentially for a number of epochs (50 epochs in the example illustrated in <xref ref-type="fig" rid="pone.0220161.g004">Fig 4C&#8211;4G</xref>). With every presentation of the stimulus the probability traces <italic>p</italic> grow accordingly (see <xref ref-type="fig" rid="pone.0220161.g004">Fig 4E</xref>), slowly evolving to their steady state value (<xref ref-type="fig" rid="pone.0220161.g004">Fig 4F</xref>). While the steady state weight matrix that results from training reveals asymmetric connectivity (<xref ref-type="fig" rid="pone.0220161.g004">Fig 4D</xref>), the sequential structure of the input is learned as early as during the first epoch, as can be observed in <xref ref-type="fig" rid="pone.0220161.g004">Fig 4G</xref>. This demonstrates that the sequential structure of the input has been successfully learned by the BCPNN rule with the help of the z-traces.</p>
<p>We characterized the relationship between the connectivity matrix (<italic>w</italic><sub><italic>self</italic></sub>, <italic>w</italic><sub><italic>next</italic></sub> and <italic>w</italic><sub><italic>prev</italic></sub>) and the training protocol parameters (the pulse time <italic>T</italic><sub><italic>P</italic></sub>, the inter-pulse-interval, IPI, and the two time constants of the synaptic traces <inline-formula id="pone.0220161.e024"><alternatives><graphic id="pone.0220161.e024g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e024" ns0:type="simple" /><ns1:math display="inline" id="M24"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> and <inline-formula id="pone.0220161.e025"><alternatives><graphic id="pone.0220161.e025g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e025" ns0:type="simple" /><ns1:math display="inline" id="M25"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula>). We summarize our findings and its relationship to the persistence time, <italic>T</italic><sub><italic>per</italic></sub>, in <xref ref-type="fig" rid="pone.0220161.g005">Fig 5</xref>. Larger values of <italic>T</italic><sub><italic>p</italic></sub> lead first to an increase in the value of <italic>w</italic><sub><italic>self</italic></sub> followed by its stabilization thereafter and to a decrease in the value of <italic>w</italic><sub><italic>next</italic></sub> (<xref ref-type="fig" rid="pone.0220161.g005">Fig 5A</xref>). This can be explained by the fact that while the ratio between self co-activation and the total training time remains more or less constant (stabilizing <italic>w</italic><sub><italic>self</italic></sub>) the co-activation between units becomes a smaller portion of the whole training protocol effectively reducing the estimating of <italic>p</italic><sub><italic>ij</italic></sub> (making <italic>w</italic><sub><italic>next</italic></sub> smaller). In consequence, the rate of <italic>T</italic><sub><italic>per</italic></sub> growth becomes constant with larger <italic>T</italic><sub><italic>p</italic></sub> giving a logarithmic encoding of time (<xref ref-type="fig" rid="pone.0220161.g005">Fig 5D</xref>). In contrast, larger IPIs lead to monotonic increments and decrements in <italic>w</italic><sub><italic>self</italic></sub> and <italic>w</italic><sub><italic>next</italic></sub> respectively (<xref ref-type="fig" rid="pone.0220161.g005">Fig 5B</xref>). The reason for this is that larger IPIs bring about an overall longer training protocol and after the co-activation of the units ceases, the product <italic>p</italic><sub><italic>i</italic></sub><italic>p</italic><sub><italic>i</italic></sub>, decreases faster than <italic>p</italic><sub><italic>ii</italic></sub> leading to a larger <italic>w</italic><sub><italic>self</italic></sub>. The value of <italic>w</italic><sub><italic>next</italic></sub>, on the other hand, is rendered smaller by larger IPIs as a consequence of the unit&#8217;s activations begin further apart in time. It follows that <italic>T</italic><sub><italic>per</italic></sub> increases faster with larger IPIs as both <italic>w</italic><sub><italic>self</italic></sub> and <italic>w</italic><sub><italic>next</italic></sub> separate farther and farther with growing inter pulse intervals (<xref ref-type="fig" rid="pone.0220161.g005">Fig 5E</xref>). The effect of the z-filters time constant <italic>&#964;</italic><sub><italic>z</italic></sub> in the weights can be described as diminishing the difference between <italic>w</italic><sub><italic>self</italic></sub> and <italic>w</italic><sub><italic>next</italic></sub> (<xref ref-type="fig" rid="pone.0220161.g005">Fig 5C</xref>). The results can be explained by interpreting the effect of increasing <inline-formula id="pone.0220161.e026"><alternatives><graphic id="pone.0220161.e026g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e026" ns0:type="simple" /><ns1:math display="inline" id="M26"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> as spreading more and more the activation in time rendering the co-activations less meaningful overall (co-activation probability drops). This results in a diminishing value of <italic>T</italic><sub><italic>per</italic></sub> as the difference between weights &#916;<italic>w</italic><sub><italic>next</italic></sub> drops with larger values of <inline-formula id="pone.0220161.e027"><alternatives><graphic id="pone.0220161.e027g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e027" ns0:type="simple" /><ns1:math display="inline" id="M27"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pone.0220161.g005">Fig 5F</xref>). Note here that the point at which <inline-formula id="pone.0220161.e028"><alternatives><graphic id="pone.0220161.e028g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e028" ns0:type="simple" /><ns1:math display="inline" id="M28"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> becomes larger than <inline-formula id="pone.0220161.e029"><alternatives><graphic id="pone.0220161.e029g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e029" ns0:type="simple" /><ns1:math display="inline" id="M29"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> (marked with a dashed red line) coincides with <italic>w</italic><sub><italic>next</italic></sub> becoming larger than <italic>w</italic><sub><italic>prev</italic></sub> as we should expect. The reasoning for <italic>w</italic><sub><italic>pre</italic></sub> is analogous to that of <italic>w</italic><sub><italic>next</italic></sub> with the only difference in synaptic time constant (<inline-formula id="pone.0220161.e030"><alternatives><graphic id="pone.0220161.e030g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e030" ns0:type="simple" /><ns1:math display="inline" id="M30"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> instead of <inline-formula id="pone.0220161.e031"><alternatives><graphic id="pone.0220161.e031g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e031" ns0:type="simple" /><ns1:math display="inline" id="M31"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula>).</p>
<fig id="pone.0220161.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Characterization of the effect of training in the connectivity weights and persistent times.</title>
<p>The equation on the inset in D relates <italic>T</italic><sub><italic>per</italic></sub> to &#916;<italic>w</italic><sub><italic>next</italic></sub> = <italic>w</italic><sub><italic>self</italic></sub> &#8722; <italic>w</italic><sub><italic>next</italic></sub> which we show as dashed red lines in each of the top figures (note that here &#916;<italic>&#946;</italic> = 0 as we trained with an homogeneous protocol). When the parameters themselves are not subjected to variation their values are: <italic>T</italic><sub><italic>p</italic></sub> = 100 <italic>ms</italic>, <italic>IPI</italic> = 0 <italic>ms</italic>, <inline-formula id="pone.0220161.e032"><alternatives><graphic id="pone.0220161.e032g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e032" ns0:type="simple" /><ns1:math display="inline" id="M32"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mn>25</ns1:mn> <ns1:mspace width="0.222em" /><ns1:mi>m</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:math></alternatives></inline-formula>, <inline-formula id="pone.0220161.e033"><alternatives><graphic id="pone.0220161.e033g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e033" ns0:type="simple" /><ns1:math display="inline" id="M33"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mn>20</ns1:mn> <ns1:mspace width="0.222em" /><ns1:mi>m</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:math></alternatives></inline-formula> for all the units. (A-C) Show how the weights depend on the training parameters <italic>T</italic><sub><italic>p</italic></sub>, inter pulse interval and <inline-formula id="pone.0220161.e034"><alternatives><graphic id="pone.0220161.e034g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e034" ns0:type="simple" /><ns1:math display="inline" id="M34"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula>, respectively, whereas (D-E) illustrate the same effects on <italic>T</italic><sub><italic>per</italic></sub>. Here we are providing the steady state values of <italic>w</italic> obtained after 100 epochs of training.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g005" ns0:type="simple" />
</fig>
<p>We have shown so far that the temporal structure of the input determines the temporal structure of the recall (<xref ref-type="fig" rid="pone.0220161.g005">Fig 5D&#8211;5F</xref>). We now show that the value IPI can change the recall phase from a sequence regime, where the patterns are tied in time, (<xref ref-type="fig" rid="pone.0220161.g006">Fig 6A</xref>) to a regime where the attractors are learned but not their temporal arrangement (<xref ref-type="fig" rid="pone.0220161.g006">Fig 6B</xref>). In this regime the network undergoes an unordered reactivation of the attractors in the recall phase. In general, to bridge a longer inter-pulse-interval, a longer <inline-formula id="pone.0220161.e035"><alternatives><graphic id="pone.0220161.e035g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e035" ns0:type="simple" /><ns1:math display="inline" id="M35"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> is required, as illustrated in <xref ref-type="fig" rid="pone.0220161.g006">Fig 6C</xref>. The idea is that <inline-formula id="pone.0220161.e036"><alternatives><graphic id="pone.0220161.e036g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e036" ns0:type="simple" /><ns1:math display="inline" id="M36"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> provides a temporal window of integration withing which patterns can be tied into a sequence. So, the larger the window is, the longer are the IPIs can be to still ensure the sequential memory.</p>
<fig id="pone.0220161.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Transition from the sequence regime to a random reactivation regime.</title>
<p>(A) An example of a sequential (ordered) activation of patterns. (B) Unordered reactivation of the learned attractors. (C) The two regimes (sequential in blue and random reactivation of attractors in red) in the relevant parameter space spammed by <inline-formula id="pone.0220161.e037"><alternatives><graphic id="pone.0220161.e037g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e037" ns0:type="simple" /><ns1:math display="inline" id="M37"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> and inter pulse interval. The examples in (A) and (B) correspond to the black dot and the star, respectively.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g006" ns0:type="simple" />
</fig>
</sec>
<sec id="sec006">
<title>2.4 Noise</title>
<p>We also tested whether sequence recall in the network was robust to noise by controlling the level of noise with the parameter <italic>&#963;</italic> in <xref ref-type="disp-formula" rid="pone.0220161.e010">Eq 1</xref>. Additive noise manifest itself in stochastic trajectories where pattern to pattern transitions happens earlier (<xref ref-type="fig" rid="pone.0220161.g007">Fig 7A</xref>). This phenomenon is illustrated clearly with the red and purple lines in <xref ref-type="fig" rid="pone.0220161.g007">Fig 7A</xref> where compared to their deterministic counterparts (solid lines) the noisy trajectories (thin lines) make the transition as soon as the variations in <italic>s</italic> drive them under the transition point (<italic>w</italic><sub><italic>next</italic></sub> <italic>o</italic>). Therefore, the persistence time in a network operating in a noisy regime will be a stochastic variable (denoted <italic>T</italic><sub><italic>per</italic>,<italic>&#963;</italic></sub>) whose mean will be lower than the persistence time <italic>T</italic><sub><italic>per</italic></sub> present in the deterministic regime. The mean value of <italic>T</italic><sub><italic>per</italic>,<italic>&#963;</italic></sub> decays systematically with increasing <italic>&#963;</italic> and quickly converges to a common value independent of the value of <italic>T</italic><sub><italic>per</italic></sub> for the deterministic regime set by controlling <italic>g</italic><sub><italic>a</italic></sub> (<xref ref-type="fig" rid="pone.0220161.g007">Fig 7B</xref>). To examine whether a sequence with lower values of <italic>T</italic><sub><italic>per</italic></sub> is less likely to be recalled correctly under the influence of noise we cued the sequence 1000 times for every value of <italic>&#963;</italic> and estimated the success rate by dividing the number of times that the sequence was correctly recalled in its entirety by the number of trials (1000). With this information we constructed the success rate vs noise profile shown in <xref ref-type="fig" rid="pone.0220161.g007">Fig 7C</xref> where we can observe that the success rate is identical for different values of <italic>T</italic><sub><italic>per</italic></sub>. We conclude that <italic>T</italic><sub><italic>per</italic></sub> has no effect on the sensitivity of the recall process is noise. This facilitates the study of the effect of noise as we can disregard variations on <italic>T</italic><sub><italic>per</italic></sub>.</p>
<fig id="pone.0220161.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Effects of noise reflected in current trajectories and persistence times.</title>
<p>(A) An example of current trajectories subjected to noise. The solid lines indicate the deterministic trajectories the system would follow in the zero noise case. In dotted, jagged and dashed lines we depict the currents induce <italic>w</italic><sub><italic>self</italic></sub>, <italic>w</italic><sub><italic>next</italic></sub> and <italic>w</italic><sub><italic>rest</italic></sub> for reference. (B) Change in the average of the actual value of <italic>T</italic><sub><italic>per</italic></sub> for different levels on noise. We Shaded the area between the 25th and the 75th percentile to convey and idea of the distribution for every value of <italic>&#963;</italic> (C) Success rate vs noise profile dependence on <italic>T</italic><sub><italic>per</italic></sub>. We ran 1000 simulations of recall and present the ratio of successful recalls as a function of <italic>&#963;</italic>. Confidence intervals from the binomial distribution are too small to be seen.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g007" ns0:type="simple" />
</fig>
<p>Next we systematically characterized the sensitivity of the network to noise as a function of the training parameters by calculating <italic>&#963;</italic><sub>50</sub> (the value at which the success rate falls below fifty percent, see <xref ref-type="sec" rid="sec015">Methods</xref>). We illustrate the nature of <italic>&#963;</italic><sub>50</sub> in <xref ref-type="fig" rid="pone.0220161.g008">Fig 8A</xref>, please note that larger <italic>&#963;</italic><sub>50</sub> implies that a system is less sensitive to noise and vice versa. Having estimated <italic>&#963;</italic><sub>50</sub> for different values of <italic>T</italic><sub><italic>p</italic></sub> we conclude that the network becomes less sensitive to noise with longer values of <italic>T</italic><sub><italic>p</italic></sub>, as shown in <xref ref-type="fig" rid="pone.0220161.g008">Fig 8B</xref>. This can be explained by the fact that training with longer pulses increases the distances between the weights (and therefore the distance between the currents), as previously shown in <xref ref-type="fig" rid="pone.0220161.g005">Fig 5A</xref>. We can see the same effect by increasing the inter pulse interval in <xref ref-type="fig" rid="pone.0220161.g008">Fig 8C</xref>, where the separation of weights produced by larger IPIs leads to a similar outcome. The opposite effect is observed with longer values of <inline-formula id="pone.0220161.e038"><alternatives><graphic id="pone.0220161.e038g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e038" ns0:type="simple" /><ns1:math display="inline" id="M38"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> where the system becomes more sensitive with longer values of <inline-formula id="pone.0220161.e039"><alternatives><graphic id="pone.0220161.e039g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e039" ns0:type="simple" /><ns1:math display="inline" id="M39"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula>, as shown in <xref ref-type="fig" rid="pone.0220161.g008">Fig 8D</xref>. We can appeal again to the structure of the weights in <xref ref-type="fig" rid="pone.0220161.g005">Fig 5C</xref> to explain these results as an outcome of the weights and therefore the current being less differentiated among themselves leading to failures in sequence recall.</p>
<fig id="pone.0220161.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Sensitivity of network performance to noise for different parameters.</title>
<p>The base reference values of the parameters of interest are: <italic>T</italic><sub><italic>p</italic></sub> = 100 <italic>ms</italic>, <italic>IPI</italic> = 0 <italic>ms</italic>, <inline-formula id="pone.0220161.e040"><alternatives><graphic id="pone.0220161.e040g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e040" ns0:type="simple" /><ns1:math display="inline" id="M40"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mn>25</ns1:mn> <ns1:mspace width="0.222em" /><ns1:mi>m</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:math></alternatives></inline-formula>, <inline-formula id="pone.0220161.e041"><alternatives><graphic id="pone.0220161.e041g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e041" ns0:type="simple" /><ns1:math display="inline" id="M41"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mn>15</ns1:mn> <ns1:mspace width="0.222em" /><ns1:mi>m</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:math></alternatives></inline-formula>, sequence length = 5, #hypercolumns = 1. (A) Two examples of the success vs noise profiles (<italic>T</italic><sub><italic>p</italic></sub> = 50 <italic>ms</italic>, 200 <italic>ms</italic>). The value of <italic>&#963;</italic><sub>50</sub> is indicated in the abscissa for clarity, note that smaller <italic>&#963;</italic><sub>50</sub> implies a network that is more sensitive to noise (the success rate decays faster). (B) <italic>&#963;</italic><sub>50</sub> variation with respect to <italic>T</italic><sub><italic>P</italic></sub>. We also indicate the <italic>&#963;</italic><sub>50</sub> for the values of <italic>T</italic><sub><italic>p</italic></sub> used in (A) with stars of corresponding colors.(C) <italic>&#963;</italic><sub>50</sub> variation with respect to the inter pulse intervals. (D) <italic>&#963;</italic><sub>50</sub> variation with respect to the value of <inline-formula id="pone.0220161.e042"><alternatives><graphic id="pone.0220161.e042g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e042" ns0:type="simple" /><ns1:math display="inline" id="M42"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula>. (E) <italic>&#963;</italic><sub>50</sub> variation with respect to sequence length. (F) <italic>&#963;</italic><sub>50</sub> variation with respect to the number of hypercolumns.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g008" ns0:type="simple" />
</fig>
<p>We also report two relevant noise effects not related to the connectivity. First, we show in <xref ref-type="fig" rid="pone.0220161.g008">Fig 8E</xref> that the network becomes more sensitive to noise for longer sequences. This can be explained by considering each pattern-to-pattern transition as a possible point of failure. Naturally, adding more links to the chain makes the recall of the sequence more likely to fail at some point (i.e. not recall all patterns in the right order). Finally, in <xref ref-type="fig" rid="pone.0220161.g008">Fig 8F</xref> we observe a scaling effect in how robust the network is with the number of hypercolumns. This can be explained using the fact a network with more hypercolumns posses a higher degree of recurrent connectivity. Every time there is a mis-transition in any of the units the recurrent connectivity channels the currents of the units where the transition occurred correctly as an error correction mechanism assuring the successful completion of the sequence more often than not. In a more abstract language the more hypercolumns the network possess, the less likely it is for enough transitions to occur such that the network state is pushed out of the basin of attraction of the next pattern. Therefore, the more hypercolumns the network possess, the more robust it is to noise and hence the observed scaling.</p>
</sec>
<sec id="sec007">
<title>2.5 Overlapping representations and sequences</title>
<p>Previous work with attractor models has shown that it is possible to store attractor states with overlapping representations (i.e. patterns that shared a unit activation in some hypercolumns) [<xref ref-type="bibr" rid="pone.0220161.ref055">55</xref>, <xref ref-type="bibr" rid="pone.0220161.ref057">57</xref>]. We test here whether our network is able to store and recall overlapping patterns successfully when they belong to sequences and are recalled as such. This is desirable to increase the storage capacity of our network and to enrich the combinatorial representations that our network can process.</p>
<p>Our aim is to characterize the capabilities of our network to store and successfully recall sequences containing patterns with some degree of overlap. As sequences can contain more than a pair of overlapped patterns we propose the following two parameters as a framework to systematically parameterize the problem: 1) the first parameter quantifies the level of overlap between the representation of two patterns and is therefore a spatial measure of overlap, we call this parameter representation overlap. 2) the second parameter is a temporal metric of overlap and quantifies how many patterns between two sequences possess some degree of representational overlap; we call this parameter sequential overlap. A schematic illustration of the general idea is presented in <xref ref-type="fig" rid="pone.0220161.g009">Fig 9A1</xref>, where the two parameters, the representational overlap and the sequential overlap, are shown in black and grey, respectively. To be more precise, the representational overlap between two patterns is defined as the proportion / ratio of hypercolumns that share units between the two patterns. We define the sequential overlap between two sequences as the number of patterns in the sequences that possess some degree of overlap (e.g. in <xref ref-type="fig" rid="pone.0220161.g009">Fig 9A1</xref> the sequential overlap is 4). In order to illustrate these concepts we present a detailed example in <xref ref-type="fig" rid="pone.0220161.g009">Fig 9B</xref>. The example consists of two six-pattern sequences (i.e. of length six) whose patterns are distributed over three hypercolumns (for example, the first pattern <italic>P</italic><sub>1<italic>a</italic></sub> of sequence a consists in the activation of the unit 10 in each of the three hypercolumns). The two sequences have two pairs of patterns that have some degree of overlap (pairs <italic>P</italic><sub>3<italic>a</italic></sub> &#8722; <italic>P</italic><sub>3<italic>b</italic></sub> and <italic>P</italic><sub>4<italic>a</italic></sub> &#8722; <italic>P</italic><sub>4<italic>b</italic></sub>) and therefore the two sequences have a sequential overlap of 2 as indicated by the gray area in <xref ref-type="fig" rid="pone.0220161.g009">Fig 9B</xref>. If we look at patterns <italic>P</italic><sub>3<italic>a</italic></sub> = (12, 3, 3) and <italic>P</italic><sub>3<italic>b</italic></sub> = (3, 3, 3) we can observe that they have the same unit activation in the last two hypercolumns (hypercolumns 2 and 3) and therefore the pair has a representational overlap of <inline-formula id="pone.0220161.e043"><alternatives><graphic id="pone.0220161.e043g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e043" ns0:type="simple" /><ns1:math display="inline" id="M43"><ns1:mfrac><ns1:mn>2</ns1:mn> <ns1:mn>3</ns1:mn></ns1:mfrac></ns1:math></alternatives></inline-formula>. The units in the hypercolumns responsible for the representational overlap between the pair are highlighted in black in <xref ref-type="fig" rid="pone.0220161.g009">Fig 9B</xref>. Note that the representational overlap is a parameter between 0 and 1, whereas the sequential overlap is an unbounded parameter as sequences can be arbitrarily long.</p>
<fig id="pone.0220161.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Overlapping representations and sequences.</title>
<p>(A1) Schematic of the parameterization framework. Black and gray stand for the representational overlap and the sequential overlap respectively (see text for details) (A2) Schematic of the sequence disambiguation problem. (B) An example of two sequences with overlap. Here each row is a hypercolumn and each column a pattern (patterns <italic>P</italic><sub>1<italic>x</italic></sub>, <italic>P</italic><sub>2<italic>x</italic></sub>, <italic>P</italic><sub>3<italic>x</italic></sub>, <italic>P</italic><sub>4<italic>x</italic></sub>, <italic>P</italic><sub>5<italic>x</italic></sub>, and <italic>P</italic><sub>6<italic>x</italic></sub>). The single entries represent the particular unit that was activated for that hypercolumn and pattern. (C) The superposition of the recall phase for the sequences in (B). Each sequence recall is highlighted by its corresponding color. We can appreciate inside the gray area that the second and third hypercolumns (sequential overlap of 2) have the same units activated (depicted in black). This reflects the fact those patterns have a representational overlap of <inline-formula id="pone.0220161.e044"><alternatives><graphic id="pone.0220161.e044g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e044" ns0:type="simple" /><ns1:math display="inline" id="M44"><ns1:mfrac><ns1:mn>2</ns1:mn> <ns1:mn>3</ns1:mn></ns1:mfrac></ns1:math></alternatives></inline-formula> (two out of three hypercolumns).</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g009" ns0:type="simple" />
</fig>
<p>The limit case when representational overlap is equal to 1 is the domain of sequence disambiguation. We show a schematic of the disambiguation problem in <xref ref-type="fig" rid="pone.0220161.g009">Fig 9A2</xref> where a representational overlap of 1 can be interpreted as the equivalence of both patterns in the sequential overlap section. In this regime the sequential overlap corresponds to the size of the disambiguation window that the network has to bridge to correctly disambiguate the sequence (i.e. ending in <italic>P</italic><sub>8</sub> <italic>a</italic> if you started in <italic>P</italic><sub>1</sub> <italic>a</italic> in <xref ref-type="fig" rid="pone.0220161.g009">Fig 9A2</xref>). Solving sequence disambiguation in the most strict sense requires the network to be able to store the contextual information required to solve correctly the bifurcation at the end of the overlapping section. That is, the network requires to hold the information of what pattern was activated before the disambiguation window for as long as the time it takes for the sequential dynamics to reach forking point.</p>
<p>In general we should expect that sequences with higher representational and sequential overlaps would be harder to process for the network. To characterize these difficulties systematically we tested for correct sequence recall for sequences in the zero noise condition for all the possible combinations of representation overlap as well as sequential overlap that the network allowed. As can be see in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10A</xref> the network can successfully recall overlapping sequences over a wide range of sequential and representational overlaps. The exception to this is the disambiguation regime in top of <xref ref-type="fig" rid="pone.0220161.g010">Fig 10A</xref> where we see a failure to recall both sequences when overlapped patterns are identical. Next we studied the recall of sequences with overlapping patterns in the presence of noise. First, we examined the dependence of the success rate on the noise level for a wide array of sequential and representational overlaps (1, 2, 3 and 4 in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10A</xref>). The results, as shown by the curves in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10B</xref>, illustrate that the success rate vs noise profiles are very similar despite different degrees of sequential and representational overlap. Second, for a fix value of representational overlap (0.5), we calculated <italic>&#963;</italic><sub>50</sub> for all the possible values of sequential overlap (green horizontal line in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10A</xref>). We also calculated the values of <italic>&#963;</italic><sub>50</sub> for a fix value of sequential overlap (5) and all the possible values of representational overlap (blue vertical line in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10A</xref>). The results (<xref ref-type="fig" rid="pone.0220161.g010">Fig 10C and 10D</xref>) show that the network is robust to noise across the spectrum of possible overlaps except when we get close to the sequence disambiguation regime (right part of <xref ref-type="fig" rid="pone.0220161.g010">Fig 10D</xref>), where the network becomes more sensitive. Those results together suggests that our neural network can consistently recall sequences correctly over a broad set of overlap conditions.</p>
<fig id="pone.0220161.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Sequence recall performance for different overlap conditions.</title>
<p>The base line values of the parameters of interest are <italic>T</italic><sub><italic>p</italic></sub> = 100 <italic>ms</italic>, &#916;<italic>T</italic><sub><italic>p</italic></sub> = 0 <italic>ms</italic>, <inline-formula id="pone.0220161.e045"><alternatives><graphic id="pone.0220161.e045g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e045" ns0:type="simple" /><ns1:math display="inline" id="M45"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mn>25</ns1:mn> <ns1:mspace width="0.222em" /><ns1:mi>m</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:math></alternatives></inline-formula>, <inline-formula id="pone.0220161.e046"><alternatives><graphic id="pone.0220161.e046g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e046" ns0:type="simple" /><ns1:math display="inline" id="M46"><ns1:mrow><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>o</ns1:mi> <ns1:mi>s</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mn>5</ns1:mn> <ns1:mspace width="0.222em" /><ns1:mi>m</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:math></alternatives></inline-formula>, sequence length = 10, <italic>H</italic> = 10 and <italic>T</italic><sub><italic>per</italic></sub> = 50 <italic>ms</italic>. (A) Success rate for pairs of two sequences with different sequential and representation overlaps. We show here the performance over the parameter space. Success here is determined by correct recall of both sequences. Note that the white corner in the top-right is undefined as it corresponds to a degree of sequential overlap that would include either the first or the last pattern in the sequence (B) Success rate vs noise level for the sequences with configurations marked as 1, 2, 3, 4 in A. The values of <italic>&#963;</italic><sub>50</sub> are marked for illustration purposes. (C) <italic>&#963;</italic><sub>50</sub> as a function of the sequential overlap. The values of <italic>&#963;</italic><sub>50</sub> are calculated over the sequences with configurations given in the green horizontal line in A. (D) <italic>&#963;</italic><sub>50</sub> as a function of the representation overlap. The values of <italic>&#963;</italic><sub>50</sub> are calculated over the sequences with configurations given in the blue vertical line in A. (E) max disambiguation as a function of <italic>T</italic><sub><italic>per</italic></sub>. The network loses disambiguation power with long lasting attractors as the memory of the earlier pattern activation reflected in the currents fades. (F) Success rate vs noise profile in the disambiguation regime. The three curves correspond to overlapping sequence configurations marked with x, y, and z in A. Shaded areas correspond to 95% confidence intervals (1000 trials).</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g010" ns0:type="simple" />
</fig>
<p>In the disambiguation regime with no noise (gray line in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10A</xref>) the network is able to solve the disambiguation problem successfully up to disambiguation windows of size 8. The disambiguation capabilities of the network are due to memory effects on the dynamics (here capacitance effects mediated by <italic>&#964;</italic><sub><italic>s</italic></sub>). In fact, we show in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10E</xref> that the longer the persistence times (and therefore the more time for the memory to fade) the smaller is the disambiguation window that the system can resolve. Contrary to the results above the network is brittle in the sequence disambiguation regime. In particular, the success rate decays extremely fast in the presence of noise as show in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10F</xref>. However, an interesting resonance phenomena occurs for low sequential overlaps (blue curve) where the success rate actually increases with noise. This can be explained with the fact that the noise effectively reduces the mean persistence time <italic>T</italic><sub><italic>per</italic>,<italic>&#963;</italic></sub> (as shown before in <xref ref-type="fig" rid="pone.0220161.g007">Fig 7B</xref>) which leads to the increased disambiguation power (c.f. 9E). In other words, by reducing the attractors life-time with noise, the network is able to leverage the short-lived information provided by the synaptic traces to successfully perform disambiguation.</p>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>3 Discussion</title>
<p>We have evaluated a Hebbian-like BCPNN learning rule with asymmetrical temporal synaptic traces as a sufficient principle underlying robust sequence learning in an attractor neural network model. The results have revealed the potential of the network to successfully encode and reliably recall multiple overlapping sequential representations even in the presence of noise. In this context, we have systematically studied the effect of network modularity as well as the role of key temporal parameters of the synaptic learning rule. We have also stressed that our network has the capability to control the temporal structure of the sequential pattern recall by means of an intrinsic adaptation mechanism.</p>
<p>Overall we have found that for a wide range of parameters the network learns sequences with no requirement of fine-tuning (see <xref ref-type="fig" rid="pone.0220161.g005">Fig 5</xref>). There exist two regimes where the network fails to learn sequences: 1) when the value of <inline-formula id="pone.0220161.e047"><alternatives><graphic id="pone.0220161.e047g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e047" ns0:type="simple" /><ns1:math display="inline" id="M47"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> is too small compared to the inter-pulse-interval (IPI) which is the case when the attractors are learned but not linked in time (see <xref ref-type="fig" rid="pone.0220161.g006">Fig 6B</xref>) and 2) when the value of <inline-formula id="pone.0220161.e048"><alternatives><graphic id="pone.0220161.e048g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e048" ns0:type="simple" /><ns1:math display="inline" id="M48"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> is so large that the structure of the network gets diluted as the weights connecting a pattern to its successor become larger than the self-excitatory weights (<italic>w</italic><sub><italic>self</italic></sub> &lt; <italic>w</italic><sub><italic>next</italic></sub>). The other parameters just modulate this process. This fact coupled with a graceful degradation of the network performance with noise (c.f. <xref ref-type="fig" rid="pone.0220161.g008">Fig 8</xref>) shows that the sequence learning capabilities of our network are robust to learning and require no fine tuning of the parameters involved.</p>
<sec id="sec009">
<title>3.1 Comparison with similar models</title>
<p>Previous models have also utilized some of the key components of our model such as the use of temporal traces for hetero-association, competition and the use of adaptation or facilitation to ensure pattern transition [<xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>, <xref ref-type="bibr" rid="pone.0220161.ref044">44</xref>, <xref ref-type="bibr" rid="pone.0220161.ref058">58</xref>]. While some of such models provide a study of individual aspects of sequence learning such as the control and characterization of persistence time [<xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>, <xref ref-type="bibr" rid="pone.0220161.ref044">44</xref>], the analysis of sequence recall under noise [<xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>], or the storage and recall of sequences with some degree of overlap [<xref ref-type="bibr" rid="pone.0220161.ref058">58</xref>], to the best of our knowledge, our approach represents the first systematic treatment of all the aforementioned phenomena under the same framework. In particular, we find that the problem of learning multiple sequences has received scant attention so far. A naive implementation of asymmetric Hebbian learning leads to weights that do not reflect the adequate transition statistics (in the Markov chain sense) of the patterns present during training (see <xref ref-type="fig" rid="pone.0220161.g011">Fig 11A</xref>). The BCPNN learning rule that we employ in this work learns the transition statistics by keeping a history of the overall pattern activity in the form of p-traces (see <xref ref-type="supplementary-material" rid="pone.0220161.s003">S2 Appendix</xref>). It is important to state that most sequence learning models do not implement a naive version of Hebbian plasticity but enhance their plasticity rules with competition motifs (competition among the weights) such as LTD or diverse forms of heterosynaptic plasticity to introduce competition and enhance robustness [<xref ref-type="bibr" rid="pone.0220161.ref041">41</xref>, <xref ref-type="bibr" rid="pone.0220161.ref042">42</xref>, <xref ref-type="bibr" rid="pone.0220161.ref058">58</xref>]. However, it is not clear how such competition mechanisms can be balanced to learn temporal associations between patterns that occur with varying frequencies due to their participation in multiple sequences. Such balance that account for this heterogeneous distribution of pattern activation probabilities is offered by the BCPNN as units are automatically connected accordingly to their activation probability history (see <xref ref-type="disp-formula" rid="pone.0220161.e019">Eq 8</xref>).</p>
<fig id="pone.0220161.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0220161.g011</object-id>
<label>Fig 11</label>
<caption>
<title>The BCPNN weights temporal co-activations against overall activations.</title>
<p>The significance of temporal associations. (A) Here we compare naive simple Hebbian learning with the BCPNN in terms of relative weighting of different temporal associations. In the presented example there are three associations <italic>E</italic> &#8594; <italic>F</italic>, <italic>E</italic> &#8594; <italic>G</italic>, and <italic>H</italic> &#8594; <italic>G</italic> that have been observed 99, 1, 1 occasions respectively. Simple Hebbian learning weights just the frequency of the associations and, as a consequence, <italic>E</italic> &#8594; <italic>G</italic> and <italic>H</italic> &#8594; <italic>G</italic> end up with the same association weight. The BCPNN, on the other hand, differentiates the weights as it takes into account the total activation probability of each unit, rendering the temporal association <italic>H</italic> &#8594; <italic>G</italic> more significant than <italic>E</italic> &#8594; <italic>G</italic>.</p>
</caption>
<graphic mimetype="image" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.g011" ns0:type="simple" />
</fig>
</sec>
<sec id="sec010">
<title>3.2 Previous work and biological context</title>
<p>Here we have followed the modelling philosophy aimed at distilling the architecture of the network to its essential characteristics that support and control the phenomenon of interest (sequence learning). In the previous models of particular relevance to our work, complex spike based dynamics and rich biological detail were promoted to provide insights into the biophysical underpinnings of sequence learning in the cortex [<xref ref-type="bibr" rid="pone.0220161.ref039">39</xref>] and as a model of memory consolidation [<xref ref-type="bibr" rid="pone.0220161.ref059">59</xref>]. While the aforementioned contributions provide a more direct mapping between biology and the network, our approach, which reduces the network to its essential characteristics, necessarily dilutes that mapping. Nevertheless, some key design principles emerging from biology are preserved. Below we discuss in more detail the main aspects of the relationship between the dynamical as well as structural properties in our network and the biological substrate that inspired them in the first place.</p>
<p>Local competition, often mediated by lateral inhibition and operating as a normalization mechanism [<xref ref-type="bibr" rid="pone.0220161.ref060">60</xref>], is one of the canonical computational motifs in cortex [<xref ref-type="bibr" rid="pone.0220161.ref047">47</xref>]. In our network competition is modelled locally within each hypercolumn with a hard-wired WTA mechanism, which is not a biologically plausible solution. Douglas and Martin [<xref ref-type="bibr" rid="pone.0220161.ref047">47</xref>] suggested that such a competition mechanism could be implemented by basket or chandelier cells. In the spiking counterparts of our attractor neural network model [<xref ref-type="bibr" rid="pone.0220161.ref039">39</xref>, <xref ref-type="bibr" rid="pone.0220161.ref059">59</xref>, <xref ref-type="bibr" rid="pone.0220161.ref061">61</xref>], this computational principle was implemented by means of fast inhibitory basket cells with fixed connectivity and produced compatible outcomes. It is important to point out that the idea of using diverse forms of local competition to achieve pattern selection in sequence recall has been examined previously and extensively in the sequence learning literature [<xref ref-type="bibr" rid="pone.0220161.ref043">43</xref>, <xref ref-type="bibr" rid="pone.0220161.ref044">44</xref>, <xref ref-type="bibr" rid="pone.0220161.ref062">62</xref>].</p>
<p>Asymmetric temporal traces have been proven successful to achieve the effect of sequence learning [<xref ref-type="bibr" rid="pone.0220161.ref037">37</xref>, <xref ref-type="bibr" rid="pone.0220161.ref038">38</xref>, <xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>, <xref ref-type="bibr" rid="pone.0220161.ref041">41</xref>, <xref ref-type="bibr" rid="pone.0220161.ref063">63</xref>, <xref ref-type="bibr" rid="pone.0220161.ref064">64</xref>]. In our model we have utilized the temporal asymmetric z-traces as the basis of probabilistic learning with the BCPNN learning rule. The degree of asymmetry of the z-traces and its effects on the connectivity matrix have been studied through variations in <inline-formula id="pone.0220161.e049"><alternatives><graphic id="pone.0220161.e049g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e049" ns0:type="simple" /><ns1:math display="inline" id="M49"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pone.0220161.g005">Fig 5C</xref>). In this framework lower values of <inline-formula id="pone.0220161.e050"><alternatives><graphic id="pone.0220161.e050g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e050" ns0:type="simple" /><ns1:math display="inline" id="M50"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> would correspond to fast AMPA dynamics [<xref ref-type="bibr" rid="pone.0220161.ref065">65</xref>] while longer values of <inline-formula id="pone.0220161.e051"><alternatives><graphic id="pone.0220161.e051g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e051" ns0:type="simple" /><ns1:math display="inline" id="M51"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> would correspond in turn to slower NMDA dynamics [<xref ref-type="bibr" rid="pone.0220161.ref066">66</xref>]. Consistently with these observations, throughout this work we have restricted the values of <inline-formula id="pone.0220161.e052"><alternatives><graphic id="pone.0220161.e052g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e052" ns0:type="simple" /><ns1:math display="inline" id="M52"><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:msub><ns1:mi>z</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>e</ns1:mi></ns1:mrow></ns1:msub></ns1:msub></ns1:math></alternatives></inline-formula> to the 5 &#8722; 150 <italic>ms</italic> range. A biological account of the z-traces and their connection to the biochemical cascades that underlie synaptic learning have been presented in a more detailed way by [<xref ref-type="bibr" rid="pone.0220161.ref056">56</xref>].</p>
<p>It is important to point out that synaptic connections learned in our network with the BCPNN learning rule violate Dale&#8217;s law, i.e. projections emanating from the same unit can mediate both excitatory and inhibitory effects on the target units. To address this issue, we propose a different interpretation for positive and negative synaptic weights. In the former, they can be straightforwardly interpreted as the conductance between two units, whereas in the latter case we interpret them as a disynaptic connection through an inhibitory interneuron. The argument for the biological plausibility of this arrangement using double bouquet cells as the inhibitory interneurons in this architecture is developed further by [<xref ref-type="bibr" rid="pone.0220161.ref067">67</xref>].</p>
</sec>
<sec id="sec011">
<title>3.3 Control of the temporal structure of the sequence</title>
<p>We have shown that the persistence time, <italic>T</italic><sub><italic>per</italic></sub>, of our attractors can be quite effectively controlled through the use of the adaptation gain <italic>g</italic><sub><italic>a</italic></sub> and less effectively by means of the adaptation time constant <italic>&#964;</italic><sub><italic>a</italic></sub> (see <xref ref-type="fig" rid="pone.0220161.g003">Fig 3</xref> and <xref ref-type="disp-formula" rid="pone.0220161.e015">Eq 4</xref>). The range of <italic>T</italic><sub><italic>per</italic></sub> values for the attractor patterns in our network model is within the 10 <italic>ms</italic> and 3.5 <italic>s</italic> range. This in turn means that the duration of our sequences corresponds to the milliseconds to minutes interval (considering sequential lengths of 10 to 100). This range of values is consistent with the variation in sequence duration that [<xref ref-type="bibr" rid="pone.0220161.ref068">68</xref>] found for biological sequences in the hippocampus. While the mechanisms for temporal phenomena under the millisecond scale (inter-aural-scale, [<xref ref-type="bibr" rid="pone.0220161.ref069">69</xref>]) and over the minute scale (circadian rhythms, [<xref ref-type="bibr" rid="pone.0220161.ref070">70</xref>]) are already well understood, the nature and origin of temporal phenomena at the intermediate time scales is still a matter of debate [<xref ref-type="bibr" rid="pone.0220161.ref071">71</xref>]. We believe our work contributes to this debate by offering an intrinsic model of time [<xref ref-type="bibr" rid="pone.0220161.ref072">72</xref>] capable of both, using the taxonomy of [<xref ref-type="bibr" rid="pone.0220161.ref071">71</xref>], the production and reproduction of temporal patterns within the discussed range.</p>
<p>Similarly to previous work [<xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>] we found a logarithmic relationship (<xref ref-type="disp-formula" rid="pone.0220161.e015">Eq 4</xref>) between the persistence time, <italic>T</italic><sub><italic>per</italic></sub>, and the network parameters. In their model, Veliz-Cuba et al. (2015) [<xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>] find that by training the network with the right combination of parameters (such as time constants and maximum facilitation), the precise timing of different patterns can be exactly replicated. In our model, we are able to reproduce this effect with only one parameter <italic>g</italic><sub><italic>a</italic></sub> for the case of orthogonal patterns (see <xref ref-type="fig" rid="pone.0220161.g003">Fig 3D</xref>). The case with patterns that share some overlap is more complicated, as it requires adjusting the adaptation gains, <italic>g</italic><sub><italic>a</italic></sub>, more selectively to preserve the duration of all the patterns that contain those units. As far as we know, a firing rate model that is able to adjust its parameters automatically during learning with unsupervised local learning (instead of fixing it by hand) is yet to be found in the literature and remains a matter of future work.</p>
<p>In the work of Murray et al. [<xref ref-type="bibr" rid="pone.0220161.ref044">44</xref>] the control of the temporal structure (<italic>T</italic><sub><italic>per</italic></sub>) is accomplished by means of input from an external network. Although the ability of our network to control the temporal structure rests on internal mechanisms, we could also exploit external input for this purpose. By adding external input to our differential equation during recall and solving the resulting expression (see <xref ref-type="supplementary-material" rid="pone.0220161.s002">S1 Appendix</xref>) we obtain an expression for the parameter <italic>B</italic> in the following form <italic>B</italic> = (&#916;<italic>w</italic><sub><italic>next</italic></sub> + &#916;<italic>&#946;</italic><sub><italic>next</italic></sub> + &#916;<italic>I</italic>(<italic>t</italic>))(<italic>g</italic><sub><italic>a</italic></sub>)<sup>&#8722;1</sup> where &#916;<italic>I</italic>(<italic>t</italic>) = <italic>I</italic><sub><italic>self</italic></sub>(<italic>t</italic>) &#8722; <italic>I</italic><sub><italic>next</italic></sub>(<italic>t</italic>) is the differential input between the consecutive units in the sequence. By controlling this differential input, the persistence time of attractor states in a given sequence can be modulated. This could be used to build a framework where a generalist network learns the sequential structure of the input and a specialized control network adjusts the temporal structure of the sequence recall suitable for the task at hand.</p>
</sec>
<sec id="sec012">
<title>3.4 Sequence disambiguation and overlapping representations</title>
<p>Sequence disambiguation or using past context to determine the trajectory of a sequence has been deemed one of the most important problems that a sequence prediction network should solve [<xref ref-type="bibr" rid="pone.0220161.ref073">73</xref>]. While some networks [<xref ref-type="bibr" rid="pone.0220161.ref074">74</xref>&#8211;<xref ref-type="bibr" rid="pone.0220161.ref076">76</xref>] have addressed the problem in their generality, their reliance on supervised learning and lack of biological plausibility remain a matter of concern. There have been a few attempts at the problem of sequence disambiguation in the attractor network framework but most of them rely on non-local learning rules or require an infeasible large number of parameters [<xref ref-type="bibr" rid="pone.0220161.ref052">52</xref>, <xref ref-type="bibr" rid="pone.0220161.ref077">77</xref>, <xref ref-type="bibr" rid="pone.0220161.ref078">78</xref>]. Minai et al. [<xref ref-type="bibr" rid="pone.0220161.ref079">79</xref>] proposed an alternative approach using the activity in a random network (what now is called a reservoir) as a source of context information for disambiguation. In their network, activity in the reservoir evolved in a path-dependent way, and inter-network connectivity between the disambiguation network and the reservoir conveyed the necessary information from the latter to the former thus allowing for successful disambiguation. While effective, such networks require another complete layer to keep a dynamical memory, an approach judged to be inefficient. To address this issue, context codes with less overhead have been proposed where, instead of a network, the state of a unit or a collection of units is determined by the dynamical history of the system and that state is then used for disambiguation [<xref ref-type="bibr" rid="pone.0220161.ref080">80</xref>, <xref ref-type="bibr" rid="pone.0220161.ref081">81</xref>]. In our network, disambiguation can be achieved by building cell assemblies containing a subset of units that are preferentially connected to the subsequent assembly in the sequence. By preferential connectivity we mean that those units posses strong excitatory connections to the units of the subsequent pattern and strong inhibitory connections to the rest. To put it more concretely, the BCPNN learning rule, following its probabilistic nature, will ensure that the non-overlapping parts in a sequence are connected in such fashion by creating excitatory connections between the units in the non-overlapping parts and the subsequent units in the sequence (as they are the only ones that actually appeared together) and strong inhibitory connections between the non-overlapping units and all the units belonging to any other pattern (as they never appeared together). In virtue of the aforementioned connectivity, activation of the units in the non-overlapping part of the assembly (context units) guarantees a transition to the subsequent (correct) pattern. As shown in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10D</xref>, the proposed mechanism is very robust to the size of the cell assembly that gets connected preferentially (the non-overlapping part); degradation of the performance under noise only becomes evident when the size of the context code becomes less than 20% of the cell assembly. This is consistent with some experimental evidence of neurons in the hippocampus that fire in such a trajectory dependent fashion [<xref ref-type="bibr" rid="pone.0220161.ref082">82</xref>].</p>
<p>Even in the absence of context units, i.e. with fully overlapping (the same) assemblies in competing sequences, our network can still solve a disambiguation task for sequences sharing two consecutive states in their trajectories (see the resonance phenomena in <xref ref-type="fig" rid="pone.0220161.g010">Fig 10F</xref>). While this phenomena allows the network to statistically solve sequence disambiguation for disambiguation windows of size 2, it does not generalize for longer sequential overlaps. One way to handle the problem in a more robust, consistent and transparent fashion is to use a mechanism that preserves the network&#8217;s dynamical history in a dynamical variable. In our future work we intend to add such mechanism to the network in the form of currents dependent on the z-traces that facilitate the longer maintenance of the information about past activations and thus support the disambiguation of sequences with more challenging overlaps.</p>
</sec>
<sec id="sec013">
<title>3.5 Learning rule stability, competition and homeostasis</title>
<p>The stability of the learning dynamics of a firing rate network subject to associative learning tends to be accomplished by introducing weight dependent terms into weight updates [<xref ref-type="bibr" rid="pone.0220161.ref083">83</xref>]. This constrain is usually motivated and biologically interpreted as a homeostatic mechanism. Sequence learning models are not exempt from this necessity. One of the simpler approaches amounts to combining STDP with hetero-synaptic plasticity [<xref ref-type="bibr" rid="pone.0220161.ref042">42</xref>]. However, it is not straightforward how these two forces should be balanced. There are a plethora of models that rely on weight clipping with arbitrarily handpicked upper and lower limits [<xref ref-type="bibr" rid="pone.0220161.ref040">40</xref>, <xref ref-type="bibr" rid="pone.0220161.ref044">44</xref>, <xref ref-type="bibr" rid="pone.0220161.ref062">62</xref>]. While this approach is analytically transparent, fine tuning between potentiation and depression is usually required. In a similar vein, Byrnes et al. [<xref ref-type="bibr" rid="pone.0220161.ref043">43</xref>] introduced a combination of subtractive and multiplicative normalization as a mechanism of weight stabilization, which also has to be arbitrarily tuned. Verduzco-Flores et al. [<xref ref-type="bibr" rid="pone.0220161.ref058">58</xref>] proposed a more complex approach that combines hetero-synaptic competition with a mechanism that limits both the total value of the weights and the total incoming current to a unit in order to achieve stability [<xref ref-type="bibr" rid="pone.0220161.ref041">41</xref>], on the other hand, resorted to a combination of synaptic normalization and multiplicative homoeostasis to avoid runaway excitation. While these two learning rules are able to prevent runaway instabilities and have varying degrees of biological plausibility, the number of parameters involved, and the complexity of the model are excessively high. As opposed to this complexity, the probabilistic nature of our BCPNN learning rule automatically accounts for weight competition during learning leading the network to a stable regime of sequential or attractor dynamics without requiring extra parameters or balancing different forces (as discussed more thoroughly elsewhere [<xref ref-type="bibr" rid="pone.0220161.ref056">56</xref>]).</p>
</sec>
<sec id="sec014">
<title>3.6 Limitations and further work</title>
<p>Although multiple studies of the cortical micro-circuitry have revealed distance dependent connectivity profiles [<xref ref-type="bibr" rid="pone.0220161.ref084">84</xref>, <xref ref-type="bibr" rid="pone.0220161.ref085">85</xref>], we have ignored this design principle in our model. Previous spiking implementations of this model architecture have included to some degree both distance dependent effects in connectivity and distance dependent delays [<xref ref-type="bibr" rid="pone.0220161.ref039">39</xref>, <xref ref-type="bibr" rid="pone.0220161.ref059">59</xref>, <xref ref-type="bibr" rid="pone.0220161.ref061">61</xref>], which had impact on the network&#8217;s temporal dynamics. In our non-spiking network model the expected implications of such spatio-temporal diversity would be prolonged (temporally spread) attractor reactivation and transition processes. Still there should be no qualitative functional changes in the network&#8217;s behaviour as the key mechanisms would not be compromised (although see [<xref ref-type="bibr" rid="pone.0220161.ref086">86</xref>] for a sequence production mechanism that arises itself from asymmetries in the spatial profile of connectivity). Due to the mesoscale nature of our model and interest in network phenomena, we obviously do not account for any dendritic related phenomena in sequence processing such as as the capacity of single neurons to work as sequence recognition devices through spatial effects [<xref ref-type="bibr" rid="pone.0220161.ref087">87</xref>] and the use of distal dendritic inputs to prime sequential activations [<xref ref-type="bibr" rid="pone.0220161.ref088">88</xref>].</p>
<p>In the presented work there are some phenomena that we have not systematically characterized in their generality. For example, in most simulations we exploited temporally homogeneous training protocols. To test the performance of our network under the conditions of varying pulse time, <italic>T</italic><sub><italic>p</italic></sub>, and inter-pulse-interval, &#916;<italic>T</italic><sub><italic>p</italic></sub>, across patterns, we have ran preliminary tests and obtained promising results. We intend to conduct a more comprehensive characterization of the network&#8217;s behaviour subject to highly variable training protocols (temporal pattern heterogeneity) in our future work.</p>
</sec>
</sec>
<sec id="sec015" sec-type="materials|methods">
<title>4 Methods</title>
<sec id="sec016">
<title>4.1 Training and recall protocol</title>
<p>For our training protocol we created a time series <bold>s</bold>(<italic>t</italic>) to represent the input. <bold>s</bold>(<italic>t</italic>) encodes the information about <italic>T</italic><sub><italic>p</italic></sub> and IPI (<xref ref-type="fig" rid="pone.0220161.g004">Fig 4B</xref>). We then performed off-line batch learning of the parameters using the integral formulation of the dynamic equations presented above (Eqs <xref ref-type="disp-formula" rid="pone.0220161.e017">6</xref> and <xref ref-type="disp-formula" rid="pone.0220161.e018">7</xref>).</p>
<p>To avoid the ill-defined case for <italic>p</italic> = 0 we set the lower bound of <italic>&#1013;</italic> = 10<sup>&#8722;</sup>7 for the argument of the logarithm. That is, if the value of <italic>p</italic> is less than <italic>&#1013;</italic> we equate it to <italic>&#1013;</italic>.</p>
<p>For training the two sequences with the overlapping representations we created the sequences in succession but separated among them by 1<italic>s</italic>. This ensured that the sequences in the training protocol were uncoupled from each other.</p>
<p>We consider a pattern to be active if the corresponding units are active for longer than <italic>&#964;</italic><sub><italic>s</italic></sub> (the smallest time constant in the system). The sequence is considered to be correctly recalled if by activating the first pattern all the others patterns in the sequence are subsequently activated in that given order. Given that for many possible tasks it suffices that the network state ends in the correct pattern or that only a part of the sequence is recalled correctly our success criteria is rather conservative.</p>
</sec>
<sec id="sec017">
<title>4.2 Control and estimation of persistence time</title>
<p>In order to estimate the <italic>T</italic><sub><italic>per</italic></sub> for a pattern <italic>P</italic> during recall we calculated the difference between the time <italic>t</italic><sub>1</sub> at which pattern <italic>P</italic> was activated and the time at which the next pattern was activated <italic>t</italic><sub>2</sub>. <italic>T</italic><sub><italic>per</italic></sub> = <italic>t</italic><sub>2</sub> &#8722; <italic>t</italic><sub>1</sub>.</p>
<p>As shown in <xref ref-type="disp-formula" rid="pone.0220161.e015">Eq 4</xref>, <italic>T</italic><sub><italic>per</italic></sub> depends on both the weight and bias differences, &#916;<italic>w</italic><sub><italic>next</italic></sub> = <italic>w</italic><sub><italic>self</italic></sub> &#8722; <italic>w</italic><sub><italic>next</italic></sub> and &#916;<italic>&#946;</italic> = <italic>&#946;</italic><sub><italic>self</italic></sub> &#8722; <italic>&#946;</italic><sub><italic>next</italic></sub>, respectively, and the adaptation gain <italic>g</italic><sub><italic>a</italic></sub>. This offers flexibility in controlling the duration of patterns activations by adjusting the adaptation gain <italic>g</italic><sub><italic>a</italic></sub> as follows: <inline-formula id="pone.0220161.e053"><alternatives><graphic id="pone.0220161.e053g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e053" ns0:type="simple" /><ns1:math display="inline" id="M53"><ns1:mrow><ns1:msub><ns1:mi>g</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub> <ns1:mo>=</ns1:mo> <ns1:mo form="prefix" stretchy="false">(</ns1:mo> <ns1:mo>&#916;</ns1:mo> <ns1:msub><ns1:mi>w</ns1:mi> <ns1:mrow><ns1:mi>n</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>x</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow></ns1:msub> <ns1:mo>+</ns1:mo> <ns1:mo>&#916;</ns1:mo> <ns1:mi>&#946;</ns1:mi> <ns1:mo form="postfix" stretchy="false">)</ns1:mo> <ns1:mo form="prefix" stretchy="false">(</ns1:mo> <ns1:mn>1</ns1:mn> <ns1:mo>&#8722;</ns1:mo> <ns1:mfrac><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>s</ns1:mi></ns1:msub> <ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub></ns1:mfrac> <ns1:mo form="postfix" stretchy="false">)</ns1:mo> <ns1:mo form="prefix" stretchy="false">(</ns1:mo> <ns1:mn>1</ns1:mn> <ns1:mo>&#8722;</ns1:mo> <ns1:mfrac><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>s</ns1:mi></ns1:msub> <ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub></ns1:mfrac> <ns1:mo>&#8722;</ns1:mo> <ns1:msup><ns1:mi>e</ns1:mi> <ns1:mfrac><ns1:msub><ns1:mi>T</ns1:mi> <ns1:mrow><ns1:mi>p</ns1:mi> <ns1:mi>e</ns1:mi> <ns1:mi>r</ns1:mi></ns1:mrow></ns1:msub> <ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>a</ns1:mi></ns1:msub></ns1:mfrac></ns1:msup> <ns1:msup><ns1:mo form="postfix" stretchy="false">)</ns1:mo> <ns1:mrow><ns1:mo>&#8722;</ns1:mo> <ns1:mn>1</ns1:mn></ns1:mrow></ns1:msup></ns1:mrow></ns1:math></alternatives></inline-formula>. We use this adjustment to control <italic>T</italic><sub><italic>per</italic></sub> during recall in order to decouple the effects of training from the recall process.</p>
</sec>
<sec id="sec018">
<title>4.3 Noise</title>
<p>Noise was included in our simulations as additive white noise with variance <inline-formula id="pone.0220161.e054"><alternatives><graphic id="pone.0220161.e054g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e054" ns0:type="simple" /><ns1:math display="inline" id="M54"><ns1:msubsup><ns1:mi>&#963;</ns1:mi> <ns1:mrow><ns1:mi>i</ns1:mi> <ns1:mi>n</ns1:mi></ns1:mrow> <ns1:mn>2</ns1:mn></ns1:msubsup></ns1:math></alternatives></inline-formula> in the differential equation for the <italic>s</italic> variable. The current <italic>s</italic>, however, behaves almost as an Ornstein&#8211;Uhlenbeck (OU) process and therefore its standard deviation is given by <inline-formula id="pone.0220161.e055"><alternatives><graphic id="pone.0220161.e055g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e055" ns0:type="simple" /><ns1:math display="inline" id="M55"><ns1:mrow><ns1:msubsup><ns1:mi>&#963;</ns1:mi> <ns1:mrow><ns1:mi>o</ns1:mi> <ns1:mi>u</ns1:mi> <ns1:mi>t</ns1:mi></ns1:mrow> <ns1:mn>2</ns1:mn></ns1:msubsup> <ns1:mo>=</ns1:mo> <ns1:mfrac><ns1:msub><ns1:mi>&#964;</ns1:mi> <ns1:mi>s</ns1:mi></ns1:msub> <ns1:mn>2</ns1:mn></ns1:mfrac> <ns1:msubsup><ns1:mi>&#963;</ns1:mi> <ns1:mrow><ns1:mi>i</ns1:mi> <ns1:mi>n</ns1:mi></ns1:mrow> <ns1:mn>2</ns1:mn></ns1:msubsup></ns1:mrow></ns1:math></alternatives></inline-formula>. Based on this fact we characterized the effects of noise with the size of <italic>&#963;</italic><sub><italic>out</italic></sub> instead of <italic>&#963;</italic><sub><italic>in</italic></sub> The rational behind this choice is that <italic>&#963;</italic><sub><italic>out</italic></sub> will be closer to the standard deviation of the variable <italic>s</italic> in <xref ref-type="disp-formula" rid="pone.0220161.e010">Eq 1</xref> and therefore comparable in magnitude to the value of currents in the network. It is important to say that thanks to the separation of times scales (<italic>&#964;</italic><sub><italic>s</italic></sub> &#8810; <italic>&#964;</italic><sub><italic>a</italic></sub>) the dynamics of <italic>s</italic> behaves mostly as an OU process and it is only the WTA mechanism around the transition points that induces deviations.</p>
<p>The incorporation of noise to the network makes the trajectories and, thereby, the recall process stochastic. To quantify the recall performance under noise (probability of successful recall at a given level of noise) we averaged the number of correct recalls in a given number of trials. The estimated probability of successful recall <inline-formula id="pone.0220161.e056"><alternatives><graphic id="pone.0220161.e056g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e056" ns0:type="simple" /><ns1:math display="inline" id="M56"><ns1:mover><ns1:mi>p</ns1:mi> <ns1:mo accent="true">^</ns1:mo></ns1:mover></ns1:math></alternatives></inline-formula> follows from a Bernoulli process and we can therefore quantify the uncertainty of our estimates with the Wald method to provide 95% confidence intervals (<italic>N</italic><sub><italic>trials</italic></sub> = 1000):
<disp-formula id="pone.0220161.e057"><alternatives><graphic id="pone.0220161.e057g" mimetype="image" position="anchor" ns0:href="info:doi/10.1371/journal.pone.0220161.e057" ns0:type="simple" /><ns1:math display="block" id="M57"><ns1:mtable displaystyle="true"><ns1:mtr><ns1:mtd columnalign="right"><ns1:mrow><ns1:mover accent="true"><ns1:mi>p</ns1:mi> <ns1:mo>^</ns1:mo></ns1:mover> <ns1:mo>&#177;</ns1:mo> <ns1:mn>1</ns1:mn> <ns1:mo>.</ns1:mo> <ns1:mn>96</ns1:mn> <ns1:msqrt><ns1:mfrac><ns1:mrow><ns1:mover accent="true"><ns1:mi>p</ns1:mi> <ns1:mo>^</ns1:mo></ns1:mover> <ns1:mrow><ns1:mo>(</ns1:mo> <ns1:mn>1</ns1:mn> <ns1:mo>-</ns1:mo> <ns1:mover accent="true"><ns1:mi>p</ns1:mi> <ns1:mo>^</ns1:mo></ns1:mover> <ns1:mo>)</ns1:mo></ns1:mrow></ns1:mrow> <ns1:msub><ns1:mi>N</ns1:mi> <ns1:mrow><ns1:mi>t</ns1:mi> <ns1:mi>r</ns1:mi> <ns1:mi>i</ns1:mi> <ns1:mi>a</ns1:mi> <ns1:mi>l</ns1:mi> <ns1:mi>s</ns1:mi></ns1:mrow></ns1:msub></ns1:mfrac></ns1:msqrt></ns1:mrow></ns1:mtd></ns1:mtr></ns1:mtable></ns1:math></alternatives> <label>(9)</label></disp-formula></p>
<p>In order to systematically characterize how different parameters of our training protocol affect the sensitivity of the resulting network to noise, we estimated <italic>&#963;</italic><sub>50</sub> as the value of noise variance <italic>&#963;</italic> for which the probability of correctly recalling a given sequence is 0.5. Finding such <italic>&#963;</italic> is an instance of the Stochastic Root Finding Problem [<xref ref-type="bibr" rid="pone.0220161.ref089">89</xref>]. To estimate this we used the naive bisection algorithm for deterministic functions by using the averages as estimates of the actual values. We stopped the algorithm as soon as the success rate corresponding to our estimate of <italic>&#963;</italic><sub>50</sub> was contained in the Wald confidence interval given in <xref ref-type="disp-formula" rid="pone.0220161.e057">Eq 9</xref>. We find that our method was consistently able to find solutions to the root finding problem (see <xref ref-type="supplementary-material" rid="pone.0220161.s001">S1 Fig</xref>).</p>
</sec>
</sec>
<sec id="sec019">
<title>Supporting information</title>
<supplementary-material id="pone.0220161.s001" mimetype="application/pdf" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.s001" ns0:type="simple">
<label>S1 Fig</label>
<caption>
<title>Calibration of <italic>&#963;</italic><sub>50</sub> estimation.</title>
<p>(A) two success rate vs noise profiles for <italic>T</italic><sub><italic>p</italic></sub> = 50 <italic>ms</italic> and <italic>T</italic><sub><italic>p</italic></sub> = 200 <italic>ms</italic>. The values of <italic>p</italic><sub>50</sub> are annotated for reference. (B-F) We show the values of <italic>p</italic><sub>50</sub> obtained after running the algorithm in <xref ref-type="fig" rid="pone.0220161.g008">Fig 8</xref>. For every value we see that the values of the found roots (<italic>p</italic><sub>50</sub>, blue lines) was within confidence bounds (here blue shaded) of the expected value (0.5, horizontal lien in gray).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0220161.s002" mimetype="application/pdf" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.s002" ns0:type="simple">
<label>S1 Appendix</label>
<caption>
<title>Complete treatment of the persistence time.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0220161.s003" mimetype="application/pdf" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.s003" ns0:type="simple">
<label>S2 Appendix</label>
<caption>
<title>Sequence transition as probabilistic inference.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Arvind Kumar for reading a draft of this work and providing valuable comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0220161.ref001">
<label>1</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Luczak</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Barth&#243;</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Marguet</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Buzs&#225;ki</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>. <article-title>Sequential structure of neocortical spontaneous activity in vivo</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>1</issue>):<fpage>347</fpage>&#8211;<lpage>352</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.0605643104" ns0:type="simple">10.1073/pnas.0605643104</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref002">
<label>2</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Jin</surname> <given-names>DZ</given-names></name>, <name name-style="western"><surname>Fujii</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Graybiel</surname> <given-names>AM</given-names></name>. <article-title>Neural representation of time in cortico-basal ganglia circuits</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2009</year>; p. pnas&#8211;0909881106. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.0909881106" ns0:type="simple">10.1073/pnas.0909881106</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref003">
<label>3</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Harvey</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Coen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title>. <source>Nature</source>. <year>2012</year>;<volume>484</volume>(<issue>7392</issue>):<fpage>62</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nature10918" ns0:type="simple">10.1038/nature10918</ext-link></comment> <object-id pub-id-type="pmid">22419153</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref004">
<label>4</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Tang</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jackson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hobbs</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Patel</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro</article-title>. <source>Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>2</issue>):<fpage>505</fpage>&#8211;<lpage>518</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.3359-07.2008" ns0:type="simple">10.1523/JNEUROSCI.3359-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18184793</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref005">
<label>5</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Barnes</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Kubota</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jin</surname> <given-names>DZ</given-names></name>, <name name-style="western"><surname>Graybiel</surname> <given-names>AM</given-names></name>. <article-title>Activity of striatal neurons reflects dynamic encoding and recoding of procedural memories</article-title>. <source>Nature</source>. <year>2005</year>;<volume>437</volume>(<issue>7062</issue>):<fpage>1158</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nature04053" ns0:type="simple">10.1038/nature04053</ext-link></comment> <object-id pub-id-type="pmid">16237445</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref006">
<label>6</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Mello</surname> <given-names>GB</given-names></name>, <name name-style="western"><surname>Soares</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Paton</surname> <given-names>JJ</given-names></name>. <article-title>A scalable population code for time in the striatum</article-title>. <source>Current Biology</source>. <year>2015</year>;<volume>25</volume>(<issue>9</issue>):<fpage>1113</fpage>&#8211;<lpage>1122</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.cub.2015.02.036" ns0:type="simple">10.1016/j.cub.2015.02.036</ext-link></comment> <object-id pub-id-type="pmid">25913405</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref007">
<label>7</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Gouv&#234;a</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Monteiro</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Motiwala</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Soares</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Paton</surname> <given-names>JJ</given-names></name>. <article-title>Striatal dynamics explain duration judgments</article-title>. <source>Elife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e11386</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.7554/eLife.11386" ns0:type="simple">10.7554/eLife.11386</ext-link></comment> <object-id pub-id-type="pmid">26641377</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref008">
<label>8</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Bakhurin</surname> <given-names>KI</given-names></name>, <name name-style="western"><surname>Goudar</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Shobe</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Claar</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>, <name name-style="western"><surname>Masmanidis</surname> <given-names>SC</given-names></name>. <article-title>Differential encoding of time by prefrontal and striatal network dynamics</article-title>. <source>Journal of Neuroscience</source>. <year>2017</year>;<volume>37</volume>(<issue>4</issue>):<fpage>854</fpage>&#8211;<lpage>870</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.1789-16.2016" ns0:type="simple">10.1523/JNEUROSCI.1789-16.2016</ext-link></comment> <object-id pub-id-type="pmid">28123021</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref009">
<label>9</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Dhawale</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Poddar</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wolff</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Normand</surname> <given-names>VA</given-names></name>, <name name-style="western"><surname>Kopelowitz</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>&#214;lveczky</surname> <given-names>BP</given-names></name>. <article-title>Automated long-term recording and analysis of neural activity in behaving animals</article-title>. <source>Elife</source>. <year>2017</year>;<volume>6</volume>:<fpage>e27702</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.7554/eLife.27702" ns0:type="simple">10.7554/eLife.27702</ext-link></comment> <object-id pub-id-type="pmid">28885141</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref010">
<label>10</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Rueda-Orozco</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Robbe</surname> <given-names>D</given-names></name>. <article-title>The striatum multiplexes contextual and kinematic information to constrain motor habits execution</article-title>. <source>Nature neuroscience</source>. <year>2015</year>;<volume>18</volume>(<issue>3</issue>):<fpage>453</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nn.3924" ns0:type="simple">10.1038/nn.3924</ext-link></comment> <object-id pub-id-type="pmid">25622144</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref011">
<label>11</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>N&#225;dasdy</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Hirase</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Czurk&#243;</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Csicsvari</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Buzs&#225;ki</surname> <given-names>G</given-names></name>. <article-title>Replay and time compression of recurring spike sequences in the hippocampus</article-title>. <source>Journal of Neuroscience</source>. <year>1999</year>;<volume>19</volume>(<issue>21</issue>):<fpage>9497</fpage>&#8211;<lpage>9507</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.19-21-09497.1999" ns0:type="simple">10.1523/JNEUROSCI.19-21-09497.1999</ext-link></comment> <object-id pub-id-type="pmid">10531452</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref012">
<label>12</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Pastalkova</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Itskov</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Amarasingham</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Buzs&#225;ki</surname> <given-names>G</given-names></name>. <article-title>Internally generated cell assembly sequences in the rat hippocampus</article-title>. <source>Science</source>. <year>2008</year>;<volume>321</volume>(<issue>5894</issue>):<fpage>1322</fpage>&#8211;<lpage>1327</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1126/science.1159775" ns0:type="simple">10.1126/science.1159775</ext-link></comment> <object-id pub-id-type="pmid">18772431</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref013">
<label>13</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Louie</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>. <article-title>Temporally structured replay of awake hippocampal ensemble activity during rapid eye movement sleep</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>29</volume>(<issue>1</issue>):<fpage>145</fpage>&#8211;<lpage>156</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/S0896-6273(01)00186-6" ns0:type="simple">10.1016/S0896-6273(01)00186-6</ext-link></comment> <object-id pub-id-type="pmid">11182087</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref014">
<label>14</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Davidson</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Kloosterman</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>. <article-title>Hippocampal replay of extended experience</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>63</volume>(<issue>4</issue>):<fpage>497</fpage>&#8211;<lpage>507</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.neuron.2009.07.027" ns0:type="simple">10.1016/j.neuron.2009.07.027</ext-link></comment> <object-id pub-id-type="pmid">19709631</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref015">
<label>15</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>MacDonald</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Carrow</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Place</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Eichenbaum</surname> <given-names>H</given-names></name>. <article-title>Distinct hippocampal time cell sequences represent odor memories in immobilized rats</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>36</issue>):<fpage>14607</fpage>&#8211;<lpage>14616</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.1537-13.2013" ns0:type="simple">10.1523/JNEUROSCI.1537-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24005311</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref016">
<label>16</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Hahnloser</surname> <given-names>RH</given-names></name>, <name name-style="western"><surname>Kozhevnikov</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Fee</surname> <given-names>MS</given-names></name>. <article-title>An ultra-sparse code underliesthe generation of neural sequences in a songbird</article-title>. <source>Nature</source>. <year>2002</year>;<volume>419</volume>(<issue>6902</issue>):<fpage>65</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nature00974" ns0:type="simple">10.1038/nature00974</ext-link></comment> <object-id pub-id-type="pmid">12214232</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref017">
<label>17</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Kozhevnikov</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Fee</surname> <given-names>MS</given-names></name>. <article-title>Singing-related activity of identified HVC neurons in the zebra finch</article-title>. <source>Journal of neurophysiology</source>. <year>2007</year>;<volume>97</volume>(<issue>6</issue>):<fpage>4271</fpage>&#8211;<lpage>4283</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1152/jn.00952.2006" ns0:type="simple">10.1152/jn.00952.2006</ext-link></comment> <object-id pub-id-type="pmid">17182906</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref018">
<label>18</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Jones</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Fontanini</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sadacca</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Katz</surname> <given-names>DB</given-names></name>. <article-title>Natural stimuli evoke dynamic sequences of states in sensory cortical ensembles</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>47</issue>):<fpage>18772</fpage>&#8211;<lpage>18777</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.0705546104" ns0:type="simple">10.1073/pnas.0705546104</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref019">
<label>19</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Crowe</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Chafee</surname> <given-names>MV</given-names></name>. <article-title>Rapid sequences of population activity patterns dynamically encode task-critical spatial information in parietal cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2010</year>;<volume>30</volume>(<issue>35</issue>):<fpage>11640</fpage>&#8211;<lpage>11653</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.0954-10.2010" ns0:type="simple">10.1523/JNEUROSCI.0954-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20810885</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref020">
<label>20</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Abeles</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gat</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Meilijson</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Seidemann</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tishby</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Cortical activity flips among quasi-stationary states</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1995</year>;<volume>92</volume>(<issue>19</issue>):<fpage>8616</fpage>&#8211;<lpage>8620</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.92.19.8616" ns0:type="simple">10.1073/pnas.92.19.8616</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref021">
<label>21</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Seidemann</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Meilijson</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Abeles</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Vaadia</surname> <given-names>E</given-names></name>. <article-title>Simultaneously recorded single units in the frontal cortex go through sequences of discrete and stable states in monkeys performing a delayed localization task</article-title>. <source>Journal of Neuroscience</source>. <year>1996</year>;<volume>16</volume>(<issue>2</issue>):<fpage>752</fpage>&#8211;<lpage>768</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.16-02-00752.1996" ns0:type="simple">10.1523/JNEUROSCI.16-02-00752.1996</ext-link></comment> <object-id pub-id-type="pmid">8551358</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref022">
<label>22</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Fujisawa</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Amarasingham</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Buzs&#225;ki</surname> <given-names>G</given-names></name>. <article-title>Behavior-dependent short-term assembly dynamics in the medial prefrontal cortex</article-title>. <source>Nature neuroscience</source>. <year>2008</year>;<volume>11</volume>(<issue>7</issue>):<fpage>823</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nn.2134" ns0:type="simple">10.1038/nn.2134</ext-link></comment> <object-id pub-id-type="pmid">18516033</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref023">
<label>23</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Chafee</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Crowe</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Georgopoulos</surname> <given-names>AP</given-names></name>. <article-title>Parallel processing of serial movements in prefrontal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2002</year>;<volume>99</volume>(<issue>20</issue>):<fpage>13172</fpage>&#8211;<lpage>13177</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.162485599" ns0:type="simple">10.1073/pnas.162485599</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref024">
<label>24</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Nakajima</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hosaka</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mushiake</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tanji</surname> <given-names>J</given-names></name>. <article-title>Covert representation of second-next movement in the pre-supplementary motor area of monkeys</article-title>. <source>Journal of neurophysiology</source>. <year>2009</year>;<volume>101</volume>(<issue>4</issue>):<fpage>1883</fpage>&#8211;<lpage>1889</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1152/jn.90636.2008" ns0:type="simple">10.1152/jn.90636.2008</ext-link></comment> <object-id pub-id-type="pmid">19164110</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref025">
<label>25</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Lapish</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Chandler</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>. <article-title>Successful choice behavior is associated with distinct and coherent network states in anterior cingulate cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2008</year>;. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.0804045105" ns0:type="simple">10.1073/pnas.0804045105</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref026">
<label>26</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Johnson</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Goel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>. <article-title>Neural dynamics of in vitro cortical networks reflects experienced temporal patterns</article-title>. <source>Nature Neuroscience</source>. <year>2010</year>;<volume>13</volume>(<issue>8</issue>):<fpage>917</fpage>&#8211;<lpage>919</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nn.2579" ns0:type="simple">10.1038/nn.2579</ext-link></comment> <object-id pub-id-type="pmid">20543842</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref027">
<label>27</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Euston</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Tatsuno</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>. <article-title>Fast-forward playback of recent memory sequences in prefrontal cortex during sleep</article-title>. <source>science</source>. <year>2007</year>;<volume>318</volume>(<issue>5853</issue>):<fpage>1147</fpage>&#8211;<lpage>1150</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1126/science.1148979" ns0:type="simple">10.1126/science.1148979</ext-link></comment> <object-id pub-id-type="pmid">18006749</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref028">
<label>28</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Ji</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>. <article-title>Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title>. <source>Nature neuroscience</source>. <year>2007</year>;<volume>10</volume>(<issue>1</issue>):<fpage>100</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nn1825" ns0:type="simple">10.1038/nn1825</ext-link></comment> <object-id pub-id-type="pmid">17173043</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref029">
<label>29</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Agster</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Fortin</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Eichenbaum</surname> <given-names>H</given-names></name>. <article-title>The hippocampus and disambiguation of overlapping sequences</article-title>. <source>Journal of Neuroscience</source>. <year>2002</year>;<volume>22</volume>(<issue>13</issue>):<fpage>5760</fpage>&#8211;<lpage>5768</lpage>.</mixed-citation>
</ref>
<ref id="pone.0220161.ref030">
<label>30</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Amari</surname> <given-names>SI</given-names></name>. <article-title>Learning patterns and pattern sequences by self-organizing nets of threshold elements</article-title>. <source>IEEE Transactions on Computers</source>. <year>1972</year>;<volume>100</volume>(<issue>11</issue>):<fpage>1197</fpage>&#8211;<lpage>1206</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1109/T-C.1972.223477" ns0:type="simple">10.1109/T-C.1972.223477</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref031">
<label>31</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Willwacher</surname> <given-names>G</given-names></name>. <article-title>Storage of a temporal pattern sequence in a network</article-title>. <source>Biological Cybernetics</source>. <year>1982</year>;<volume>43</volume>(<issue>2</issue>):<fpage>115</fpage>&#8211;<lpage>126</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1007/BF00336974" ns0:type="simple">10.1007/BF00336974</ext-link></comment> <object-id pub-id-type="pmid">7059627</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref032">
<label>32</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the national academy of sciences</source>. <year>1982</year>;<volume>79</volume>(<issue>8</issue>):<fpage>2554</fpage>&#8211;<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.79.8.2554" ns0:type="simple">10.1073/pnas.79.8.2554</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref033">
<label>33</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>Neurons with graded response have collective computational properties like those of two-state neurons</article-title>. <source>Proceedings of the national academy of sciences</source>. <year>1984</year>;<volume>81</volume>(<issue>10</issue>):<fpage>3088</fpage>&#8211;<lpage>3092</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.81.10.3088" ns0:type="simple">10.1073/pnas.81.10.3088</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref034">
<label>34</label>
<mixed-citation publication-type="book" ns0:type="simple">
<name name-style="western"><surname>K&#252;hn</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>van Hemmen</surname> <given-names>JL</given-names></name>. <chapter-title>Temporal association</chapter-title>. In: <source>Models of neural networks</source>. <publisher-name>Springer</publisher-name>; <year>1991</year>. p. <fpage>213</fpage>&#8211;<lpage>280</lpage>.</mixed-citation>
</ref>
<ref id="pone.0220161.ref035">
<label>35</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Kleinfeld</surname> <given-names>D</given-names></name>. <article-title>Sequential state generation by model neural networks</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1986</year>;<volume>83</volume>(<issue>24</issue>):<fpage>9469</fpage>&#8211;<lpage>9473</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1073/pnas.83.24.9469" ns0:type="simple">10.1073/pnas.83.24.9469</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref036">
<label>36</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kanter</surname> <given-names>I</given-names></name>. <article-title>Temporal association in asymmetric neural networks</article-title>. <source>Physical review letters</source>. <year>1986</year>;<volume>57</volume>(<issue>22</issue>):<fpage>2861</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1103/PhysRevLett.57.2861" ns0:type="simple">10.1103/PhysRevLett.57.2861</ext-link></comment> <object-id pub-id-type="pmid">10033885</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref037">
<label>37</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Herz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sulzer</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>K&#252;hn</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Van Hemmen</surname> <given-names>J</given-names></name>. <article-title>Hebbian learning reconsidered: Representation of static and dynamic objects in associative neural nets</article-title>. <source>Biological cybernetics</source>. <year>1989</year>;<volume>60</volume>(<issue>6</issue>):<fpage>457</fpage>&#8211;<lpage>467</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1007/BF00204701" ns0:type="simple">10.1007/BF00204701</ext-link></comment> <object-id pub-id-type="pmid">11455966</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref038">
<label>38</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Coolen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gielen</surname> <given-names>C</given-names></name>. <article-title>Delays in neural networks</article-title>. <source>EPL (Europhysics Letters)</source>. <year>1988</year>;<volume>7</volume>(<issue>3</issue>):<fpage>281</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1209/0295-5075/7/3/016" ns0:type="simple">10.1209/0295-5075/7/3/016</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref039">
<label>39</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Tully</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Lind&#233;n</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hennig</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>. <article-title>Spike-based Bayesian-Hebbian learning of temporal sequences</article-title>. <source>PLoS computational biology</source>. <year>2016</year>;<volume>12</volume>(<issue>5</issue>):<fpage>e1004954</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1371/journal.pcbi.1004954" ns0:type="simple">10.1371/journal.pcbi.1004954</ext-link></comment> <object-id pub-id-type="pmid">27213810</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref040">
<label>40</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Veliz-Cuba</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Shouval</surname> <given-names>HZ</given-names></name>, <name name-style="western"><surname>Josi&#263;</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kilpatrick</surname> <given-names>ZP</given-names></name>. <article-title>Networks that learn the precise timing of event sequences</article-title>. <source>Journal of computational neuroscience</source>. <year>2015</year>;<volume>39</volume>(<issue>3</issue>):<fpage>235</fpage>&#8211;<lpage>254</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1007/s10827-015-0574-4" ns0:type="simple">10.1007/s10827-015-0574-4</ext-link></comment> <object-id pub-id-type="pmid">26334992</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref041">
<label>41</label>
<mixed-citation publication-type="other" ns0:type="simple">Pereira U, Brunel N. Unsupervised learning of persistent and sequential activity. bioRxiv. 2018; p. 414813.</mixed-citation>
</ref>
<ref id="pone.0220161.ref042">
<label>42</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Fiete</surname> <given-names>IR</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>CZ</given-names></name>, <name name-style="western"><surname>Hahnloser</surname> <given-names>RH</given-names></name>. <article-title>Spike-time-dependent plasticity and heterosynaptic competition organize networks to produce long scale-free sequences of neural activity</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>65</volume>(<issue>4</issue>):<fpage>563</fpage>&#8211;<lpage>576</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.neuron.2010.02.003" ns0:type="simple">10.1016/j.neuron.2010.02.003</ext-link></comment> <object-id pub-id-type="pmid">20188660</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref043">
<label>43</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Byrnes</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Burkitt</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Grayden</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Meffin</surname> <given-names>H</given-names></name>. <article-title>Learning a sparse code for temporal sequences using STDP and sequence compression</article-title>. <source>Neural computation</source>. <year>2011</year>;<volume>23</volume>(<issue>10</issue>):<fpage>2567</fpage>&#8211;<lpage>2598</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1162/NECO_a_00184" ns0:type="simple">10.1162/NECO_a_00184</ext-link></comment> <object-id pub-id-type="pmid">21732857</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref044">
<label>44</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Murray</surname> <given-names>JM</given-names></name>, <etal>et al</etal>. <article-title>Learning multiple variable-speed sequences in striatum via cortical tutoring</article-title>. <source>eLife</source>. <year>2017</year>;<volume>6</volume>:<fpage>e26084</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.7554/eLife.26084" ns0:type="simple">10.7554/eLife.26084</ext-link></comment> <object-id pub-id-type="pmid">28481200</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref045">
<label>45</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Marklund</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sikstr&#246;m</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Nilsson</surname> <given-names>LG</given-names></name>. <article-title>Reactivation in working memory: an attractor network model of free recall</article-title>. <source>PLoS One</source>. <year>2013</year>;<volume>8</volume>(<issue>8</issue>):<fpage>e73776</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1371/journal.pone.0073776" ns0:type="simple">10.1371/journal.pone.0073776</ext-link></comment> <object-id pub-id-type="pmid">24023690</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref046">
<label>46</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ekeberg</surname> <given-names>&#214;</given-names></name>. <article-title>A one-layer feedback artificial neural network with a Bayesian learning rule</article-title>. <source>International journal of neural systems</source>. <year>1989</year>;<volume>1</volume>(<issue>01</issue>):<fpage>77</fpage>&#8211;<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1142/S0129065789000499" ns0:type="simple">10.1142/S0129065789000499</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref047">
<label>47</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Douglas</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>KA</given-names></name>. <article-title>Neuronal circuits of the neocortex</article-title>. <source>Annu Rev Neurosci</source>. <year>2004</year>;<volume>27</volume>:<fpage>419</fpage>&#8211;<lpage>451</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1146/annurev.neuro.27.070203.144152" ns0:type="simple">10.1146/annurev.neuro.27.070203.144152</ext-link></comment> <object-id pub-id-type="pmid">15217339</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref048">
<label>48</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>. <article-title>Associative memory models: from the cell-assembly theory to biophysically detailed cortex simulations</article-title>. <source>Trends in neurosciences</source>. <year>2009</year>;<volume>32</volume>(<issue>3</issue>):<fpage>178</fpage>&#8211;<lpage>186</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.tins.2008.12.002" ns0:type="simple">10.1016/j.tins.2008.12.002</ext-link></comment> <object-id pub-id-type="pmid">19187979</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref049">
<label>49</label>
<mixed-citation publication-type="other" ns0:type="simple">Foldiak P. Sparse coding in the primate cortex. The handbook of brain theory and neural networks. 2003;.</mixed-citation>
</ref>
<ref id="pone.0220161.ref050">
<label>50</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Cowan</surname> <given-names>JD</given-names></name>. <article-title>Excitatory and inhibitory interactions in localized populations of model neurons</article-title>. <source>Biophysical journal</source>. <year>1972</year>;<volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>&#8211;<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/S0006-3495(72)86068-5" ns0:type="simple">10.1016/S0006-3495(72)86068-5</ext-link></comment> <object-id pub-id-type="pmid">4332108</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref051">
<label>51</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Fumarola</surname> <given-names>F</given-names></name>. <article-title>Mathematical equivalence of two common forms of firing rate models of neural networks</article-title>. <source>Neural computation</source>. <year>2012</year>;<volume>24</volume>(<issue>1</issue>):<fpage>25</fpage>&#8211;<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1162/NECO_a_00221" ns0:type="simple">10.1162/NECO_a_00221</ext-link></comment> <object-id pub-id-type="pmid">22023194</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref052">
<label>52</label>
<mixed-citation publication-type="book" ns0:type="simple">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>. <source>Modeling brain function: The world of attractor neural networks</source>. <publisher-name>Cambridge university press</publisher-name>; <year>1992</year>.</mixed-citation>
</ref>
<ref id="pone.0220161.ref053">
<label>53</label>
<mixed-citation publication-type="book" ns0:type="simple">
<name name-style="western"><surname>van Hemmen</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Schulten</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Domany</surname> <given-names>E</given-names></name>. <source>Models of neural networks</source>. <publisher-name>Springer</publisher-name>; <year>1991</year>.</mixed-citation>
</ref>
<ref id="pone.0220161.ref054">
<label>54</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Dominey</surname> <given-names>PF</given-names></name>, <name name-style="western"><surname>Ramus</surname> <given-names>F</given-names></name>. <article-title>Neural network processing of natural language: I. Sensitivity to serial, temporal and abstract structure of language in the infant</article-title>. <source>Language and Cognitive Processes</source>. <year>2000</year>;<volume>15</volume>(<issue>1</issue>):<fpage>87</fpage>&#8211;<lpage>127</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1080/016909600386129" ns0:type="simple">10.1080/016909600386129</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref055">
<label>55</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Sandberg</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Petersson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ekeberg</surname> <given-names>O</given-names></name>. <article-title>A Bayesian attractor network with incremental learning</article-title>. <source>Network: Computation in neural systems</source>. <year>2002</year>;<volume>13</volume>(<issue>2</issue>):<fpage>179</fpage>&#8211;<lpage>194</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1080/net.13.2.179.194" ns0:type="simple">10.1080/net.13.2.179.194</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref056">
<label>56</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Tully</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Hennig</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>. <article-title>Synaptic and nonsynaptic plasticity approximating probabilistic inference</article-title>. <source>Frontiers in synaptic neuroscience</source>. <year>2014</year>;<volume>6</volume>:<fpage>8</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.3389/fnsyn.2014.00008" ns0:type="simple">10.3389/fnsyn.2014.00008</ext-link></comment> <object-id pub-id-type="pmid">24782758</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref057">
<label>57</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Meli</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>. <article-title>A modular attractor associative memory with patchy connectivity and weight pruning</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2013</year>;<volume>24</volume>(<issue>4</issue>):<fpage>129</fpage>&#8211;<lpage>150</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.3109/0954898X.2013.859323" ns0:type="simple">10.3109/0954898X.2013.859323</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref058">
<label>58</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Verduzco-Flores</surname> <given-names>SO</given-names></name>, <name name-style="western"><surname>Bodner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ermentrout</surname> <given-names>B</given-names></name>. <article-title>A model for complex sequence learning and reproduction in neural populations</article-title>. <source>Journal of computational neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>3</issue>):<fpage>403</fpage>&#8211;<lpage>423</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1007/s10827-011-0360-x" ns0:type="simple">10.1007/s10827-011-0360-x</ext-link></comment> <object-id pub-id-type="pmid">21887499</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref059">
<label>59</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Fiebig</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>. <article-title>A spiking working memory model based on Hebbian short-term potentiation</article-title>. <source>Journal of Neuroscience</source>. <year>2017</year>;<volume>37</volume>(<issue>1</issue>):<fpage>83</fpage>&#8211;<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.1989-16.2016" ns0:type="simple">10.1523/JNEUROSCI.1989-16.2016</ext-link></comment> <object-id pub-id-type="pmid">28053032</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref060">
<label>60</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Carandini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Normalization as a canonical neural computation</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2012</year>;<volume>13</volume>(<issue>1</issue>):<fpage>51</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nrn3136" ns0:type="simple">10.1038/nrn3136</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref061">
<label>61</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Lundqvist</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rehn</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Djurfeldt</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lansner</surname> <given-names>A</given-names></name>. <article-title>Attractor dynamics in a modular network model of neocortex</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2006</year>;<volume>17</volume>(<issue>3</issue>):<fpage>253</fpage>&#8211;<lpage>276</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1080/09548980600774619" ns0:type="simple">10.1080/09548980600774619</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref062">
<label>62</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Mostafa</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Indiveri</surname> <given-names>G</given-names></name>. <article-title>Sequential activity in asymmetrically coupled winner-take-all circuits</article-title>. <source>Neural computation</source>. <year>2014</year>;<volume>26</volume>(<issue>9</issue>):<fpage>1973</fpage>&#8211;<lpage>2004</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1162/NECO_a_00619" ns0:type="simple">10.1162/NECO_a_00619</ext-link></comment> <object-id pub-id-type="pmid">24877737</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref063">
<label>63</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Blum</surname> <given-names>KI</given-names></name>. <article-title>Functional significance of long-term potentiation for sequence learning and prediction</article-title>. <source>Cerebral cortex</source>. <year>1996</year>;<volume>6</volume>(<issue>3</issue>):<fpage>406</fpage>&#8211;<lpage>416</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1093/cercor/6.3.406" ns0:type="simple">10.1093/cercor/6.3.406</ext-link></comment> <object-id pub-id-type="pmid">8670667</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref064">
<label>64</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Lawrence</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Trappenberg</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Fine</surname> <given-names>A</given-names></name>. <article-title>Rapid learning and robust recall of long sequences in modular associator networks</article-title>. <source>Neurocomputing</source>. <year>2006</year>;<volume>69</volume>(<issue>7-9</issue>):<fpage>634</fpage>&#8211;<lpage>641</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.neucom.2005.12.003" ns0:type="simple">10.1016/j.neucom.2005.12.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref065">
<label>65</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Holthoff</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Zecevic</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Konnerth</surname> <given-names>A</given-names></name>. <article-title>Rapid time course of action potentials in spines and remote dendrites of mouse visual cortex neurons</article-title>. <source>The Journal of physiology</source>. <year>2010</year>;<volume>588</volume>(<issue>7</issue>):<fpage>1085</fpage>&#8211;<lpage>1096</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1113/jphysiol.2009.184960" ns0:type="simple">10.1113/jphysiol.2009.184960</ext-link></comment> <object-id pub-id-type="pmid">20156851</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref066">
<label>66</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Paoletti</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bellone</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>Q</given-names></name>. <article-title>NMDA receptor subunit diversity: impact on receptor properties, synaptic plasticity and disease</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2013</year>;<volume>14</volume>(<issue>6</issue>):<fpage>383</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1038/nrn3504" ns0:type="simple">10.1038/nrn3504</ext-link></comment> <object-id pub-id-type="pmid">23686171</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref067">
<label>67</label>
<mixed-citation publication-type="other" ns0:type="simple">Chrysanthidis N, Fiebig F, Lansner A. Introducing double bouquet cells into a modular cortical associative memory model. bioRxiv. 2018; p. 462010.</mixed-citation>
</ref>
<ref id="pone.0220161.ref068">
<label>68</label>
<mixed-citation publication-type="other" ns0:type="simple">Bhalla US. Dendrites, deep learning, and sequences in the hippocampus. Hippocampus. 2017;.</mixed-citation>
</ref>
<ref id="pone.0220161.ref069">
<label>69</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Carr</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Konishi</surname> <given-names>M</given-names></name>. <article-title>A circuit for detection of interaural time differences in the brain stem of the barn owl</article-title>. <source>Journal of Neuroscience</source>. <year>1990</year>;<volume>10</volume>(<issue>10</issue>):<fpage>3227</fpage>&#8211;<lpage>3246</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.10-10-03227.1990" ns0:type="simple">10.1523/JNEUROSCI.10-10-03227.1990</ext-link></comment> <object-id pub-id-type="pmid">2213141</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref070">
<label>70</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Golombek</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Bussi</surname> <given-names>IL</given-names></name>, <name name-style="western"><surname>Agostino</surname> <given-names>PV</given-names></name>. <article-title>Minutes, days and years: molecular interactions among different scales of biological timing</article-title>. <source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source>. <year>2014</year>;<volume>369</volume>(<issue>1637</issue>):<fpage>20120465</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1098/rstb.2012.0465" ns0:type="simple">10.1098/rstb.2012.0465</ext-link></comment> <object-id pub-id-type="pmid">24446499</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref071">
<label>71</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Paton</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>. <article-title>The neural basis of timing: Distributed mechanisms for diverse functions</article-title>. <source>Neuron</source>. <year>2018</year>;<volume>98</volume>(<issue>4</issue>):<fpage>687</fpage>&#8211;<lpage>705</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.neuron.2018.03.045" ns0:type="simple">10.1016/j.neuron.2018.03.045</ext-link></comment> <object-id pub-id-type="pmid">29772201</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref072">
<label>72</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Ivry</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Schlerf</surname> <given-names>JE</given-names></name>. <article-title>Dedicated and intrinsic models of time perception</article-title>. <source>Trends in cognitive sciences</source>. <year>2008</year>;<volume>12</volume>(<issue>7</issue>):<fpage>273</fpage>&#8211;<lpage>280</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.tics.2008.04.002" ns0:type="simple">10.1016/j.tics.2008.04.002</ext-link></comment> <object-id pub-id-type="pmid">18539519</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref073">
<label>73</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Levy</surname> <given-names>WB</given-names></name>. <article-title>A sequence predicting CA3 is a flexible associator that learns and uses context to solve hippocampal-like tasks</article-title>. <source>Hippocampus</source>. <year>1996</year>;<volume>6</volume>(<issue>6</issue>):<fpage>579</fpage>&#8211;<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1002/(SICI)1098-1063(1996)6:6&lt;579::AID-HIPO3&gt;3.0.CO;2-C" ns0:type="simple">10.1002/(SICI)1098-1063(1996)6:6&lt;579::AID-HIPO3&gt;3.0.CO;2-C</ext-link></comment> <object-id pub-id-type="pmid">9034847</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref074">
<label>74</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Generating coherent patterns of activity from chaotic neural networks</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>63</volume>(<issue>4</issue>):<fpage>544</fpage>&#8211;<lpage>557</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.neuron.2009.07.018" ns0:type="simple">10.1016/j.neuron.2009.07.018</ext-link></comment> <object-id pub-id-type="pmid">19709635</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref075">
<label>75</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Rajan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Harvey</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>Recurrent network models of sequence generation and memory</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>90</volume>(<issue>1</issue>):<fpage>128</fpage>&#8211;<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.neuron.2016.02.009" ns0:type="simple">10.1016/j.neuron.2016.02.009</ext-link></comment> <object-id pub-id-type="pmid">26971945</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref076">
<label>76</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Rothkopf</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Triesch</surname> <given-names>J</given-names></name>. <article-title>A model of human motor sequence learning explains facilitation and interference effects based on spike-timing dependent plasticity</article-title>. <source>PLoS computational biology</source>. <year>2017</year>;<volume>13</volume>(<issue>8</issue>):<fpage>e1005632</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1371/journal.pcbi.1005632" ns0:type="simple">10.1371/journal.pcbi.1005632</ext-link></comment> <object-id pub-id-type="pmid">28767646</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref077">
<label>77</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Fukushima</surname> <given-names>K</given-names></name>. <article-title>A model of associative memory in the brain</article-title>. <source>Kybernetik</source>. <year>1973</year>;<volume>12</volume>(<issue>2</issue>):<fpage>58</fpage>&#8211;<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1007/BF00272461" ns0:type="simple">10.1007/BF00272461</ext-link></comment> <object-id pub-id-type="pmid">4694254</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref078">
<label>78</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Guyon</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Personnaz</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dreyfus</surname> <given-names>G</given-names></name>. <article-title>Storage and retrieval of complex sequences in neural networks</article-title>. <source>Physical Review A</source>. <year>1988</year>;<volume>38</volume>(<issue>12</issue>):<fpage>6365</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1103/PhysRevA.38.6365" ns0:type="simple">10.1103/PhysRevA.38.6365</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref079">
<label>79</label>
<mixed-citation publication-type="other" ns0:type="simple">Minai AA, Barrows GL, Levy WB. Disambiguation of pattern sequences with recurrent networks. In: Proc. WCNN, San Diego. vol. 4; 1994. p. 176&#8211;180.</mixed-citation>
</ref>
<ref id="pone.0220161.ref080">
<label>80</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Sohal</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Hasselmo</surname> <given-names>ME</given-names></name>. <article-title>GABAB modulation improves sequence disambiguation in computational models of hippocampal region CA3</article-title>. <source>Hippocampus</source>. <year>1998</year>;<volume>8</volume>(<issue>2</issue>):<fpage>171</fpage>&#8211;<lpage>193</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1002/(SICI)1098-1063(1998)8:2&lt;171::AID-HIPO9&gt;3.0.CO;2-O" ns0:type="simple">10.1002/(SICI)1098-1063(1998)8:2&lt;171::AID-HIPO9&gt;3.0.CO;2-O</ext-link></comment> <object-id pub-id-type="pmid">9572723</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref081">
<label>81</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Samura</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hattori</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ishizaki</surname> <given-names>S</given-names></name>. <article-title>Sequence disambiguation and pattern completion by cooperation between autoassociative and heteroassociative memories of functionally divided hippocampal CA3</article-title>. <source>Neurocomputing</source>. <year>2008</year>;<volume>71</volume>(<issue>16-18</issue>):<fpage>3176</fpage>&#8211;<lpage>3183</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1016/j.neucom.2008.04.026" ns0:type="simple">10.1016/j.neucom.2008.04.026</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0220161.ref082">
<label>82</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Lipton</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>White</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Eichenbaum</surname> <given-names>H</given-names></name>. <article-title>Disambiguation of overlapping experiences by neurons in the medial entorhinal cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2007</year>;<volume>27</volume>(<issue>21</issue>):<fpage>5787</fpage>&#8211;<lpage>5795</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.1063-07.2007" ns0:type="simple">10.1523/JNEUROSCI.1063-07.2007</ext-link></comment> <object-id pub-id-type="pmid">17522322</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref083">
<label>83</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Van Rossum</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Bi</surname> <given-names>GQ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname> <given-names>GG</given-names></name>. <article-title>Stable Hebbian learning from spike timing-dependent plasticity</article-title>. <source>Journal of neuroscience</source>. <year>2000</year>;<volume>20</volume>(<issue>23</issue>):<fpage>8812</fpage>&#8211;<lpage>8821</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1523/JNEUROSCI.20-23-08812.2000" ns0:type="simple">10.1523/JNEUROSCI.20-23-08812.2000</ext-link></comment> <object-id pub-id-type="pmid">11102489</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref084">
<label>84</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Xu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Olivas</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Ikrar</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Nie</surname> <given-names>Q</given-names></name>, <etal>et al</etal>. <article-title>Primary visual cortex shows laminar-specific and balanced circuit organization of excitatory and inhibitory synaptic connectivity</article-title>. <source>The Journal of physiology</source>. <year>2016</year>;<volume>594</volume>(<issue>7</issue>):<fpage>1891</fpage>&#8211;<lpage>1910</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1113/JP271891" ns0:type="simple">10.1113/JP271891</ext-link></comment> <object-id pub-id-type="pmid">26844927</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref085">
<label>85</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Jiang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cadwell</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sinz</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <etal>et al</etal>. <article-title>Principles of connectivity among morphologically defined cell types in adult neocortex</article-title>. <source>Science</source>. <year>2015</year>;<volume>350</volume>(<issue>6264</issue>):<fpage>aac9462</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1126/science.aac9462" ns0:type="simple">10.1126/science.aac9462</ext-link></comment> <object-id pub-id-type="pmid">26612957</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref086">
<label>86</label>
<mixed-citation publication-type="other" ns0:type="simple">Spreizer S, Aertsen A, Kumar A. From space to time: Spatial inhomogeneities lead to the emergence of spatio-temporal activity sequences in spiking neuronal networks. bioRxiv. 2018; p. 428649.</mixed-citation>
</ref>
<ref id="pone.0220161.ref087">
<label>87</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Branco</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>H&#228;usser</surname> <given-names>M</given-names></name>. <article-title>Dendritic discrimination of temporal input sequences in cortical neurons</article-title>. <source>Science</source>. <year>2010</year>;<volume>329</volume>(<issue>5999</issue>):<fpage>1671</fpage>&#8211;<lpage>1675</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1126/science.1189664" ns0:type="simple">10.1126/science.1189664</ext-link></comment> <object-id pub-id-type="pmid">20705816</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref088">
<label>88</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Hawkins</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ahmad</surname> <given-names>S</given-names></name>. <article-title>Why neurons have thousands of synapses, a theory of sequence memory in neocortex</article-title>. <source>Frontiers in neural circuits</source>. <year>2016</year>;<volume>10</volume>:<fpage>23</fpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.3389/fncir.2016.00023" ns0:type="simple">10.3389/fncir.2016.00023</ext-link></comment> <object-id pub-id-type="pmid">27065813</object-id></mixed-citation>
</ref>
<ref id="pone.0220161.ref089">
<label>89</label>
<mixed-citation publication-type="journal" ns0:type="simple">
<name name-style="western"><surname>Pasupathy</surname> <given-names>R</given-names></name>. <article-title>On choosing parameters in retrospective-approximation algorithms for stochastic root finding and simulation optimization</article-title>. <source>Operations Research</source>. <year>2010</year>;<volume>58</volume>(<issue>4-part-1</issue>):<fpage>889</fpage>&#8211;<lpage>901</lpage>. <comment>doi: <ext-link ext-link-type="uri" ns0:href="https://doi.org/10.1287/opre.1090.0773" ns0:type="simple">10.1287/opre.1090.0773</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
<sub-article article-type="author-comment" id="pone.0220161.r001" specific-use="rebutted-decision-letter-unavailable">
<front-stub>
<article-id pub-id-type="doi">10.1371/journal.pone.0220161.r001</article-id>
<title-group>
<article-title>Author response to previous submission</article-title>
</title-group>
<custom-meta-group>
<custom-meta>
<meta-name>Submission Version</meta-name>
<meta-value>0</meta-value>
</custom-meta>
</custom-meta-group>
</front-stub>
<body>
<boxed-text id="pone-0220161-box001" position="float" specific-use="prior_peer_review_unavailable">
<sec id="sec020">
<title>Transfer Alert</title>
<p>This paper was transferred from another journal. As a result, its full editorial history (including decision letters, peer reviews and author responses) may not be present.</p>
</sec>
</boxed-text>
<p>
<named-content content-type="author-response-date">8 Jul 2019</named-content>
</p>
<supplementary-material id="pone.0220161.s004" mimetype="application/pdf" position="float" ns0:href="info:doi/10.1371/journal.pone.0220161.s004" ns0:type="simple">
<label>Attachment</label>
<caption>
<p>Submitted filename: <named-content content-type="submitted-filename">2019-06-26(Response to the reviewers).pdf</named-content></p>
</caption>
</supplementary-material>
</body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0220161.r002" specific-use="decision-letter">
<front-stub>
<article-id pub-id-type="doi">10.1371/journal.pone.0220161.r002</article-id>
<title-group>
<article-title>Decision Letter 0</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name name-style="western">
<surname>Morrison</surname>
<given-names>Abigail</given-names>
</name>
<role>Guest Editor</role>
</contrib>
</contrib-group>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Abigail Morrison</copyright-holder>
<license ns0:href="http://creativecommons.org/licenses/by/4.0/">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" ns0:href="http://creativecommons.org/licenses/by/4.0/" ns0:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<related-object document-id="10.1371/journal.pone.0220161" document-id-type="doi" document-type="article" id="rel-obj002" link-type="peer-reviewed-article" />
<custom-meta-group>
<custom-meta>
<meta-name>Submission Version</meta-name>
<meta-value>0</meta-value>
</custom-meta>
</custom-meta-group>
</front-stub>
<body>
<p>
<named-content content-type="letter-date">10 Jul 2019</named-content>
</p>
<p>Probabilistic associative learning suffices for learning the temporal structure of multiple sequences</p>
<p>PONE-D-19-19151</p>
<p>Dear Dr. Martinez Mayorquin,</p>
<p>Thank you for transferring you manuscript to PLOS ONE. We are pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it complies with all outstanding technical requirements.</p>
<p>Within one week, you will receive an e-mail containing information on the amendments required prior to publication. When all required modifications have been addressed, you will receive a formal acceptance letter and your manuscript will proceed to our production department and be scheduled for publication.</p>
<p>Shortly after the formal acceptance letter is sent, an invoice for payment will follow. To ensure an efficient production and billing process, please log into Editorial Manager at <ext-link ext-link-type="uri" ns0:href="https://www.editorialmanager.com/pone/" ns0:type="simple">https://www.editorialmanager.com/pone/</ext-link>, click the "Update My Information" link at the top of the page, and update your user information. If you have any billing related questions, please contact our Author Billing department directly at <email ns0:type="simple">authorbilling@plos.org</email>.</p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper to enable them to help maximize its impact. If they will be preparing press materials for this manuscript, you must inform our press team as soon as possible and no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email ns0:type="simple">onepress@plos.org</email>.</p>
<p>With kind regards,</p>
<p>Abigail Morrison</p>
<p>Transfer Guest Editor</p>
<p>PLOS ONE</p>
<p>Additional Editor Comments (optional):</p>
<p>Reviewers' comments:</p>
</body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0220161.r003" specific-use="acceptance-letter">
<front-stub>
<article-id pub-id-type="doi">10.1371/journal.pone.0220161.r003</article-id>
<title-group>
<article-title>Acceptance letter</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name name-style="western">
<surname>Morrison</surname>
<given-names>Abigail</given-names>
</name>
<role>Guest Editor</role>
</contrib>
</contrib-group>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Abigail Morrison</copyright-holder>
<license ns0:href="http://creativecommons.org/licenses/by/4.0/">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" ns0:href="http://creativecommons.org/licenses/by/4.0/" ns0:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<related-object document-id="10.1371/journal.pone.0220161" document-id-type="doi" document-type="article" id="rel-obj003" link-type="peer-reviewed-article" />
</front-stub>
<body>
<p>
<named-content content-type="letter-date">19 Jul 2019</named-content>
</p>
<p>PONE-D-19-19151 </p>
<p>Probabilistic associative learning suffices for learning the temporal structure of multiple sequences </p>
<p>Dear Dr. Martinez:</p>
<p>I am pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
<p>If your institution or institutions have a press office, please notify them about your upcoming paper at this point, to enable them to help maximize its impact. If they will be preparing press materials for this manuscript, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email ns0:type="simple">onepress@plos.org</email>.</p>
<p>For any other questions or concerns, please email <email ns0:type="simple">plosone@plos.org</email>. </p>
<p>Thank you for submitting your work to PLOS ONE.</p>
<p>With kind regards,</p>
<p>PLOS ONE Editorial Office Staff</p>
<p>on behalf of</p>
<p>Professor Abigail Morrison  </p>
<p>Transfer Guest Editor</p>
<p>PLOS ONE</p>
</body>
</sub-article>
</article>